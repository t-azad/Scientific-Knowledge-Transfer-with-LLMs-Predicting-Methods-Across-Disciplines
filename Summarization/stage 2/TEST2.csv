domain,title,abstract,verified methods list,inital masked extraction,final masked extraction,leakage detected
Biology,"Revolutionizing agriculture with artificial intelligence: plant disease detection methods, applications, and their limitations","Accurate and rapid plant disease detection is critical for enhancing long-term agricultural yield. Disease infection poses the most significant challenge in crop production, potentially leading to economic losses. Viruses, fungi, bacteria, and other infectious organisms can affect numerous plant parts, including roots, stems, and leaves. Traditional techniques for plant disease detection are time-consuming, require expertise, and are resource-intensive. Therefore, automated leaf disease diagnosis using artificial intelligence (AI) with Internet of Things (IoT) sensors methodologies are considered for the analysis and detection. This research examines four crop diseases: tomato, chilli, potato, and cucumber. It also highlights the most prevalent diseases and infections in these four types of vegetables, along with their symptoms. This review provides detailed predetermined steps to predict plant diseases using AI. Predetermined steps include image acquisition, preprocessing, segmentation, feature selection, and classification. Machine learning (ML) and deep understanding (DL) detection models are discussed. A comprehensive examination of various existing ML and DL-based studies to detect the disease of the following four crops is discussed, including the datasets used to evaluate these studies. We also provided the list of plant disease detection datasets. Finally, different ML and DL application problems are identified and discussed, along with future research prospects, by combining AI with IoT platforms like smart drones for field-based disease detection and monitoring. This work will help other practitioners in surveying different plant disease detection strategies and the limits of present systems.","['machine learning (ML)', 'deep learning (DL)']","The research addresses the critical need for accurate and rapid detection of plant diseases to enhance long-term agricultural yield, as disease infections pose significant challenges in crop production and can lead to economic losses. Various infectious organisms such as viruses, fungi, and bacteria affect multiple parts of plants, including roots, stems, and leaves, making timely and effective disease identification essential. The study focuses on four important crops—tomato, chilli, potato, and cucumber—and highlights the most prevalent diseases and their symptoms in these vegetables. The primary objective of the research is to examine and review existing approaches for detecting diseases in these four crops, providing detailed steps and insights into disease diagnosis, as well as identifying current challenges and future prospects in plant disease detection to support practitioners in improving detection strategies.","The research addresses the critical need for accurate and rapid detection of plant diseases to enhance long-term agricultural yield, as disease infections pose significant challenges in crop production and can lead to economic losses. Various infectious organisms such as viruses, fungi, and bacteria affect multiple parts of plants, including roots, stems, and leaves, making timely and effective disease identification essential. The study focuses on four important crops—tomato, chilli, potato, and cucumber—and highlights the most prevalent diseases and their symptoms in these vegetables. The primary objective of the research is to examine and review existing approaches for detecting diseases in these four crops, providing detailed steps and insights into disease diagnosis, as well as identifying current challenges and future prospects in plant disease detection to support practitioners in improving detection strategies.",FALSE
Biology,Deep-STP: a deep learning-based approach to predict snake toxin proteins by using word embeddings,"Snake venom contains many toxic proteins that can destroy the circulatory system or nervous system of prey. Studies have found that these snake venom proteins have the potential to treat cardiovascular and nervous system diseases. Therefore, the study of snake venom protein is conducive to the development of related drugs. The research technologies based on traditional biochemistry can accurately identify these proteins, but the experimental cost is high and the time is long. Artificial intelligence technology provides a new means and strategy for large-scale screening of snake venom proteins from the perspective of computing. In this paper, we developed a sequence-based computational method to recognize snake toxin proteins. Specially, we utilized three different feature descriptors, namely g-gap , natural vector and word 2 vector, to encode snake toxin protein sequences. The analysis of variance (ANOVA), gradient-boost decision tree algorithm (GBDT) combined with incremental feature selection (IFS) were used to optimize the features, and then the optimized features were input into the deep learning model for model training. The results show that our model can achieve a prediction performance with an accuracy of 82.00% in 10-fold cross-validation. The model is further verified on independent data, and the accuracy rate reaches to 81.14%, which demonstrated that our model has excellent prediction performance and robustness.","['gradient-boost decision tree algorithm (GBDT)', 'incremental feature selection (IFS)', 'deep learning model']","The study addresses the presence of toxic proteins in snake venom that can damage the circulatory or nervous systems of prey, highlighting their potential use in treating cardiovascular and nervous system diseases. Understanding and identifying these snake venom proteins is important for the development of related drugs. The primary aim of the study is to recognize and accurately identify snake toxin proteins to facilitate their study and potential therapeutic application. This research seeks to improve the identification process of these proteins to support drug development efforts.","The study addresses the presence of toxic proteins in snake venom that can damage the circulatory or nervous systems of prey, highlighting their potential use in treating cardiovascular and nervous system diseases. Understanding and identifying these snake venom proteins is important for the development of related drugs. The primary aim of the study is to recognize and accurately identify snake toxin proteins to facilitate their study and potential therapeutic application. This research seeks to improve the identification process of these proteins to support drug development efforts.",FALSE
Biology,Feature reduction for hepatocellular carcinoma prediction using machine learning algorithms,"Abstract Hepatocellular carcinoma (HCC) is a highly prevalent form of liver cancer that necessitates accurate prediction models for early diagnosis and effective treatment. Machine learning algorithms have demonstrated promising results in various medical domains, including cancer prediction. In this study, we propose a comprehensive approach for HCC prediction by comparing the performance of different machine learning algorithms before and after applying feature reduction methods. We employ popular feature reduction techniques, such as weighting features, hidden features correlation, feature selection, and optimized selection, to extract a reduced feature subset that captures the most relevant information related to HCC. Subsequently, we apply multiple algorithms, including Naive Bayes, support vector machines (SVM), Neural Networks, Decision Tree, and K nearest neighbors (KNN), to both the original high-dimensional dataset and the reduced feature set. By comparing the predictive accuracy, precision, F Score, recall, and execution time of each algorithm, we assess the effectiveness of feature reduction in enhancing the performance of HCC prediction models. Our experimental results, obtained using a comprehensive dataset comprising clinical features of HCC patients, demonstrate that feature reduction significantly improves the performance of all examined algorithms. Notably, the reduced feature set consistently outperforms the original high-dimensional dataset in terms of prediction accuracy and execution time. After applying feature reduction techniques, the employed algorithms, namely decision trees, Naive Bayes, KNN, neural networks, and SVM achieved accuracies of 96%, 97.33%, 94.67%, 96%, and 96.00%, respectively.","['Naive Bayes', 'support vector machines (SVM)', 'Neural Networks', 'Decision Tree', 'K nearest neighbors (KNN)']","The study addresses the challenge of accurately predicting hepatocellular carcinoma (HCC), a highly prevalent form of liver cancer, which is crucial for early diagnosis and effective treatment. Improving prediction accuracy is essential to better identify patients at risk and enhance clinical outcomes. The primary aim of the study is to evaluate and compare the effectiveness of different approaches in predicting HCC by identifying the most relevant clinical features associated with the disease. This involves assessing how reducing the number of features impacts the accuracy and efficiency of HCC prediction, ultimately aiming to improve the reliability of diagnostic methods for this type of liver cancer.","The study addresses the challenge of accurately predicting hepatocellular carcinoma (HCC), a highly prevalent form of liver cancer, which is crucial for early diagnosis and effective treatment. Improving prediction accuracy is essential to better identify patients at risk and enhance clinical outcomes. The primary aim of the study is to evaluate and compare the effectiveness of different approaches in predicting HCC by identifying the most relevant clinical features associated with the disease. This involves assessing how reducing the number of features impacts the accuracy and efficiency of HCC prediction, ultimately aiming to improve the reliability of diagnostic methods for this type of liver cancer.",FALSE
Biology,Optimization of hepatological clinical guidelines interpretation by large language models: a retrieval augmented generation-based framework,"Abstract Large language models (LLMs) can potentially transform healthcare, particularly in providing the right information to the right provider at the right time in the hospital workflow. This study investigates the integration of LLMs into healthcare, specifically focusing on improving clinical decision support systems (CDSSs) through accurate interpretation of medical guidelines for chronic Hepatitis C Virus infection management. Utilizing OpenAI’s GPT-4 Turbo model, we developed a customized LLM framework that incorporates retrieval augmented generation (RAG) and prompt engineering. Our framework involved guideline conversion into the best-structured format that can be efficiently processed by LLMs to provide the most accurate output. An ablation study was conducted to evaluate the impact of different formatting and learning strategies on the LLM’s answer generation accuracy. The baseline GPT-4 Turbo model’s performance was compared against five experimental setups with increasing levels of complexity: inclusion of in-context guidelines, guideline reformatting, and implementation of few-shot learning. Our primary outcome was the qualitative assessment of accuracy based on expert review, while secondary outcomes included the quantitative measurement of similarity of LLM-generated responses to expert-provided answers using text-similarity scores. The results showed a significant improvement in accuracy from 43 to 99% ( p &lt; 0.001), when guidelines were provided as context in a coherent corpus of text and non-text sources were converted into text. In addition, few-shot learning did not seem to improve overall accuracy. The study highlights that structured guideline reformatting and advanced prompt engineering (data quality vs. data quantity) can enhance the efficacy of LLM integrations to CDSSs for guideline delivery.","['OpenAI’s GPT-4 Turbo model', 'retrieval augmented generation (RAG)', 'few-shot learning']",The study addresses the challenge of improving clinical decision support systems for the management of chronic Hepatitis C Virus infection by ensuring accurate interpretation and delivery of medical guidelines within healthcare settings. It focuses on the need to provide the right information to healthcare providers at the right time to enhance patient care and treatment outcomes. The primary aim of the study is to investigate how the structuring and presentation of clinical guidelines can improve the accuracy of guideline interpretation and support clinical decision-making for chronic Hepatitis C Virus infection management. The study seeks to evaluate the impact of different guideline formatting strategies on the accuracy of clinical recommendations to ultimately enhance the effectiveness of guideline delivery in healthcare workflows.,The study addresses the challenge of improving clinical decision support systems for the management of chronic Hepatitis C Virus infection by ensuring accurate interpretation and delivery of medical guidelines within healthcare settings. It focuses on the need to provide the right information to healthcare providers at the right time to enhance patient care and treatment outcomes. The primary aim of the study is to investigate how the structuring and presentation of clinical guidelines can improve the accuracy of guideline interpretation and support clinical decision-making for chronic Hepatitis C Virus infection management. The study seeks to evaluate the impact of different guideline formatting strategies on the accuracy of clinical recommendations to ultimately enhance the effectiveness of guideline delivery in healthcare workflows.,FALSE
Biology,Survival Prediction Across Diverse Cancer Types Using Neural Networks,"Gastric cancer and Colon adenocarcinoma represent widespread and challenging malignancies with high mortality rates and complex treatment landscapes. In response to the critical need for accurate prognosis in cancer patients, the medical community has embraced the 5-year survival rate as a vital metric for estimating patient outcomes. This study introduces a pioneering approach to enhance survival prediction models for gastric and Colon adenocarcinoma patients. Leveraging advanced image analysis techniques, we sliced whole slide images (WSI) of these cancers, extracting comprehensive features to capture nuanced tumor characteristics. Subsequently, we constructed patient-level graphs, encapsulating intricate spatial relationships within tumor tissues. These graphs served as inputs for a sophisticated 4-layer graph convolutional neural network (GCN), designed to exploit the inherent connectivity of the data for comprehensive analysis and prediction. By integrating patients' total survival time and survival status, we computed C-index values for gastric cancer and Colon adenocarcinoma, yielding 0.57 and 0.64, respectively. Significantly surpassing previous convolutional neural network models, these results underscore the efficacy of our approach in accurately predicting patient survival outcomes. This research holds profound implications for both the medical and AI communities, offering insights into cancer biology and progression while advancing personalized treatment strategies. Ultimately, our study represents a significant stride in leveraging AI-driven methodologies to revolutionize cancer prognosis and improve patient outcomes on a global scale.","['graph convolutional neural network (GCN)', 'convolutional neural network']","The study addresses the critical challenge of accurately predicting survival outcomes for patients with gastric cancer and colon adenocarcinoma, two malignancies characterized by high mortality rates and complex treatment options. Given the importance of the 5-year survival rate as a key metric for estimating patient prognosis, there is a pressing need to improve the precision of survival predictions in these cancers. The primary objective of this research is to enhance survival prediction for gastric and colon adenocarcinoma patients by capturing detailed tumor characteristics and spatial relationships within tumor tissues. This aims to provide more accurate estimates of patient outcomes, thereby contributing to better-informed clinical decision-making and personalized treatment strategies.","The study addresses the critical challenge of accurately predicting survival outcomes for patients with gastric cancer and colon adenocarcinoma, two malignancies characterized by high mortality rates and complex treatment options. Given the importance of the 5-year survival rate as a key metric for estimating patient prognosis, there is a pressing need to improve the precision of survival predictions in these cancers. The primary objective of this research is to enhance survival prediction for gastric and colon adenocarcinoma patients by capturing detailed tumor characteristics and spatial relationships within tumor tissues. This aims to provide more accurate estimates of patient outcomes, thereby contributing to better-informed clinical decision-making and personalized treatment strategies.",FALSE
Biology,A systematic review of hyperspectral imaging in precision agriculture: Analysis of its current state and future prospects,"Hyperspectral sensor adaptability in precision agriculture to digital images is still at its nascent stage. Hyperspectral imaging (HSI) is data rich in solving agricultural problems like disease detection, weed detection, stress detection, crop monitoring, nutrient application, soil mineralogy, yield estimation, and sorting applications. With modern precision agriculture, the challenge now is to bring these applications to the field for real-time solutions, where machines are enabled to conduct analyses without expert supervision and communicate the results to users for better management of farmlands; a necessary step to gain complete autonomy in agricultural farmlands. Significant advancements in HSI technology for precision agriculture are required to fully realize its potential. As a wide-ranging collection of the status of HSI and analysis in precision agriculture is lacking, this review endeavors to provide a comprehensive overview of the recent advancements and trends of HSI in precision agriculture for real-time applications. In this study, a systematic review of 163 scientific articles published over the past twenty years (2003–2023) was conducted. Of these, 97 were selected for further analysis based on their relevance to the topic at hand. Topics include conventional data preprocessing techniques, hyperspectral data acquisition, data compression methods, and segmentation methods. The hardware implementation of field-programmable gate arrays (FPGAs) and graphics processing units (GPUs) for high-speed data processing and application of machine learning and deep learning technologies were explored. This review highlights the potential of HSI as a powerful tool for precision agriculture, particularly in real-time applications, discusses limitations, and provides insights into future research directions.","['machine learning', 'deep learning']","The research idea centers on the emerging role of hyperspectral imaging (HSI) in precision agriculture, particularly its potential to address critical agricultural challenges such as disease detection, weed detection, stress detection, crop monitoring, nutrient application, soil mineralogy, yield estimation, and sorting. Despite its promise, the adaptation of hyperspectral sensors for real-time field applications remains in the early stages, and significant advancements are needed to fully harness HSI technology for autonomous and efficient farmland management. The primary objective of this study is to provide a comprehensive overview of recent advancements and trends in the use of hyperspectral imaging within precision agriculture, focusing on real-time applications. This review aims to synthesize current knowledge, highlight the potential and limitations of HSI, and offer insights into future research directions to support the development of effective real-time solutions in agricultural practices.","The research idea centers on the emerging role of hyperspectral imaging (HSI) in precision agriculture, particularly its potential to address critical agricultural challenges such as disease detection, weed detection, stress detection, crop monitoring, nutrient application, soil mineralogy, yield estimation, and sorting. Despite its promise, the adaptation of hyperspectral sensors for real-time field applications remains in the early stages, and significant advancements are needed to fully harness HSI technology for autonomous and efficient farmland management. The primary objective of this study is to provide a comprehensive overview of recent advancements and trends in the use of hyperspectral imaging within precision agriculture, focusing on real-time applications. This review aims to synthesize current knowledge, highlight the potential and limitations of HSI, and offer insights into future research directions to support the development of effective real-time solutions in agricultural practices.",FALSE
Biology,Critical review on water quality analysis using IoT and machine learning models,"Water quality and its management are the most precise concerns confronting humanity globally. This article evaluates the various sensors used for water quality monitoring and focuses on the water quality index considering the multiple physical, chemical, and biological parameters. A Review of Internet of Things (IoT) research for water quality monitoring and analysis, sensors used for water quality can help remote monitoring of the water quality parameters using various IoT-based sensors that convey the assembled estimations utilizing Low-Power Wide Area Network innovations. Overall, the IoT system was 95 % accurate in measuring pH, Turbidity, TDS, and Temperature, while the traditional method was only 85 % accurate. Also, this study reviewed the different A.I. techniques used to assess water quality, including conventional machine learning techniques, Support Vector Machines, Deep Neural Networks, and K-nearest neighbors. Compared to traditional methods, machine learning and deep learning can significantly increase the accuracy of measurements of groundwater quality. However, various variables, such as the caliber of the training data, the water quality metrics' complexity, and the monitoring frequency, will affect the accuracy. The geographical information system (GIS) is used for spatial data analysis and managing water resources. The quality of its data is also reviewed in the paper. Based on these analyses, the study has forecasted the future sensors, Geospatial Technology, and machine learning techniques for water quality monitoring and analysis.","['Support Vector Machines', 'Deep Neural Networks', 'K-nearest neighbors']","The research idea centers on the critical global concern of water quality and its management, emphasizing the importance of monitoring multiple physical, chemical, and biological parameters to assess water quality accurately. The study addresses the need for effective evaluation and management of water quality to ensure environmental safety and public health. The primary objective of the study is to evaluate various sensors used for water quality monitoring, focusing on the water quality index derived from multiple parameters, and to review the effectiveness of different approaches for measuring and managing water quality. The study aims to provide insights into improving the accuracy and reliability of water quality assessment methods to support better water resource management.","The research idea centers on the critical global concern of water quality and its management, emphasizing the importance of monitoring multiple physical, chemical, and biological parameters to assess water quality accurately. The study addresses the need for effective evaluation and management of water quality to ensure environmental safety and public health. The primary objective of the study is to evaluate various sensors used for water quality monitoring, focusing on the water quality index derived from multiple parameters, and to review the effectiveness of different approaches for measuring and managing water quality. The study aims to provide insights into improving the accuracy and reliability of water quality assessment methods to support better water resource management.",FALSE
Psychology,3WC-GBNRS++: A novel three-way classifier with granular-ball neighborhood rough sets based on uncertainty,"Three-way decision with neighborhood rough sets (3WDNRS) is adept at addressing uncertain problems involving continuous data by configuring the neighborhood radius. However, on one hand, the inputs of 3WDNRS are individual neighborhood granules, which reduce the decision efficiency and generality; on other hand, the thresholds of 3WDNRS require prior knowledge to be approximately set in advance, making it difficult to apply in cases where such knowledge is unavailable. To address these issues, we introduce granular-ball computing (GBC) into 3WDNRS from the perspective of uncertainty. Firstly, we propose an enhanced granular-ball generation method based on DBSCAN called DBGBC. Subsequently, we present an improved granular-ball neighborhood rough sets model (GBNRS++) by combining DBGBC with a quality index. Furthermore, we construct a three-way classifier with granular-ball neighborhood rough sets (3WC-GBNRS++) based on the principle of minimum fuzziness loss. This approach provides an objective and efficient way to determine the thresholds. To further enhance classification accuracy, we design an adaptive granular-ball neighborhood within the subsequent classification process of 3WC-GBNRS++. Finally, experimental results demonstrate that, 3WC-GBNRS++ almost outperformed other comparison methods in terms of effectiveness and robustness, including 4 state-of-the-art granular-balls-based classifiers and 5 classical machine learning classifiers on 12 public benchmark datasets. Moreover, we discuss the limitations of our work and the outlook for future research.","['three-way decision with neighborhood rough sets (3WDNRS)', 'DBSCAN']","The research idea centers on addressing challenges in decision-making processes involving uncertain and continuous data, particularly the limitations posed by reliance on individual neighborhood granules and the need for prior knowledge to set thresholds. These issues reduce decision efficiency and hinder applicability in situations where such prior knowledge is unavailable. The study aims to overcome these obstacles by introducing a novel approach that enhances the determination of thresholds and improves decision accuracy in uncertain environments. The primary objective of the study is to develop an improved method for configuring neighborhood-based decision frameworks that objectively and efficiently determine thresholds without requiring prior knowledge, thereby enhancing classification accuracy and robustness in handling uncertain data.","The research idea centers on addressing challenges in decision-making processes involving uncertain and continuous data, particularly the limitations posed by reliance on individual neighborhood granules and the need for prior knowledge to set thresholds. These issues reduce decision efficiency and hinder applicability in situations where such prior knowledge is unavailable. The study aims to overcome these obstacles by introducing a novel approach that enhances the determination of thresholds and improves decision accuracy in uncertain environments. The primary objective of the study is to develop an improved method for configuring neighborhood-based decision frameworks that objectively and efficiently determine thresholds without requiring prior knowledge, thereby enhancing classification accuracy and robustness in handling uncertain data.",FALSE
Psychology,Role of Artificial Intelligence in Higher Education- An Empirical Investigation,"The importance of artificial intelligence (AI) is growing in all economic sectors and thus also in higher education. In recent years, there have been significant developments in this concept of ""Artificial Intelligence in Education (AIED)"". The purpose of this study was to find out how the concept of artificial intelligence can be applied to teaching and learning in higher education and the implications of the use of artificial intelligence in higher education. The impact of the development of technologies on learning is often studied on the methods and scope of learning and teaching. Artificial intelligence enables higher education services to become easily accessible with extraordinary speed, not only in the classroom but also outside the classroom. This report seeks to explore how AI will become an integral part of universities and seeks to examine its immediate and future impact on various aspects of higher education. The challenges of implementing AI in these institutes were also explored. As artificial intelligence (AI) research in education increases, many researchers in the field believe that the role of teachers, schools and leaders in education will change. In this regard, the aim of this study is to investigate which are the possible scenarios for the arrival of artificial intelligence in education and what impact it can have on the future of schools. In this research, it confirmed that artificial intelligence has been widely adopted and used in various forms in education, especially educational institutions. Artificial intelligence was initially implemented in the form of computers and computer-related technologies, moving to web-based and web-based intelligent educational systems, and finally with the use of embedded computing systems and other technologies such as humanoid robots and web-based chatbots teachers &amp; tasks and assignments independently or with tutors. With these platforms, teachers could perform various administrative tasks such as grading and Work more effectively and efficiently and achieve higher quality in your learning activities. On the other hand, because the systems use machine learning and adaptability, the curriculum and content are adapted which improved uptake and retention, which improved the student experience and the overall quality of education.",['machine learning'],"The research idea centers on understanding the growing influence of emerging technologies in higher education, particularly how these advancements affect teaching and learning processes both inside and outside the classroom. The study addresses the broader implications of integrating such technologies into educational institutions and the potential changes in the roles of teachers, schools, and educational leaders. The research objective is to investigate the possible scenarios for the introduction of these technologies in education and to examine their immediate and future impact on various aspects of higher education. Additionally, the study aims to explore the challenges associated with implementing these innovations in educational settings and how they may transform educational practices and experiences.","The research idea centers on understanding the growing influence of emerging technologies in higher education, particularly how these advancements affect teaching and learning processes both inside and outside the classroom. The study addresses the broader implications of integrating such technologies into educational institutions and the potential changes in the roles of teachers, schools, and educational leaders. The research objective is to investigate the possible scenarios for the introduction of these technologies in education and to examine their immediate and future impact on various aspects of higher education. Additionally, the study aims to explore the challenges associated with implementing these innovations in educational settings and how they may transform educational practices and experiences.",FALSE
Psychology,Enhancing black-box models: Advances in explainable artificial intelligence for ethical decision-making,"Transparency, trust, and accountability are among the issues raised by artificial intelligence's (AI) growing reliance on black-box models, especially in high-stakes industries like healthcare, finance, and criminal justice. These models, which are frequently distinguished by their intricacy and opacity, are capable of producing extremely accurate forecasts, but users and decision-makers are still unable to fully understand how they operate. In response to this challenge, the field of Explainable AI (XAI) has emerged with the goal of demystifying these models by offering insights into their decision-making processes. Our ability to interpret model behavior has greatly improved with recent developments in XAI techniques, such as SHAP (Shapley Additive Explanations), LIME (Local Interpretable Model-agnostic Explanations), and counterfactual explanations. These instruments make it easier to recognize bias, promote trust, and guarantee adherence to moral principles and laws like the GDPR and the AI Act. Modern XAI techniques are reviewed in this research along with how they are used in moral decision-making. It looks at how explainability can improve fairness, reduce the risks of AI bias and discrimination, and assist well-informed decision-making in a variety of industries. It also examines the trade-offs between performance and interpretability of models, as well as the growing trends toward user-centric explainability techniques. In order to ensure responsible AI development and deployment, XAI's role in fostering accountability and transparency will become increasingly important as AI becomes more integrated into critical systems.","['SHAP (Shapley Additive Explanations)', 'LIME (Local Interpretable Model-agnostic Explanations)', 'counterfactual explanations']","The research addresses the critical issues of transparency, trust, and accountability in decision-making processes within high-stakes industries such as healthcare, finance, and criminal justice. It highlights the challenge that users and decision-makers face in understanding complex and opaque systems that, despite their accuracy, lack clear interpretability. The study is motivated by the need to improve fairness, reduce bias and discrimination, and support well-informed decisions through enhanced explainability. The primary objective of the study is to review contemporary approaches to explainability and their application in ethical decision-making, focusing on how increased transparency can promote fairness, accountability, and adherence to moral and legal standards. It also aims to explore the balance between performance and interpretability and the shift toward user-centered approaches to foster responsible development and deployment in critical contexts.","The research addresses the critical issues of transparency, trust, and accountability in decision-making processes within high-stakes industries such as healthcare, finance, and criminal justice. It highlights the challenge that users and decision-makers face in understanding complex and opaque systems that, despite their accuracy, lack clear interpretability. The study is motivated by the need to improve fairness, reduce bias and discrimination, and support well-informed decisions through enhanced explainability. The primary objective of the study is to review contemporary approaches to explainability and their application in ethical decision-making, focusing on how increased transparency can promote fairness, accountability, and adherence to moral and legal standards. It also aims to explore the balance between performance and interpretability and the shift toward user-centered approaches to foster responsible development and deployment in critical contexts.",FALSE
Psychology,Active inference as a theory of sentient behavior,"This review paper offers an overview of the history and future of active inference—a unifying perspective on action and perception. Active inference is based upon the idea that sentient behavior depends upon our brains' implicit use of internal models to predict, infer, and direct action. Our focus is upon the conceptual roots and development of this theory of (basic) sentience and does not follow a rigid chronological narrative. We trace the evolution from Helmholtzian ideas on unconscious inference, through to a contemporary understanding of action and perception. In doing so, we touch upon related perspectives, the neural underpinnings of active inference, and the opportunities for future development. Key steps in this development include the formulation of predictive coding models and related theories of neuronal message passing, the use of sequential models for planning and policy optimization, and the importance of hierarchical (temporally) deep internal (i.e., generative or world) models. Active inference has been used to account for aspects of anatomy and neurophysiology, to offer theories of psychopathology in terms of aberrant precision control, and to unify extant psychological theories. We anticipate further development in all these areas and note the exciting early work applying active inference beyond neuroscience. This suggests a future not just in biology, but in robotics, machine learning, and artificial intelligence.","['active inference', 'predictive coding models', 'sequential models for planning and policy optimization']","The research idea centers on understanding sentient behavior through the brain's implicit use of internal models to predict, infer, and direct action, highlighting a unifying perspective on action and perception. The study addresses the conceptual roots and development of a theory of basic sentience, tracing its evolution from early ideas on unconscious inference to contemporary understandings of how action and perception are integrated. The research objective is to provide an overview of the history and future directions of this theory, focusing on its conceptual foundations and development rather than a strict chronological account. The study aims to explore key theoretical advancements and their implications for understanding anatomy, neurophysiology, and psychopathology, as well as to anticipate future developments within psychological science.","The research idea centers on understanding sentient behavior through the brain's implicit use of internal models to predict, infer, and direct action, highlighting a unifying perspective on action and perception. The study addresses the conceptual roots and development of a theory of basic sentience, tracing its evolution from early ideas on unconscious inference to contemporary understandings of how action and perception are integrated. The research objective is to provide an overview of the history and future directions of this theory, focusing on its conceptual foundations and development rather than a strict chronological account. The study aims to explore key theoretical advancements and their implications for understanding anatomy, neurophysiology, and psychopathology, as well as to anticipate future developments within psychological science.",FALSE
Psychology,A self-attention-based CNN-Bi-LSTM model for accurate state-of-charge estimation of lithium-ion batteries,"In the quest for clean and efficient energy solutions, lithium-ion batteries have emerged at the forefront of technological innovation. Accurate state-of-charge (SOC) estimation across a broad temperature range is essential for extending battery longevity, and enduring effective management of overcharge and over-discharge conditions. However, prevailing challenges persist in achieving precise SOC estimates and generalizing across a wide temperature range, particularly at lower temperatures. Our comparative analysis reveals that, while a single-layer bidirectional LSTM model with a self-attention mechanism achieves remarkable SOC estimation accuracy at room temperature, the intricacies of SOC estimation at lower temperatures necessitate the incorporation of more hidden layers and more complex network architecture to capture intricate features influencing battery dynamics. Hence, we propose a deep learning model, based on convolutional neural networks integrating bidirectional long short-term memory and self-attention mechanism (CNN-Bi-LSTM-AM), specifically designed to tackle the challenges of achieving accurate SOC estimations across a wide temperature range. The proposed model demonstrates proficiency in capturing both spatial and temporal dependencies critical for lithium-ion battery SOC estimation. Furthermore, the integration of a self-attention mechanism enhances the model's adeptness to discern pertinent features and patterns within the dataset, thereby improving its overall performance and robustness, even in sub-room temperature environments.","['single-layer bidirectional LSTM model with a self-attention mechanism', 'deep learning model based on convolutional neural networks integrating bidirectional long short-term memory and self-attention mechanism (CNN-Bi-LSTM-AM)']","The research idea centers on the challenge of accurately estimating the state-of-charge (SOC) of lithium-ion batteries across a wide temperature range, which is crucial for extending battery longevity and effectively managing overcharge and over-discharge conditions. Existing methods struggle particularly with precise SOC estimation at lower temperatures, highlighting the need for improved approaches to address these limitations. The primary objective of the study is to develop a solution that enhances the accuracy of SOC estimation across varying temperatures, with a focus on overcoming difficulties encountered in sub-room temperature environments. This aims to ensure more reliable battery management and performance under diverse thermal conditions.","The research idea centers on the challenge of accurately estimating the state-of-charge (SOC) of lithium-ion batteries across a wide temperature range, which is crucial for extending battery longevity and effectively managing overcharge and over-discharge conditions. Existing methods struggle particularly with precise SOC estimation at lower temperatures, highlighting the need for improved approaches to address these limitations. The primary objective of the study is to develop a solution that enhances the accuracy of SOC estimation across varying temperatures, with a focus on overcoming difficulties encountered in sub-room temperature environments. This aims to ensure more reliable battery management and performance under diverse thermal conditions.",FALSE
Psychology,Artificial Intelligence in Point-of-Care Biosensing: Challenges and Opportunities,"The integration of artificial intelligence (AI) into point-of-care (POC) biosensing has the potential to revolutionize diagnostic methodologies by offering rapid, accurate, and accessible health assessment directly at the patient level. This review paper explores the transformative impact of AI technologies on POC biosensing, emphasizing recent computational advancements, ongoing challenges, and future prospects in the field. We provide an overview of core biosensing technologies and their use at the POC, highlighting ongoing issues and challenges that may be solved with AI. We follow with an overview of AI methodologies that can be applied to biosensing, including machine learning algorithms, neural networks, and data processing frameworks that facilitate real-time analytical decision-making. We explore the applications of AI at each stage of the biosensor development process, highlighting the diverse opportunities beyond simple data analysis procedures. We include a thorough analysis of outstanding challenges in the field of AI-assisted biosensing, focusing on the technical and ethical challenges regarding the widespread adoption of these technologies, such as data security, algorithmic bias, and regulatory compliance. Through this review, we aim to emphasize the role of AI in advancing POC biosensing and inform researchers, clinicians, and policymakers about the potential of these technologies in reshaping global healthcare landscapes.",['neural networks'],"The research idea centers on the potential to transform diagnostic methodologies by enabling rapid, accurate, and accessible health assessments directly at the patient level through advancements in point-of-care biosensing. The study addresses ongoing issues and challenges in current biosensing technologies that impact their effectiveness and accessibility in healthcare settings. The primary objective of the study is to explore the transformative impact of recent advancements on point-of-care biosensing, highlighting the challenges and future prospects in the field. It aims to inform researchers, clinicians, and policymakers about the opportunities and obstacles involved in advancing these diagnostic approaches to improve global healthcare outcomes.","The research idea centers on the potential to transform diagnostic methodologies by enabling rapid, accurate, and accessible health assessments directly at the patient level through advancements in point-of-care biosensing. The study addresses ongoing issues and challenges in current biosensing technologies that impact their effectiveness and accessibility in healthcare settings. The primary objective of the study is to explore the transformative impact of recent advancements on point-of-care biosensing, highlighting the challenges and future prospects in the field. It aims to inform researchers, clinicians, and policymakers about the opportunities and obstacles involved in advancing these diagnostic approaches to improve global healthcare outcomes.",FALSE
Psychology,Evaluating the persuasive influence of political microtargeting with large language models,"Recent advancements in large language models (LLMs) have raised the prospect of scalable, automated, and fine-grained political microtargeting on a scale previously unseen; however, the persuasive influence of microtargeting with LLMs remains unclear. Here, we build a custom web application capable of integrating self-reported demographic and political data into GPT-4 prompts in real-time, facilitating the live creation of unique messages tailored to persuade individual users on four political issues. We then deploy this application in a preregistered randomized control experiment ( n = 8,587) to investigate the extent to which access to individual-level data increases the persuasive influence of GPT-4. Our approach yields two key findings. First, messages generated by GPT-4 were broadly persuasive, in some cases increasing support for an issue stance by up to 12 percentage points. Second, in aggregate, the persuasive impact of microtargeted messages was not statistically different from that of non-microtargeted messages (4.83 vs. 6.20 percentage points, respectively, P = 0.226). These trends hold even when manipulating the type and number of attributes used to tailor the message. These findings suggest—contrary to widespread speculation—that the influence of current LLMs may reside not in their ability to tailor messages to individuals but rather in the persuasiveness of their generic, nontargeted messages. We release our experimental dataset, GPTarget2024 , as an empirical baseline for future research.",['GPT-4'],"The research idea centers on understanding the persuasive influence of personalized political messaging, specifically examining whether tailoring messages to individuals based on their demographic and political characteristics enhances persuasion. Despite the growing interest in microtargeting as a strategy to influence political opinions, it remains unclear if such individualized approaches are more effective than generic messaging. The study aims to clarify the extent to which access to individual-level data increases the persuasive impact of political messages. The primary objective of the study is to investigate whether microtargeted political messages, customized using personal demographic and political information, have a greater persuasive effect on individuals compared to non-microtargeted, generic messages across multiple political issues.","The research idea centers on understanding the persuasive influence of personalized political messaging, specifically examining whether tailoring messages to individuals based on their demographic and political characteristics enhances persuasion. Despite the growing interest in microtargeting as a strategy to influence political opinions, it remains unclear if such individualized approaches are more effective than generic messaging. The study aims to clarify the extent to which access to individual-level data increases the persuasive impact of political messages. The primary objective of the study is to investigate whether microtargeted political messages, customized using personal demographic and political information, have a greater persuasive effect on individuals compared to non-microtargeted, generic messages across multiple political issues.",FALSE
Medicine,"Revolutionizing agriculture with artificial intelligence: plant disease detection methods, applications, and their limitations","Accurate and rapid plant disease detection is critical for enhancing long-term agricultural yield. Disease infection poses the most significant challenge in crop production, potentially leading to economic losses. Viruses, fungi, bacteria, and other infectious organisms can affect numerous plant parts, including roots, stems, and leaves. Traditional techniques for plant disease detection are time-consuming, require expertise, and are resource-intensive. Therefore, automated leaf disease diagnosis using artificial intelligence (AI) with Internet of Things (IoT) sensors methodologies are considered for the analysis and detection. This research examines four crop diseases: tomato, chilli, potato, and cucumber. It also highlights the most prevalent diseases and infections in these four types of vegetables, along with their symptoms. This review provides detailed predetermined steps to predict plant diseases using AI. Predetermined steps include image acquisition, preprocessing, segmentation, feature selection, and classification. Machine learning (ML) and deep understanding (DL) detection models are discussed. A comprehensive examination of various existing ML and DL-based studies to detect the disease of the following four crops is discussed, including the datasets used to evaluate these studies. We also provided the list of plant disease detection datasets. Finally, different ML and DL application problems are identified and discussed, along with future research prospects, by combining AI with IoT platforms like smart drones for field-based disease detection and monitoring. This work will help other practitioners in surveying different plant disease detection strategies and the limits of present systems.","['machine learning (ML)', 'deep learning (DL)']","The research addresses the critical need for accurate and rapid detection of plant diseases to enhance long-term agricultural yield, as disease infections pose significant challenges in crop production and can lead to substantial economic losses. Various infectious organisms such as viruses, fungi, and bacteria affect multiple parts of plants, including roots, stems, and leaves, making timely and effective diagnosis essential. The primary objective of the study is to examine and highlight the most prevalent diseases and infections in four major crops—tomato, chilli, potato, and cucumber—along with their symptoms. Additionally, the study aims to provide a comprehensive review of existing approaches for plant disease detection, discuss the limitations of current methods, and outline future research prospects to improve disease diagnosis and monitoring in these crops.","The research addresses the critical need for accurate and rapid detection of plant diseases to enhance long-term agricultural yield, as disease infections pose significant challenges in crop production and can lead to substantial economic losses. Various infectious organisms such as viruses, fungi, and bacteria affect multiple parts of plants, including roots, stems, and leaves, making timely and effective diagnosis essential. The primary objective of the study is to examine and highlight the most prevalent diseases and infections in four major crops—tomato, chilli, potato, and cucumber—along with their symptoms. Additionally, the study aims to provide a comprehensive review of existing approaches for plant disease detection, discuss the limitations of current methods, and outline future research prospects to improve disease diagnosis and monitoring in these crops.",FALSE
Medicine,Optimization of hepatological clinical guidelines interpretation by large language models: a retrieval augmented generation-based framework,"Abstract Large language models (LLMs) can potentially transform healthcare, particularly in providing the right information to the right provider at the right time in the hospital workflow. This study investigates the integration of LLMs into healthcare, specifically focusing on improving clinical decision support systems (CDSSs) through accurate interpretation of medical guidelines for chronic Hepatitis C Virus infection management. Utilizing OpenAI’s GPT-4 Turbo model, we developed a customized LLM framework that incorporates retrieval augmented generation (RAG) and prompt engineering. Our framework involved guideline conversion into the best-structured format that can be efficiently processed by LLMs to provide the most accurate output. An ablation study was conducted to evaluate the impact of different formatting and learning strategies on the LLM’s answer generation accuracy. The baseline GPT-4 Turbo model’s performance was compared against five experimental setups with increasing levels of complexity: inclusion of in-context guidelines, guideline reformatting, and implementation of few-shot learning. Our primary outcome was the qualitative assessment of accuracy based on expert review, while secondary outcomes included the quantitative measurement of similarity of LLM-generated responses to expert-provided answers using text-similarity scores. The results showed a significant improvement in accuracy from 43 to 99% ( p &lt; 0.001), when guidelines were provided as context in a coherent corpus of text and non-text sources were converted into text. In addition, few-shot learning did not seem to improve overall accuracy. The study highlights that structured guideline reformatting and advanced prompt engineering (data quality vs. data quantity) can enhance the efficacy of LLM integrations to CDSSs for guideline delivery.","['OpenAI’s GPT-4 Turbo model', 'retrieval augmented generation (RAG)', 'few-shot learning']","The research idea addresses the challenge of improving clinical decision support systems (CDSSs) in healthcare by enhancing the accurate interpretation and delivery of medical guidelines for managing chronic Hepatitis C Virus infection. There is a need to provide the right information to healthcare providers at the right time within the hospital workflow to support better clinical decisions. The study focuses on overcoming limitations in guideline presentation to improve the accuracy of guideline-based recommendations in clinical practice. The primary objective of the study is to investigate how the integration of structured and well-formatted medical guidelines can improve the accuracy of clinical decision support for chronic Hepatitis C Virus infection management. The study aims to assess the impact of different guideline formatting strategies on the accuracy of guideline interpretation and delivery, with the goal of enhancing the effectiveness of clinical decision support systems in healthcare settings.","The research idea addresses the challenge of improving clinical decision support systems (CDSSs) in healthcare by enhancing the accurate interpretation and delivery of medical guidelines for managing chronic Hepatitis C Virus infection. There is a need to provide the right information to healthcare providers at the right time within the hospital workflow to support better clinical decisions. The study focuses on overcoming limitations in guideline presentation to improve the accuracy of guideline-based recommendations in clinical practice. The primary objective of the study is to investigate how the integration of structured and well-formatted medical guidelines can improve the accuracy of clinical decision support for chronic Hepatitis C Virus infection management. The study aims to assess the impact of different guideline formatting strategies on the accuracy of guideline interpretation and delivery, with the goal of enhancing the effectiveness of clinical decision support systems in healthcare settings.",FALSE
Medicine,Employing deep learning and transfer learning for accurate brain tumor detection,"Abstract Artificial intelligence-powered deep learning methods are being used to diagnose brain tumors with high accuracy, owing to their ability to process large amounts of data. Magnetic resonance imaging stands as the gold standard for brain tumor diagnosis using machine vision, surpassing computed tomography, ultrasound, and X-ray imaging in its effectiveness. Despite this, brain tumor diagnosis remains a challenging endeavour due to the intricate structure of the brain. This study delves into the potential of deep transfer learning architectures to elevate the accuracy of brain tumor diagnosis. Transfer learning is a machine learning technique that allows us to repurpose pre-trained models on new tasks. This can be particularly useful for medical imaging tasks, where labelled data is often scarce. Four distinct transfer learning architectures were assessed in this study: ResNet152, VGG19, DenseNet169, and MobileNetv3. The models were trained and validated on a dataset from benchmark database: Kaggle. Five-fold cross validation was adopted for training and testing. To enhance the balance of the dataset and improve the performance of the models, image enhancement techniques were applied to the data for the four categories: pituitary, normal, meningioma, and glioma. MobileNetv3 achieved the highest accuracy of 99.75%, significantly outperforming other existing methods. This demonstrates the potential of deep transfer learning architectures to revolutionize the field of brain tumor diagnosis.","['deep learning', 'deep transfer learning', 'transfer learning', 'ResNet152', 'VGG19', 'DenseNet169', 'MobileNetv3']","The study addresses the challenge of accurately diagnosing brain tumors, which remains difficult due to the complex structure of the brain. Magnetic resonance imaging is recognized as the gold standard for brain tumor diagnosis, yet improving diagnostic accuracy continues to be a critical need in medical practice. The research focuses on enhancing the precision of brain tumor identification to support better clinical outcomes. The primary aim of the study is to evaluate different approaches to improve the accuracy of brain tumor diagnosis using magnetic resonance imaging data. The objective is to assess and compare various methods to determine which can most effectively distinguish between different types of brain tumors, including pituitary, meningioma, glioma, and normal brain tissue, thereby advancing diagnostic capabilities in this field.","The study addresses the challenge of accurately diagnosing brain tumors, which remains difficult due to the complex structure of the brain. Magnetic resonance imaging is recognized as the gold standard for brain tumor diagnosis, yet improving diagnostic accuracy continues to be a critical need in medical practice. The research focuses on enhancing the precision of brain tumor identification to support better clinical outcomes. The primary aim of the study is to evaluate different approaches to improve the accuracy of brain tumor diagnosis using magnetic resonance imaging data. The objective is to assess and compare various methods to determine which can most effectively distinguish between different types of brain tumors, including pituitary, meningioma, glioma, and normal brain tissue, thereby advancing diagnostic capabilities in this field.",FALSE
Medicine,Transformative Breast Cancer Diagnosis using CNNs with Optimized ReduceLROnPlateau and Early Stopping Enhancements,"Abstract Breast cancer stands as a paramount public health concern worldwide, underscoring an imperative necessity within the research sphere for precision-driven and efficacious methodologies facilitating accurate detection. The existing diagnostic approaches in breast cancer often suffer from limitations in accuracy and efficiency, leading to delayed detection and subsequent challenges in personalized treatment planning. The primary focus of this research is to overcome these shortcomings by harnessing the power of advanced deep learning techniques, thereby revolutionizing the precision and reliability of breast cancer classification. This research addresses the critical need for improved breast cancer diagnostics by introducing a novel Convolutional Neural Network (CNN) model integrated with an Early Stopping callback and ReduceLROnPlateau callback. By enhancing the precision and reliability of breast cancer classification, the study aims to overcome the limitations of existing diagnostic methods, ultimately leading to better patient outcomes and reduced mortality rates. The comprehensive methodology includes diverse datasets, meticulous image preprocessing, robust model training, and validation strategies, emphasizing the model's adaptability and reliability in varied clinical contexts. The findings showcase the CNN model's exceptional performance, achieving a 95.2% accuracy rate in distinguishing cancerous and non-cancerous breast tissue in the integrated dataset, thereby demonstrating its potential for enhancing clinical decision-making and fostering the development of AI-driven diagnostic solutions.","['Convolutional Neural Network (CNN)', 'Early Stopping callback']","Breast cancer remains a significant public health challenge globally, highlighting the urgent need for more accurate and efficient diagnostic methods. Current approaches often face limitations that result in delayed detection and complicate personalized treatment planning. The study is motivated by the necessity to improve the precision and reliability of breast cancer classification to address these diagnostic shortcomings. The primary objective of this research is to enhance breast cancer diagnostics by developing a method that improves the accuracy and reliability of distinguishing cancerous from non-cancerous breast tissue, ultimately aiming to facilitate better patient outcomes and reduce mortality rates.","Breast cancer remains a significant public health challenge globally, highlighting the urgent need for more accurate and efficient diagnostic methods. Current approaches often face limitations that result in delayed detection and complicate personalized treatment planning. The study is motivated by the necessity to improve the precision and reliability of breast cancer classification to address these diagnostic shortcomings. The primary objective of this research is to enhance breast cancer diagnostics by developing a method that improves the accuracy and reliability of distinguishing cancerous from non-cancerous breast tissue, ultimately aiming to facilitate better patient outcomes and reduce mortality rates.",FALSE
Medicine,Real-Time Plant Disease Dataset Development and Detection of Plant Disease Using Deep Learning,"Agriculture plays a significant role in meeting food needs and providing food security for the increasingly growing global population, which has increased by 0.88% since 2022. Plant diseases can reduce food production and affect food security. Worldwide crop loss due to plant disease is estimated to be around 14.1%. The lack of proper identification of plant disease at the early stages of infection can result in inappropriate disease control measures. Therefore, the automatic identification and diagnosis of plant diseases are highly recommended. Lack of availability of large amounts of data that are not processed to a large extent is one of the main challenges in plant disease diagnosis. In the current manuscript, we developed datasets for food grains specifically for rice, wheat, and maize to address the identified challenges. The developed datasets consider the common diseases (two bacterial diseases and two fungal diseases of rice, four fungal diseases of maize, and four fungal diseases of wheat) that affect crop yields and cause damage to the whole plant. The datasets developed were applied to eight fine-tuned deep learning models with the same training hyperparameters. The experimental results based on eight fine-tuned deep learning models show that, while recognizing maize leaf diseases, the models Xception and MobileNet performed best with a testing accuracy of 0.9580 and 0.9464 respectively. Similarly, while recognizing the wheat leaf diseases, the models MobileNetV2 and MobileNet performed best with a testing accuracy of 0.9632 and 0.9628 respectively. The Xception and Inception V3 models performed best, with a testing accuracy of 0.9728 and 0.9620, respectively, for recognizing rice leaf diseases. The research also proposes a new convolutional neural network (CNN) model trained from scratch on all three food grain datasets developed. The proposed model performs well and shows a testing accuracy of 0.9704, 0.9706, and 0.9808 respectively on the maize, rice, and wheat datasets.","['fine-tuned deep learning models', 'Xception', 'MobileNet', 'MobileNetV2', 'Inception V3', 'convolutional neural network (CNN) model trained from scratch']","The research addresses the significant impact of plant diseases on food production and food security, emphasizing that worldwide crop loss due to plant disease is estimated to be around 14.1%. Early and accurate identification of plant diseases is crucial to prevent inappropriate disease control measures that can further harm crop yields. The study highlights the challenge posed by the lack of properly processed and comprehensive data for diagnosing plant diseases, particularly in important food grains such as rice, wheat, and maize. The primary objective of the study is to develop datasets for common bacterial and fungal diseases affecting these food grains to improve the identification and diagnosis of plant diseases, thereby supporting efforts to reduce crop damage and enhance food security.","The research addresses the significant impact of plant diseases on food production and food security, emphasizing that worldwide crop loss due to plant disease is estimated to be around 14.1%. Early and accurate identification of plant diseases is crucial to prevent inappropriate disease control measures that can further harm crop yields. The study highlights the challenge posed by the lack of properly processed and comprehensive data for diagnosing plant diseases, particularly in important food grains such as rice, wheat, and maize. The primary objective of the study is to develop datasets for common bacterial and fungal diseases affecting these food grains to improve the identification and diagnosis of plant diseases, thereby supporting efforts to reduce crop damage and enhance food security.",FALSE
Medicine,Oral squamous cell carcinoma detection using EfficientNet on histopathological images,"Introduction Oral Squamous Cell Carcinoma (OSCC) poses a significant challenge in oncology due to the absence of precise diagnostic tools, leading to delays in identifying the condition. Current diagnostic methods for OSCC have limitations in accuracy and efficiency, highlighting the need for more reliable approaches. This study aims to explore the discriminative potential of histopathological images of oral epithelium and OSCC. By utilizing a database containing 1224 images from 230 patients, captured at varying magnifications and publicly available, a customized deep learning model based on EfficientNetB3 was developed. The model’s objective was to differentiate between normal epithelium and OSCC tissues by employing advanced techniques such as data augmentation, regularization, and optimization. Methods The research utilized a histopathological imaging database for Oral Cancer analysis, incorporating 1224 images from 230 patients. These images, taken at various magnifications, formed the basis for training a specialized deep learning model built upon the EfficientNetB3 architecture. The model underwent training to distinguish between normal epithelium and OSCC tissues, employing sophisticated methodologies including data augmentation, regularization techniques, and optimization strategies. Results The customized deep learning model achieved significant success, showcasing a remarkable 99% accuracy when tested on the dataset. This high accuracy underscores the model’s efficacy in effectively discerning between normal epithelium and OSCC tissues. Furthermore, the model exhibited impressive precision, recall, and F1-score metrics, reinforcing its potential as a robust diagnostic tool for OSCC. Discussion This research demonstrates the promising potential of employing deep learning models to address the diagnostic challenges associated with OSCC. The model’s ability to achieve a 99% accuracy rate on the test dataset signifies a considerable leap forward in earlier and more accurate detection of OSCC. Leveraging advanced techniques in machine learning, such as data augmentation and optimization, has shown promising results in improving patient outcomes through timely and precise identification of OSCC.",['deep learning model based on EfficientNetB3'],"Oral Squamous Cell Carcinoma (OSCC) presents a significant challenge in oncology due to the lack of precise diagnostic tools, which often results in delays in identifying the condition. Current diagnostic methods for OSCC are limited in both accuracy and efficiency, underscoring the need for more reliable approaches to improve early detection. The primary aim of this study is to explore the discriminative potential of histopathological images of oral epithelium and OSCC. Specifically, the study seeks to differentiate between normal epithelium and OSCC tissues using a comprehensive database of histopathological images, with the goal of enhancing the accuracy and reliability of OSCC diagnosis.","Oral Squamous Cell Carcinoma (OSCC) presents a significant challenge in oncology due to the lack of precise diagnostic tools, which often results in delays in identifying the condition. Current diagnostic methods for OSCC are limited in both accuracy and efficiency, underscoring the need for more reliable approaches to improve early detection. The primary aim of this study is to explore the discriminative potential of histopathological images of oral epithelium and OSCC. Specifically, the study seeks to differentiate between normal epithelium and OSCC tissues using a comprehensive database of histopathological images, with the goal of enhancing the accuracy and reliability of OSCC diagnosis.",FALSE
Medicine,Damage identification of steel bridge based on data augmentation and adaptive optimization neural network,"With the advancement of deep learning, data-driven structural damage identification (SDI) has shown considerable development. However, collecting vibration signals related to structural damage poses certain challenges, which can undermine the accuracy of the identification results produced by data-driven SDI methods in scenarios where data is scarce. This paper introduces an innovative approach to bridge SDI in a few-shot context by integrating an adaptive simulated annealing particle swarm optimization-convolutional neural network (ASAPSO-CNN) as the foundational framework, augmented by data enhancement techniques. Firstly, three specific types of noise are introduced to augment the source signals used for training. Subsequently, the source signals and augmented signals are recombined to construct a four-dimensional matrix as the input to the CNN, while defining the damage feature vector as the output. Secondly, a CNN is constructed to establish the mapping relationship between the input and output. Then, an adaptive fitness function is proposed that simultaneously considers the accuracy of SDI, model complexity, and training efficiency. The ASAPSO is employed to adaptively optimize the hyperparameters of the CNN. The proposed method is validated on an experimental model of a three-span continuous beam. It is compared with four other data-driven methods, demonstrating good effectiveness and robustness of SDI under cases of scarce data. Finally, the effectiveness of this SDI method is validated in a real-world case of a steel truss bridge.",['convolutional neural network (CNN)'],"The research idea addresses the challenge of accurately identifying structural damage when vibration signal data related to such damage is scarce, which can compromise the reliability of existing identification methods. The study is motivated by the need to improve structural damage identification in scenarios where collecting sufficient vibration signals is difficult, thereby enhancing the detection accuracy and robustness under limited data conditions. The primary objective of the study is to develop and validate a novel approach that improves the identification of structural damage using limited vibration signal data. This approach aims to enhance the effectiveness and robustness of damage detection in both experimental models and real-world structures, such as steel truss bridges, particularly in cases where data availability is constrained.","The research idea addresses the challenge of accurately identifying structural damage when vibration signal data related to such damage is scarce, which can compromise the reliability of existing identification methods. The study is motivated by the need to improve structural damage identification in scenarios where collecting sufficient vibration signals is difficult, thereby enhancing the detection accuracy and robustness under limited data conditions. The primary objective of the study is to develop and validate a novel approach that improves the identification of structural damage using limited vibration signal data. This approach aims to enhance the effectiveness and robustness of damage detection in both experimental models and real-world structures, such as steel truss bridges, particularly in cases where data availability is constrained.",FALSE
Engineering,Sequence Training and Data Shuffling to Enhance the Accuracy of Recurrent Neural Network Based Battery Voltage Models,"&lt;div class=""section abstract""&gt;&lt;div class=""htmlview paragraph""&gt;Battery terminal voltage modelling is crucial for various applications, including electric vehicles, renewable energy systems, and portable electronics. Terminal voltage models are used to determine how a battery will respond under load and can be used to calculate run-time, power capability, and heat generation and as a component of state estimation approaches, such as for state of charge. Previous studies have shown better voltage modelling accuracy for long short-term memory (LSTM) recurrent neural networks than other traditional methods (e.g., equivalent circuit and electrochemical models). This study presents two new approaches – sequence training and data shuffling – to improve LSTM battery voltage models further, making them an even better candidate for the high-accuracy modelling of lithium-ion batteries. Because the LSTM memory captures information from past time steps, it must typically be trained using one series of continuous data. Instead, the proposed sequence training approach feeds a fixed window of prior data (e.g., 100 seconds) into the LSTM at each time step to initialize the memory states properly and then only uses the output at the current time step. With this method, the LSTM just requires the prior data window to be continuous, thereby allowing the handling of discontinuities. This also means that during the training process, the data can be shuffled randomly, enabling mini-batches to speed up the training significantly. When these approaches were applied, LSTM voltage estimation error was reduced by 22%, from 28.5 mV to 22.3 mV RMS error over four drive cycles and temperatures from -20 to 25°C.&lt;/div&gt;&lt;/div&gt;","['long short-term memory (LSTM) recurrent neural networks', 'sequence training']","The research addresses the critical need for accurate battery terminal voltage modeling, which is essential for applications such as electric vehicles, renewable energy systems, and portable electronics. Accurate voltage models help predict battery response under load, enabling the calculation of run-time, power capability, and heat generation, as well as supporting state estimation tasks like determining the state of charge. The primary objective of the study is to enhance the accuracy of lithium-ion battery voltage models by introducing new approaches that improve the handling of prior data during the modeling process. This aims to reduce voltage estimation errors across various operating conditions, including different drive cycles and temperature ranges.","The research addresses the critical need for accurate battery terminal voltage modeling, which is essential for applications such as electric vehicles, renewable energy systems, and portable electronics. Accurate voltage models help predict battery response under load, enabling the calculation of run-time, power capability, and heat generation, as well as supporting state estimation tasks like determining the state of charge. The primary objective of the study is to enhance the accuracy of lithium-ion battery voltage models by introducing new approaches that improve the handling of prior data during the modeling process. This aims to reduce voltage estimation errors across various operating conditions, including different drive cycles and temperature ranges.",FALSE
Engineering,Semantic and Instance Segmentation in Coastal Urban Spatial Perception: A Multi-Task Learning Framework with an Attention Mechanism,"With the continuous acceleration of urbanization, urban planning and design require more in-depth research and development. Street view images can express rich urban features and guide residents’ emotions toward a city, thereby providing the most intuitive reflection of their perception of the city’s spatial quality. However, current researchers mainly conduct research on urban spatial quality through subjective experiential judgment, which includes problems such as a high cost and a low judgment accuracy. In response to these problems, this study proposes a multi-task learning urban spatial attribute perception model that integrates an attention mechanism. Via this model, the existing attributes of urban street scenes are analyzed. Then, the model is improved by introducing semantic segmentation and instance segmentation to identify and match the qualities of the urban space. The experimental results show that the multi-task learning urban spatial attribute perception model with an integrated attention mechanism has prediction accuracies of 79.54%, 78.62%, 79.68%, 77.42%, 78.45%, and 76.98% for the urban spatial attributes of beauty, boredom, depression, liveliness, safety, and richness, respectively. The accuracy of the multi-task learning urban spatial scene feature image segmentation model with an integrated attention mechanism is 95.4, 94.8, 96.2, 92.1, and 96.7 for roads, walls, sky, vehicles, and buildings, respectively. The multi-task learning urban spatial scene feature image segmentation model with an integrated attention mechanism has a higher recognition accuracy for urban spatial buildings than other models. These research results indicate the model’s effectiveness in matching urban spatial quality with public perception.","['multi-task learning', 'attention mechanism', 'semantic segmentation', 'instance segmentation']","The research addresses the challenge of accurately assessing urban spatial quality, which is essential for urban planning and design amid rapid urbanization. Traditional methods rely heavily on subjective experiential judgment, leading to high costs and low accuracy in evaluating the spatial attributes of urban environments. The study aims to improve the evaluation of urban street scenes by analyzing existing urban spatial attributes and enhancing the identification and matching of these qualities within urban spaces. The primary objective is to develop and validate an approach that effectively matches urban spatial quality with public perception, thereby providing a more accurate and objective assessment of urban environments.","The research addresses the challenge of accurately assessing urban spatial quality, which is essential for urban planning and design amid rapid urbanization. Traditional methods rely heavily on subjective experiential judgment, leading to high costs and low accuracy in evaluating the spatial attributes of urban environments. The study aims to improve the evaluation of urban street scenes by analyzing existing urban spatial attributes and enhancing the identification and matching of these qualities within urban spaces. The primary objective is to develop and validate an approach that effectively matches urban spatial quality with public perception, thereby providing a more accurate and objective assessment of urban environments.",FALSE
Engineering,A comprehensive analysis of the emerging modern trends in research on photovoltaic systems and desalination in the era of artificial intelligence and machine learning,"Integration of photovoltaic (PV) systems, desalination technologies, and Artificial Intelligence (AI) combined with Machine Learning (ML) has introduced a new era of remarkable research and innovation. This review article thoroughly examines the recent advancements in the field, focusing on the interplay between PV systems and water desalination within the framework of AI and ML applications, along with it analyses current research to identify significant patterns, obstacles, and prospects in this interdisciplinary field. Furthermore, review examines the incorporation of AI and ML methods in improving the performance of PV systems. This includes raising their efficiency, implementing predictive maintenance strategies, and enabling real-time monitoring. It also explores the transformative influence of intelligent algorithms on desalination techniques, specifically addressing concerns pertaining to energy usage, scalability, and environmental sustainability. This article provides a thorough analysis of the current literature, identifying areas where research is lacking and suggesting potential future avenues for investigation. These advancements have resulted in increased efficiency, decreased expenses, and improved sustainability of PV system. By utilizing artificial intelligence technologies, freshwater productivity can increase by 10 % and efficiency. This review offers significant and informative perspectives for researchers, engineers, and policymakers involved in renewable energy and water technology. It sheds light on the latest advancements in photovoltaic systems and desalination, which are facilitated by AI and ML. The review aims to guide towards a more sustainable and technologically advanced future.",['Machine Learning (ML)'],"The research idea centers on the integration of photovoltaic systems and water desalination technologies to address challenges related to energy usage, scalability, and environmental sustainability in renewable energy and water treatment. The study highlights the need to improve the efficiency and sustainability of these systems to meet growing demands for freshwater production and energy optimization. The primary objective of the study is to thoroughly review recent advancements in photovoltaic systems and desalination technologies, focusing on their performance improvements and the potential for increased freshwater productivity. Additionally, the study aims to identify current obstacles, significant patterns, and future research directions to support the development of more efficient, cost-effective, and sustainable solutions in this interdisciplinary field.","The research idea centers on the integration of photovoltaic systems and water desalination technologies to address challenges related to energy usage, scalability, and environmental sustainability in renewable energy and water treatment. The study highlights the need to improve the efficiency and sustainability of these systems to meet growing demands for freshwater production and energy optimization. The primary objective of the study is to thoroughly review recent advancements in photovoltaic systems and desalination technologies, focusing on their performance improvements and the potential for increased freshwater productivity. Additionally, the study aims to identify current obstacles, significant patterns, and future research directions to support the development of more efficient, cost-effective, and sustainable solutions in this interdisciplinary field.",FALSE
Engineering,Optimum tuned mass damper inerter under near-fault pulse-like ground motions of buildings including soil-structure interaction,"This study investigates the effectiveness of the tuned mass damper inerter (TMDI) in mitigating building response, considering the soil structure interaction (SSI). Three types of models are examined: single degree of freedom (SDOF), low-rise multi-degree of freedom (MDOF), and high-rise MDOF. Additionally, the natural period of the SDOF model is varied to explore the TMDI's efficacy across different ranges. Frequency and time domain analysis are conducted under pulse-like ground motions. The H2 and genetic algorithm (GA) are used to optimize the parameters of the TMDI. In this optimization method the transfer function for displacement response is minimized. In time domain analysis we used Newmark's integration method to solve the equation of motion for all the cases considered. It is found that the optimized TMDI proves highly effective in mitigating the displacement response of the buildings, accounting for SSI. Notably, its efficiency is more pronounced when pulse period aligns closely with the buildings' natural period. In addition, a notable pattern emerges, wherein the TMDI excels in mitigating response for buildings experiencing large motion, thereby enhancing safety under severe conditions. These findings offer valuable insights into the application and optimization of the TMDI to enhance seismic performance in various buildings, while considering complex interaction with the soil.",['genetic algorithm (GA)'],"The research idea centers on addressing the challenge of mitigating building responses during seismic events by considering the interaction between the soil and the structure. The study focuses on evaluating the effectiveness of a tuned mass damper inerter (TMDI) in reducing displacement responses in buildings of varying heights and dynamic characteristics under pulse-like ground motions. It highlights the importance of accounting for soil-structure interaction to improve the accuracy and reliability of vibration mitigation strategies. The primary objective of the study is to investigate the performance of the TMDI in controlling building displacement responses across different structural models, including single degree of freedom and multi-degree of freedom systems, while varying the natural period to assess its efficacy under different conditions. The study aims to demonstrate how optimizing the TMDI parameters can enhance seismic safety, particularly when the pulse period of ground motion closely matches the natural period of the building, thereby providing insights for improving seismic performance in buildings considering complex soil-structure interactions.","The research idea centers on addressing the challenge of mitigating building responses during seismic events by considering the interaction between the soil and the structure. The study focuses on evaluating the effectiveness of a tuned mass damper inerter (TMDI) in reducing displacement responses in buildings of varying heights and dynamic characteristics under pulse-like ground motions. It highlights the importance of accounting for soil-structure interaction to improve the accuracy and reliability of vibration mitigation strategies. The primary objective of the study is to investigate the performance of the TMDI in controlling building displacement responses across different structural models, including single degree of freedom and multi-degree of freedom systems, while varying the natural period to assess its efficacy under different conditions. The study aims to demonstrate how optimizing the TMDI parameters can enhance seismic safety, particularly when the pulse period of ground motion closely matches the natural period of the building, thereby providing insights for improving seismic performance in buildings considering complex soil-structure interactions.",FALSE
Engineering,A machine learning-based framework for clustering residential electricity load profiles to enhance demand response programs,"Load shapes derived from smart meter data are frequently employed to analyze daily energy consumption patterns, particularly in the context of applications like Demand Response (DR). Nevertheless, one of the most important challenges to this endeavor lies in identifying the most suitable consumer clusters with similar consumption behaviors. In this paper, we present a novel machine learning based framework in order to achieve optimal load profiling through a real case study, utilizing data from almost 5000 households in London. Four widely used clustering algorithms are applied specifically K-means, K-medoids, Hierarchical Agglomerative Clustering and Density-based Spatial Clustering. An empirical analysis as well as multiple evaluation metrics are leveraged to assess those algorithms. Following that, we redefine the problem as a probabilistic classification one, with the classifier emulating the behavior of a clustering algorithm, leveraging Explainable AI (xAI) to enhance the interpretability of our solution. According to the clustering algorithm analysis the optimal number of clusters for this case is seven. Despite that, our methodology shows that two of the clusters, almost 10% of the dataset, exhibit significant internal dissimilarity. As a result, these clusters have been excluded from consideration for DR programs. The scalability and versatility of our solution makes it an ideal choice for power utility companies aiming to segment their users for creating more targeted DR programs.","['K-means', 'K-medoids', 'Hierarchical Agglomerative Clustering', 'Density-based Spatial Clustering', 'probabilistic classification', 'Explainable AI (xAI)']","The study addresses the challenge of accurately identifying consumer groups with similar daily energy consumption patterns to improve the effectiveness of Demand Response (DR) programs. This problem is critical because proper segmentation of consumers based on their load profiles enables more targeted and efficient energy management strategies. The primary objective of the research is to achieve optimal load profiling by analyzing consumption data from nearly 5000 households in London, determining the most appropriate number of consumer clusters, and refining the segmentation to exclude groups with significant internal dissimilarity. This aims to support power utility companies in better segmenting their users for the development of more effective DR initiatives.","The study addresses the challenge of accurately identifying consumer groups with similar daily energy consumption patterns to improve the effectiveness of Demand Response (DR) programs. This problem is critical because proper segmentation of consumers based on their load profiles enables more targeted and efficient energy management strategies. The primary objective of the research is to achieve optimal load profiling by analyzing consumption data from nearly 5000 households in London, determining the most appropriate number of consumer clusters, and refining the segmentation to exclude groups with significant internal dissimilarity. This aims to support power utility companies in better segmenting their users for the development of more effective DR initiatives.",FALSE
Engineering,Deep Learning for Integrated Origin–Destination Estimation and Traffic Sensor Location Problems,"Traffic control and management applications require the full realization of traffic flow data. Frequently, such data are acquired by traffic sensors with two issues: it is not practicable or even possible to place traffic sensors on every link in a network; sensors do not provide direct information about origin–destination (O–D) demand flows. Therefore, it is imperative to locate the best places to deploy traffic sensors and then augment the knowledge obtained from this link flow sample to predict the entire traffic flow of the network. This article provides a resilient deep learning (DL) architecture combined with a global sensitivity analysis tool to solve O–D estimation and sensor location problems simultaneously. The proposed DL architecture is based on the stacked sparse autoencoder (SAE) model for accurately estimating the entire O–D flows of the network using link flows, thus reversing the conventional traffic assignment problem. The SAE model extracts traffic flow characteristics and derives a meaningful relationship between traffic flow data and network topology. To train the proposed DL architecture, synthetic link flow data were created randomly from the historical demand data of the network. Finally, a global sensitivity analysis was implemented to prioritize the importance of each link in the O–D estimation step to solve the sensor location problem. Two networks of different sizes were used to validate the performance of the model. The efficiency of the proposed method for solving the combination of traffic flow estimation and sensor location problems was confirmed from a low root-mean-square error with a reduction in the number of link flows required.",['stacked sparse autoencoder (SAE) model'],"The research addresses the challenge of obtaining comprehensive traffic flow data for effective traffic control and management, given the practical limitations of placing sensors on every network link and the inability of sensors to directly measure origin–destination demand flows. It is crucial to determine optimal locations for traffic sensor deployment and to enhance the information gathered from limited link flow measurements to estimate the complete traffic flow across the network. The primary objective of the study is to accurately estimate the entire origin–destination traffic flows within a network using available link flow data while simultaneously identifying the best locations for sensor placement. This aims to improve the understanding of traffic patterns and optimize sensor deployment to support more effective traffic management strategies.","The research addresses the challenge of obtaining comprehensive traffic flow data for effective traffic control and management, given the practical limitations of placing sensors on every network link and the inability of sensors to directly measure origin–destination demand flows. It is crucial to determine optimal locations for traffic sensor deployment and to enhance the information gathered from limited link flow measurements to estimate the complete traffic flow across the network. The primary objective of the study is to accurately estimate the entire origin–destination traffic flows within a network using available link flow data while simultaneously identifying the best locations for sensor placement. This aims to improve the understanding of traffic patterns and optimize sensor deployment to support more effective traffic management strategies.",FALSE
Engineering,Evaluation of artificial intelligence-powered screening for sexually transmitted infections-related skin lesions using clinical images and metadata,"Abstract Background Sexually transmitted infections (STIs) pose a significant global public health challenge. Early diagnosis and treatment reduce STI transmission, but rely on recognising symptoms and care-seeking behaviour of the individual. Digital health software that distinguishes STI skin conditions could improve health-seeking behaviour. We developed and evaluated a deep learning model to differentiate STIs from non-STIs based on clinical images and symptoms. Methods We used 4913 clinical images of genital lesions and metadata from the Melbourne Sexual Health Centre collected during 2010–2023. We developed two binary classification models to distinguish STIs from non-STIs: (1) a convolutional neural network (CNN) using images only and (2) an integrated model combining both CNN and fully connected neural network (FCN) using images and metadata. We evaluated the model performance by the area under the ROC curve (AUC) and assessed metadata contributions to the Image-only model. Results Our study included 1583 STI and 3330 non-STI images. Common STI diagnoses were syphilis (34.6%), genital warts (24.5%) and herpes (19.4%), while most non-STIs (80.3%) were conditions such as dermatitis, lichen sclerosis and balanitis. In both STI and non-STI groups, the most frequently observed groups were 25–34 years (48.6% and 38.2%, respectively) and heterosexual males (60.3% and 45.9%, respectively). The Image-only model showed a reasonable performance with an AUC of 0.859 (SD 0.013). The Image + Metadata model achieved a significantly higher AUC of 0.893 (SD 0.018) compared to the Image-only model ( p &lt; 0.01). Out of 21 metadata, the integration of demographic and dermatological metadata led to the most significant improvement in model performance, increasing AUC by 6.7% compared to the baseline Image-only model. Conclusions The Image + Metadata model outperformed the Image-only model in distinguishing STIs from other skin conditions. Using it as a screening tool in a clinical setting may require further development and evaluation with larger datasets.","['convolutional neural network (CNN)', 'fully connected neural network (FCN)']","The research addresses the significant global public health challenge posed by sexually transmitted infections (STIs), emphasizing the importance of early diagnosis and treatment to reduce transmission. The study highlights the need for improved recognition of STI-related skin conditions to enhance health-seeking behavior among individuals. The primary objective of the study is to develop and evaluate a method to differentiate STIs from non-STIs based on clinical images and associated patient information. This aims to improve the accuracy of distinguishing STI skin conditions from other dermatological issues, potentially serving as a screening tool in clinical settings.","The research addresses the significant global public health challenge posed by sexually transmitted infections (STIs), emphasizing the importance of early diagnosis and treatment to reduce transmission. The study highlights the need for improved recognition of STI-related skin conditions to enhance health-seeking behavior among individuals. The primary objective of the study is to develop and evaluate a method to differentiate STIs from non-STIs based on clinical images and associated patient information. This aims to improve the accuracy of distinguishing STI skin conditions from other dermatological issues, potentially serving as a screening tool in clinical settings.",FALSE
Social Sciences,Unmasking bias in artificial intelligence: a systematic review of bias detection and mitigation strategies in electronic health record-based models,"Abstract Objectives Leveraging artificial intelligence (AI) in conjunction with electronic health records (EHRs) holds transformative potential to improve healthcare. However, addressing bias in AI, which risks worsening healthcare disparities, cannot be overlooked. This study reviews methods to handle various biases in AI models developed using EHR data. Materials and Methods We conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and Meta-analyses guidelines, analyzing articles from PubMed, Web of Science, and IEEE published between January 01, 2010 and December 17, 2023. The review identified key biases, outlined strategies for detecting and mitigating bias throughout the AI model development, and analyzed metrics for bias assessment. Results Of the 450 articles retrieved, 20 met our criteria, revealing 6 major bias types: algorithmic, confounding, implicit, measurement, selection, and temporal. The AI models were primarily developed for predictive tasks, yet none have been deployed in real-world healthcare settings. Five studies concentrated on the detection of implicit and algorithmic biases employing fairness metrics like statistical parity, equal opportunity, and predictive equity. Fifteen studies proposed strategies for mitigating biases, especially targeting implicit and selection biases. These strategies, evaluated through both performance and fairness metrics, predominantly involved data collection and preprocessing techniques like resampling and reweighting. Discussion This review highlights evolving strategies to mitigate bias in EHR-based AI models, emphasizing the urgent need for both standardized and detailed reporting of the methodologies and systematic real-world testing and evaluation. Such measures are essential for gauging models’ practical impact and fostering ethical AI that ensures fairness and equity in healthcare.","['resampling', 'reweighting']","The research idea centers on the critical issue of bias in healthcare applications that utilize electronic health records, which poses a risk of exacerbating existing healthcare disparities. Addressing these biases is essential to ensure fairness and equity in healthcare delivery. The study recognizes the transformative potential of improving healthcare outcomes but emphasizes that bias cannot be overlooked in this context. The primary objective of the study is to review and synthesize existing approaches for identifying and mitigating various types of bias in healthcare-related applications using electronic health records. It aims to highlight the need for standardized reporting and real-world evaluation to assess the practical impact of these approaches and promote ethical practices that support fairness in healthcare.","The research idea centers on the critical issue of bias in healthcare applications that utilize electronic health records, which poses a risk of exacerbating existing healthcare disparities. Addressing these biases is essential to ensure fairness and equity in healthcare delivery. The study recognizes the transformative potential of improving healthcare outcomes but emphasizes that bias cannot be overlooked in this context. The primary objective of the study is to review and synthesize existing approaches for identifying and mitigating various types of bias in healthcare-related applications using electronic health records. It aims to highlight the need for standardized reporting and real-world evaluation to assess the practical impact of these approaches and promote ethical practices that support fairness in healthcare.",FALSE
Social Sciences,Data extraction for evidence synthesis using a large language model: A proof‐of‐concept study,"Abstract Data extraction is a crucial, yet labor‐intensive and error‐prone part of evidence synthesis. To date, efforts to harness machine learning for enhancing efficiency of the data extraction process have fallen short of achieving sufficient accuracy and usability. With the release of large language models (LLMs), new possibilities have emerged to increase efficiency and accuracy of data extraction for evidence synthesis. The objective of this proof‐of‐concept study was to assess the performance of an LLM (Claude 2) in extracting data elements from published studies, compared with human data extraction as employed in systematic reviews. Our analysis utilized a convenience sample of 10 English‐language, open‐access publications of randomized controlled trials included in a single systematic review. We selected 16 distinct types of data, posing varying degrees of difficulty (160 data elements across 10 studies). We used the browser version of Claude 2 to upload the portable document format of each publication and then prompted the model for each data element. Across 160 data elements, Claude 2 demonstrated an overall accuracy of 96.3% with a high test–retest reliability (replication 1: 96.9%; replication 2: 95.0% accuracy). Overall, Claude 2 made 6 errors on 160 data items. The most common errors ( n = 4) were missed data items. Importantly, Claude 2's ease of use was high; it required no technical expertise or labeled training data for effective operation (i.e., zero‐shot learning). Based on findings of our proof‐of‐concept study, leveraging LLMs has the potential to substantially enhance the efficiency and accuracy of data extraction for evidence syntheses.",['zero‐shot learning'],"The research idea addresses the challenge that data extraction in evidence synthesis is a crucial but labor-intensive and error-prone process, with previous efforts to improve efficiency and accuracy falling short. This study is motivated by the need to find more effective ways to enhance the accuracy and usability of data extraction in systematic reviews. The primary objective of the study was to assess the performance of a new approach in extracting data elements from published studies, comparing its accuracy and reliability to human data extraction methods commonly used in systematic reviews. The study aimed to evaluate whether this approach could improve the efficiency and accuracy of data extraction for evidence synthesis without requiring specialized expertise or training.","The research idea addresses the challenge that data extraction in evidence synthesis is a crucial but labor-intensive and error-prone process, with previous efforts to improve efficiency and accuracy falling short. This study is motivated by the need to find more effective ways to enhance the accuracy and usability of data extraction in systematic reviews. The primary objective of the study was to assess the performance of a new approach in extracting data elements from published studies, comparing its accuracy and reliability to human data extraction methods commonly used in systematic reviews. The study aimed to evaluate whether this approach could improve the efficiency and accuracy of data extraction for evidence synthesis without requiring specialized expertise or training.",FALSE
Social Sciences,CFSSynergy: Combining Feature-Based and Similarity-Based Methods for Drug Synergy Prediction,"Drug synergy prediction plays a vital role in cancer treatment. Because experimental approaches are labor-intensive and expensive, computational-based approaches get more attention. There are two types of computational methods for drug synergy prediction: feature-based and similarity-based. In feature-based methods, the main focus is to extract more discriminative features from drug pairs and cell lines to pass to the task predictor. In similarity-based methods, the similarities among all drugs and cell lines are utilized as features and fed into the task predictor. In this work, a novel approach, called CFSSynergy, that combines these two viewpoints is proposed. First, a discriminative representation is extracted for paired drugs and cell lines as input. We have utilized transformer-based architecture for drugs. For cell lines, we have created a similarity matrix between proteins using the Node2Vec algorithm. Then, the new cell line representation is computed by multiplying the protein–protein similarity matrix and the initial cell line representation. Next, we compute the similarity between unique drugs and unique cells using the learned representation for paired drugs and cell lines. Then, we compute a new representation for paired drugs and cell lines based on the similarity-based features and the learned features. Finally, these features are fed to XGBoost as a task predictor. Two well-known data sets were used to evaluate the performance of our proposed method: DrugCombDB and OncologyScreen. The CFSSynergy approach consistently outperformed existing methods in comparative evaluations. This substantiates the efficacy of our approach in capturing complex synergistic interactions between drugs and cell lines, setting it apart from conventional similarity-based or feature-based methods.","['transformer-based architecture', 'Node2Vec algorithm', 'XGBoost']","The research addresses the challenge of predicting drug synergy in cancer treatment, emphasizing the importance of identifying effective drug combinations to improve therapeutic outcomes. Experimental methods for determining drug synergy are labor-intensive and costly, creating a need for more efficient approaches to support cancer therapy development. The study aims to enhance the prediction of synergistic drug interactions by integrating different perspectives on drug and cell line characteristics. Specifically, the primary objective is to develop a novel approach that combines distinct viewpoints on drug pairs and cell lines to better capture the complex interactions that contribute to drug synergy, ultimately improving the accuracy of synergy predictions in cancer treatment.","The research addresses the challenge of predicting drug synergy in cancer treatment, emphasizing the importance of identifying effective drug combinations to improve therapeutic outcomes. Experimental methods for determining drug synergy are labor-intensive and costly, creating a need for more efficient approaches to support cancer therapy development. The study aims to enhance the prediction of synergistic drug interactions by integrating different perspectives on drug and cell line characteristics. Specifically, the primary objective is to develop a novel approach that combines distinct viewpoints on drug pairs and cell lines to better capture the complex interactions that contribute to drug synergy, ultimately improving the accuracy of synergy predictions in cancer treatment.",FALSE
Social Sciences,A voting gray wolf optimizer-based ensemble learning models for intrusion detection in the Internet of Things,"Abstract The Internet of Things (IoT) has garnered considerable attention from academic and industrial circles as a pivotal technology in recent years. The escalation of security risks is observed to be associated with the growing interest in IoT applications. Intrusion detection systems (IDS) have been devised as viable instruments for identifying and averting malicious actions in this context. Several techniques described in academic papers are thought to be very accurate, but they cannot be used in the real world because the datasets used to build and test the models do not accurately reflect and simulate the IoT network. Existing methods, on the other hand, deal with these issues, but they are not good enough for commercial use because of their lack of precision, low detection rate, receiver operating characteristic (ROC), and false acceptance rate (FAR). The effectiveness of these solutions is predominantly dependent on individual learners and is consequently influenced by the inherent limitations of each learning algorithm. This study introduces a new approach for detecting intrusion attacks in an IoT network, which involves the use of an ensemble learning technique based on gray wolf optimizer (GWO). The novelty of this study lies in the proposed voting gray wolf optimizer (GWO) ensemble model, which incorporates two crucial components: a traffic analyzer and a classification phase engine. The model employs a voting technique to combine the probability averages of the base learners. Secondly, the combination of feature selection and feature extraction techniques is to reduce dimensionality. Thirdly, the utilization of GWO is employed to optimize the parameters of ensemble models. Similarly, the approach employs the most authentic intrusion detection datasets that are accessible and amalgamates multiple learners to generate ensemble learners. The hybridization of information gain (IG) and principal component analysis (PCA) was employed to reduce dimensionality. The study utilized a novel GWO ensemble learning approach that incorporated a decision tree, random forest, K-nearest neighbor, and multilayer perceptron for classification. To evaluate the efficacy of the proposed model, two authentic datasets, namely, BoT-IoT and UNSW-NB15, were scrutinized. The GWO-optimized ensemble model demonstrates superior accuracy when compared to other machine learning-based and deep learning models. Specifically, the model achieves an accuracy rate of 99.98%, a DR of 99.97%, a precision rate of 99.94%, an ROC rate of 99.99%, and an FAR rate of 1.30 on the BoT-IoT dataset. According to the experimental results, the proposed ensemble model optimized by GWO achieved an accuracy of 100%, a DR of 99.9%, a precision of 99.59%, an ROC of 99.40%, and an FAR of 1.5 when tested on the UNSW-NB15 dataset.","['ensemble learning technique', 'gray wolf optimizer (GWO)', 'voting technique', 'feature selection', 'information gain (IG)', 'principal component analysis (PCA)', 'decision tree', 'random forest', 'K-nearest neighbor', 'multilayer perceptron']","The research idea centers on the increasing security risks associated with the widespread adoption of Internet of Things (IoT) applications, highlighting the challenge of effectively identifying and preventing malicious actions within IoT networks. Existing solutions for intrusion detection are limited in their real-world applicability due to issues such as lack of precision, low detection rates, and high false acceptance rates, which hinder their commercial viability. The study is motivated by the need to develop more accurate and reliable methods to address these security concerns in IoT environments. The primary objective of the study is to propose and evaluate a novel approach for detecting intrusion attacks in IoT networks that improves accuracy and detection performance. This approach aims to overcome the limitations of current methods by enhancing the identification of malicious activities, thereby contributing to more secure IoT applications.","The research idea centers on the increasing security risks associated with the widespread adoption of Internet of Things (IoT) applications, highlighting the challenge of effectively identifying and preventing malicious actions within IoT networks. Existing solutions for intrusion detection are limited in their real-world applicability due to issues such as lack of precision, low detection rates, and high false acceptance rates, which hinder their commercial viability. The study is motivated by the need to develop more accurate and reliable methods to address these security concerns in IoT environments. The primary objective of the study is to propose and evaluate a novel approach for detecting intrusion attacks in IoT networks that improves accuracy and detection performance. This approach aims to overcome the limitations of current methods by enhancing the identification of malicious activities, thereby contributing to more secure IoT applications.",FALSE
Social Sciences,UANet: An Uncertainty-Aware Network for Building Extraction From Remote Sensing Images,"Building extraction aims to segment building pixels from remote sensing images and plays an essential role in many applications, such as city planning and urban dynamic monitoring. Over the past few years, deep learning methods with encoder–decoder architectures have achieved remarkable performance due to their powerful feature representation capability. Nevertheless, due to the varying scales and styles of buildings, conventional deep learning models always suffer from uncertain predictions and cannot accurately distinguish the complete footprints of the building from the complex distribution of ground objects, leading to a large degree of omission and commission. In this paper, we realize the importance of uncertain prediction and propose a novel and straightforward Uncertainty-Aware Network (UANet) to alleviate this problem. Specifically, we first apply a general encoder–decoder network to obtain a building extraction map with relatively high uncertainty. Second, in order to aggregate the useful information in the highest-level features, we design a Prior Information Guide Module to guide the highest-level features in learning the prior information from the conventional extraction map. Third, based on the uncertain extraction map, we introduce an Uncertainty Rank Algorithm to measure the uncertainty level of each pixel belonging to the foreground and the background. We further combine this algorithm with the proposed Uncertainty-Aware Fusion Module to facilitate level-by-level feature refinement and obtain the final refined extraction map with low uncertainty. To verify the performance of our proposed UANet, we conduct extensive experiments on three public building datasets, including the WHU building dataset, the Massachusetts building dataset, and the Inria aerial image dataset. Results demonstrate that the proposed UANet outperforms other state-of-the-art algorithms by a large margin. The source code of the proposed UANet is available at https://github.com/Henryjiepanli/Uncertainty-aware-Network.","['deep learning methods with encoder–decoder architectures', 'encoder–decoder network']","The research addresses the challenge of accurately identifying building footprints from remote sensing images, which is crucial for applications such as city planning and monitoring urban dynamics. The problem arises due to the varying scales and styles of buildings, as well as the complex distribution of ground objects, leading to uncertain and incomplete building extraction results. The primary aim of the study is to improve the precision of building segmentation by addressing the uncertainty in predictions and enhancing the completeness of building footprint identification from remote sensing imagery. This objective seeks to reduce errors of omission and commission in building extraction to support more reliable urban analysis and planning.","The research addresses the challenge of accurately identifying building footprints from remote sensing images, which is crucial for applications such as city planning and monitoring urban dynamics. The problem arises due to the varying scales and styles of buildings, as well as the complex distribution of ground objects, leading to uncertain and incomplete building extraction results. The primary aim of the study is to improve the precision of building segmentation by addressing the uncertainty in predictions and enhancing the completeness of building footprint identification from remote sensing imagery. This objective seeks to reduce errors of omission and commission in building extraction to support more reliable urban analysis and planning.",FALSE
Social Sciences,One-Step Multi-View Clustering With Diverse Representation,"Multi-View clustering has attracted broad attention due to its capacity to utilize consistent and complementary information among views. Although tremendous progress has been made recently, most existing methods undergo high complexity, preventing them from being applied to large-scale tasks. Multi-View clustering via matrix factorization is a representative to address this issue. However, most of them map the data matrices into a fixed dimension, limiting the model's expressiveness. Moreover, a range of methods suffers from a two-step process, i.e., multimodal learning and the subsequent <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""> <tex-math notation=""LaTeX"">$k$</tex-math> </inline-formula> -means, inevitably causing a suboptimal clustering result. In light of this, we propose a one-step multi-view clustering with diverse representation (OMVCDR) method, which incorporates multi-view learning and <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""> <tex-math notation=""LaTeX"">$k$</tex-math> </inline-formula> -means into a unified framework. Specifically, we first project original data matrices into various latent spaces to attain comprehensive information and auto-weight them in a self-supervised manner. Then, we directly use the information matrices under diverse dimensions to obtain consensus discrete clustering labels. The unified work of representation learning and clustering boosts the quality of the final results. Furthermore, we develop an efficient optimization algorithm with proven convergence to solve the resultant problem. Comprehensive experiments on various datasets demonstrate the promising clustering performance of our proposed method. The code is publicly available at https://github.com/wanxinhang/OMVCDR.","['Multi-View clustering via matrix factorization', 'k-means', 'multi-view learning', 'representation learning']","The research idea centers on addressing the challenges in multi-view clustering, particularly the limitations posed by high complexity and fixed-dimensional data representation, which hinder the applicability and expressiveness of existing methods in handling large-scale tasks. The study highlights the problem of suboptimal clustering results caused by the common two-step process of multimodal learning followed by clustering. The primary objective of the study is to develop a unified approach that integrates multi-view learning and clustering into a single framework to improve the quality of clustering outcomes. This approach aims to capture comprehensive information from diverse data representations and achieve consensus clustering labels more effectively than previous methods.","The research idea centers on addressing the challenges in multi-view clustering, particularly the limitations posed by high complexity and fixed-dimensional data representation, which hinder the applicability and expressiveness of existing methods in handling large-scale tasks. The study highlights the problem of suboptimal clustering results caused by the common two-step process of multimodal learning followed by clustering. The primary objective of the study is to develop a unified approach that integrates multi-view learning and clustering into a single framework to improve the quality of clustering outcomes. This approach aims to capture comprehensive information from diverse data representations and achieve consensus clustering labels more effectively than previous methods.",FALSE
Social Sciences,Crafting personalized learning paths with AI for lifelong learning: a systematic literature review,"The rapid evolution of knowledge requires constantly acquiring and updating skills, making lifelong learning crucial. Despite decades of artificial intelligence, recent advances promote new solutions to personalize learning in this context. The purpose of this article is to explore the current state of research on the development of artificial intelligence-mediated solutions for the design of personalized learning paths. To achieve this, a systematic literature review (SRL) of 78 articles published between 2019 and 2024 from the Scopus and Web or Science databases was conducted, answering seven questions grouped into three themes: characteristics of the published research, context of the research, and type of solution analyzed. This study identified that: (a) the greatest production of scientific research on the topic is developed in China, India and the United States, (b) the focus is mainly directed towards the educational context at the higher education level with areas of opportunity for application in the work context, and (c) the development of adaptive learning technologies predominates; however, there is a growing interest in the application of generative language models. This article contributes to the growing interest and literature related to personalized learning under artificial intelligence mediated solutions that will serve as a basis for academic institutions and organizations to design programs under this model.",['generative language models'],"The rapid evolution of knowledge necessitates continuous skill acquisition and updating, making lifelong learning increasingly important. This study addresses the growing need to understand how personalized learning paths can be designed to support individuals in this context. The primary aim of the study is to explore the current state of research on the development of solutions for personalized learning paths by reviewing existing literature. It seeks to identify key characteristics, contexts, and types of solutions in the field to inform academic institutions and organizations in designing effective personalized learning programs.","The rapid evolution of knowledge necessitates continuous skill acquisition and updating, making lifelong learning increasingly important. This study addresses the growing need to understand how personalized learning paths can be designed to support individuals in this context. The primary aim of the study is to explore the current state of research on the development of solutions for personalized learning paths by reviewing existing literature. It seeks to identify key characteristics, contexts, and types of solutions in the field to inform academic institutions and organizations in designing effective personalized learning programs.",FALSE
