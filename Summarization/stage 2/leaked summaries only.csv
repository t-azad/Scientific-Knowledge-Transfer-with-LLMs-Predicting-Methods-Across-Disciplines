domain,title,abstract,verified methods list,inital masked extraction,final masked extraction,leakage detected
Biology,"A Comprehensive Survey of Continual Learning: Theory, Method and Application","To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance drop of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efficiency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative strategies address continual learning, and how they are adapted to particular challenges in various applications. Through an in-depth discussion of promising directions, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this field and beyond.",['continual learning'],"The research idea centers on the challenge of continually acquiring, updating, and utilizing knowledge over time without losing previously learned information, a problem known as catastrophic forgetting. This issue is significant because learning new tasks often leads to a decline in performance on earlier tasks, highlighting the complexity of maintaining both stability and adaptability in dynamic environments. The study’s primary objective is to comprehensively summarize and categorize existing approaches to continual learning, focusing on achieving a balance between retaining old knowledge and integrating new information effectively. Additionally, it aims to analyze how different strategies address these challenges and to provide insights that can guide future research in this area.","The research idea centers on the challenge of continually acquiring, updating, and utilizing knowledge over time without losing previously learned information, a problem known as catastrophic forgetting. This issue is significant because learning new tasks often leads to a decline in performance on earlier tasks, highlighting the complexity of maintaining both stability and adaptability in dynamic environments. The study's primary objective is to comprehensively summarize and categorize existing approaches to continual knowledge acquisition, focusing on achieving a balance between retaining old knowledge and integrating new information effectively. Additionally, it aims to analyze how different strategies address these challenges and to provide insights that can guide future research in this area.",True
Biology,Genotype by Environment Interaction and Adaptation in Barley Breeding: Basic Concepts and Methods of Analysis,"Genotype by environment interaction (GE) has important consequences in barley breeding. It often complicates testing and selection of superior genotypes, reducing genetic progress in breeding programs. This drawback may be overcome by a better understanding of the genetic and environmental factors that determine GE and adaptation of genotypes. An important array of statistical techniques is nowadays available to breeders and researchers to cope with the presence of relevant GE in multi-environment trials. This paper begins with a review of recent literature on the latest barley studies on GE and adaptation, including potential biotic and abiotic causes underlying GE. Most studies reported are empirical, describing postdictively genotypic performance across environments. As an alternative, methods allowing a more analytical approach are proposed, in which genotypes and environments are characterized in terms of external variables that affect genotypic performance. These methods are applied to a real barley data set. After data description, a number of selected multiplicative models are developed, namely the additive main effects and multiplicative interaction (AMMI) model, and the factorial regression model. Finally, the implications of GE in barley breeding are discussed. As an appendix, the SAS programs are given for the models described. Key-words: genotype by environment interaction, adaptation, AMMI, factorial regression, breeding programs",['additive main effects and multiplicative interaction (AMMI) model'],"The research idea centers on the significant impact of genotype by environment interaction (GE) in barley breeding, which complicates the testing and selection of superior genotypes and consequently reduces genetic progress in breeding programs. Understanding the genetic and environmental factors that influence GE and the adaptation of genotypes is crucial to overcoming these challenges. The primary objective of the study is to review recent barley research on GE and adaptation, including the biotic and abiotic causes underlying GE, and to propose an analytical approach that characterizes genotypes and environments based on external variables affecting genotypic performance. This approach aims to improve the understanding of GE and its implications for barley breeding programs.","The research idea centers on the significant impact of genotype by environment interaction (GE) in barley breeding, which complicates the testing and selection of superior genotypes and consequently reduces genetic progress in breeding programs. Understanding the genetic and environmental factors that influence GE and the adaptation of genotypes is crucial to overcoming these challenges. The primary objective of the study is to review recent barley research on GE and adaptation, including the biotic and abiotic causes underlying GE, and to propose an approach that characterizes genotypes and environments based on external variables affecting genotypic performance. This approach aims to improve the understanding of GE and its implications for barley breeding programs.",True
Biology,Can large language models replace humans in systematic reviews? Evaluating <scp>GPT</scp>‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages,"Systematic reviews are vital for guiding practice, research and policy, although they are often slow and labour-intensive. Large language models (LLMs) could speed up and automate systematic reviews, but their performance in such tasks has yet to be comprehensively evaluated against humans, and no study has tested Generative Pre-Trained Transformer (GPT)-4, the biggest LLM so far. This pre-registered study uses a ""human-out-of-the-loop"" approach to evaluate GPT-4's capability in title/abstract screening, full-text review and data extraction across various literature types and languages. Although GPT-4 had accuracy on par with human performance in some tasks, results were skewed by chance agreement and dataset imbalance. Adjusting for these caused performance scores to drop across all stages: for data extraction, performance was moderate, and for screening, it ranged from none in highly balanced literature datasets (~1:1) to moderate in those datasets where the ratio of inclusion to exclusion in studies was imbalanced (~1:3). When screening full-text literature using highly reliable prompts, GPT-4's performance was more robust, reaching ""human-like"" levels. Although our findings indicate that, currently, substantial caution should be exercised if LLMs are being used to conduct systematic reviews, they also offer preliminary evidence that, for certain review tasks delivered under specific conditions, LLMs can rival human performance.",['Generative Pre-Trained Transformer (GPT)-4'],"The research idea centers on the importance of systematic reviews for guiding practice, research, and policy, while highlighting the challenges posed by their slow and labor-intensive nature. The study addresses the need to evaluate new approaches that could potentially accelerate and automate the process of conducting systematic reviews. The primary objective of the study is to comprehensively assess the capability of GPT-4 in performing key tasks involved in systematic reviews, including title and abstract screening, full-text review, and data extraction across various types of literature and languages. The study aims to determine how well GPT-4’s performance compares to human reviewers in these tasks and to identify the conditions under which it may achieve human-like accuracy.","The research idea centers on the importance of systematic reviews for guiding practice, research, and policy, while highlighting the challenges posed by their slow and labor-intensive nature. The study addresses the need to evaluate new approaches that could potentially accelerate and streamline the process of conducting systematic reviews. The primary objective of the study is to comprehensively assess alternative methods for performing key tasks involved in systematic reviews, including title and abstract screening, full-text review, and data extraction across various types of literature and languages. The study aims to determine how well these alternative approaches compare to human reviewers in these tasks and to identify the conditions under which they may achieve human-like accuracy.",True
Biology,CIFAKE: Image Classification and Explainable Identification of AI-Generated Synthetic Images,"Recent advances in synthetic data have enabled the generation of images with such high quality that human beings cannot tell the difference between real-life photographs and Artificial Intelligence (AI) generated images. Given the critical necessity of data reliability and authentication, this article proposes to enhance our ability to recognise AI-generated images through computer vision. Initially, a synthetic dataset is generated that mirrors the ten classes of the already available CIFAR-10 dataset with latent diffusion, providing a contrasting set of images for comparison to real photographs. The model is capable of generating complex visual attributes, such as photorealistic reflections in water. The two sets of data present as a binary classification problem with regard to whether the photograph is real or generated by AI. This study then proposes the use of a Convolutional Neural Network (CNN) to classify the images into two categories; Real or Fake. Following hyperparameter tuning and the training of 36 individual network topologies, the optimal approach could correctly classify the images with 92.98% accuracy. Finally, this study implements explainable AI via Gradient Class Activation Mapping to explore which features within the images are useful for classification. Interpretation reveals interesting concepts within the image, in particular, noting that the actual entity itself does not hold useful information for classification; instead, the model focuses on small visual imperfections in the background of the images. The complete dataset engineered for this study, referred to as the CIFAKE dataset, is made publicly available to the research community for future work.","['latent diffusion', 'Convolutional Neural Network (CNN)', 'Gradient Class Activation Mapping']","The research idea addresses the challenge of distinguishing between real-life photographs and synthetic images that are generated with such high quality that they are indistinguishable to the human eye. Given the critical necessity of data reliability and authentication, the study focuses on enhancing the ability to recognize artificially generated images. The primary objective of the study is to develop a method to classify images into real or AI-generated categories by comparing synthetic images that mirror established photographic classes with authentic photographs. Additionally, the study aims to identify specific visual features that differentiate real images from synthetic ones, thereby improving understanding of the distinguishing characteristics between these two types of images.","The research idea addresses the challenge of distinguishing between real-life photographs and synthetic images that are generated with such high quality that they are indistinguishable to the human eye. Given the critical necessity of data reliability and authentication, the study focuses on enhancing the ability to recognize artificially generated images. The primary objective of the study is to develop a method to classify images into real or synthetic categories by comparing artificially created images that mirror established photographic classes with authentic photographs. Additionally, the study aims to identify specific visual features that differentiate real images from synthetic ones, thereby improving understanding of the distinguishing characteristics between these two types of images.",True
Biology,Aspect-based drug review classification through a hybrid model with ant colony optimization using deep learning,"Abstract The task of aspect-level sentiment analysis is intricately designed to determine the sentiment polarity directed towards a specific target within a sentence. With the increasing availability of online reviews and the growing importance of healthcare decisions, analyzing drug reviews has become a critical task. Traditional sentiment analysis, which categorizes a whole review as positive, negative, or neutral, provides limited insights for consumers and healthcare professionals. Aspect-based sentiment analysis (ABSA) aims to overcome these limitations by identifying and evaluating the sentiment associated with specific aspects or attributes of drugs mentioned in the reviews. Various fields, including business, politics, and medicine, have been explored in the context of sentiment analysis. Automation of online user reviews allows pharmaceutical companies to assess large amounts of user feedback. This helps extract pharmacological efficacy and side effect insights. The data collected could improve pharmacovigilance. Reviewing user comments can provide valuable data that can be used to improve drug safety and efficacy monitoring procedures. This improves pharmacovigilance processes, improving pharmaceutical outcomes understanding and corporate decision-making. Therefore, we propose a pre-trained RoBERTa with a Bi-LSTM model to categorise drug reviews from online sources and pre-process the text data. Ant Colony Optimization can be used in feature selection for ABSA, helping to identify the most relevant aspects and sentiments. Further, RoBERTa is fine-tuned to perform ABSA on the dataset, enabling the system to categorize aspects and determine the associated sentiment. The outcomes reveal that the suggested framework has achieved higher accuracy (96.78%) and F1 score (98.29%) on druglib.com, and 95.02% on the drugs.com dataset, than several prior state-of-the-art methods.","['pre-trained RoBERTa', 'Bi-LSTM model', 'fine-tuned RoBERTa']","The research addresses the challenge of understanding specific sentiments expressed toward particular aspects of drugs within online reviews, which is crucial given the increasing reliance on such reviews for healthcare decisions. Traditional methods that classify entire reviews as simply positive, negative, or neutral fail to provide detailed insights needed by consumers and healthcare professionals. This study focuses on improving the extraction of nuanced sentiment information related to distinct drug attributes to enhance pharmacovigilance and drug safety monitoring. The primary objective of the study is to categorize drug reviews from online sources by identifying and evaluating the sentiment associated with specific drug aspects mentioned in the reviews. This aims to provide more detailed and actionable insights into pharmacological efficacy and side effects, thereby supporting improved drug safety and efficacy monitoring procedures.","The research addresses the challenge of understanding specific sentiments expressed toward particular aspects of drugs within online reviews, which is crucial given the increasing reliance on such reviews for healthcare decisions. Methods that classify entire reviews as simply positive, negative, or neutral fail to provide detailed insights needed by consumers and healthcare professionals. This study focuses on improving the extraction of nuanced sentiment information related to distinct drug attributes to enhance pharmacovigilance and drug safety monitoring. The primary objective of the study is to categorize drug reviews from online sources by identifying and evaluating the sentiment associated with specific drug aspects mentioned in the reviews. This aims to provide more detailed and actionable insights into pharmacological efficacy and side effects, thereby supporting improved drug safety and efficacy monitoring procedures.",True
Biology,"A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions","Automating the monitoring of industrial processes has the potential to enhance efficiency and optimize quality by promptly detecting abnormal events and thus facilitating timely interventions. Deep learning, with its capacity to discern non-trivial patterns within large datasets, plays a pivotal role in this process. Standard deep learning methods are suitable to solve a specific task given a specific type of data. During training, deep learning demands large volumes of labeled data. However, due to the dynamic nature of the industrial processes and environment, it is impractical to acquire large-scale labeled data for standard deep learning training for every slightly different case anew. Deep transfer learning offers a solution to this problem. By leveraging knowledge from related tasks and accounting for variations in data distributions, the transfer learning framework solves new tasks with little or even no additional labeled data. The approach bypasses the need to retrain a model from scratch for every new setup and dramatically reduces the labeled data requirement. This survey first provides an in-depth review of deep transfer learning, examining the problem settings of transfer learning and classifying the prevailing deep transfer learning methods. Moreover, we delve into applications of deep transfer learning in the context of a broad spectrum of time series anomaly detection tasks prevalent in primary industrial domains, e.g., manufacturing process monitoring, predictive maintenance, energy management, and infrastructure facility monitoring. We discuss the challenges and limitations of deep transfer learning in industrial contexts and conclude the survey with practical directions and actionable suggestions to address the need to leverage diverse time series data for anomaly detection in an increasingly dynamic production environment.","['deep learning', 'deep transfer learning', 'transfer learning framework']","The research idea centers on the challenge of monitoring industrial processes efficiently by detecting abnormal events promptly to enhance operational quality and enable timely interventions. Due to the dynamic nature of industrial environments, acquiring large-scale labeled data for every variation in the process is impractical, creating a need for approaches that can adapt to changing conditions with limited new data. The primary objective of the study is to review and classify existing methods that address this challenge by leveraging knowledge from related tasks to detect anomalies in time series data across various industrial domains. Additionally, the study aims to discuss the challenges and limitations of these methods in industrial settings and provide practical recommendations for improving anomaly detection in dynamic production environments.","The research idea centers on the challenge of monitoring industrial processes efficiently by detecting abnormal events promptly to enhance operational quality and enable timely interventions. Due to the dynamic nature of industrial environments, acquiring large-scale labeled data for every variation in the process is impractical, creating a need for approaches that can adapt to changing conditions with limited new data. The primary objective of the study is to review and classify existing methods that address this challenge by transferring knowledge from related tasks to detect anomalies in time series data across various industrial domains. Additionally, the study aims to discuss the challenges and limitations of these methods in industrial settings and provide practical recommendations for improving anomaly detection in dynamic production environments.",True
Biology,Enhancing precision agriculture: A comprehensive review of machine learning and AI vision applications in all-terrain vehicle for farm automation,"The automation of all-terrain vehicles (ATVs) through the integration of advanced technologies such as machine learning (ML) and artificial intelligence (AI) vision has significantly changed precision agriculture. This paper aims to analyse and develop trends to provide comprehensive knowledge of the current state of ATV-based precision agriculture and the future possibilities of ML and AI. A bibliometric analysis was conducted through network diagram with keywords taken from previous publications in the domain. This review comprehensively analyses the potential of machine learning and artificial intelligence in transforming farming operations through the automation of tasks and the deployment of all-terrain vehicles. The research extensively analyses how machine learning methods have influenced several aspects of agricultural activities, such as planting, harvesting, spraying, weeding, crop monitoring, and others. AI vision systems are being researched for their ability to enhance precise and prompt decision-making in ATV-driven agricultural automation. These technologies have been thoroughly tested to show how they can improve crop yield, reducing overall investment, and make farming more efficient. Examples include machine learning-based seeding accuracy, AI-enabled crop health monitoring, and the use of AI vision for accurate pesticide application. The assessment examines challenges such as data privacy problems and scalability constraints, along with potential advancements and future prospects in the field. This will assist researchers and practitioners in making well-informed judgments regarding farming practices that are efficient, sustainable, and technologically robust.",['machine learning'],"The study addresses the evolving role of all-terrain vehicles (ATVs) in precision agriculture and the impact of integrating advanced technologies to transform farming operations. It highlights the motivation to understand how these innovations can improve various agricultural activities such as planting, harvesting, spraying, weeding, and crop monitoring, ultimately aiming to enhance crop yield and farming efficiency. The primary objective of the research is to provide a comprehensive overview of the current state and future possibilities of ATV-based precision agriculture, focusing on how these advancements can automate tasks and support precise decision-making in agricultural practices. The study also aims to identify challenges and potential developments to guide researchers and practitioners toward more efficient, sustainable, and effective farming methods.","The study addresses the evolving role of all-terrain vehicles (ATVs) in precision agriculture and the impact of integrating advanced technologies to transform farming operations. It highlights the motivation to understand how these innovations can improve various agricultural activities such as planting, harvesting, spraying, weeding, and crop monitoring, ultimately aiming to enhance crop yield and farming efficiency. The primary objective of the research is to provide a comprehensive overview of the current state and future possibilities of ATV-based precision agriculture, focusing on how these advancements can streamline tasks and support precise decision-making in agricultural practices. The study also aims to identify challenges and potential developments to guide researchers and practitioners toward more efficient, sustainable, and effective farming methods.",True
Biology,An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study,"Background Large language models (LLMs) have shown remarkable capabilities in natural language processing (NLP), especially in domains where labeled data are scarce or expensive, such as the clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. Objective The objective of this study is to assess the effectiveness of various prompt engineering techniques, including 2 newly introduced types—heuristic and ensemble prompts, for zero-shot and few-shot clinical information extraction using pretrained language models. Methods This comprehensive experimental study evaluated different prompt types (simple prefix, simple cloze, chain of thought, anticipatory, heuristic, and ensemble) across 5 clinical NLP tasks: clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, and medication attribute extraction. The performance of these prompts was assessed using 3 state-of-the-art language models: GPT-3.5 (OpenAI), Gemini (Google), and LLaMA-2 (Meta). The study contrasted zero-shot with few-shot prompting and explored the effectiveness of ensemble approaches. Results The study revealed that task-specific prompt tailoring is vital for the high performance of LLMs for zero-shot clinical NLP. In clinical sense disambiguation, GPT-3.5 achieved an accuracy of 0.96 with heuristic prompts and 0.94 in biomedical evidence extraction. Heuristic prompts, alongside chain of thought prompts, were highly effective across tasks. Few-shot prompting improved performance in complex scenarios, and ensemble approaches capitalized on multiple prompt strengths. GPT-3.5 consistently outperformed Gemini and LLaMA-2 across tasks and prompt types. Conclusions This study provides a rigorous evaluation of prompt engineering methodologies and introduces innovative techniques for clinical information extraction, demonstrating the potential of in-context learning in the clinical domain. These findings offer clear guidelines for future prompt-based clinical NLP research, facilitating engagement by non-NLP experts in clinical NLP advancements. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative artificial intelligence, and we hope that it will inspire and inform future research in this area.","['in-context learning', 'few-shot prompting', 'zero-shot prompting']","The research idea addresses the challenge of extracting valuable clinical knowledge from large language models without relying on task-specific training data, focusing on how to effectively guide these models to perform various clinical information extraction tasks. This study is motivated by the need to improve clinical natural language processing in scenarios where labeled clinical data are scarce or expensive, emphasizing the importance of designing appropriate prompts to unlock the potential of these models for clinical applications. The primary objective of the study is to evaluate the effectiveness of different prompt engineering techniques, including newly introduced heuristic and ensemble prompts, for zero-shot and few-shot clinical information extraction across multiple clinical tasks. The study aims to provide a comprehensive assessment of these approaches to enhance clinical information extraction performance and offer practical guidelines for future research in clinical natural language processing.","The research idea addresses the challenge of extracting valuable clinical knowledge from text resources without relying on task-specific training data, focusing on how to effectively guide information processing to perform various clinical information extraction tasks. This study is motivated by the need to improve clinical natural language processing in scenarios where labeled clinical data are scarce or expensive, emphasizing the importance of designing appropriate prompts to unlock the potential of text analysis for clinical applications. The primary objective of the study is to evaluate the effectiveness of different prompt engineering techniques, including newly introduced heuristic and ensemble prompts, for information extraction across multiple clinical tasks with minimal examples. The study aims to provide a comprehensive assessment of these approaches to enhance clinical information extraction performance and offer practical guidelines for future research in clinical natural language processing.",True
Biology,A novel framework for developing environmentally sustainable and cost-effective ultra-high-performance concrete (UHPC) using advanced machine learning and multi-objective optimization techniques,"This study aims to propose a novel framework for strength prediction and multi-objective optimization (MOO) of economical and environmentally sustainable ultra-high-performance concrete (UHPC) which aids in intelligent, sustainable, and resilient construction. Different tree- and boosting ensemble-based machine learning (ML) models are integrated to form an accurate and reliable prediction model for the uniaxial compressive strength of UHPC. The optimized models are integrated into a super learner model, resulting in a robust predictive model that is used as one of the objective functions in the MOO problem. A total of 19 objective functions are considered, including cost, uniaxial compressive strength, and 17 environmental impact categories that comprehensively evaluate the environmental sustainability of the UHPC mix. The resulting impacts from the mid-point indicators were calculated using the Eco-invent v3.7 Life Cycle Inventory database. The results showed that the super learner model accurately predicted the uniaxial compressive strength of UHPC. The MOO resulted in Pareto fronts, demonstrating the trade-off among the uniaxial compressive strength, cost, and environmental sustainability of the mix and a broad range of solutions that can be obtained for the 19 objectives. The study provides a useful tool for designers and decision-makers to select the optimal UHPC mixture that meets specific project requirements. Finally, for the practical application of the ML predictive model and MOO algorithm for UHPC, a graphical user interface-based software tool, FAI-OSUSCONCRET, was developed. This software tool offers fast, accurate, and intelligent predictions and multi-objective optimizations tailored to specific project requirements, thus resulting in a UHPC mixture that perfectly meets project needs.","['tree-based ensemble machine learning models', 'boosting ensemble-based machine learning models', 'super learner model']","The study addresses the challenge of developing ultra-high-performance concrete (UHPC) that is both economically viable and environmentally sustainable, focusing on achieving a balance between strength, cost, and environmental impact. It highlights the need for optimizing UHPC mixtures to support sustainable and resilient construction practices by considering multiple objectives including mechanical performance and environmental sustainability. The primary aim of the study is to accurately predict the uniaxial compressive strength of UHPC and to perform multi-objective optimization that evaluates trade-offs among strength, cost, and various environmental impact categories. This enables the identification of optimal UHPC mixtures that meet specific project requirements while promoting sustainability in construction.","The study addresses the challenge of developing ultra-high-performance concrete (UHPC) that is both economically viable and environmentally sustainable, focusing on achieving a balance between strength, cost, and environmental impact. It highlights the need for optimizing UHPC mixtures to support sustainable and resilient construction practices by considering multiple objectives including mechanical performance and environmental sustainability. The primary aim of the study is to accurately determine the uniaxial compressive strength of UHPC and to perform multi-objective optimization that evaluates trade-offs among strength, cost, and various environmental impact categories. This enables the identification of optimal UHPC mixtures that meet specific project requirements while promoting sustainability in construction.",True
Biology,Utilizing Hybrid Machine Learning and Soft Computing Techniques for Landslide Susceptibility Mapping in a Drainage Basin,"The hydrological system of thebasin of Lake Urmia is complex, deriving its supply from a network comprising 13 perennial rivers, along withnumerous small springs and direct precipitation onto the lake’s surface. Among these contributors, approximately half of the inflow is attributed to the Zarrineh River and the Simineh River. Remarkably, Lake Urmia lacks a natural outlet, with its water loss occurring solely through evaporation processes. This study employed a comprehensive methodology integrating ground surveys, remote sensing analyses, and meticulous documentation of historical landslides within the basin as primary information sources. Through this investigative approach, we preciselyidentified and geolocated a total of 512 historical landslide occurrences across the Urmia Lake drainage basin, leveraging GPS technology for precision. Thisarticle introduces a suite of hybrid machine learning predictive models, such as support-vector machine (SVM), random forest (RF), decision trees (DT), logistic regression (LR), fuzzy logic (FL), and the technique for order of preference by similarity to the ideal solution (TOPSIS). These models were strategically deployed to assess landslide susceptibility within the region. The outcomes of the landslide susceptibility assessment reveal that the main high susceptible zones for landslide occurrence are concentrated in the northwestern, northern, northeastern, and some southern and southeastern areas of the region. Moreover, when considering the implementation of predictions using different algorithms, it became evident that SVM exhibited superior performance regardingboth accuracy (0.89) and precision (0.89), followed by RF, with and accuracy of 0.83 and a precision of 0.83. However, it is noteworthy that TOPSIS yielded the lowest accuracy value among the algorithms assessed.","['support-vector machine (SVM)', 'random forest (RF)', 'decision trees (DT)', 'logistic regression (LR)']","The hydrological system of the Lake Urmia basin is complex, relying on inflows from 13 perennial rivers, numerous small springs, and direct precipitation, with approximately half of the inflow coming from the Zarrineh and Simineh Rivers. Lake Urmia lacks a natural outlet, and its water loss occurs solely through evaporation, making the basin's environmental dynamics particularly sensitive. Understanding the spatial distribution and susceptibility to landslides within this basin is crucial due to the potential impacts on the hydrological system and surrounding ecosystems. The study aims to identify and precisely locate historical landslide occurrences across the Urmia Lake drainage basin to assess landslide susceptibility in the region. By documenting 512 historical landslides and analyzing their spatial patterns, the research seeks to determine the main zones prone to landslide occurrence, thereby contributing to better management and mitigation strategies for the basin’s environmental stability.","The hydrological system of the Lake Urmia basin is complex, relying on inflows from 13 perennial rivers, numerous small springs, and direct precipitation, with approximately half of the inflow coming from the Zarrineh and Simineh Rivers. Lake Urmia lacks a natural outlet, and its water loss occurs solely through evaporation, making the basin's environmental dynamics particularly sensitive. Understanding the spatial distribution and susceptibility to landslides within this basin is crucial due to the potential impacts on the hydrological system and surrounding ecosystems. The study aims to identify and precisely locate historical landslide occurrences across the Urmia Lake drainage basin to assess landslide susceptibility in the region. By documenting 512 historical landslides and analyzing their spatial patterns, the research seeks to determine the main zones prone to landslide occurrence, thereby contributing to better management and mitigation strategies for the basin's environmental stability.",True
Biology,Comparison of Random Forest and XGBoost Classifiers Using Integrated Optical and SAR Features for Mapping Urban Impervious Surface,"The integration of optical and SAR datasets through ensemble machine learning models shows promising results in urban remote sensing applications. The integration of multi-sensor datasets enhances the accuracy of information extraction. This research presents a comparison of two ensemble machine learning classifiers (random forest and extreme gradient boost (XGBoost)) classifiers using an integration of optical and SAR features and simple layer stacking (SLS) techniques. Therefore, Sentinel-1 (SAR) and Landsat 8 (optical) datasets were used with SAR textures and enhanced modified indices to extract features for the year 2023. The classification process utilized two machine learning algorithms, random forest and XGBoost, for urban impervious surface extraction. The study focused on three significant East Asian cities with diverse urban dynamics: Jakarta, Manila, and Seoul. This research proposed a novel index called the Normalized Blue Water Index (NBWI), which distinguishes water from other features and was utilized as an optical feature. Results showed an overall accuracy of 81% for UIS classification using XGBoost and 77% with RF while classifying land use land cover into four major classes (water, vegetation, bare soil, and urban impervious). However, the proposed framework with the XGBoost classifier outperformed the RF algorithm and Dynamic World (DW) data product and comparatively showed higher classification accuracy. Still, all three results show poor separability with bare soil class compared to ground truth data. XGBoost outperformed random forest and Dynamic World in classification accuracy, highlighting its potential use in urban remote sensing applications.","['ensemble machine learning models', 'random forest', 'extreme gradient boost (XGBoost)']","The study addresses the challenge of accurately extracting urban impervious surfaces and distinguishing land cover types in rapidly changing urban environments, particularly in East Asian cities with diverse urban dynamics. Improving the classification of land use and land cover, including water, vegetation, bare soil, and urban impervious surfaces, is essential for effective urban remote sensing applications. The research aims to compare the effectiveness of different approaches for integrating optical and radar satellite data to enhance the accuracy of urban land cover classification. Specifically, the primary objective is to evaluate and compare the performance of classification methods in extracting urban impervious surfaces and other land cover classes in Jakarta, Manila, and Seoul, while introducing a novel index to better distinguish water features from other land cover types.","The study addresses the challenge of accurately extracting urban impervious surfaces and distinguishing land cover types in rapidly changing urban environments, particularly in East Asian cities with diverse urban dynamics. Improving the classification of land use and land cover, including water, vegetation, bare soil, and urban impervious surfaces, is essential for effective urban remote sensing applications. The research aims to compare the effectiveness of different approaches for integrating optical and radar satellite data to enhance the accuracy of urban land cover classification. Specifically, the primary objective is to evaluate and compare the performance of various analytical methods in extracting urban impervious surfaces and other land cover classes in Jakarta, Manila, and Seoul, while introducing a new index to better distinguish water features from other land cover types.",True
Biology,Estimating compressive strength of concrete containing rice husk ash using interpretable machine learning-based models,"The construction sector is a major contributor to global greenhouse gas emissions. Using recycled and waste materials in concrete is a practical solution to address environmental challenges. Currently, agricultural waste is widely used as a substitute for cement in the production of eco-friendly concrete. However, traditional methods for assessing the strength of such materials are both expensive and time-consuming. Therefore, this study uses machine learning techniques to develop prediction models for the compressive strength (CS) of rice husk ash (RHA) concrete. The ML techniques used in the present study include random forest (RF), light gradient boosting machine (LightGBM), ridge regression, and extreme gradient boosting (XGBoost). A total of 348 values of CS were collected from the experimental studies, and five characteristics of RHA concrete were taken as input variables. For the performance assessment of the models, multiple statistical metrics were used. During the training phase, the correlation coefficients (R) obtained for ridge regression, RF, XGBoost, and LightGBM were 0.943, 0.981, 0.985, and 0.996, respectively. In the testing set, these values demonstrated even higher performance, with correlation coefficients of 0.971, 0.993, 0.992, and 0.998 for ridge regression, RF, XGBoost, and LightGBM, respectively. The statistical analysis revealed that the LightGBM model outperformed other models, whereas the ridge regression model exhibited comparatively lower accuracy. SHapley Additive exPlanation (SHAP) method was employed for the interpretability of the developed model. The SHAP analysis revealed that water-to-cement is a controlling parameter in estimating the CS of RHA concrete. In conclusion, this study provides valuable guidance for builders and researchers to estimate the CS of RHA concrete. However, it is suggested that more input variables be incorporated and hybrid models utilized to further enhance the reliability and precision of the models.","['random forest (RF)', 'light gradient boosting machine (LightGBM)', 'ridge regression', 'extreme gradient boosting (XGBoost)', 'SHapley Additive exPlanation (SHAP)']","The construction sector significantly contributes to global greenhouse gas emissions, prompting the need for sustainable alternatives in building materials. Using recycled and waste materials, such as agricultural waste, as substitutes for cement in concrete production offers an eco-friendly solution to reduce environmental impact. However, traditional methods for assessing the strength of such materials are costly and time-consuming, creating a barrier to their widespread adoption. This study aims to develop reliable approaches to predict the compressive strength of rice husk ash concrete, an eco-friendly material derived from agricultural waste. The primary objective of the study is to establish accurate prediction models for the compressive strength of rice husk ash concrete based on key material characteristics, thereby providing valuable guidance for builders and researchers in estimating its performance.","The construction sector significantly contributes to global greenhouse gas emissions, prompting the need for sustainable alternatives in building materials. Using recycled and waste materials, such as agricultural waste, as substitutes for cement in concrete production offers an eco-friendly solution to reduce environmental impact. However, traditional methods for assessing the strength of such materials are costly and time-consuming, creating a barrier to their widespread adoption. This study aims to develop reliable approaches to estimate the compressive strength of rice husk ash concrete, an eco-friendly material derived from agricultural waste. The primary objective of the study is to establish accurate assessment methods for the compressive strength of rice husk ash concrete based on key material characteristics, thereby providing valuable guidance for builders and researchers in evaluating its performance.",True
Biology,Improving Thyroid Disorder Diagnosis via Ensemble Stacking and Bidirectional Feature Selection,"Thyroid disorders represent a significant global health challenge with hypothyroidism and hyperthyroidism as two common conditions arising from dysfunction in the thyroid gland.Accurate and timely diagnosis of these disorders is crucial for effective treatment and patient care.This research introduces a comprehensive approach to improve the accuracy of thyroid disorder diagnosis through the integration of ensemble stacking and advanced feature selection techniques.Sequential forward feature selection, sequential backward feature elimination, and bidirectional feature elimination are investigated in this study.In ensemble learning, random forest, adaptive boosting, and bagging classifiers are employed.The effectiveness of these techniques is evaluated using two different datasets obtained from the University of California Irvine-Machine Learning Repository, both of which undergo preprocessing steps, including outlier removal, addressing missing data, data cleansing, and feature reduction.Extensive experimentation demonstrates the remarkable success of proposed ensemble stacking and bidirectional feature elimination achieving 100% and 99.86% accuracy in identifying hyperthyroidism and hypothyroidism, respectively.Beyond enhancing detection accuracy, the ensemble stacking model also demonstrated a streamlined computational complexity which is pivotal for practical medical applications.It significantly outperformed existing studies with similar objectives underscoring the viability and effectiveness of the proposed scheme.This research offers an innovative perspective and sets the platform for improved thyroid disorder diagnosis with broader implications for healthcare and patient well-being.","['ensemble stacking', 'sequential forward feature selection', 'sequential backward feature elimination', 'random forest', 'adaptive boosting', 'bagging classifiers']","Thyroid disorders, including hypothyroidism and hyperthyroidism, represent a significant global health challenge due to dysfunction in the thyroid gland. Accurate and timely diagnosis of these conditions is crucial for effective treatment and patient care. The primary aim of this study is to improve the accuracy of thyroid disorder diagnosis by exploring and integrating various feature selection techniques and classification approaches. This research seeks to enhance the identification of hyperthyroidism and hypothyroidism, ultimately contributing to better healthcare outcomes and patient well-being.","Thyroid disorders, including hypothyroidism and hyperthyroidism, represent a significant global health challenge due to dysfunction in the thyroid gland. Accurate and timely diagnosis of these conditions is crucial for effective treatment and patient care. The primary aim of this study is to improve the accuracy of thyroid disorder diagnosis by exploring and integrating various analytical methods and diagnostic approaches. This research seeks to enhance the identification of hyperthyroidism and hypothyroidism, ultimately contributing to better healthcare outcomes and patient well-being.",True
Biology,Assessing water quality of an ecologically critical urban canal incorporating machine learning approaches,"This study assessed water quality (WQ) in Tongi Canal, an ecologically critical and economically important urban canal in Bangladesh. The researchers employed the Root Mean Square Water Quality Index (RMS-WQI) model, utilizing seven WQ indicators, including temperature, dissolve oxygen, electrical conductivity, lead, cadmium, and iron to calculate the water quality index (WQI) score. The results showed that most of the water sampling locations showed poor WQ, with many indicators violating Bangladesh's environmental conservation regulations. This study employed eight machine learning algorithms, where the Gaussian process regression (GPR) model demonstrated superior performance (training RMSE = 1.77, testing RMSE = 0.0006) in predicting WQI scores. To validate the GPR model's performance, several performance measures, including the coefficient of determination (R2), the Nash-Sutcliffe efficiency (NSE), the model efficiency factor (MEF), Z statistics, and Taylor diagram analysis, were employed. The GPR model exhibited higher sensitivity (R2 = 1.0) and efficiency (NSE = 1.0, MEF = 0.0) in predicting WQ. The analysis of model uncertainty (standard uncertainty = 7.08 ± 0.9025; expanded uncertainty = 7.08 ± 1.846) indicates that the RMS-WQI model holds potential for assessing the WQ of inland waterbodies. These findings indicate that the RMS-WQI model could be an effective approach for assessing inland waters across Bangladesh. The study's results showed that most of the WQ indicators did not meet the recommended guidelines, indicating that the water in the Tongi Canal is unsafe and unsuitable for various purposes. The study's implications extend beyond the Tongi Canal and could contribute to WQ management initiatives across Bangladesh.",['Gaussian process regression (GPR)'],"The study addresses the critical issue of water quality in Tongi Canal, an ecologically important and economically significant urban waterbody in Bangladesh. It highlights concerns about the poor condition of the canal’s water, with many indicators exceeding the limits set by environmental conservation regulations, posing risks to its safety and suitability for various uses. The primary aim of the study is to assess the water quality of Tongi Canal by calculating a comprehensive water quality index based on multiple indicators such as temperature, dissolved oxygen, electrical conductivity, and heavy metals like lead, cadmium, and iron. The research seeks to evaluate the current status of the canal’s water quality and provide insights that could support water quality management efforts not only for Tongi Canal but also for inland waterbodies across Bangladesh.","The study addresses the critical issue of water quality in Tongi Canal, an ecologically important and economically significant urban waterbody in Bangladesh. It highlights concerns about the poor condition of the canal's water, with many indicators exceeding the limits set by environmental conservation regulations, posing risks to its safety and suitability for various uses. The primary aim of the study is to assess the water quality of Tongi Canal by calculating a comprehensive water quality index based on multiple indicators such as temperature, dissolved oxygen, electrical conductivity, and heavy metals like lead, cadmium, and iron. The research seeks to evaluate the current status of the canal's water quality and provide insights that could support water quality management efforts not only for Tongi Canal but also for inland waterbodies across Bangladesh.",True
Biology,Compressive strength prediction of sustainable concrete incorporating rice husk ash (RHA) using hybrid machine learning algorithms and parametric analyses,"The construction industry is making efforts to reduce the environmental impact of cement production in concrete by incorporating alternative and supplementary cementitious materials, as well as lowering carbon emissions. One such material that has gained popularity in this context is rice husk ash (RHA) due to its pozzolanic reactions. This study aims to forecast the compressive strength (CS) of RHA-based concrete (RBC) by examining the effects of several factors such as cement, RHA content, curing age, water usage, aggregate amount, and superplasticizer content. To accomplish this, the study collected and analyzed data from literature, resulting in a dataset of 1404 observations. Several machine learning (ML) models, such as light gradient boosting (LGB), extreme gradient boosting (XGB), and random forest (RF), as well as hybrid machine learning (HML) approaches like XGB-LGB and XGB-RF were employed to thoroughly analyze these parameters and assess their impact on strength. The dataset was split into training and testing groups, and statistical analyses were performed to determine the relationships between the input parameters and CS. Moreover, the performance of all the models was evaluated using various statistical evaluation criteria, including mean absolute percentage error (MAPE), coefficient of efficiency (CE), root mean square error (RMSE), and coefficient of determination (R2). The hybrid XGB-LGB model was found to have higher precision (R2 = 0.95, and RMSE = 5.255 MPa) as compared to other models. SHAP (SHapley Additive exPlanations) analysis revealed that cement, RHA, and superplasticizer had a positive effect on strength. Overall, the study's findings suggest that the hybrid XGB-LGB model with the identified input parameters can be used to accurately predict the CS of RBC. The application of such technologies in the construction sector can facilitate the rapid and low-cost identification of material qualities and the impact of input parameters.","['light gradient boosting (LGB)', 'extreme gradient boosting (XGB)', 'random forest (RF)']","The research idea centers on addressing the environmental impact of cement production in concrete by incorporating alternative materials such as rice husk ash (RHA), which is valued for its pozzolanic properties. The study is motivated by the need to understand how various factors, including cement, RHA content, curing age, water usage, aggregate amount, and superplasticizer content, influence the compressive strength of RHA-based concrete (RBC). The primary objective of the study is to examine the effects of these factors on the compressive strength of RBC and to accurately forecast its strength by analyzing a comprehensive dataset compiled from existing literature. This aims to provide insights that can support the development of more sustainable concrete materials with reduced environmental impact.","The research idea centers on addressing the environmental impact of cement production in concrete by incorporating alternative materials such as rice husk ash (RHA), which is valued for its pozzolanic properties. The study is motivated by the need to understand how various factors, including cement, RHA content, curing age, water usage, aggregate amount, and superplasticizer content, influence the compressive strength of RHA-based concrete (RBC). The primary objective of the study is to examine the effects of these factors on the compressive strength of RBC and to determine reliable strength estimation methods by analyzing a comprehensive dataset compiled from existing literature. This aims to provide insights that can support the development of more sustainable concrete materials with reduced environmental impact.",True
Biology,Data-driven evolution of water quality models: An in-depth investigation of innovative outlier detection approaches-A case study of Irish Water Quality Index (IEWQI) model,"Recently, there has been a significant advancement in the water quality index (WQI) models utilizing data-driven approaches, especially those integrating machine learning and artificial intelligence (ML/AI) technology. Although, several recent studies have revealed that the data-driven model has produced inconsistent results due to the data outliers, which significantly impact model reliability and accuracy. The present study was carried out to assess the impact of data outliers on a recently developed Irish Water Quality Index (IEWQI) model, which relies on data-driven techniques. To the author's best knowledge, there has been no systematic framework for evaluating the influence of data outliers on such models. For the purposes of assessing the outlier impact of the data outliers on the water quality (WQ) model, this was the first initiative in research to introduce a comprehensive approach that combines machine learning with advanced statistical techniques. The proposed framework was implemented in Cork Harbour, Ireland, to evaluate the IEWQI model's sensitivity to outliers in input indicators to assess the water quality. In order to detect the data outlier, the study utilized two widely used ML techniques, including Isolation Forest (IF) and Kernel Density Estimation (KDE) within the dataset, for predicting WQ with and without these outliers. For validating the ML results, the study used five commonly used statistical measures. The performance metric (R2) indicates that the model performance improved slightly (R2 increased from 0.92 to 0.95) in predicting WQ after removing the data outlier from the input. But the IEWQI scores revealed that there were no statistically significant differences among the actual values, predictions with outliers, and predictions without outliers, with a 95% confidence interval at p < 0.05. The results of model uncertainty also revealed that the model contributed <1% uncertainty to the final assessment results for using both datasets (with and without outliers). In addition, all statistical measures indicated that the ML techniques provided reliable results that can be utilized for detecting outliers and their impacts on the IEWQI model. The findings of the research reveal that although the data outliers had no significant impact on the IEWQI model architecture, they had moderate impacts on the rating schemes' of the model. This finding indicated that detecting the data outliers could improve the accuracy of the IEWQI model in rating WQ as well as be helpful in mitigating the model eclipsing problem. In addition, the results of the research provide evidence of how the data outliers influenced the data-driven model in predicting WQ and reliability, particularly since the study confirmed that the IEWQI model's could be effective for accurately rating WQ despite the presence of the data outliers in the input. It could occur due to the spatio-temporal variability inherent in WQ indicators. However, the research assesses the influence of data input outliers on the IEWQI model and underscores important areas for future investigation. These areas include expanding temporal analysis using multi-year data, examining spatial outlier patterns, and evaluating detection methods. Moreover, it is essential to explore the real-world impacts of revised rating categories, involve stakeholders in outlier management, and fine-tune model parameters. Analysing model performance across varying temporal and spatial resolutions and incorporating additional environmental data can significantly enhance the accuracy of WQ assessment. Consequently, this study offers valuable insights to strengthen the IEWQI model's robustness and provides avenues for enhancing its utility in broader WQ assessment applications. Moreover, the study successfully adopted the framework for evaluating how data input outliers affect the data-driven model, such as the IEWQI model. The current study has been carried out in Cork Harbour for only a single year of WQ data. The framework should be tested across various domains for evaluating the response of the IEWQI model's in terms of the spatio-temporal resolution of the domain. Nevertheless, the study recommended that future research should be conducted to adjust or revise the IEWQI model's rating schemes and investigate the practical effects of data outliers on updated rating categories. However, the study provides potential recommendations for enhancing the IEWQI model's adaptability and reveals its effectiveness in expanding its applicability in more general WQ assessment scenarios.",['Isolation Forest (IF)'],"The study addresses the challenge of assessing water quality accurately in the presence of data outliers, which can affect the reliability and accuracy of water quality index models. Specifically, it focuses on understanding how these outliers influence the recently developed Irish Water Quality Index (IEWQI) model, given that no systematic framework has previously been established to evaluate their impact. The research aims to assess the effect of data outliers on the IEWQI model’s performance in rating water quality, particularly by evaluating the model’s sensitivity to outliers in input indicators. The primary objective is to determine whether the presence of data outliers significantly affects the accuracy and reliability of the IEWQI model in predicting water quality and to provide insights for improving the model’s rating schemes and overall robustness in water quality assessment.","The study addresses the challenge of assessing water quality accurately in the presence of data outliers, which can affect the reliability and accuracy of water quality index models. Specifically, it focuses on understanding how these outliers influence the recently developed Irish Water Quality Index (IEWQI) model, given that no systematic framework has previously been established to evaluate their impact. The research aims to assess the effect of data outliers on the IEWQI model's performance in rating water quality, particularly by evaluating the model's sensitivity to outliers in input indicators. The primary objective is to determine whether the presence of data outliers significantly affects the accuracy and reliability of the IEWQI model in predicting water quality and to provide insights for improving the model's rating schemes and overall robustness in water quality assessment.",True
Biology,Vision–language foundation model for echocardiogram interpretation,"Abstract The development of robust artificial intelligence models for echocardiography has been limited by the availability of annotated clinical data. Here, to address this challenge and improve the performance of cardiac imaging models, we developed EchoCLIP, a vision–language foundation model for echocardiography, that learns the relationship between cardiac ultrasound images and the interpretations of expert cardiologists across a wide range of patients and indications for imaging. After training on 1,032,975 cardiac ultrasound videos and corresponding expert text, EchoCLIP performs well on a diverse range of benchmarks for cardiac image interpretation, despite not having been explicitly trained for individual interpretation tasks. EchoCLIP can assess cardiac function (mean absolute error of 7.1% when predicting left ventricular ejection fraction in an external validation dataset) and identify implanted intracardiac devices (area under the curve (AUC) of 0.84, 0.92 and 0.97 for pacemakers, percutaneous mitral valve repair and artificial aortic valves, respectively). We also developed a long-context variant (EchoCLIP-R) using a custom tokenizer based on common echocardiography concepts. EchoCLIP-R accurately identified unique patients across multiple videos (AUC of 0.86), identified clinical transitions such as heart transplants (AUC of 0.79) and cardiac surgery (AUC 0.77) and enabled robust image-to-text search (mean cross-modal retrieval rank in the top 1% of candidate text reports). These capabilities represent a substantial step toward understanding and applying foundation models in cardiovascular imaging for preliminary interpretation of echocardiographic findings.",['vision–language foundation model'],"The research addresses the challenge of limited availability of annotated clinical data for echocardiography, which has hindered the development of robust methods for cardiac imaging interpretation. There is a need to improve the understanding and assessment of cardiac ultrasound images in relation to expert cardiologists' interpretations across diverse patient populations and imaging indications. The primary objective of the study is to develop a comprehensive approach that learns the relationship between cardiac ultrasound images and expert clinical interpretations to enhance the accuracy and scope of echocardiographic assessment. This includes the ability to evaluate cardiac function, identify implanted intracardiac devices, and recognize significant clinical transitions such as heart transplants and cardiac surgeries.","The research addresses the challenge of limited availability of annotated clinical data for echocardiography, which has hindered the development of robust methods for cardiac imaging interpretation. There is a need to improve the understanding and assessment of cardiac ultrasound images in relation to expert cardiologists' interpretations across diverse patient populations and imaging indications. The primary objective of the study is to develop a comprehensive approach that establishes relationships between cardiac ultrasound images and expert clinical interpretations to enhance the accuracy and scope of echocardiographic assessment. This includes the ability to evaluate cardiac function, identify implanted intracardiac devices, and recognize significant clinical transitions such as heart transplants and cardiac surgeries.",True
Biology,Performance assessment of machine learning algorithms for mapping of land use/land cover using remote sensing data,"The rapid increase in population accelerates the rate of change of Land use/Land cover (LULC) in various parts of the world. This phenomenon caused a huge strain for natural resources. Hence, continues monitoring of LULC changes gained a significant importance for management of natural resources and assessing the climate change impacts. Recently, application of machine learning algorithms on RS (remote sensing) data for rapid and accurate mapping of LULC gained significant importance due to growing need of LULC estimation for ecosystem services, natural resource management and environmental management. Hence, it is crucial to access and compare the performance of different machine learning classifiers for accurate mapping of LULC. The primary objective of this study was to compare the performance of CART (Classification and Regression Tree), RF (Random Forest) and SVM (Support Vector Machine) for LULC estimation by processing RS data on Google Earth Engine (GEE). In total four classes of LULC (Water Bodies, Vegetation Cover, Urban Land and Barren Land) for city of Lahore were extracted using satellite images from Landsat-7, Landsat-8 and Landsat-9 for years 2008, 2015 and 2022, respectively. According to results, RF is the best performing classifier with maximum overall accuracy of 95.2% and highest Kappa coefficient value of 0.87, SVM achieved maximum accuracy of 89.8% with highest Kappa of 0.84 and CART showed maximum overall accuracy of 89.7% with Kappa value of 0.79. Results from this study can give assistance for decision makers, planners and RS experts to choose a suitable machine learning algorithm for LULC classification in an unplanned urbanized city like Lahore.","['Classification and Regression Tree (CART)', 'Random Forest (RF)', 'Support Vector Machine (SVM)']","The rapid increase in population accelerates the rate of change of Land use/Land cover (LULC) in various parts of the world, causing significant strain on natural resources. Continuous monitoring of LULC changes has become critically important for the management of natural resources and for assessing the impacts of climate change. The primary objective of this study was to compare the performance of different approaches for accurate mapping of LULC by processing satellite imagery data. Specifically, the study aimed to evaluate and compare the effectiveness of various classification methods in estimating four classes of LULC—Water Bodies, Vegetation Cover, Urban Land, and Barren Land—in the city of Lahore using satellite images from different years.","The rapid increase in population accelerates the rate of change of Land use/Land cover (LULC) in various parts of the world, causing significant strain on natural resources. Continuous monitoring of LULC changes has become critically important for the management of natural resources and for assessing the impacts of climate change. The primary objective of this study was to compare the performance of different approaches for accurate mapping of LULC by processing satellite imagery data. Specifically, the study aimed to evaluate and compare the effectiveness of various analytical methods in estimating four classes of LULC—Water Bodies, Vegetation Cover, Urban Land, and Barren Land—in the city of Lahore using satellite images from different years.",True
Biology,CCL-DTI: contributing the contrastive loss in drug–target interaction prediction,"Abstract Background The Drug–Target Interaction (DTI) prediction uses a drug molecule and a protein sequence as inputs to predict the binding affinity value. In recent years, deep learning-based models have gotten more attention. These methods have two modules: the feature extraction module and the task prediction module. In most deep learning-based approaches, a simple task prediction loss (i.e., categorical cross entropy for the classification task and mean squared error for the regression task) is used to learn the model. In machine learning, contrastive-based loss functions are developed to learn more discriminative feature space. In a deep learning-based model, extracting more discriminative feature space leads to performance improvement for the task prediction module. Results In this paper, we have used multimodal knowledge as input and proposed an attention-based fusion technique to combine this knowledge. Also, we investigate how utilizing contrastive loss function along the task prediction loss could help the approach to learn a more powerful model. Four contrastive loss functions are considered: (1) max-margin contrastive loss function, (2) triplet loss function, (3) Multi-class N-pair Loss Objective, and (4) NT-Xent loss function. The proposed model is evaluated using four well-known datasets: Wang et al. dataset, Luo's dataset, Davis, and KIBA datasets. Conclusions Accordingly, after reviewing the state-of-the-art methods, we developed a multimodal feature extraction network by combining protein sequences and drug molecules, along with protein–protein interaction networks and drug–drug interaction networks. The results show it performs significantly better than the comparable state-of-the-art approaches.","['contrastive loss function', 'max-margin contrastive loss function', 'triplet loss function', 'Multi-class N-pair Loss Objective', 'NT-Xent loss function']","The research idea centers on improving the prediction of drug–target interactions by enhancing the representation of drug molecules and protein sequences to better understand their binding affinity. Current approaches often rely on simple loss functions that may limit the ability to learn discriminative features necessary for accurate prediction. By addressing the challenge of extracting more informative and distinct biological features, the study aims to advance the understanding of how drugs interact with their protein targets. The primary objective of the study is to develop a method that integrates multiple biological data sources, including protein sequences, drug molecules, protein–protein interaction networks, and drug–drug interaction networks, to improve the prediction of drug–target binding affinity. The study specifically aims to investigate how combining these diverse biological inputs can enhance the learning of more powerful representations that lead to better predictive performance compared to existing approaches.","The research idea centers on improving the prediction of drug–target interactions by enhancing the representation of drug molecules and protein sequences to better understand their binding affinity. Current approaches often rely on simple evaluation metrics that may limit the ability to capture discriminative features necessary for accurate prediction. By addressing the challenge of extracting more informative and distinct biological features, the study aims to advance the understanding of how drugs interact with their protein targets. The primary objective of the study is to develop a method that integrates multiple biological data sources, including protein sequences, drug molecules, protein–protein interaction networks, and drug–drug interaction networks, to improve the prediction of drug–target binding affinity. The study specifically aims to investigate how combining these diverse biological inputs can enhance the derivation of more powerful representations that lead to better predictive performance compared to existing approaches.",True
Biology,Pedestrian 3D Shape Understanding for Person Re-Identification via Multi-View Learning,"Recent development in computing power has resulted in performance improvements on holistic(none-occluded) person Re-Identification (ReID) tasks. Nevertheless, the precision of the recent research will diminish when a pedestrian is obstructed by obstacles. Within the realm of 2D space, the loss of information from obstructed objects continues to pose significant challenges in the context of person ReID. Person is a 3D non-grid object, and thus semantic representation learning in only 2D space limits the understanding of occluded person. In the present work, we propose a network based on 3D multi-view learning, allowing it to acquire geometric and shape details of an occluded pedestrian from 3D space. Simultaneously, it capitalizes on advancements in 2D-based networks to extract semantic representations from 3D multi-views. Specifically, the surface random selection strategy is proposed to convert images of 2D RGB into 3D multi-views. Using this strategy, we build four extensive 3D multi-view data collections for person ReID. After that, Pedestrian 3D Shape Understanding for Person Re-Identification via Multi-View Learning(MV-3DSReID), is proposed for identifying the person by learning person geometry and structure representation from the groups of multi-view images. In comparison to alternative data formats (e.g., 2D RGB, 3D point cloud), multi-view images complement each other's detailed features of the 3D object by adjusting rendering viewpoints, thus facilitating a more comprehensive understanding of the person for both holistic and occluded ReID situations. Experiments on occluded and holistic ReID tasks demonstrate performance levels comparable to state-of-the-art methods, validating the effectiveness of our proposed approach in tackling challenges related to occlusion. The code is available at https://github.com/hangjiaqi1/MV-TransReID.",['3D multi-view learning'],"The research idea addresses the challenge of accurately identifying pedestrians when they are partially obstructed by obstacles, which leads to loss of information in traditional 2D representations. Since a person is a three-dimensional object, relying solely on 2D semantic representations limits the understanding of occluded individuals. This study focuses on overcoming the difficulties posed by occlusion in person identification by enhancing the representation of pedestrian geometry and structure.

The primary objective of the study is to improve person re-identification by learning geometric and shape details of occluded pedestrians through multi-view 3D representations. The research aims to capture comprehensive features of individuals by integrating information from multiple viewpoints, thereby facilitating better identification in both occluded and unobstructed scenarios.","The research idea addresses the challenge of accurately identifying pedestrians when they are partially obstructed by obstacles, which leads to loss of information in traditional 2D representations. Since a person is a three-dimensional object, relying solely on 2D semantic representations limits the understanding of occluded individuals. This study focuses on overcoming the difficulties posed by occlusion in person identification by enhancing the representation of pedestrian geometry and structure.

The primary objective of the study is to improve person re-identification by developing methods to capture geometric and shape details of occluded pedestrians through multi-view 3D representations. The research aims to capture comprehensive features of individuals by integrating information from multiple viewpoints, thereby facilitating better identification in both occluded and unobstructed scenarios.",True
Biology,"admetSAR3.0: a comprehensive platform for exploration, prediction and optimization of chemical ADMET properties","Abstract Absorption, distribution, metabolism, excretion and toxicity (ADMET) properties play a crucial role in drug discovery and chemical safety assessment. Built on the achievements of admetSAR and its successor, admetSAR2.0, this paper introduced the new version of the series, admetSAR3.0, as a comprehensive platform for chemical ADMET assessment, including search, prediction and optimization modules. In the search module, admetSAR3.0 hosted over 370 000 high-quality experimental ADMET data for 104 652 unique compounds, and supplemented chemical structure similarity search function to facilitate read-across. In the prediction module, we introduced comprehensive ADMET endpoints and two new sections for environmental and cosmetic risk assessments, empowering admetSAR3.0 to provide prediction for 119 endpoints, more than double numbers compared to the previous version. Furthermore, the advanced multi-task graph neural network framework offered robust and reliable support for ADMET prediction. In particular, a module named ADMETopt was added to automatically optimize the ADMET properties of query molecules through transformation rules or scaffold hopping. Finally, admetSAR3.0 provides user-friendly interfaces for multiple types of input data, such as SMILES string, chemical structure and batch molecule file, and supports various output types, including digital, chart displays and file downloads. In summary, admetSAR3.0 is anticipated to be a valuable and powerful tool in drug discovery and chemical safety assessment at http://lmmd.ecust.edu.cn/admetsar3/.",['multi-task graph neural network framework'],"The research idea centers on the critical importance of absorption, distribution, metabolism, excretion, and toxicity (ADMET) properties in drug discovery and chemical safety assessment. Understanding and accurately assessing these properties is essential for evaluating the safety and efficacy of chemical compounds. The study aims to enhance the capability to assess chemical ADMET properties comprehensively, addressing the need for reliable and extensive experimental data to support these evaluations. The primary objective of the study is to present an updated and comprehensive platform that integrates a vast collection of high-quality experimental ADMET data for numerous unique compounds and expands the scope of ADMET endpoints, including environmental and cosmetic risk assessments. This platform is designed to facilitate the search, prediction, and optimization of ADMET properties, thereby supporting more effective drug discovery and chemical safety evaluations.","The research idea centers on the critical importance of absorption, distribution, metabolism, excretion, and toxicity (ADMET) properties in drug discovery and chemical safety assessment. Understanding and accurately assessing these properties is essential for evaluating the safety and efficacy of chemical compounds. The study aims to enhance the capability to assess chemical ADMET properties comprehensively, addressing the need for reliable and extensive experimental data to support these evaluations. The primary objective of the study is to present an updated and comprehensive platform that integrates a vast collection of high-quality experimental ADMET data for numerous unique compounds and expands the scope of ADMET endpoints, including environmental and cosmetic risk assessments. This platform is designed to facilitate the search, analysis, and optimization of ADMET properties, thereby supporting more effective drug discovery and chemical safety evaluations.",True
Biology,GAN based augmentation using a hybrid loss function for dermoscopy images,"Dermatology is the most appropriate field to utilize pattern recognition-based automated techniques for objective, accurate, and rapid diagnosis because diagnosis mainly relies on visual examinations of skin lesions. Recent approaches utilizing deep learning techniques have shown remarkable results in this field. However, they necessitate a substantial quantity of images and the availability of dermoscopy images is often limited. Also, even if enough images are available, their labeling requires expert knowledge and is time-consuming. To overcome these issues, an efficient augmentation approach is needed to expand training datasets from input images. Therefore, in this work, a generative adversarial network has been developed using a new hybrid loss function constructed with traditional loss functions to enhance the generation power of the architecture. Also, the effect of the proposed approach and different generative network-based augmentations, which have been used with dermoscopy images in the literature, on the classification of skin lesions has been investigated. Therefore, the main contributions of this work are: (i) introducing a new generative model for the augmentation of dermoscopy images; (ii) presenting the effect of the proposed model on the classification of the images; (iii) comparative evaluations of the effectiveness of different generative network-based augmentations in the classification of seven forms of skin lesions. The classification accuracy when the proposed augmentation is used is 93.12%, which is higher than its counterparts. Experimental results indicate the significance of augmentation techniques in the classification of skin lesions and the efficiency of the proposed structure in improving the classification accuracy.",['generative adversarial network'],"The research idea centers on the challenge of diagnosing skin lesions in dermatology, which primarily depends on visual examination but is limited by the availability and labeling of dermoscopy images. Since obtaining a large number of expert-labeled images is difficult and time-consuming, there is a need for methods to effectively expand the dataset of skin lesion images to improve diagnostic accuracy. The study’s primary objective is to develop and evaluate a new approach for augmenting dermoscopy images to enhance the classification of different types of skin lesions. Specifically, the research aims to introduce a novel generative model for image augmentation, assess its impact on the classification accuracy of skin lesions, and compare its effectiveness with other augmentation methods across seven forms of skin lesions.","The research idea centers on the challenge of diagnosing skin lesions in dermatology, which primarily depends on visual examination but is limited by the availability and labeling of dermoscopy images. Since obtaining a large number of expert-labeled images is difficult and time-consuming, there is a need for methods to effectively expand the dataset of skin lesion images to improve diagnostic accuracy. The study's primary objective is to develop and evaluate a new approach for augmenting dermoscopy images to enhance the identification of different types of skin lesions. Specifically, the research aims to introduce a novel image synthesis technique for expanding datasets, assess its impact on the diagnostic accuracy of skin lesions, and compare its effectiveness with other image expansion methods across seven forms of skin lesions.",True
Biology,Traffic Sign Detection and Recognition Using YOLO Object Detection Algorithm: A Systematic Review,"Context: YOLO (You Look Only Once) is an algorithm based on deep neural networks with real-time object detection capabilities. This state-of-the-art technology is widely available, mainly due to its speed and precision. Since its conception, YOLO has been applied to detect and recognize traffic signs, pedestrians, traffic lights, vehicles, and so on. Objective: The goal of this research is to systematically analyze the YOLO object detection algorithm, applied to traffic sign detection and recognition systems, from five relevant aspects of this technology: applications, datasets, metrics, hardware, and challenges. Method: This study performs a systematic literature review (SLR) of studies on traffic sign detection and recognition using YOLO published in the years 2016–2022. Results: The search found 115 primary studies relevant to the goal of this research. After analyzing these investigations, the following relevant results were obtained. The most common applications of YOLO in this field are vehicular security and intelligent and autonomous vehicles. The majority of the sign datasets used to train, test, and validate YOLO-based systems are publicly available, with an emphasis on datasets from Germany and China. It has also been discovered that most works present sophisticated detection, classification, and processing speed metrics for traffic sign detection and recognition systems by using the different versions of YOLO. In addition, the most popular desktop data processing hardwares are Nvidia RTX 2080 and Titan Tesla V100 and, in the case of embedded or mobile GPU platforms, Jetson Xavier NX. Finally, seven relevant challenges that these systems face when operating in real road conditions have been identified. With this in mind, research has been reclassified to address these challenges in each case. Conclusions: This SLR is the most relevant and current work in the field of technology development applied to the detection and recognition of traffic signs using YOLO. In addition, insights are provided about future work that could be conducted to improve the field.",['YOLO (You Look Only Once)'],"The research idea addresses the need for effective detection and recognition of traffic signs, which is crucial for vehicular security and the development of intelligent and autonomous vehicles. Accurate identification of traffic signs in real road conditions presents several challenges that impact the safety and efficiency of transportation systems. The study aims to systematically analyze existing approaches to traffic sign detection and recognition to better understand their applications, datasets, performance metrics, hardware requirements, and the challenges they face. The primary objective of this research is to comprehensively review and categorize the current state of traffic sign detection and recognition technologies, focusing on identifying key challenges and providing insights for future improvements in this field.","The research idea addresses the need for effective detection and recognition of traffic signs, which is crucial for vehicular security and the development of advanced and automated vehicles. Accurate identification of traffic signs in real road conditions presents several challenges that impact the safety and efficiency of transportation systems. The study aims to systematically analyze existing approaches to traffic sign detection and recognition to better understand their applications, datasets, performance metrics, hardware requirements, and the challenges they face. The primary objective of this research is to comprehensively review and categorize the current state of traffic sign detection and recognition technologies, focusing on identifying key challenges and providing insights for future improvements in this field.",True
Biology,Multi-task aquatic toxicity prediction model based on multi-level features fusion,"With the escalating menace of organic compounds in environmental pollution imperiling the survival of aquatic organisms, the investigation of organic compound toxicity across diverse aquatic species assumes paramount significance for environmental protection. Understanding how different species respond to these compounds helps assess the potential ecological impact of pollution on aquatic ecosystems as a whole. Compared with traditional experimental methods, deep learning methods have higher accuracy in predicting aquatic toxicity, faster data processing speed and better generalization ability. This article presents ATFPGT-multi, an advanced multi-task deep neural network prediction model for organic toxicity. The model integrates molecular fingerprints and molecule graphs to characterize molecules, enabling the simultaneous prediction of acute toxicity for the same organic compound across four distinct fish species. Furthermore, to validate the advantages of multi-task learning, we independently construct prediction models, named ATFPGT-single, for each fish species. We employ cross-validation in our experiments to assess the performance and generalization ability of ATFPGT-multi. The experimental results indicate, first, that ATFPGT-multi outperforms ATFPGT-single on four fish datasets with AUC improvements of 9.8%, 4%, 4.8%, and 8.2%, respectively, demonstrating the superiority of multi-task learning over single-task learning. Furthermore, in comparison with previous algorithms, ATFPGT-multi outperforms comparative methods, emphasizing that our approach exhibits higher accuracy and reliability in predicting aquatic toxicity. Moreover, ATFPGT-multi utilizes attention scores to identify molecular fragments associated with fish toxicity in organic molecules, as demonstrated by two organic molecule examples in the main text, demonstrating the interpretability of ATFPGT-multi. In summary, ATFPGT-multi provides important support and reference for the further development of aquatic toxicity assessment. All of codes and datasets are freely available online at https://github.com/zhaoqi106/ATFPGT-multi.","['deep learning methods', 'multi-task deep neural network prediction model', 'multi-task learning', 'single-task learning']","The research addresses the growing threat posed by organic compounds in environmental pollution, which endangers the survival of aquatic organisms. Investigating the toxicity of these organic compounds across various aquatic species is crucial for understanding and protecting aquatic ecosystems from ecological harm. The primary aim of the study is to predict the acute toxicity of the same organic compounds across four different fish species, thereby providing a more comprehensive assessment of their potential ecological impact. This work seeks to enhance the accuracy and reliability of aquatic toxicity prediction to support environmental protection efforts.","The research addresses the growing threat posed by organic compounds in environmental pollution, which endangers the survival of aquatic organisms. Investigating the toxicity of these organic compounds across various aquatic species is crucial for understanding and protecting aquatic ecosystems from ecological harm. The primary aim of the study is to assess the acute toxicity of the same organic compounds across four different fish species, thereby providing a more comprehensive assessment of their potential ecological impact. This work seeks to enhance the accuracy and reliability of aquatic toxicity evaluation to support environmental protection efforts.",True
Biology,A survey on training challenges in generative adversarial networks for biomedical image analysis,"Abstract In biomedical image analysis, the applicability of deep learning methods is directly impacted by the quantity of image data available. This is due to deep learning models requiring large image datasets to provide high-level performance. Generative Adversarial Networks (GANs) have been widely utilized to address data limitations through the generation of synthetic biomedical images. GANs consist of two models. The generator, a model that learns how to produce synthetic images based on the feedback it receives. The discriminator, a model that classifies an image as synthetic or real and provides feedback to the generator. Throughout the training process, a GAN can experience several technical challenges that impede the generation of suitable synthetic imagery. First, the mode collapse problem whereby the generator either produces an identical image or produces a uniform image from distinct input features. Second, the non-convergence problem whereby the gradient descent optimizer fails to reach a Nash equilibrium. Thirdly, the vanishing gradient problem whereby unstable training behavior occurs due to the discriminator achieving optimal classification performance resulting in no meaningful feedback being provided to the generator. These problems result in the production of synthetic imagery that is blurry, unrealistic, and less diverse. To date, there has been no survey article outlining the impact of these technical challenges in the context of the biomedical imagery domain. This work presents a review and taxonomy based on solutions to the training problems of GANs in the biomedical imaging domain. This survey highlights important challenges and outlines future research directions about the training of GANs in the domain of biomedical imagery.","['deep learning', 'Generative Adversarial Networks (GANs)', 'discriminator', 'gradient descent optimizer']","The research idea addresses the challenge of limited availability of biomedical image data, which affects the ability to generate high-quality synthetic biomedical images. The study highlights the technical difficulties encountered during the generation of synthetic images, such as producing blurry, unrealistic, and less diverse images due to specific training problems. The research objective is to review and categorize the existing solutions to these training challenges in the context of biomedical image generation. This work aims to highlight important challenges and propose future research directions for improving the generation of synthetic biomedical imagery.","The research idea addresses the challenge of limited availability of biomedical image data, which affects the ability to generate high-quality synthetic biomedical images. The study highlights the technical difficulties encountered during the generation of synthetic images, such as producing blurry, unrealistic, and less diverse images due to specific training problems. The research objective is to review and categorize the existing solutions to these challenges in the context of biomedical image generation. This work aims to highlight important challenges and propose future research directions for improving the generation of synthetic biomedical imagery.",True
Biology,Machine learning for the management of biochar yield and properties of biomass sources for sustainable energy,"Abstract Biochar is emerging as a potential solution for biomass conversion to meet the ever increasing demand for sustainable energy. Efficient management systems are needed in order to exploit fully the potential of biochar. Modern machine learning (ML) techniques, and in particular ensemble approaches and explainable AI methods, are valuable for forecasting the properties and efficiency of biochar properly. Machine‐learning‐based forecasts, optimization, and feature selection are critical for improving biomass management techniques. In this research, we explore the influences of these techniques on the accurate forecasting of biochar yield and properties for a range of biomass sources. We emphasize the importance of the interpretability of a model, as this improves human comprehension and trust in ML predictions. Sensitivity analysis is shown to be an effective technique for finding crucial biomass characteristics that influence the synthesis of biochar. Precision prognostics have far‐reaching ramifications, influencing industries such as biomass logistics, conversion technologies, and the successful use of biomass as renewable energy. These advances can make a substantial contribution to a greener future and can encourage the development of a circular biobased economy. This work emphasizes the importance of using sophisticated data‐driven methodologies such as ML in biochar synthesis, to usher in ecologically friendly energy solutions. These breakthroughs hold the key to a more sustainable and environmentally friendly future.","['ensemble approaches', 'feature selection']","The research idea centers on addressing the growing need for sustainable energy by improving the conversion of biomass into biochar, a promising renewable energy source. Efficient management and understanding of biochar properties are essential to fully exploit its potential in biomass conversion. The study highlights the significance of accurately forecasting biochar yield and properties from various biomass sources to enhance biomass management and conversion technologies. This is important for advancing sustainable energy solutions and supporting the development of a circular biobased economy.

The primary objective of the study is to explore the factors influencing the accurate prediction of biochar yield and properties across different biomass sources. The research aims to identify key biomass characteristics that affect biochar synthesis and to improve the understanding of these relationships. Ultimately, the study seeks to contribute to more effective biomass management and conversion strategies that promote environmentally friendly and sustainable energy production.","The research idea centers on addressing the growing need for sustainable energy by improving the conversion of biomass into biochar, a promising renewable energy source. Efficient management and understanding of biochar properties are essential to fully exploit its potential in biomass conversion. The study highlights the significance of accurately determining biochar yield and properties from various biomass sources to enhance biomass management and conversion technologies. This is important for advancing sustainable energy solutions and supporting the development of a circular biobased economy.

The primary objective of the study is to explore the factors influencing the accurate estimation of biochar yield and properties across different biomass sources. The research aims to identify key biomass characteristics that affect biochar synthesis and to improve the understanding of these relationships. Ultimately, the study seeks to contribute to more effective biomass management and conversion strategies that promote environmentally friendly and sustainable energy production.",True
Biology,An ensemble penalized regression method for multi-ancestry polygenic risk prediction,"Abstract Great efforts are being made to develop advanced polygenic risk scores (PRS) to improve the prediction of complex traits and diseases. However, most existing PRS are primarily trained on European ancestry populations, limiting their transferability to non-European populations. In this article, we propose a novel method for generating multi-ancestry Polygenic Risk scOres based on enSemble of PEnalized Regression models (PROSPER). PROSPER integrates genome-wide association studies (GWAS) summary statistics from diverse populations to develop ancestry-specific PRS with improved predictive power for minority populations. The method uses a combination of $${{{{{{\mathscr{L}}}}}}}_{1}$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:msub> <mml:mrow> <mml:mi>L</mml:mi> </mml:mrow> <mml:mrow> <mml:mn>1</mml:mn> </mml:mrow> </mml:msub> </mml:math> (lasso) and $${{{{{{\mathscr{L}}}}}}}_{2}$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:msub> <mml:mrow> <mml:mi>L</mml:mi> </mml:mrow> <mml:mrow> <mml:mn>2</mml:mn> </mml:mrow> </mml:msub> </mml:math> (ridge) penalty functions, a parsimonious specification of the penalty parameters across populations, and an ensemble step to combine PRS generated across different penalty parameters. We evaluate the performance of PROSPER and other existing methods on large-scale simulated and real datasets, including those from 23andMe Inc., the Global Lipids Genetics Consortium, and All of Us. Results show that PROSPER can substantially improve multi-ancestry polygenic prediction compared to alternative methods across a wide variety of genetic architectures. In real data analyses, for example, PROSPER increased out-of-sample prediction R 2 for continuous traits by an average of 70% compared to a state-of-the-art Bayesian method (PRS-CSx) in the African ancestry population. Further, PROSPER is computationally highly scalable for the analysis of large SNP contents and many diverse populations.","['lasso', 'ridge', 'ensemble']","The study addresses the challenge that most existing polygenic risk scores (PRS) are primarily developed using data from European ancestry populations, which limits their effectiveness and transferability to non-European populations. This limitation hinders accurate prediction of complex traits and diseases in diverse populations, particularly minority groups. The research is motivated by the need to improve the predictive power of PRS across multiple ancestries to enable better genetic risk assessment for a broader range of populations. The primary objective of the study is to develop a novel approach for generating multi-ancestry polygenic risk scores by integrating genome-wide association study summary statistics from diverse populations, aiming to produce ancestry-specific PRS with enhanced predictive accuracy for minority populations. The study seeks to demonstrate that this approach can substantially improve polygenic prediction across various genetic architectures compared to existing methods.","The study addresses the challenge that most existing polygenic risk scores (PRS) are primarily developed using data from European ancestry populations, which limits their effectiveness and transferability to non-European populations. This limitation hinders accurate prediction of complex traits and diseases in diverse populations, particularly minority groups. The research is motivated by the need to improve the predictive power of PRS across multiple ancestries to enable better genetic risk assessment for a broader range of populations. The primary objective of the study is to develop a new approach for generating multi-ancestry polygenic risk scores by integrating genome-wide association study summary statistics from diverse populations, aiming to produce ancestry-specific PRS with enhanced predictive accuracy for minority populations. The study seeks to demonstrate that this approach can substantially improve polygenic prediction across various genetic architectures compared to existing methods.",True
Biology,A Survey of Text Classification With Transformers: How Wide? How Large? How Long? How Accurate? How Expensive? How Safe?,"Text classification is a basic task in natural language processing (NLP) with applications from sentiment analysis to question-answering with chat bots. In recent years, transformer-based models have emerged as the prevailing framework in NLP, demonstrating excellent results across many benchmarks. This paper recommends an expanded taxonomy of applications and provides a review of the performance of different models across these applications. The use of traditional research techniques plus co-citation and bibliographic coupling provides a comprehensive view of the current and past research in this area. The study begins by providing an overview of the history of transformer-based models with an emphasis on recent large language models (LLM). Next, uni-modal (text only) inputs and the emerging area of multi-modal classification are discussed to provide a comparison of current and emerging research in this area. Gaps are highlighted in the use of multi-modal text/numeric/columnar data and recommendations for future research are provided. Finally, the length of text input variables (tokens) is reviewed to explore the evolution from short-text to longer document applications. Furthermore, the accuracy on 358 datasets across 20 applications is reviewed and unexpected results emerge which show that LLMs are not always the most accurate or least expensive option. In addition to model performance, the safety implications of transformer-based models are reviewed, and a summary of issues related to ethics, bias, social implications, and copyright are explored.",['transformer-based models'],"The research idea addresses the broad application of text classification within natural language processing, highlighting its importance across various tasks such as sentiment analysis and question-answering. The study is motivated by the need to understand the performance and evolution of different approaches in this area, including the challenges and gaps related to multi-modal data and varying lengths of text inputs. The primary objective of the study is to provide a comprehensive review of the performance of different models across multiple applications, identify gaps in current research particularly concerning multi-modal data, and offer recommendations for future investigations. Additionally, the study aims to examine the implications of these models in terms of safety, ethics, bias, social impact, and copyright considerations.","The research idea addresses the broad application of text classification within natural language processing, highlighting its importance across various tasks such as sentiment analysis and question-answering. The study is motivated by the need to understand the performance and evolution of different approaches in this area, including the challenges and gaps related to multi-modal data and varying lengths of text inputs. The primary objective of the study is to provide a comprehensive review of the performance of different methodologies across multiple applications, identify gaps in current research particularly concerning multi-modal data, and offer recommendations for future investigations. Additionally, the study aims to examine the implications of these approaches in terms of safety, ethics, bias, social impact, and copyright considerations.",True
Biology,"A stacking ANN ensemble model of ML models for stream water quality prediction of Godavari River Basin, India","The importance of water quality models has increased as their inputs are critical to the development of risk assessment framework for environmental management and monitoring of rivers. However, with the advent of a plethora of recent advances in ML algorithms better predictions are possible. This study proposes a causal and effect model by considering climatological such as temperature and precipitation along with geospatial information related to the agricultural land use factor (ALUF), the forest land use factor (FLUF), the grassland usage factor (GLUF), the shrub land use factor (SLUF), and the urban land use factor (ULUF). All these factors are included in the input data, whereas four Stream Water Quality parameters (SWQPs) such as Electrical Conductivity (EC), Biochemical Oxygen Demand (BOD), Nitrate, and Dissolved Oxygen (DO) from 2019 to 2021 are taken as outputs to predict the Godavari River Basin water quality. In the preliminary investigation, out of these four SWQPs, nitrate's coefficient of variation (CV) is high, revealing a close association with climate parameters and land use practices across the sampling stations. In the authors' earlier study, a model using a single-layer Feed-Forward Neural Network (FFNN) showed improved performance in predicting cause and effect factors linked to water quality metrics. To achieve better prediction, a stacked ANN meta-model and nine conventional machine learning (ML) models, including Extreme Gradient Boosting (XGB), Extra Trees (ET), Bagging (BG), Random Forest (RF), AdaBoost or Adaptive Boosting (ADB), Decision Tree (DT), Highest Gradient Boosting (HGB), Light Gradient Boosting Method (LGBM), and Gradient Boosting (GB), were compared in this study. According to the study's findings, Bagging and Boosting models outperformed stand-alone earlier FFNN for the same dataset and showed superior predictive capabilities in terms of accuracy in forecasting the variable of interest. For instance, during testing, the coefficient of determination (R2) of Biochemical Oxygen Demand (BOD) increased from 0.72 to 0.87. Furthermore, a stacked Artificial Neural Network (ANN) meta model that was reinforced using Extreme Gradient Boosting (XGB), Random Forest (RF), and Extra Trees (ET) as base models performed better than the individual ML models (from R2 = 0.87 to 0.91 for BOD in testing). By using this new framework, the effort for hyperparameter tuning can be minimized.","['Feed-Forward Neural Network (FFNN)', 'stacked ANN meta-model', 'Extreme Gradient Boosting (XGB)', 'Extra Trees (ET)', 'Bagging (BG)', 'Random Forest (RF)', 'AdaBoost or Adaptive Boosting (ADB)', 'Decision Tree (DT)', 'Light Gradient Boosting Method (LGBM)', 'Gradient Boosting (GB)', 'stacked Artificial Neural Network (ANN) meta model']","The study addresses the critical need for accurate water quality assessment in river ecosystems, emphasizing the importance of understanding how climatological factors such as temperature and precipitation, along with various land use types including agricultural, forest, grassland, shrub, and urban areas, influence stream water quality parameters. This is essential for developing effective risk assessment frameworks for environmental management and monitoring of rivers like the Godavari River Basin. The primary objective of the study is to improve the prediction of key stream water quality parameters—Electrical Conductivity, Biochemical Oxygen Demand, Nitrate, and Dissolved Oxygen—by examining their relationships with climate variables and land use factors over the period from 2019 to 2021. The study aims to enhance the accuracy of forecasting these water quality metrics to better understand the cause-and-effect dynamics affecting river health.","The study addresses the critical need for accurate water quality assessment in river ecosystems, emphasizing the importance of understanding how climatological factors such as temperature and precipitation, along with various land use types including agricultural, forest, grassland, shrub, and urban areas, influence stream water quality parameters. This is essential for developing effective risk assessment frameworks for environmental management and monitoring of rivers like the Godavari River Basin. The primary objective of the study is to improve the understanding of key stream water quality parameters—Electrical Conductivity, Biochemical Oxygen Demand, Nitrate, and Dissolved Oxygen—by examining their relationships with climate variables and land use factors over the period from 2019 to 2021. The study aims to enhance the accuracy of estimating these water quality metrics to better understand the cause-and-effect dynamics affecting river health.",True
Biology,CAR-Toner: an AI-driven approach for CAR tonic signaling prediction and optimization,"2][3] Our previous work has elucidated that positively charged patches (PCPs) on the surface of the CAR antigenbinding domain facilitate CAR clustering, thereby triggering CAR tonic signals.To quantify these PCPs, which are indicative of CAR tonic signaling, we previously developed a bioinformatic method to determine the PCP score. 1 This calculation method starts with constructing three-dimensional (3D) homology models for CAR's single-chain variable fragments (scFvs) using the SWISS homology modeler.Subsequently, the BindUP web server is used to determine the total count of residues within the top three largest patches containing continuous positively charged residues on the surface of CAR scFv.However, this PCP score calculation method has several limitations: 1. reliance on two external servers; 2. each calculation taking a few days, significantly hindering efficiency; 3. lack of batch calculation capability; 4. no optimization strategies provided for finetuning PCP scores.Given these constraints, we aimed to develop an artificial intelligence (AI)-based PCP score calculator and optimizer to overcome these bottlenecks.Protein databases, structural biology, and advanced deep learning models are all integrated into our AI-based PCP score calculator (Fig. 1a).A comprehensive protein structure database consisting of over 170,000 entries was established by extracting 3D structural information from the Protein Data Bank (PDB) and AlphaFold predictions, followed by stringent quality control procedures.We further developed an in-house algorithm tailored for calculating PCP scores based on the obtained 3D structure information (Supplementary Information), subsequently generating a dataset comprising approximately 170,000 protein sequences along with their associated PCP scores.For model training and evaluation, 70% of the data are allocated as the training dataset, while the remaining 30% serve as the test dataset.The ESM2 model, developed by the FAIR (Meta Fundamental AI Research Protein Team), is utilized for fine-tuning tasks related to PCP prediction. 4,5SM2 is a transformer-based language model using an attention mechanism to learn interaction patterns between pairs of amino acids in the input sequence.Pre-trained on over 60 million protein sequences from the UniProt Reference Clusters (UniRef) database, ESM2 demonstrates strong adaptability to downstream protein structure-related tasks. 5The ESM2-8M model was used to fine-tune the training dataset.Following updating parameters, the ESM2 model was transformed into the PCP-AI prediction model, referred to as CAR-Tonic Signal Tuner (abbreviated as CAR-Toner; http://cartfitness.slst.shanghaitech.edu.cn/CAR-fitness/).This model encompasses three key functionalities: proficient PCP calculation for individual proteins, streamlined batch processing, and an integrated optimization strategy for refining PCP scores (Fig. 1b).","['ESM2 model', 'transformer-based language model using an attention mechanism', 'fine-tuning']","The research idea centers on the role of positively charged patches (PCPs) on the surface of the CAR antigen-binding domain, which facilitate CAR clustering and trigger CAR tonic signaling. Quantifying these PCPs is important because they serve as indicators of CAR tonic signaling, a critical factor in understanding CAR function. However, existing methods for calculating PCP scores have significant limitations, including reliance on external resources, lengthy processing times, lack of batch processing, and absence of optimization strategies, which hinder efficient and effective analysis. The study aims to address these challenges to improve the quantification of PCPs.

The primary objective of the study is to develop a more efficient and optimized approach for calculating PCP scores that overcomes the limitations of previous methods. This includes enabling faster and batch processing of PCP calculations and incorporating strategies to refine PCP scores for better accuracy. The goal is to facilitate improved quantification of positively charged patches on CAR antigen-binding domains to better understand and potentially modulate CAR tonic signaling.","The research idea centers on the role of positively charged patches (PCPs) on the surface of the CAR antigen-binding domain, which facilitate CAR clustering and trigger CAR tonic signaling. Quantifying these PCPs is important because they serve as indicators of CAR tonic signaling, a critical factor in understanding CAR function. However, existing methods for calculating PCP scores have significant limitations, including reliance on external resources, lengthy processing times, lack of batch processing, and absence of refinement strategies, which hinder efficient and effective analysis. The study aims to address these challenges to improve the quantification of PCPs.

The primary objective of the study is to develop a more efficient and streamlined approach for calculating PCP scores that overcomes the limitations of previous methods. This includes enabling faster and batch processing of PCP calculations and incorporating strategies to refine PCP scores for better accuracy. The goal is to facilitate improved quantification of positively charged patches on CAR antigen-binding domains to better understand and potentially modulate CAR tonic signaling.",True
Biology,Accuracy of GPT-4 in histopathological image detection and classification of colorectal adenomas,"Aims To evaluate the accuracy of Chat Generative Pre-trained Transformer (ChatGPT) powered by GPT-4 in histopathological image detection and classification of colorectal adenomas using the diagnostic consensus provided by pathologists as a reference standard. Methods A study was conducted with 100 colorectal polyp photomicrographs, comprising an equal number of adenomas and non-adenomas, classified by two pathologists. These images were analysed by classic GPT-4 for 1 time in October 2023 and custom GPT-4 for 20 times in December 2023. GPT-4’s responses were compared against the reference standard through statistical measures to evaluate its proficiency in histopathological diagnosis, with the pathologists further assessing the model’s descriptive accuracy. Results GPT-4 demonstrated a median sensitivity of 74% and specificity of 36% for adenoma detection. The median accuracy of polyp classification varied, ranging from 16% for non-specific changes to 36% for tubular adenomas. Its diagnostic consistency, indicated by low kappa values ranging from 0.06 to 0.11, suggested only poor to slight agreement. All of the microscopic descriptions corresponded with their diagnoses. GPT-4 also commented about the limitations in its diagnoses (eg, slide diagnosis best done by pathologists, the inadequacy of single-image diagnostic conclusions, the need for clinical data and a higher magnification view). Conclusions GPT-4 showed high sensitivity but low specificity in detecting adenomas and varied accuracy for polyp classification. However, its diagnostic consistency was low. This artificial intelligence tool acknowledged its diagnostic limitations, emphasising the need for a pathologist’s expertise and additional clinical context.",['Chat Generative Pre-trained Transformer (ChatGPT) powered by GPT-4'],"The research idea addresses the challenge of accurately detecting and classifying colorectal adenomas through histopathological examination, highlighting the importance of reliable diagnostic methods in pathology. Given the critical role of pathologists in providing consensus diagnoses, there is a need to evaluate alternative approaches that could assist or complement traditional histopathological analysis. The study aims to assess the accuracy and consistency of colorectal polyp classification, which is essential for effective diagnosis and treatment planning. The primary objective of the study is to evaluate the accuracy of histopathological image detection and classification of colorectal adenomas by comparing diagnostic results against the consensus provided by expert pathologists. The study seeks to determine the sensitivity, specificity, and overall diagnostic consistency in identifying adenomas and non-adenomas, as well as to understand the limitations inherent in single-image diagnoses and the necessity of clinical context and expert interpretation.","The research idea addresses the challenge of accurately detecting and classifying colorectal adenomas through histopathological examination, highlighting the importance of reliable diagnostic methods in pathology. Given the critical role of pathologists in providing consensus diagnoses, there is a need to evaluate alternative approaches that could assist or complement traditional histopathological analysis. The study aims to assess the accuracy and consistency of colorectal polyp classification, which is essential for effective diagnosis and treatment planning. The primary objective of the study is to evaluate the accuracy of histopathological examination and classification of colorectal adenomas by comparing diagnostic results against the consensus provided by expert pathologists. The study seeks to determine the sensitivity, specificity, and overall diagnostic consistency in identifying adenomas and non-adenomas, as well as to understand the limitations inherent in single-image diagnoses and the necessity of clinical context and expert interpretation.",True
Biology,Discovering Consensus Regions for Interpretable Identification of RNA N6-Methyladenosine Modification Sites via Graph Contrastive Clustering,"As a pivotal post-transcriptional modification of RNA, N6-methyladenosine (m6A) has a substantial influence on gene expression modulation and cellular fate determination. Although a variety of computational models have been developed to accurately identify potential m6A modification sites, few of them are capable of interpreting the identification process with insights gained from consensus knowledge. To overcome this problem, we propose a deep learning model, namely M6A-DCR, by discovering consensus regions for interpretable identification of m6A modification sites. In particular, M6A-DCR first constructs an instance graph for each RNA sequence by integrating specific positions and types of nucleotides. The discovery of consensus regions is then formulated as a graph clustering problem in light of aggregating all instance graphs. After that, M6A-DCR adopts a motif-aware graph reconstruction optimization process to learn high-quality embeddings of input RNA sequences, thus achieving the identification of m6A modification sites in an end-to-end manner. Experimental results demonstrate the superior performance of M6A-DCR by comparing it with several state-of-the-art identification models. The consideration of consensus regions empowers our model to make interpretable predictions at the motif level. The analysis of cross validation through different species and tissues further verifies the consistency between the identification results of M6A-DCR and the evolutionary relationships among species","['deep learning model', 'graph clustering']","The study addresses the important role of N6-methyladenosine (m6A) as a key post-transcriptional modification of RNA that significantly influences gene expression regulation and cellular fate determination. Despite the existence of various approaches to identify potential m6A modification sites, there remains a challenge in interpreting the identification process with insights derived from consensus biological knowledge. The primary aim of the study is to improve the identification of m6A modification sites by discovering consensus regions that allow for interpretable predictions at the motif level. Additionally, the study seeks to validate the consistency of these identification results across different species and tissues, reflecting evolutionary relationships.","The study addresses the important role of N6-methyladenosine (m6A) as a key post-transcriptional modification of RNA that significantly influences gene expression regulation and cellular fate determination. Despite the existence of various approaches to identify potential m6A modification sites, there remains a challenge in interpreting the identification process with insights derived from consensus biological knowledge. The primary aim of the study is to improve the identification of m6A modification sites by discovering consensus regions that allow for interpretable observations at the motif level. Additionally, the study seeks to validate the consistency of these identification results across different species and tissues, reflecting evolutionary relationships.",True
Biology,"Machine learning models for gully erosion susceptibility assessment in the Tensift catchment, Haouz Plain, Morocco for sustainable development","Gully erosion is a widespread environmental danger, threatening global socio-economic stability and sustainable development. This study comprehensively applied seven machine learning (ML) models including SVM, KNN, RF, XGBoost, ANN, DT, and LR, and evaluated gully erosion susceptibility in the Tensift catchment and predict it within the Haouz plain, Morocco. To ensure the reliability of the findings, the study employed a robust combination of gully erosion inventory, sentinel images, and Digital Surface Model. Eighteen predictors, encompassing topographical, geomorphological, environmental, and hydrological factors, were selected after multicollinearity analyses. The gully erosion susceptibility of the study revealed that approximately 28.18% of the Tensift catchment is at a very high risk of erosion. Furthermore, 15.13% and 31.28% of the catchment are categorized as low and very low respectively. These findings extend to the Haouz plain, where 7.84% of the surface area are very highly risking erosion, while 18.25% and 55.18% are characterized as low and very low risk areas. To gauge the performance of the ML models, an array of metrics including specificity, precision, sensitivity, and accuracy were employed. The study highlights XGBoost and KNN as the most promising models, achieving AUC ROC values of 0.96 and 0.93 in the test phase. The remaining models namely RF (AUC ROC = 0.89), LR (AUC ROC = 0.80), SVM (AUC ROC = 0.81), DT (AUC ROC = 0.86), and ANN (AUC ROC = 0.78), also displayed commendable performance. The novelty of this research is its innovative approach to combat gully erosion through cutting edge ML models, offering practical solutions for watershed conservation, sustainable management, and the prevention of land degradation. These insights are invaluable for addressing the challenges posed by gully erosion within the region, and beyond its geographical boundaries and can be used for defining appropriate mitigation strategies at local to national scale.","['SVM', 'KNN', 'RF', 'XGBoost', 'ANN', 'DT', 'LR']","The study addresses the widespread environmental threat of gully erosion, which poses significant risks to global socio-economic stability and sustainable development. Gully erosion leads to land degradation, impacting watershed conservation and the overall health of affected ecosystems. The primary aim of the study is to evaluate gully erosion susceptibility in the Tensift catchment and predict it within the Haouz plain, Morocco, by identifying areas at varying levels of erosion risk. This research seeks to provide valuable insights for defining appropriate mitigation strategies to prevent land degradation and promote sustainable management of the affected regions.","The study addresses the widespread environmental threat of gully erosion, which poses significant risks to global socio-economic stability and sustainable development. Gully erosion leads to land degradation, impacting watershed conservation and the overall health of affected ecosystems. The primary aim of the study is to evaluate gully erosion susceptibility in the Tensift catchment and assess it within the Haouz plain, Morocco, by identifying areas at varying levels of erosion risk. This research seeks to provide valuable insights for defining appropriate mitigation strategies to prevent land degradation and promote sustainable management of the affected regions.",True
Biology,A Deep Bidirectional LSTM Model Enhanced by Transfer-Learning-Based Feature Extraction for Dynamic Human Activity Recognition,"Dynamic human activity recognition (HAR) is a domain of study that is currently receiving considerable attention within the fields of computer vision and pattern recognition. The growing need for artificial-intelligence (AI)-driven systems to evaluate human behaviour and bolster security underscores the timeliness of this research. Despite the strides made by numerous researchers in developing dynamic HAR frameworks utilizing diverse pre-trained architectures for feature extraction and classification, persisting challenges include suboptimal performance accuracy and the computational intricacies inherent in existing systems. These challenges arise due to the vast video-based datasets and the inherent similarity in the data. To address these challenges, we propose an innovative, dynamic HAR technique employing a deep-learning-based, deep bidirectional long short-term memory (Deep BiLSTM) model facilitated by a pre-trained transfer-learning-based feature-extraction approach. Our approach begins with the utilization of Convolutional Neural Network (CNN) models, specifically MobileNetV2, for extracting deep-level features from video frames. Subsequently, these features are fed into an optimized deep bidirectional long short-term memory (Deep BiLSTM) network to discern dependencies and process data, enabling optimal predictions. During the testing phase, an iterative fine-tuning procedure is introduced to update the high parameters of the trained model, ensuring adaptability to varying scenarios. The proposed model’s efficacy was rigorously evaluated using three benchmark datasets, namely UCF11, UCF Sport, and JHMDB, achieving notable accuracies of 99.20%, 93.3%, and 76.30%, respectively. This high-performance accuracy substantiates the superiority of our proposed model, signaling a promising advancement in the domain of activity recognition.","['deep bidirectional long short-term memory (Deep BiLSTM)', 'pre-trained transfer-learning-based feature-extraction approach', 'Convolutional Neural Network (CNN)', 'MobileNetV2']","The study addresses the challenge of accurately recognizing dynamic human activities, which is important for evaluating human behavior and enhancing security. Despite previous efforts, existing approaches face difficulties due to the complexity of video-based datasets and the similarity within the data, leading to suboptimal performance. The primary objective of the study is to develop a novel technique for dynamic human activity recognition that improves accuracy and adaptability across different scenarios. This approach aims to effectively capture and interpret human activity patterns to achieve superior recognition performance on benchmark datasets.","The study addresses the challenge of accurately recognizing dynamic human activities, which is important for evaluating human behavior and enhancing security. Despite previous efforts, existing approaches face difficulties due to the complexity of video-based datasets and the similarity within the data, leading to suboptimal performance. The primary objective of the study is to develop a new methodology for dynamic human activity recognition that improves accuracy and adaptability across different scenarios. This approach aims to effectively capture and interpret human activity patterns to achieve superior recognition performance on benchmark datasets.",True
Biology,An Empirical Study on Correlations Between Deep Neural Network Fairness and Neuron Coverage Criteria,"Recently, with the widespread use of deep neural networks (DNNs) in high-stakes decision-making systems (such as fraud detection and prison sentencing), concerns have arisen about the fairness of DNNs in terms of the potential negative impact they may have on individuals and society. Therefore, fairness testing has become an important research topic in DNN testing. At the same time, the neural network coverage criteria (such as criteria based on neuronal activation) is considered as an adequacy test for DNN white-box testing. It is implicitly assumed that improving the coverage can enhance the quality of test suites. Nevertheless, the correlation between DNN fairness (a test property) and coverage criteria (a test method) has not been adequately explored. To address this issue, we conducted a systematic empirical study on seven coverage criteria, six fairness metrics, three fairness testing techniques, and five bias mitigation methods on five DNN models and nine fairness datasets to assess the correlation between coverage criteria and DNN fairness. Our study achieved the following findings: 1) with the increase in the size of the test suite, some of the coverage and fairness metrics changed significantly, as the size of the test suite increased; 2) the statistical correlation between coverage criteria and DNN fairness is limited; and 3) after bias mitigation for improving the fairness of DNN, the change pattern in coverage criteria is different; 4) Models debiased by different bias mitigation methods have a lower correlation between coverage and fairness compared to the original models. Our findings cast doubt on the validity of coverage criteria concerning DNN fairness (i.e., increasing the coverage may even have a negative impact on the fairness of DNNs). Therefore, we warn DNN testers against blindly pursuing higher coverage of coverage criteria at the cost of test properties of DNNs (such as fairness).","['deep neural networks (DNNs)', 'bias mitigation methods']","The research idea addresses concerns about fairness in decision-making systems that use deep neural networks, highlighting the potential negative impacts these systems may have on individuals and society. It emphasizes the importance of fairness testing in evaluating these systems and questions the assumed relationship between test coverage criteria and fairness. The study aims to investigate whether improving test coverage actually correlates with enhanced fairness in these networks. The primary objective of the study is to systematically assess the correlation between various coverage criteria and fairness metrics across multiple models and datasets. It seeks to determine how changes in test suite size and bias mitigation methods affect both coverage and fairness, ultimately evaluating the validity of using coverage criteria as an indicator of fairness in these systems.","The research idea addresses concerns about fairness in decision-making systems, highlighting the potential negative impacts these systems may have on individuals and society. It emphasizes the importance of fairness testing in evaluating these systems and questions the assumed relationship between test coverage criteria and fairness. The study aims to investigate whether improving test coverage actually correlates with enhanced fairness in these systems. The primary objective of the study is to systematically assess the correlation between various coverage criteria and fairness metrics across multiple models and datasets. It seeks to determine how changes in test suite size and bias mitigation methods affect both coverage and fairness, ultimately evaluating the validity of using coverage criteria as an indicator of fairness in these systems.",True
Biology,Investigating the Impact of Train / Test Split Ratio on the Performance of Pre-Trained Models with Custom Datasets,"The proper allocation of data between training and testing is a critical factor influencing the performance of deep learning models, especially those built upon pre-trained architectures. Having the suitable training set size is an important factor for the classification model’s generalization performance. The main goal of this study is to find the appropriate training set size for three pre-trained networks using different custom datasets. For this aim, the study presented in this paper explores the effect of varying the train / test split ratio on the performance of three popular pre-trained models, namely MobileNetV2, ResNet50v2 and VGG19, with a focus on image classification task. In this work, three balanced datasets never seen by the models have been used, each containing 1000 images divided into two classes. The train / test split ratios used for this study are: 60-40, 70-30, 80-20 and 90-10. The focus was on the critical metrics of sensitivity, specificity and overall accuracy to evaluate the performance of the classifiers under the different ratios. Experimental results show that, the performance of the classifiers is affected by varying the training / testing split ratio for the three custom datasets. Moreover, with the three pre-trained models, using more than 70% of the dataset images for the training task gives better performance.","['MobileNetV2', 'ResNet50v2', 'VGG19']","The study addresses the challenge of determining the appropriate allocation of data between training and testing to optimize the performance of classification models in biological image analysis. Proper training set size is crucial for achieving good generalization performance when classifying biological images. The primary aim of the study is to identify the suitable training set size by examining the effect of different train/test split ratios on the classification performance using three balanced biological image datasets. The research focuses on evaluating how varying the proportion of training data influences sensitivity, specificity, and overall accuracy in classifying images into two classes.","The study addresses the challenge of determining the appropriate allocation of data between training and testing to optimize the performance of classification approaches in biological image analysis. Proper dataset division is crucial for achieving good generalization when categorizing biological images. The primary aim of the study is to identify the suitable proportion of data for development by examining the effect of different data split ratios on the classification performance using three balanced biological image datasets. The research focuses on evaluating how varying the proportion of development data influences sensitivity, specificity, and overall accuracy in categorizing images into two classes.",True
Biology,"Comparative performance analysis of Boruta, SHAP, and Borutashap for disease diagnosis: A study with multiple machine learning algorithms","Interpretable machine learning models are instrumental in disease diagnosis and clinical decision-making, shedding light on relevant features. Notably, Boruta, SHAP (SHapley Additive exPlanations), and BorutaShap were employed for feature selection, each contributing to the identification of crucial features. These selected features were then utilized to train six machine learning algorithms, including LR, SVM, ETC, AdaBoost, RF, and LR, using diverse medical datasets obtained from public sources after rigorous preprocessing. The performance of each feature selection technique was evaluated across multiple ML models, assessing accuracy, precision, recall, and F1-score metrics. Among these, SHAP showcased superior performance, achieving average accuracies of 80.17%, 85.13%, 90.00%, and 99.55% across diabetes, cardiovascular, statlog, and thyroid disease datasets, respectively. Notably, the LGBM emerged as the most effective algorithm, boasting an average accuracy of 91.00% for most disease states. Moreover, SHAP enhanced the interpretability of the models, providing valuable insights into the underlying mechanisms driving disease diagnosis. This comprehensive study contributes significant insights into feature selection techniques and machine learning algorithms for disease diagnosis, benefiting researchers and practitioners in the medical field. Further exploration of feature selection methods and algorithms holds promise for advancing disease diagnosis methodologies, paving the way for more accurate and interpretable diagnostic models.","['Boruta', 'SHAP (SHapley Additive exPlanations)', 'LR', 'SVM', 'AdaBoost', 'RF', 'LGBM']","The study addresses the challenge of identifying crucial features that contribute to accurate disease diagnosis and clinical decision-making. It highlights the importance of selecting relevant biological markers to improve understanding of the underlying mechanisms driving various diseases. The primary aim of the research is to evaluate different feature selection techniques in order to identify key features associated with diseases such as diabetes, cardiovascular conditions, statlog, and thyroid disorders. By comparing these techniques, the study seeks to enhance the accuracy and interpretability of disease diagnosis, ultimately benefiting medical research and practice.","The study addresses the challenge of identifying crucial features that contribute to accurate disease diagnosis and clinical decision-making. It highlights the importance of selecting relevant biological markers to improve understanding of the underlying mechanisms driving various diseases. The primary aim of the research is to evaluate different methodologies for identifying key features associated with diseases such as diabetes, cardiovascular conditions, statlog, and thyroid disorders. By comparing these approaches, the study seeks to enhance the accuracy and interpretability of disease diagnosis, ultimately benefiting medical research and practice.",True
Biology,Motif-Aware miRNA-Disease Association Prediction via Hierarchical Attention Network,"As post-transcriptional regulators of gene expression, micro-ribonucleic acids (miRNAs) are regarded as potential biomarkers for a variety of diseases. Hence, the prediction of miRNA-disease associations (MDAs) is of great significance for an in-depth understanding of disease pathogenesis and progression. Existing prediction models are mainly concentrated on incorporating different sources of biological information to perform the MDA prediction task while failing to consider the fully potential utility of MDA network information at the motif-level. To overcome this problem, we propose a novel motif-aware MDA prediction model, namely MotifMDA, by fusing a variety of high- and low-order structural information. In particular, we first design several motifs of interest considering their ability to characterize how miRNAs are associated with diseases through different network structural patterns. Then, MotifMDA adopts a two-layer hierarchical attention to identify novel MDAs. Specifically, the first attention layer learns high-order motif preferences based on their occurrences in the given MDA network, while the second one learns the final embeddings of miRNAs and diseases through coupling high- and low-order preferences. Experimental results on two benchmark datasets have demonstrated the superior performance of MotifMDA over several state-of-the-art prediction models. This strongly indicates that accurate MDA prediction can be achieved by relying solely on MDA network information. Furthermore, our case studies indicate that the incorporation of motif-level structure information allows MotifMDA to discover novel MDAs from different perspectives. The data and codes are available at <uri xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">https://github.com/stevejobws/MotifMDA.</uri>",['two-layer hierarchical attention'],"The study addresses the importance of micro-ribonucleic acids (miRNAs) as post-transcriptional regulators of gene expression and their potential role as biomarkers for various diseases. Understanding the associations between miRNAs and diseases is crucial for gaining deeper insights into disease pathogenesis and progression. The primary aim of the study is to improve the prediction of miRNA-disease associations by exploring the utility of network information at the motif level, which characterizes how miRNAs are linked to diseases through different structural patterns. This approach seeks to identify novel miRNA-disease associations to enhance the understanding of their biological relationships.","The study addresses the importance of micro-ribonucleic acids (miRNAs) as post-transcriptional regulators of gene expression and their potential role as biomarkers for various diseases. Understanding the associations between miRNAs and diseases is crucial for gaining deeper insights into disease pathogenesis and progression. The primary aim of the study is to improve the identification of miRNA-disease associations by exploring the utility of network information at the motif level, which characterizes how miRNAs are linked to diseases through different structural patterns. This approach seeks to identify novel miRNA-disease associations to enhance the understanding of their biological relationships.",True
Biology,DeepKEGG: a multi-omics data integration framework with biological insights for cancer recurrence prediction and biomarker discovery,"Abstract Deep learning-based multi-omics data integration methods have the capability to reveal the mechanisms of cancer development, discover cancer biomarkers and identify pathogenic targets. However, current methods ignore the potential correlations between samples in integrating multi-omics data. In addition, providing accurate biological explanations still poses significant challenges due to the complexity of deep learning models. Therefore, there is an urgent need for a deep learning-based multi-omics integration method to explore the potential correlations between samples and provide model interpretability. Herein, we propose a novel interpretable multi-omics data integration method (DeepKEGG) for cancer recurrence prediction and biomarker discovery. In DeepKEGG, a biological hierarchical module is designed for local connections of neuron nodes and model interpretability based on the biological relationship between genes/miRNAs and pathways. In addition, a pathway self-attention module is constructed to explore the correlation between different samples and generate the potential pathway feature representation for enhancing the prediction performance of the model. Lastly, an attribution-based feature importance calculation method is utilized to discover biomarkers related to cancer recurrence and provide a biological interpretation of the model. Experimental results demonstrate that DeepKEGG outperforms other state-of-the-art methods in 5-fold cross validation. Furthermore, case studies also indicate that DeepKEGG serves as an effective tool for biomarker discovery. The code is available at https://github.com/lanbiolab/DeepKEGG.","['deep learning-based multi-omics data integration methods', 'attribution-based feature importance calculation method']","The study addresses the challenge of understanding cancer development by integrating multi-omics data, with a focus on revealing mechanisms of cancer recurrence and identifying relevant biomarkers. It highlights the limitations of current approaches that overlook potential correlations between samples and struggle to provide accurate biological explanations due to the complexity of existing models. The primary aim of the study is to develop a method for multi-omics data integration that explores the correlations between samples and enhances interpretability to improve cancer recurrence prediction and biomarker discovery. This approach seeks to provide meaningful biological insights into cancer recurrence and facilitate the identification of pathogenic targets.","The study addresses the challenge of understanding cancer development by integrating multi-omics data, with a focus on revealing mechanisms of cancer recurrence and identifying relevant biomarkers. It highlights the limitations of current approaches that overlook potential correlations between samples and struggle to provide accurate biological explanations due to the complexity of existing analytical frameworks. The primary aim of the study is to develop a method for multi-omics data integration that explores the correlations between samples and enhances interpretability to improve cancer recurrence prediction and biomarker discovery. This approach seeks to provide meaningful biological insights into cancer recurrence and facilitate the identification of pathogenic targets.",True
Biology,Optimizing cancer classification: a hybrid RDO-XGBoost approach for feature selection and predictive insights,"The identification of relevant biomarkers from high-dimensional cancer data remains a significant challenge due to the complexity and heterogeneity inherent in various cancer types. Conventional feature selection methods often struggle to effectively navigate the vast solution space while maintaining high predictive accuracy. In response to these challenges, we introduce a novel feature selection approach that integrates Random Drift Optimization (RDO) with XGBoost, specifically designed to enhance the performance of cancer classification tasks. Our proposed framework not only improves classification accuracy but also offers valuable insights into the underlying biological mechanisms driving cancer progression. Through comprehensive experiments conducted on real-world cancer datasets, including Central Nervous System (CNS), Leukemia, Breast, and Ovarian cancers, we demonstrate the efficacy of our method in identifying a smaller subset of unique and relevant genes. This selection results in significantly improved classification efficiency and accuracy. When compared with popular classifiers such as Support Vector Machine, K-Nearest Neighbor, and Naive Bayes, our approach consistently outperforms these models in terms of both accuracy and F-measure metrics. For instance, our framework achieved an accuracy of 97.24% in the CNS dataset, 99.14% in Leukemia, 95.21% in Ovarian, and 87.62% in Breast cancer, showcasing its robustness and effectiveness across different types of cancer data. These results underline the potential of our RDO-XGBoost framework as a promising solution for feature selection in cancer data analysis, offering enhanced predictive performance and valuable biological insights.","['XGBoost', 'Support Vector Machine', 'K-Nearest Neighbor', 'Naive Bayes']","The identification of relevant biomarkers from high-dimensional cancer data remains a significant challenge due to the complexity and heterogeneity inherent in various cancer types. Conventional methods often struggle to effectively navigate the vast solution space while maintaining high accuracy in cancer classification. The primary aim of this study is to improve the identification of a smaller subset of unique and relevant genes that can enhance classification efficiency and accuracy across different types of cancer, including Central Nervous System, Leukemia, Breast, and Ovarian cancers. This objective seeks to provide valuable insights into the underlying biological mechanisms driving cancer progression and to demonstrate the effectiveness of the proposed approach in cancer data analysis.","The identification of relevant biomarkers from high-dimensional cancer data remains a significant challenge due to the complexity and heterogeneity inherent in various cancer types. Conventional approaches often struggle to effectively navigate the vast solution space while maintaining high accuracy in cancer classification. The primary aim of this study is to improve the identification of a smaller subset of unique and relevant genes that can enhance classification efficiency and accuracy across different types of cancer, including Central Nervous System, Leukemia, Breast, and Ovarian cancers. This objective seeks to provide valuable insights into the underlying biological mechanisms driving cancer progression and to demonstrate the effectiveness of the proposed methodology in cancer data analysis.",True
Biology,Cross-Domain Class Incremental Broad Network for Continuous Diagnosis of Rotating Machinery Faults Under Variable Operating Conditions,"Machine learning models have been widely successful in the field of intelligent fault diagnosis. Most of the existing machine learning models are deployed in static environments and rely on precollected datasets for offline training, which makes it impossible to update the models further once they are established. However, in the open and dynamic environment in reality, there is always incoming data in the form of streams, including new categories of data that are constantly generated over time. In addition, the operating conditions of mechanical equipment are time-varying, which results in continuous stream data that are nonindependently and homogeneously distributed. In industrial applications, the diagnosis problem of nonindependent and identically distributed continuous streaming data is referred to as the cross-domain class incremental diagnosis problem. To address the cross-domain class incremental problem, a novel cross-domain class incremental broad network (CDCIBN) is proposed. Specifically, to solve the nonindependent identically distributed problem, a novel domain-adaptation learning loss function is first designed, which enables the conventional broad network to handle the category increment task well. Then, a cross-domain class incremental learning mechanism is designed, which learns new categories while retaining the knowledge of old categories well enough without replaying old category data. The effectiveness of the proposed method is evaluated through multiple mechanical failure increment cases. Experimental analysis demonstrates that the designed CDCIBN has significant advantages in the variable working condition class incremental application.",['cross-domain class incremental learning mechanism'],"The research idea addresses the challenge of diagnosing faults in mechanical equipment operating under dynamic and continuously changing conditions, where data are received as streams that include new and evolving categories over time. Traditional approaches rely on static datasets and cannot adapt to new information once established, which limits their effectiveness in real-world environments with time-varying operating conditions and nonindependently distributed data. The study focuses on the problem of diagnosing faults when data distributions change across different domains and new fault categories emerge incrementally. The primary objective of the study is to develop a method that can effectively handle the diagnosis of mechanical faults in such cross-domain, class-incremental scenarios by learning new fault categories while retaining knowledge of previously learned categories, without requiring access to old data. The aim is to improve fault diagnosis performance under variable working conditions and continuous data streams in industrial applications.","The research idea addresses the challenge of diagnosing faults in mechanical equipment operating under dynamic and continuously changing conditions, where data are received as streams that include new and evolving categories over time. Traditional approaches rely on static datasets and cannot adapt to new information once established, which limits their effectiveness in real-world environments with time-varying operating conditions and nonindependently distributed data. The study focuses on the problem of diagnosing faults when data distributions change across different domains and new fault categories emerge incrementally. The primary objective of the study is to develop a method that can effectively handle the diagnosis of mechanical faults in such cross-domain, class-incremental scenarios by recognizing new fault categories while retaining knowledge of previously identified categories, without requiring access to old data. The aim is to improve fault diagnosis performance under variable working conditions and continuous data streams in industrial applications.",True
Biology,Enhancing MPPT performance for partially shaded photovoltaic arrays through backstepping control with Genetic Algorithm-optimized gains,"As the significance and complexity of solar panel performance, particularly at their maximum power point (MPP), continue to grow, there is a demand for improved monitoring systems. The presence of variable weather conditions in Maroua, including potential partial shadowing caused by cloud cover or urban buildings, poses challenges to the efficiency of solar systems. This study introduces a new approach to tracking the Global Maximum Power Point (GMPP) in photovoltaic systems within the context of solar research conducted in Cameroon. The system utilizes Genetic Algorithm (GA) and Backstepping Controller (BSC) methodologies. The Backstepping Controller (BSC) dynamically adjusts the duty cycle of the Single Ended Primary Inductor Converter (SEPIC) to align with the reference voltage of the Genetic Algorithm (GA) in Maroua's dynamic environment. This environment, characterized by intermittent sunlight and the impact of local factors and urban shadowing, affects the production of energy. The Genetic Algorithm is employed to enhance the efficiency of BSC gains in Maroua's solar environment. This optimization technique expedites the tracking process and minimizes oscillations in the GMPP. The adaptability of the learning algorithm to specific conditions improves energy generation, even in the challenging environment of Maroua. This study introduces a novel approach to enhance the efficiency of photovoltaic systems in Maroua, Cameroon, by tailoring them to the specific solar dynamics of the region. In terms of performance, our approach surpasses the INC-BSC, P&O-BSC, GA-BSC, and PSO-BSC methodologies. In practice, the stabilization period following shadowing typically requires fewer than three iterations. Additionally, our Maximum Power Point Tracking (MPPT) technology is based on the Global Maximum Power Point (GMPP) methodology, contrasting with alternative technologies that prioritize the Local Maximum Power Point (LMPP). This differentiation is particularly relevant in areas with partial shading, such as Maroua, where the use of LMPP-based technologies can result in power losses. The proposed method demonstrates significant performance by achieving a minimum 33% reduction in power losses.",['Genetic Algorithm (GA)'],"The research idea centers on addressing the challenges posed by variable weather conditions and partial shadowing from cloud cover or urban buildings on the efficiency of solar panel performance, particularly in the context of Maroua, Cameroon. The complexity of maintaining optimal energy production at the maximum power point (MPP) under these dynamic environmental factors motivates the need for improved approaches tailored to the specific solar dynamics of the region. The primary objective of the study is to enhance the efficiency of photovoltaic systems in Maroua by developing a novel approach for tracking the Global Maximum Power Point (GMPP) that reduces power losses caused by partial shading. This approach aims to improve energy generation stability and performance in the region’s challenging solar environment, achieving significant reductions in power losses compared to existing methods.","The research idea centers on addressing the challenges posed by variable weather conditions and partial shadowing from cloud cover or urban buildings on the efficiency of solar panel performance, particularly in the context of Maroua, Cameroon. The complexity of maintaining optimal energy production at the maximum power point (MPP) under these dynamic environmental factors motivates the need for improved approaches tailored to the specific solar dynamics of the region. The primary objective of the study is to enhance the efficiency of photovoltaic systems in Maroua by developing an innovative method for tracking the Global Maximum Power Point (GMPP) that reduces power losses caused by partial shading. This approach aims to improve energy generation stability and performance in the region's challenging solar environment, achieving significant reductions in power losses compared to existing methods.",True
Biology,GAM-MDR: probing miRNA–drug resistance using a graph autoencoder based on random path masking,"Abstract MicroRNAs (miRNAs) are found ubiquitously in biological cells and play a pivotal role in regulating the expression of numerous target genes. Therapies centered around miRNAs are emerging as a promising strategy for disease treatment, aiming to intervene in disease progression by modulating abnormal miRNA expressions. The accurate prediction of miRNA–drug resistance (MDR) is crucial for the success of miRNA therapies. Computational models based on deep learning have demonstrated exceptional performance in predicting potential MDRs. However, their effectiveness can be compromised by errors in the data acquisition process, leading to inaccurate node representations. To address this challenge, we introduce the GAM-MDR model, which combines the graph autoencoder (GAE) with random path masking techniques to precisely predict potential MDRs. The reliability and effectiveness of the GAM-MDR model are mainly reflected in two aspects. Firstly, it efficiently extracts the representations of miRNA and drug nodes in the miRNA–drug network. Secondly, our designed random path masking strategy efficiently reconstructs critical paths in the network, thereby reducing the adverse impact of noisy data. To our knowledge, this is the first time that a random path masking strategy has been integrated into a GAE to infer MDRs. Our method was subjected to multiple validations on public datasets and yielded promising results. We are optimistic that our model could offer valuable insights for miRNA therapeutic strategies and deepen the understanding of the regulatory mechanisms of miRNAs. Our data and code are publicly available at GitHub:https://github.com/ZZCrazy00/GAM-MDR.","['deep learning', 'graph autoencoder (GAE)']","The study addresses the critical role of microRNAs (miRNAs) in regulating gene expression and the emerging potential of miRNA-centered therapies for disease treatment by modulating abnormal miRNA expressions. A key challenge in advancing these therapies is the accurate prediction of miRNA–drug resistance (MDR), which is essential for their success. The primary aim of the study is to improve the prediction of potential miRNA–drug resistance to support the development of effective miRNA therapeutic strategies. This objective seeks to enhance the understanding of the regulatory mechanisms of miRNAs and contribute valuable insights for intervening in disease progression.","The study addresses the critical role of microRNAs (miRNAs) in regulating gene expression and the emerging potential of miRNA-centered therapies for disease treatment by modulating abnormal miRNA expressions. A key challenge in advancing these therapies is the accurate identification of miRNA–drug resistance (MDR), which is essential for their success. The primary aim of the study is to improve the determination of potential miRNA–drug resistance to support the development of effective miRNA therapeutic strategies. This objective seeks to enhance the understanding of the regulatory mechanisms of miRNAs and contribute valuable insights for intervening in disease progression.",True
Biology,Integrative analysis of AI-driven optimization in HIV treatment regimens,"The integration of artificial intelligence (AI) into HIV treatment regimens has revolutionized the approach to personalized care and optimization strategies. This study presents an in-depth analysis of the role of AI in transforming HIV treatment, focusing on its ability to tailor therapy to individual patient needs and enhance treatment outcomes. AI-driven optimization in HIV treatment involves the utilization of advanced algorithms and computational techniques to analyze vast amounts of patient data, including genetic information, viral load measurements, and treatment history. By harnessing the power of machine learning and predictive analytics, AI algorithms can identify patterns and trends in patient data that may not be readily apparent to human clinicians. One of the key benefits of AI-driven optimization is its ability to personalize treatment regimens based on individual patient characteristics and disease progression. By considering factors such as drug resistance profiles, comorbidities, and lifestyle factors, AI algorithms can recommend the most effective and well-tolerated treatment options for each patient, leading to improved adherence and clinical outcomes. Furthermore, AI enables continuous monitoring and adjustment of treatment regimens in real time, allowing healthcare providers to respond rapidly to changes in patient status and evolving viral dynamics. This proactive approach to HIV management can help prevent treatment failure and the development of drug resistance, ultimately leading to better long-term outcomes for patients. Despite its transformative potential, AI-driven optimization in HIV treatment is not without challenges. Ethical considerations, data privacy concerns, and the need for robust validation and regulatory oversight are all important factors that must be addressed to ensure the safe and effective implementation of AI algorithms in clinical practice. In conclusion, the integrative analysis presented in this study underscores the significant impact of AI-driven optimization on the personalization and optimization of HIV treatment regimens. By leveraging AI technologies, healthcare providers can tailor treatment approaches to individual patient needs, leading to improved outcomes and quality of life for people living with HIV. Keywords: Integrative Analysis, AI- Driven, Optimization, HIV Treatment, Regimens.",['machine learning'],"The study addresses the challenge of improving HIV treatment by focusing on the need for personalized care that accounts for individual patient characteristics and disease progression. It highlights the importance of tailoring therapy to optimize treatment outcomes and manage factors such as drug resistance, comorbidities, and lifestyle influences. The research aims to explore how treatment regimens can be customized to enhance adherence and clinical results for people living with HIV. Specifically, the study’s primary objective is to examine approaches that enable continuous monitoring and adjustment of HIV therapies in response to changes in patient status and viral dynamics, ultimately preventing treatment failure and improving long-term patient outcomes.","The study addresses the challenge of improving HIV treatment by focusing on the need for personalized care that accounts for individual patient characteristics and disease progression. It highlights the importance of tailoring therapy to optimize treatment outcomes and manage factors such as drug resistance, comorbidities, and lifestyle influences. The research aims to explore how treatment regimens can be customized to enhance adherence and clinical results for people living with HIV. Specifically, the study's primary objective is to examine approaches that enable continuous monitoring and adjustment of HIV therapies in response to changes in patient status and viral dynamics, ultimately preventing treatment failure and improving long-term patient outcomes.",True
Biology,Deep Learning-Based Mask Identification System Using ResNet Transfer Learning Architecture,"Recently, the coronavirus disease 2019 has shown excellent attention in the global community regarding health and the economy.World Health Organization (WHO) and many others advised controlling Corona Virus Disease in 2019.The limited treatment resources, medical resources, and unawareness of immunity is an essential horizon to unfold.Among all resources, wearing a mask is the primary non-pharmaceutical intervention to stop the spreading of the virus caused by Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) droplets.All countries made masks mandatory to prevent infection.For such enforcement, automatic and effective face detection systems are crucial.This study presents a face mask identification approach for static photos and real-time movies that distinguishes between images with and without masks.To contribute to society, we worked on mask detection of an individual to adhere to the rule and provide awareness to the public or organization.The paper aims to get detection accuracy using transfer learning from Residual Neural Network 50 (ResNet-50) architecture and works on detection localization.The experiment is tested with other popular pre-trained models such as Deep Convolutional Neural Networks (AlexNet), Residual Neural Networks (ResNet), and Visual Geometry Group Networks (VGG-Net) advanced architecture.The proposed system generates an accuracy of 98.4% when modeled using Residual Neural Network 50 (ResNet-50).Also, the precision and recall values are proved as better when compared to the existing models.This outstanding work also can be used in video surveillance applications.","['transfer learning', 'Residual Neural Network 50 (ResNet-50)', 'Deep Convolutional Neural Networks (AlexNet)', 'Residual Neural Networks (ResNet)', 'Visual Geometry Group Networks (VGG-Net)']","The research idea centers on addressing the global health challenge posed by the coronavirus disease 2019 (COVID-19), particularly focusing on the importance of wearing masks as a primary non-pharmaceutical intervention to prevent the spread of the virus caused by SARS-CoV-2 droplets. Given the limited treatment and medical resources, as well as public unawareness regarding immunity, enforcing mask-wearing has become essential worldwide. The study aims to support this enforcement by identifying whether individuals are wearing masks, thereby contributing to public health awareness and compliance with safety measures. The primary objective of the study is to develop an approach that can accurately distinguish between images of individuals with and without masks in both static photos and real-time videos, facilitating adherence to mask-wearing rules and enhancing public or organizational awareness.","The research idea centers on addressing the global health challenge posed by the coronavirus disease 2019 (COVID-19), particularly focusing on the importance of wearing masks as a primary non-pharmaceutical intervention to prevent the spread of the virus caused by SARS-CoV-2 droplets. Given the limited treatment and medical resources, as well as public unawareness regarding immunity, enforcing mask-wearing has become essential worldwide. The study aims to support this enforcement by identifying whether individuals are wearing masks, thereby contributing to public health awareness and compliance with safety measures. The primary objective of the study is to develop a method that can accurately distinguish between images of individuals with and without masks in both static photos and real-time videos, facilitating adherence to mask-wearing rules and enhancing public or organizational awareness.",True
Biology,A hyperspectral deep learning attention model for predicting lettuce chlorophyll content,"Abstract Background The phenotypic traits of leaves are the direct reflection of the agronomic traits in the growth process of leafy vegetables, which plays a vital role in the selection of high-quality leafy vegetable varieties. The current image-based phenotypic traits extraction research mainly focuses on the morphological and structural traits of plants or leaves, and there are few studies on the phenotypes of physiological traits of leaves. The current research has developed a deep learning model aimed at predicting the total chlorophyll of greenhouse lettuce directly from the full spectrum of hyperspectral images. Results A CNN-based one-dimensional deep learning model with spectral attention module was utilized for the estimate of the total chlorophyll of greenhouse lettuce from the full spectrum of hyperspectral images. Experimental results demonstrate that the deep neural network with spectral attention module outperformed the existing standard approaches, including partial least squares regression (PLSR) and random forest (RF), with an average R 2 of 0.746 and an average RMSE of 2.018. Conclusions This study unveils the capability of leveraging deep attention networks and hyperspectral imaging for estimating lettuce chlorophyll levels. This approach offers a convenient, non-destructive, and effective estimation method for the automatic monitoring and production management of leafy vegetables.","['CNN-based one-dimensional deep learning model with spectral attention module', 'partial least squares regression (PLSR)', 'random forest (RF)']","The phenotypic traits of leaves directly reflect the agronomic traits during the growth process of leafy vegetables and play a vital role in selecting high-quality leafy vegetable varieties. While most research has focused on morphological and structural traits, there is a lack of studies on the physiological phenotypes of leaves, such as chlorophyll content. The primary aim of this study is to develop a method for predicting the total chlorophyll content of greenhouse lettuce leaves using spectral information. This approach seeks to provide a convenient, non-destructive, and effective way to estimate chlorophyll levels for automatic monitoring and production management of leafy vegetables.","The phenotypic traits of leaves directly reflect the agronomic traits during the growth process of leafy vegetables and play a vital role in selecting high-quality leafy vegetable varieties. While most research has focused on morphological and structural traits, there is a lack of studies on the physiological phenotypes of leaves, such as chlorophyll content. The primary aim of this study is to develop a method for estimating the total chlorophyll content of greenhouse lettuce leaves using spectral information. This approach seeks to provide a convenient, non-destructive, and effective way to estimate chlorophyll levels for monitoring and production management of leafy vegetables.",True
Biology,"A Comprehensive Survey of Continual Learning: Theory, Method and Application","To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance drop of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efficiency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative strategies address continual learning, and how they are adapted to particular challenges in various applications. Through an in-depth discussion of promising directions, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this field and beyond.",['continual learning'],"The research idea centers on the challenge of continual learning, which involves the ability to incrementally acquire, update, accumulate, and exploit knowledge over time while coping with real-world dynamics. A key problem addressed is catastrophic forgetting, where learning new information causes a significant decline in performance on previously learned tasks. The study highlights the growing importance and complexity of understanding continual learning in biological and cognitive contexts. The primary objective of the study is to comprehensively summarize and analyze the fundamental principles, challenges, and strategies related to continual learning, aiming to ensure a proper balance between stability and adaptability, as well as effective generalization across tasks within resource constraints.","The research idea centers on the challenge of continual learning, which involves the ability to incrementally acquire, update, accumulate, and exploit knowledge over time while coping with real-world dynamics. A key problem addressed is knowledge retention, where acquiring new information causes a significant decline in performance on previously learned tasks. The study highlights the growing importance and complexity of understanding continual learning in biological and cognitive contexts. The primary objective of the study is to comprehensively summarize and analyze the fundamental principles, challenges, and strategies related to continual learning, aiming to ensure a proper balance between stability and adaptability, as well as effective generalization across tasks within resource constraints.",True
Biology,Chatbots and Large Language Models in Radiology: A Practical Primer for Clinical and Research Applications,"Although chatbots have existed for decades, the emergence of transformer-based large language models (LLMs) has captivated the world through the most recent wave of artificial intelligence chatbots, including ChatGPT. Transformers are a type of neural network architecture that enables better contextual understanding of language and efficient training on massive amounts of unlabeled data, such as unstructured text from the internet. As LLMs have increased in size, their improved performance and emergent abilities have revolutionized natural language processing. Since language is integral to human thought, applications based on LLMs have transformative potential in many industries. In fact, LLM-based chatbots have demonstrated human-level performance on many professional benchmarks, including in radiology. LLMs offer numerous clinical and research applications in radiology, several of which have been explored in the literature with encouraging results. Multimodal LLMs can simultaneously interpret text and images to generate reports, closely mimicking current diagnostic pathways in radiology. Thus, from requisition to report, LLMs have the opportunity to positively impact nearly every step of the radiology journey. Yet, these impressive models are not without limitations. This article reviews the limitations of LLMs and mitigation strategies, as well as potential uses of LLMs, including multimodal models. Also reviewed are existing LLM-based applications that can enhance efficiency in supervised settings.","['transformer-based large language models (LLMs)', 'Transformers', 'Multimodal LLMs']","The research idea centers on the transformative potential of advanced language models in the field of radiology, highlighting their ability to understand and generate human-like language and to interpret both text and images in a manner that closely mimics current diagnostic workflows. This advancement offers the possibility to positively impact nearly every step of the radiology process, from requisition to report generation, thereby enhancing clinical and research applications. The study acknowledges that despite these promising capabilities, there are notable limitations that need to be addressed. The primary objective of the study is to review the limitations and mitigation strategies of these language models, explore their potential uses including multimodal applications, and examine existing implementations that can improve efficiency within supervised radiology settings.","The research idea centers on the transformative potential of advanced interpretive systems in the field of radiology, highlighting their ability to understand and process information and to interpret both text and images in a manner that aligns with current diagnostic workflows. This advancement offers the possibility to positively impact nearly every step of the radiology process, from requisition to report generation, thereby enhancing clinical and research applications. The study acknowledges that despite these promising capabilities, there are notable limitations that need to be addressed. The primary objective of the study is to review the limitations and mitigation strategies of these interpretive systems, explore their potential uses including multimodal applications, and examine existing implementations that can improve efficiency within supervised radiology settings.",True
Biology,Fingerprint Based Gender Classification,"Male-female classification from a fingerprint is an important step in forensic science, anthropological and medical studies to reduce the efforts required for searching a person.The aim of this research is to establish a relationship between gender and the fingerprint using some special features such as ridge density, ridge thickness to valley thickness ratio (RTVTR) and ridge width.showed that male-female classification can be done correctly up to 88.5% based on white lines count, RTVTR & ridge count using Neural Network as Classifier.We have used RTVTR, ridge width and ridge density for classification and SVM as classifier.We have found male-female can be correctly classified up to 91%.Gender classification plays an active role in several applications such as biometrics, criminology, surveillance, human computer interaction, commercial profiling.Though biometric traits such as face, gait, iris and hand shape are used for gender classification in the past, majority of the work is based on face as it contains more prominent features than others.In this paper we have analyzed fingerprints for gender classification with a hope that it has great potential for future research.We have employed a three convolutional layer CNN with rectified linear and activation functions on NIST database which contains a set of 4000 images and achieved 99% accuracy.Performance of the proposed system demonstrated that fingerprints contains vital features to discriminate gender of a person.Humans have distinctive and unique traits which can be used to distinguish them thus, acting as a form of identification.Biometrics identify people by measuring some aspect of individual's anatomy or physiology such as hand geometry or fingerprint which consists of a pattern of interleaved ridges and valleys.The year 2015 election in Nigeria was greeted by some petitions including under-aged voters.The need for an age and gender detector system is a major concern for organizations at all levels where integrity of information cannot be compromised.This work developed a system that determines human age-range and gender using fingerprint analysis trained with Back Propagation Neural Network (for gender classification) and DWT+PCA (for age classification).A total of 280 fingerprint samples of people with various age and gender were collected.140 of these samples were used for training the system""s Database; 70 males and 70 females respectively.This was done for age groups 1-10, 11-20, 21-30, 31-40, 41-50, 51-60 and 61-70 accordingly.In order to determine the gender of an individual, the Ridge Thickness Valley Thickness Ratio (RTVTR) of the person was put into consideration.Result showed 80.00 % classification accuracy for females and 72.86 % for males while 115 subjects out of 140 (82.14%)were correctly classified in age.","['Neural Network', 'SVM', 'CNN', 'Back Propagation Neural Network']","The research addresses the importance of accurately classifying male and female individuals based on fingerprint characteristics, which is a significant task in forensic science, anthropological, and medical studies to streamline the identification process. It highlights the potential of fingerprint features such as ridge density and ridge thickness to valley thickness ratio (RTVTR) as reliable indicators for gender determination, emphasizing the need for effective gender classification methods in various applications including biometrics and criminology. The primary aim of the study is to establish a clear relationship between gender and fingerprint features by analyzing specific characteristics like ridge density, RTVTR, and ridge width to improve the accuracy of male-female classification. Additionally, the research seeks to develop a system capable of determining both age range and gender from fingerprint samples, thereby contributing to the integrity of identification processes in contexts where accurate demographic information is crucial.","The research addresses the importance of accurately classifying male and female individuals based on fingerprint characteristics, which is a significant task in forensic science, anthropological, and medical studies to streamline the identification process. It highlights the potential of fingerprint features such as ridge density and ridge thickness to valley thickness ratio (RTVTR) as reliable indicators for gender determination, emphasizing the need for effective gender classification methods in various applications including biometrics and criminology. The primary aim of the study is to establish a clear relationship between gender and fingerprint features by analyzing specific characteristics like ridge density, RTVTR, and ridge width to improve the accuracy of male-female differentiation. Additionally, the research seeks to develop a methodology capable of determining both age range and gender from fingerprint samples, thereby contributing to the integrity of identification processes in contexts where accurate demographic information is crucial.",True
Biology,Deep-STP: a deep learning-based approach to predict snake toxin proteins by using word embeddings,"Snake venom contains many toxic proteins that can destroy the circulatory system or nervous system of prey. Studies have found that these snake venom proteins have the potential to treat cardiovascular and nervous system diseases. Therefore, the study of snake venom protein is conducive to the development of related drugs. The research technologies based on traditional biochemistry can accurately identify these proteins, but the experimental cost is high and the time is long. Artificial intelligence technology provides a new means and strategy for large-scale screening of snake venom proteins from the perspective of computing. In this paper, we developed a sequence-based computational method to recognize snake toxin proteins. Specially, we utilized three different feature descriptors, namely g-gap , natural vector and word 2 vector, to encode snake toxin protein sequences. The analysis of variance (ANOVA), gradient-boost decision tree algorithm (GBDT) combined with incremental feature selection (IFS) were used to optimize the features, and then the optimized features were input into the deep learning model for model training. The results show that our model can achieve a prediction performance with an accuracy of 82.00% in 10-fold cross-validation. The model is further verified on independent data, and the accuracy rate reaches to 81.14%, which demonstrated that our model has excellent prediction performance and robustness.","['gradient-boost decision tree algorithm (GBDT)', 'incremental feature selection (IFS)', 'deep learning model']","The research idea centers on the fact that snake venom contains many toxic proteins capable of destroying the circulatory or nervous systems of prey, and these proteins have potential therapeutic applications for treating cardiovascular and nervous system diseases. Studying snake venom proteins is important for the development of related drugs. The primary objective of the study is to accurately identify snake venom proteins to facilitate drug development, addressing the limitations of traditional biochemical methods that are costly and time-consuming. This study aims to improve the recognition of snake toxin proteins to support large-scale screening efforts.","The research idea centers on the fact that snake venom contains many toxic proteins capable of destroying the circulatory or nervous systems of prey, and these proteins have potential therapeutic applications for treating cardiovascular and nervous system diseases. Studying snake venom proteins is important for the development of related drugs. The primary objective of the study is to accurately identify snake venom proteins to facilitate drug development, addressing the limitations of traditional biochemical methods that are costly and time-consuming. This study aims to improve the identification and characterization of snake toxin proteins to support large-scale screening efforts.",True
Biology,Can large language models replace humans in systematic reviews? Evaluating <scp>GPT</scp>‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages,"Systematic reviews are vital for guiding practice, research and policy, although they are often slow and labour-intensive. Large language models (LLMs) could speed up and automate systematic reviews, but their performance in such tasks has yet to be comprehensively evaluated against humans, and no study has tested Generative Pre-Trained Transformer (GPT)-4, the biggest LLM so far. This pre-registered study uses a ""human-out-of-the-loop"" approach to evaluate GPT-4's capability in title/abstract screening, full-text review and data extraction across various literature types and languages. Although GPT-4 had accuracy on par with human performance in some tasks, results were skewed by chance agreement and dataset imbalance. Adjusting for these caused performance scores to drop across all stages: for data extraction, performance was moderate, and for screening, it ranged from none in highly balanced literature datasets (~1:1) to moderate in those datasets where the ratio of inclusion to exclusion in studies was imbalanced (~1:3). When screening full-text literature using highly reliable prompts, GPT-4's performance was more robust, reaching ""human-like"" levels. Although our findings indicate that, currently, substantial caution should be exercised if LLMs are being used to conduct systematic reviews, they also offer preliminary evidence that, for certain review tasks delivered under specific conditions, LLMs can rival human performance.",['Generative Pre-Trained Transformer (GPT)-4'],"The research idea centers on the importance of systematic reviews for guiding practice, research, and policy, while highlighting the challenges posed by their slow and labor-intensive nature. The study addresses the need to evaluate new approaches that could potentially accelerate and automate the process of conducting systematic reviews. The primary objective of the study is to comprehensively assess the capability of a large language model, GPT-4, in performing key tasks involved in systematic reviews, including title/abstract screening, full-text review, and data extraction across various types of literature and languages. The study aims to determine how well GPT-4’s performance compares to human reviewers in these tasks and to identify the conditions under which it may achieve human-like accuracy.","The research idea centers on the importance of systematic reviews for guiding practice, research, and policy, while highlighting the challenges posed by their slow and labor-intensive nature. The study addresses the need to evaluate new approaches that could potentially accelerate and streamline the process of conducting systematic reviews. The primary objective of the study is to comprehensively assess the capability of advanced text analysis tools in performing key tasks involved in systematic reviews, including title/abstract screening, full-text review, and data extraction across various types of literature and languages. The study aims to determine how well these tools' performance compares to human reviewers in these tasks and to identify the conditions under which they may achieve human-like accuracy.",True
Biology,MemBrain v2: an end-to-end tool for the analysis of membranes in cryo-electron tomography,"A bstract MemBrain v2 is a deep learning-enabled program aimed at the efficient analysis of membranes in cryo-electron tomography (cryo-ET). The final v2 release of MemBrain will comprise three main modules: 1) MemBrain-seg, which provides automated membrane segmentation, 2) MemBrain-pick, which provides automated picking of particles along segmented membranes, and 3) MemBrain-stats, which provides quantitative statistics of particle distributions and membrane morphometrics. This initial version of the manuscript is focused on the beta release of MemBrain-seg, which combines iterative training with diverse data and specialized Fourier-based data augmentations. These augmentations are specifically designed to enhance the tool’s adaptability to a variety of tomographic data and address common challenges in cryo-ET analysis. A key feature of MemBrain-seg is the implementation of the Surface-Dice loss function, which improves the network’s focus on membrane connectivity and allows for the effective incorporation of manual annotations from different sources. This function is beneficial in handling the variability inherent in membrane structures and annotations. Our ongoing collaboration with the cryo-ET community plays an important role in continually improving MemBrain v2 with a wide array of training data. This collaborative approach ensures that MemBrain v2 remains attuned to the field’s needs, enhancing its robustness and generalizability across different types of tomographic data. The current version of MemBrain-seg is available at https://github.com/teamtomo/membrain-seg , and the predecessor of MemBrain-pick (also called MemBrain v1) is deposited at https://github.com/CellArchLab/MemBrain . This preprint will be updated concomitantly with the code until the three integrated modules of MemBrain v2 are complete.","['deep learning', 'Surface-Dice loss function']","The research addresses the challenge of accurately analyzing membranes in cryo-electron tomography (cryo-ET), which is complicated by the variability and complexity of membrane structures and the need for reliable segmentation and particle identification along membranes. Efficient and precise membrane analysis is crucial for understanding membrane morphology and the distribution of particles associated with membranes in biological samples. The primary aim of the study is to develop and improve a tool that enables automated and robust membrane segmentation, particle picking along membranes, and quantitative assessment of particle distributions and membrane morphometrics in cryo-ET data. This version of the study focuses on enhancing membrane segmentation by incorporating strategies that improve the handling of membrane connectivity and variability in manual annotations, thereby supporting more accurate and adaptable analysis of membrane structures.","The research addresses the challenge of accurately analyzing membranes in cryo-electron tomography (cryo-ET), which is complicated by the variability and complexity of membrane structures and the need for reliable segmentation and particle identification along membranes. Efficient and precise membrane analysis is crucial for understanding membrane morphology and the distribution of particles associated with membranes in biological samples. The primary aim of the study is to develop and improve a tool that enables systematic and robust membrane segmentation, particle picking along membranes, and quantitative assessment of particle distributions and membrane morphometrics in cryo-ET data. This version of the study focuses on enhancing membrane segmentation by incorporating strategies that improve the handling of membrane connectivity and variability in manual annotations, thereby supporting more accurate and adaptable analysis of membrane structures.",True
Biology,FI-NPI: Exploring Optimal Control in Parallel Platform Systems,"Typically, the current and speed loop closure of servo motor of the parallel platform is accomplished with incremental PI regulation. The control method has strong robustness, but the parameter tuning process is cumbersome, and it is difficult to achieve the optimal control state. In order to further optimize the performance, this paper proposes a double-loop control structure based on fuzzy integral and neuron proportional integral (FI-NPI). The structure makes full use of the control advantages of the fuzzy controller and integrator to improve the performance of speed closed-loop control. And through the feedforward branch, the speed error is used as the teacher signal for neuron supervised learning, which improves the effect of current closed-loop control. Through comparative simulation experiments, this paper verifies that the FI-NPI controller has a faster dynamic response speed than the traditional PI controller. Finally, in this paper, the FI-NPI controller is implemented in C language in the servo-driven lower computer, and the speed closed-loop test of the BLDC motor is carried out. The experimental results show that the FI-NPI double-loop controller is better than the traditional double-PI controller in performance indicators such as convergence rate and RMSE, which confirms that the FI-NPI double-loop controller is more suitable for BLDC servo control.",['fuzzy integral'],"The research addresses the challenge of optimizing the current and speed loop closure of servo motors in parallel platforms, where traditional incremental PI regulation methods, despite their robustness, involve a cumbersome parameter tuning process and struggle to achieve optimal control performance. The study is motivated by the need to enhance the control performance and dynamic response speed of servo motor systems, particularly for brushless DC (BLDC) motors. The primary objective of the study is to develop and evaluate a double-loop control structure that improves speed closed-loop control performance and current closed-loop control effectiveness, ultimately demonstrating superior performance compared to traditional double-PI controllers in terms of convergence rate and error metrics during BLDC motor speed closed-loop tests.","The research addresses the challenge of optimizing the current and speed loop closure of servo motors in parallel platforms, where traditional incremental PI regulation methods, despite their robustness, involve a cumbersome parameter adjustment process and struggle to achieve optimal control performance. The study is motivated by the need to enhance the control performance and dynamic response speed of servo motor systems, particularly for brushless DC (BLDC) motors. The primary objective of the study is to develop and evaluate a double-loop control structure that improves speed closed-loop control performance and current closed-loop control effectiveness, ultimately demonstrating superior performance compared to traditional double-PI controllers in terms of response rate and error measurements during BLDC motor speed closed-loop tests.",True
Biology,CIFAKE: Image Classification and Explainable Identification of AI-Generated Synthetic Images,"Recent advances in synthetic data have enabled the generation of images with such high quality that human beings cannot tell the difference between real-life photographs and Artificial Intelligence (AI) generated images. Given the critical necessity of data reliability and authentication, this article proposes to enhance our ability to recognise AI-generated images through computer vision. Initially, a synthetic dataset is generated that mirrors the ten classes of the already available CIFAR-10 dataset with latent diffusion, providing a contrasting set of images for comparison to real photographs. The model is capable of generating complex visual attributes, such as photorealistic reflections in water. The two sets of data present as a binary classification problem with regard to whether the photograph is real or generated by AI. This study then proposes the use of a Convolutional Neural Network (CNN) to classify the images into two categories; Real or Fake. Following hyperparameter tuning and the training of 36 individual network topologies, the optimal approach could correctly classify the images with 92.98% accuracy. Finally, this study implements explainable AI via Gradient Class Activation Mapping to explore which features within the images are useful for classification. Interpretation reveals interesting concepts within the image, in particular, noting that the actual entity itself does not hold useful information for classification; instead, the model focuses on small visual imperfections in the background of the images. The complete dataset engineered for this study, referred to as the CIFAKE dataset, is made publicly available to the research community for future work.","['latent diffusion', 'Convolutional Neural Network (CNN)', 'Gradient Class Activation Mapping']","The research idea addresses the challenge of distinguishing between real-life photographs and synthetic images that are generated with such high quality that they are indistinguishable to the human eye. Given the critical necessity of data reliability and authentication, the study focuses on improving the ability to recognize artificially generated images. The primary objective of the study is to develop a method to classify images into real or AI-generated categories by creating a synthetic dataset that mirrors existing photographic classes and analyzing visual features that differentiate real photographs from generated images. The study aims to identify specific visual attributes, particularly subtle imperfections in the background, that can be used to reliably distinguish between authentic and synthetic images.","The research idea addresses the challenge of distinguishing between real-life photographs and synthetic images that are generated with such high quality that they are indistinguishable to the human eye. Given the critical necessity of data reliability and authentication, the study focuses on improving the ability to recognize artificially generated images. The primary objective of the study is to develop a method to categorize images into real or synthetically-generated categories by creating a comparative dataset that mirrors existing photographic classes and analyzing visual features that differentiate real photographs from generated images. The study aims to identify specific visual attributes, particularly subtle imperfections in the background, that can be used to reliably distinguish between authentic and synthetic images.",True
Biology,Survival Prediction Across Diverse Cancer Types Using Neural Networks,"Gastric cancer and Colon adenocarcinoma represent widespread and challenging malignancies with high mortality rates and complex treatment landscapes. In response to the critical need for accurate prognosis in cancer patients, the medical community has embraced the 5-year survival rate as a vital metric for estimating patient outcomes. This study introduces a pioneering approach to enhance survival prediction models for gastric and Colon adenocarcinoma patients. Leveraging advanced image analysis techniques, we sliced whole slide images (WSI) of these cancers, extracting comprehensive features to capture nuanced tumor characteristics. Subsequently, we constructed patient-level graphs, encapsulating intricate spatial relationships within tumor tissues. These graphs served as inputs for a sophisticated 4-layer graph convolutional neural network (GCN), designed to exploit the inherent connectivity of the data for comprehensive analysis and prediction. By integrating patients' total survival time and survival status, we computed C-index values for gastric cancer and Colon adenocarcinoma, yielding 0.57 and 0.64, respectively. Significantly surpassing previous convolutional neural network models, these results underscore the efficacy of our approach in accurately predicting patient survival outcomes. This research holds profound implications for both the medical and AI communities, offering insights into cancer biology and progression while advancing personalized treatment strategies. Ultimately, our study represents a significant stride in leveraging AI-driven methodologies to revolutionize cancer prognosis and improve patient outcomes on a global scale.","['graph convolutional neural network (GCN)', 'convolutional neural network']","The research idea centers on addressing the high mortality rates and complex treatment challenges associated with gastric cancer and colon adenocarcinoma. There is a critical need for accurate prognosis in patients with these malignancies, with the 5-year survival rate serving as an essential metric for estimating patient outcomes. The study aims to improve the prediction of survival outcomes by capturing detailed tumor characteristics and spatial relationships within tumor tissues. The primary objective of the study is to enhance survival prediction for gastric and colon adenocarcinoma patients by developing a method that integrates comprehensive tumor features and patient survival data to provide more accurate estimates of patient prognosis, ultimately contributing to better-informed treatment strategies.","The research idea centers on addressing the high mortality rates and complex treatment challenges associated with gastric cancer and colon adenocarcinoma. There is a critical need for accurate prognosis in patients with these malignancies, with the 5-year survival rate serving as an essential metric for estimating patient outcomes. The study aims to improve the evaluation of survival outcomes by capturing detailed tumor characteristics and spatial relationships within tumor tissues. The primary objective of the study is to enhance survival assessment for gastric and colon adenocarcinoma patients by developing an approach that integrates comprehensive tumor features and patient survival data to provide more accurate estimates of patient prognosis, ultimately contributing to better-informed treatment strategies.",True
Biology,Aspect-based drug review classification through a hybrid model with ant colony optimization using deep learning,"Abstract The task of aspect-level sentiment analysis is intricately designed to determine the sentiment polarity directed towards a specific target within a sentence. With the increasing availability of online reviews and the growing importance of healthcare decisions, analyzing drug reviews has become a critical task. Traditional sentiment analysis, which categorizes a whole review as positive, negative, or neutral, provides limited insights for consumers and healthcare professionals. Aspect-based sentiment analysis (ABSA) aims to overcome these limitations by identifying and evaluating the sentiment associated with specific aspects or attributes of drugs mentioned in the reviews. Various fields, including business, politics, and medicine, have been explored in the context of sentiment analysis. Automation of online user reviews allows pharmaceutical companies to assess large amounts of user feedback. This helps extract pharmacological efficacy and side effect insights. The data collected could improve pharmacovigilance. Reviewing user comments can provide valuable data that can be used to improve drug safety and efficacy monitoring procedures. This improves pharmacovigilance processes, improving pharmaceutical outcomes understanding and corporate decision-making. Therefore, we propose a pre-trained RoBERTa with a Bi-LSTM model to categorise drug reviews from online sources and pre-process the text data. Ant Colony Optimization can be used in feature selection for ABSA, helping to identify the most relevant aspects and sentiments. Further, RoBERTa is fine-tuned to perform ABSA on the dataset, enabling the system to categorize aspects and determine the associated sentiment. The outcomes reveal that the suggested framework has achieved higher accuracy (96.78%) and F1 score (98.29%) on druglib.com, and 95.02% on the drugs.com dataset, than several prior state-of-the-art methods.","['pre-trained RoBERTa', 'Bi-LSTM model', 'fine-tuned RoBERTa']","The study addresses the challenge of understanding specific sentiments expressed towards different aspects of drugs within online reviews, which is important due to the increasing availability of such reviews and their impact on healthcare decisions. Traditional methods that classify entire reviews as positive, negative, or neutral offer limited insights for consumers and healthcare professionals, highlighting the need for more detailed sentiment evaluation related to particular drug attributes. This detailed understanding can enhance pharmacovigilance by providing valuable information on drug efficacy and side effects from user feedback. The primary aim of the study is to improve the categorization of drug reviews by identifying and evaluating sentiments associated with specific drug aspects mentioned in online user comments, thereby contributing to better monitoring of drug safety and efficacy.","The study addresses the challenge of understanding specific sentiments expressed towards different aspects of drugs within online reviews, which is important due to the increasing availability of such reviews and their impact on healthcare decisions. Traditional approaches that classify entire reviews as positive, negative, or neutral offer limited insights for consumers and healthcare professionals, highlighting the need for more detailed sentiment evaluation related to particular drug attributes. This detailed understanding can enhance pharmacovigilance by providing valuable information on drug efficacy and side effects from user feedback. The primary aim of the study is to improve the categorization of drug reviews by identifying and evaluating sentiments associated with specific drug aspects mentioned in online user comments, thereby contributing to better monitoring of drug safety and efficacy.",True
Biology,An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study,"Background Large language models (LLMs) have shown remarkable capabilities in natural language processing (NLP), especially in domains where labeled data are scarce or expensive, such as the clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. Objective The objective of this study is to assess the effectiveness of various prompt engineering techniques, including 2 newly introduced types—heuristic and ensemble prompts, for zero-shot and few-shot clinical information extraction using pretrained language models. Methods This comprehensive experimental study evaluated different prompt types (simple prefix, simple cloze, chain of thought, anticipatory, heuristic, and ensemble) across 5 clinical NLP tasks: clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, and medication attribute extraction. The performance of these prompts was assessed using 3 state-of-the-art language models: GPT-3.5 (OpenAI), Gemini (Google), and LLaMA-2 (Meta). The study contrasted zero-shot with few-shot prompting and explored the effectiveness of ensemble approaches. Results The study revealed that task-specific prompt tailoring is vital for the high performance of LLMs for zero-shot clinical NLP. In clinical sense disambiguation, GPT-3.5 achieved an accuracy of 0.96 with heuristic prompts and 0.94 in biomedical evidence extraction. Heuristic prompts, alongside chain of thought prompts, were highly effective across tasks. Few-shot prompting improved performance in complex scenarios, and ensemble approaches capitalized on multiple prompt strengths. GPT-3.5 consistently outperformed Gemini and LLaMA-2 across tasks and prompt types. Conclusions This study provides a rigorous evaluation of prompt engineering methodologies and introduces innovative techniques for clinical information extraction, demonstrating the potential of in-context learning in the clinical domain. These findings offer clear guidelines for future prompt-based clinical NLP research, facilitating engagement by non-NLP experts in clinical NLP advancements. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative artificial intelligence, and we hope that it will inspire and inform future research in this area.","['in-context learning', 'ensemble approaches', 'zero-shot prompting', 'few-shot prompting']","The research idea addresses the challenge of extracting valuable clinical knowledge from large language models without relying on task-specific training data, focusing on the need to design effective prompts that guide these models to perform specific clinical information extraction tasks. This is motivated by the scarcity and expense of labeled clinical data and the importance of unlocking hidden clinical insights to improve various clinical natural language processing tasks. The primary objective of the study is to assess the effectiveness of various prompt engineering techniques, including newly introduced heuristic and ensemble prompts, for zero-shot and few-shot clinical information extraction across multiple clinical tasks such as clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, and medication attribute extraction. The study aims to provide a rigorous evaluation of these approaches to inform and guide future clinical information extraction research.","The research idea addresses the challenge of extracting valuable clinical knowledge from textual resources without relying on task-specific training data, focusing on the need to design effective prompts that guide information processing to perform specific clinical information extraction tasks. This is motivated by the scarcity and expense of labeled clinical data and the importance of unlocking hidden clinical insights to improve various clinical natural language processing tasks. The primary objective of the study is to assess the effectiveness of various prompt engineering techniques, including newly introduced heuristic and ensemble prompts, for resource-efficient clinical information extraction across multiple clinical tasks such as clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, and medication attribute extraction. The study aims to provide a rigorous evaluation of these approaches to inform and guide future clinical information extraction research.",True
Biology,A systematic review of hyperspectral imaging in precision agriculture: Analysis of its current state and future prospects,"Hyperspectral sensor adaptability in precision agriculture to digital images is still at its nascent stage. Hyperspectral imaging (HSI) is data rich in solving agricultural problems like disease detection, weed detection, stress detection, crop monitoring, nutrient application, soil mineralogy, yield estimation, and sorting applications. With modern precision agriculture, the challenge now is to bring these applications to the field for real-time solutions, where machines are enabled to conduct analyses without expert supervision and communicate the results to users for better management of farmlands; a necessary step to gain complete autonomy in agricultural farmlands. Significant advancements in HSI technology for precision agriculture are required to fully realize its potential. As a wide-ranging collection of the status of HSI and analysis in precision agriculture is lacking, this review endeavors to provide a comprehensive overview of the recent advancements and trends of HSI in precision agriculture for real-time applications. In this study, a systematic review of 163 scientific articles published over the past twenty years (2003–2023) was conducted. Of these, 97 were selected for further analysis based on their relevance to the topic at hand. Topics include conventional data preprocessing techniques, hyperspectral data acquisition, data compression methods, and segmentation methods. The hardware implementation of field-programmable gate arrays (FPGAs) and graphics processing units (GPUs) for high-speed data processing and application of machine learning and deep learning technologies were explored. This review highlights the potential of HSI as a powerful tool for precision agriculture, particularly in real-time applications, discusses limitations, and provides insights into future research directions.","['machine learning', 'deep learning']","The research idea centers on the emerging potential of hyperspectral imaging (HSI) technology in precision agriculture, particularly its ability to address critical agricultural challenges such as disease detection, weed detection, stress detection, crop monitoring, nutrient application, soil mineralogy, yield estimation, and sorting. Despite its promise, the adaptation of HSI for real-time, field-level applications remains underdeveloped, limiting its practical use for autonomous farmland management. The study’s primary objective is to provide a comprehensive overview of recent advancements and trends in the use of hyperspectral imaging within precision agriculture, focusing on its application for real-time solutions. This review aims to highlight the current status, potential, limitations, and future research directions of HSI technology to better realize its benefits in agricultural practices.","The research idea centers on the emerging potential of hyperspectral imaging (HSI) technology in precision agriculture, particularly its ability to address critical agricultural challenges such as disease detection, weed detection, stress detection, crop monitoring, nutrient application, soil mineralogy, yield estimation, and sorting. Despite its promise, the adaptation of HSI for real-time, field-level applications remains underdeveloped, limiting its practical use for autonomous farmland management. The study's primary objective is to provide a comprehensive overview of recent advancements and trends in the use of hyperspectral imaging within precision agriculture, focusing on its application for real-time solutions. This review aims to highlight the current status, potential, limitations, and future research directions of HSI technology to better realize its benefits in agricultural practices.",True
Biology,Critical review on water quality analysis using IoT and machine learning models,"Water quality and its management are the most precise concerns confronting humanity globally. This article evaluates the various sensors used for water quality monitoring and focuses on the water quality index considering the multiple physical, chemical, and biological parameters. A Review of Internet of Things (IoT) research for water quality monitoring and analysis, sensors used for water quality can help remote monitoring of the water quality parameters using various IoT-based sensors that convey the assembled estimations utilizing Low-Power Wide Area Network innovations. Overall, the IoT system was 95 % accurate in measuring pH, Turbidity, TDS, and Temperature, while the traditional method was only 85 % accurate. Also, this study reviewed the different A.I. techniques used to assess water quality, including conventional machine learning techniques, Support Vector Machines, Deep Neural Networks, and K-nearest neighbors. Compared to traditional methods, machine learning and deep learning can significantly increase the accuracy of measurements of groundwater quality. However, various variables, such as the caliber of the training data, the water quality metrics' complexity, and the monitoring frequency, will affect the accuracy. The geographical information system (GIS) is used for spatial data analysis and managing water resources. The quality of its data is also reviewed in the paper. Based on these analyses, the study has forecasted the future sensors, Geospatial Technology, and machine learning techniques for water quality monitoring and analysis.","['Support Vector Machines', 'Deep Neural Networks', 'K-nearest neighbors']","The research idea centers on the critical global concern of water quality and its management, emphasizing the importance of monitoring multiple physical, chemical, and biological parameters to assess water quality accurately. The study addresses the need for effective evaluation of water quality through various sensors and indices to ensure safe and sustainable water resources. The primary objective of the study is to evaluate the different sensors used for water quality monitoring, focusing on the water quality index and the assessment of multiple parameters such as pH, turbidity, total dissolved solids, and temperature. Additionally, the study aims to review the quality of spatial data related to water resources and forecast future advancements in sensor technology and geospatial methods for improved water quality monitoring.","The research idea centers on the critical global concern of water quality and its management, emphasizing the importance of monitoring multiple physical, chemical, and biological parameters to assess water quality accurately. The study addresses the need for effective evaluation of water quality through various sensors and indices to ensure safe and sustainable water resources. The primary objective of the study is to evaluate the different sensors used for water quality monitoring, focusing on the water quality index and the assessment of multiple parameters such as pH, turbidity, total dissolved solids, and temperature. Additionally, the study aims to review the quality of spatial data related to water resources and examine potential future advancements in sensor technology and geospatial methods for improved water quality monitoring.",True
Biology,A novel and dynamic land use/cover change research framework based on an improved PLUS model and a fuzzy multiobjective programming model,"Spatial reconstruction and scenario simulation of historical processes and future trends of land use/cover change (LUCC) can help to reveal the historical background of land conversion and the spatial distribution of future land. Moreover, there is a close relationship between the spatiotemporal dynamics of land use/cover and changes in different ecosystem services (ESs). Using this relationship to simulate future land use scenarios is important. In this study, an LUCC dynamic analysis framework (LSTM-PLUS-FMOP) was constructed based on a deep learning time series forecasting model (LSTM), a parallelized urban land use simulation (PLUS) model and a fuzzy multiobjective programming (FMOP) model. The PLUS model was used to analyze the driving mechanism of land expansion and explore the land conversion pattern. In addition, three land conversion scenarios were established: natural land expansion (NLE), economic development priority (EDP) and regional sustainable development (RSD). The FMOP model and the relationship between LUCC and ES were used to perform a spatial simulation of land conversion. The uncertainty parameters in the model were treated by intuitionistic fuzzy numbers (IFSs). This study applied the constructed framework to the Yellow River Basin of Shaanxi Province (YRB-SX). The results showed that (1) from 2000 to 2020, the cropland area of the YRB-SX continuously decreased by 12.67 × 104 ha, while the built-up area continuously increased by 28.25 × 104 ha. The net reduction in woodland and grassland area was 13.90 × 104 ha. (2) The relative error range of land prediction using the LSTM model was 0.0003– 0.0042. This model had better accuracy than the Markov chain prediction model. (3) The cropland area decreased by 0.26% (NLE), 0.85% (EDP) and 1.68% (RSD) under the three scenarios. The built-up area increased by 25.01%, 32.76% and 14.72%, respectively. The RSD scenario followed the principles of ecological protection and spatial constraints, which mitigated the degradation of the ecosystem to some extent. This coupled simulation framework will help to obtain land allocation schemes that meet the requirements of ecological protection and provide solutions for rational land management.",['LSTM'],"The study addresses the problem of understanding the historical processes and future trends of land use and land cover change (LUCC) and their impact on ecosystem services (ESs). It highlights the importance of revealing the spatial distribution of land conversion and the close relationship between LUCC dynamics and changes in different ecosystem services. This understanding is crucial for simulating future land use scenarios that can inform ecological protection and sustainable land management. The primary aim of the study is to analyze the driving mechanisms of land expansion and land conversion patterns in the Yellow River Basin of Shaanxi Province, and to simulate future land use scenarios under different conditions, including natural land expansion, economic development priority, and regional sustainable development. The study seeks to provide land allocation schemes that balance ecological protection with land use demands, thereby offering solutions for rational land management.","The study addresses the problem of understanding the historical processes and future trends of land use and land cover change (LUCC) and their impact on ecosystem services (ESs). It highlights the importance of revealing the spatial distribution of land conversion and the close relationship between LUCC dynamics and changes in different ecosystem services. This understanding is crucial for projecting future land use scenarios that can inform ecological protection and sustainable land management. The primary aim of the study is to analyze the driving mechanisms of land expansion and land conversion patterns in the Yellow River Basin of Shaanxi Province, and to forecast future land use scenarios under different conditions, including natural land expansion, economic development priority, and regional sustainable development. The study seeks to provide land allocation schemes that balance ecological protection with land use demands, thereby offering solutions for rational land management.",True
Biology,Comparison of Random Forest and XGBoost Classifiers Using Integrated Optical and SAR Features for Mapping Urban Impervious Surface,"The integration of optical and SAR datasets through ensemble machine learning models shows promising results in urban remote sensing applications. The integration of multi-sensor datasets enhances the accuracy of information extraction. This research presents a comparison of two ensemble machine learning classifiers (random forest and extreme gradient boost (XGBoost)) classifiers using an integration of optical and SAR features and simple layer stacking (SLS) techniques. Therefore, Sentinel-1 (SAR) and Landsat 8 (optical) datasets were used with SAR textures and enhanced modified indices to extract features for the year 2023. The classification process utilized two machine learning algorithms, random forest and XGBoost, for urban impervious surface extraction. The study focused on three significant East Asian cities with diverse urban dynamics: Jakarta, Manila, and Seoul. This research proposed a novel index called the Normalized Blue Water Index (NBWI), which distinguishes water from other features and was utilized as an optical feature. Results showed an overall accuracy of 81% for UIS classification using XGBoost and 77% with RF while classifying land use land cover into four major classes (water, vegetation, bare soil, and urban impervious). However, the proposed framework with the XGBoost classifier outperformed the RF algorithm and Dynamic World (DW) data product and comparatively showed higher classification accuracy. Still, all three results show poor separability with bare soil class compared to ground truth data. XGBoost outperformed random forest and Dynamic World in classification accuracy, highlighting its potential use in urban remote sensing applications.","['ensemble machine learning models', 'random forest', 'extreme gradient boost (XGBoost)']","The study addresses the challenge of accurately extracting urban impervious surfaces and distinguishing land cover types in rapidly changing urban environments, particularly in diverse East Asian cities such as Jakarta, Manila, and Seoul. Improving the integration of multi-sensor datasets, including optical and synthetic aperture radar (SAR) data, is crucial for enhancing the precision of urban land use and land cover classification. The primary aim of the research is to compare the effectiveness of different classification approaches using integrated optical and SAR features to improve the accuracy of urban impervious surface extraction. Additionally, the study proposes a novel index, the Normalized Blue Water Index (NBWI), to better differentiate water bodies from other land cover types, with the goal of achieving higher classification accuracy across major land cover classes including water, vegetation, bare soil, and urban impervious surfaces.","The study addresses the challenge of accurately extracting urban impervious surfaces and distinguishing land cover types in rapidly changing urban environments, particularly in diverse East Asian cities such as Jakarta, Manila, and Seoul. Improving the integration of multi-sensor datasets, including optical and synthetic aperture radar (SAR) data, is crucial for enhancing the precision of urban land use and land cover classification. The primary aim of the research is to compare the effectiveness of different analytical methods using integrated optical and SAR features to improve the accuracy of urban impervious surface extraction. Additionally, the study proposes a novel index, the Normalized Blue Water Index (NBWI), to better differentiate water bodies from other land cover types, with the goal of achieving higher classification accuracy across major land cover classes including water, vegetation, bare soil, and urban impervious surfaces.",True
Biology,Estimating compressive strength of concrete containing rice husk ash using interpretable machine learning-based models,"The construction sector is a major contributor to global greenhouse gas emissions. Using recycled and waste materials in concrete is a practical solution to address environmental challenges. Currently, agricultural waste is widely used as a substitute for cement in the production of eco-friendly concrete. However, traditional methods for assessing the strength of such materials are both expensive and time-consuming. Therefore, this study uses machine learning techniques to develop prediction models for the compressive strength (CS) of rice husk ash (RHA) concrete. The ML techniques used in the present study include random forest (RF), light gradient boosting machine (LightGBM), ridge regression, and extreme gradient boosting (XGBoost). A total of 348 values of CS were collected from the experimental studies, and five characteristics of RHA concrete were taken as input variables. For the performance assessment of the models, multiple statistical metrics were used. During the training phase, the correlation coefficients (R) obtained for ridge regression, RF, XGBoost, and LightGBM were 0.943, 0.981, 0.985, and 0.996, respectively. In the testing set, these values demonstrated even higher performance, with correlation coefficients of 0.971, 0.993, 0.992, and 0.998 for ridge regression, RF, XGBoost, and LightGBM, respectively. The statistical analysis revealed that the LightGBM model outperformed other models, whereas the ridge regression model exhibited comparatively lower accuracy. SHapley Additive exPlanation (SHAP) method was employed for the interpretability of the developed model. The SHAP analysis revealed that water-to-cement is a controlling parameter in estimating the CS of RHA concrete. In conclusion, this study provides valuable guidance for builders and researchers to estimate the CS of RHA concrete. However, it is suggested that more input variables be incorporated and hybrid models utilized to further enhance the reliability and precision of the models.","['random forest (RF)', 'light gradient boosting machine (LightGBM)', 'ridge regression', 'extreme gradient boosting (XGBoost)', 'SHapley Additive exPlanation (SHAP)']","The construction sector significantly contributes to global greenhouse gas emissions, prompting the need for sustainable building materials. Using recycled and waste materials, such as agricultural waste, as substitutes for cement in concrete production offers an eco-friendly solution to reduce environmental impact. However, traditional methods for assessing the strength of such materials are costly and time-consuming, creating a challenge in efficiently evaluating their performance. This study aims to develop reliable approaches to predict the compressive strength of rice husk ash concrete, an eco-friendly material made by incorporating agricultural waste. The primary objective of the study is to establish accurate prediction models for the compressive strength of rice husk ash concrete based on key material characteristics, thereby providing valuable guidance for builders and researchers in estimating its performance. Additionally, the study seeks to identify the most influential factors affecting compressive strength, with the goal of improving the reliability and precision of strength estimation for sustainable concrete materials.","The construction sector significantly contributes to global greenhouse gas emissions, prompting the need for sustainable building materials. Using recycled and waste materials, such as agricultural waste, as substitutes for cement in concrete production offers an eco-friendly solution to reduce environmental impact. However, traditional methods for assessing the strength of such materials are costly and time-consuming, creating a challenge in efficiently evaluating their performance. This study aims to develop reliable approaches to estimate the compressive strength of rice husk ash concrete, an eco-friendly material made by incorporating agricultural waste. The primary objective of the study is to establish accurate estimation methods for the compressive strength of rice husk ash concrete based on key material characteristics, thereby providing valuable guidance for builders and researchers in evaluating its performance. Additionally, the study seeks to identify the most influential factors affecting compressive strength, with the goal of improving the reliability and precision of strength estimation for sustainable concrete materials.",True
Biology,Improving Thyroid Disorder Diagnosis via Ensemble Stacking and Bidirectional Feature Selection,"Thyroid disorders represent a significant global health challenge with hypothyroidism and hyperthyroidism as two common conditions arising from dysfunction in the thyroid gland.Accurate and timely diagnosis of these disorders is crucial for effective treatment and patient care.This research introduces a comprehensive approach to improve the accuracy of thyroid disorder diagnosis through the integration of ensemble stacking and advanced feature selection techniques.Sequential forward feature selection, sequential backward feature elimination, and bidirectional feature elimination are investigated in this study.In ensemble learning, random forest, adaptive boosting, and bagging classifiers are employed.The effectiveness of these techniques is evaluated using two different datasets obtained from the University of California Irvine-Machine Learning Repository, both of which undergo preprocessing steps, including outlier removal, addressing missing data, data cleansing, and feature reduction.Extensive experimentation demonstrates the remarkable success of proposed ensemble stacking and bidirectional feature elimination achieving 100% and 99.86% accuracy in identifying hyperthyroidism and hypothyroidism, respectively.Beyond enhancing detection accuracy, the ensemble stacking model also demonstrated a streamlined computational complexity which is pivotal for practical medical applications.It significantly outperformed existing studies with similar objectives underscoring the viability and effectiveness of the proposed scheme.This research offers an innovative perspective and sets the platform for improved thyroid disorder diagnosis with broader implications for healthcare and patient well-being.","['ensemble stacking', 'sequential forward feature selection', 'sequential backward feature elimination', 'random forest', 'adaptive boosting', 'bagging classifiers']","Thyroid disorders, including hypothyroidism and hyperthyroidism, represent a significant global health challenge due to dysfunction in the thyroid gland. Accurate and timely diagnosis of these conditions is crucial for effective treatment and improving patient care. The primary aim of this study is to enhance the accuracy of thyroid disorder diagnosis by investigating and integrating various feature selection techniques and classification approaches. This research seeks to improve the identification of hyperthyroidism and hypothyroidism, ultimately contributing to better healthcare outcomes and patient well-being.","Thyroid disorders, including hypothyroidism and hyperthyroidism, represent a significant global health challenge due to dysfunction in the thyroid gland. Accurate and timely diagnosis of these conditions is crucial for effective treatment and improving patient care. The primary aim of this study is to enhance the accuracy of thyroid disorder diagnosis by investigating and integrating various analysis methods and diagnostic approaches. This research seeks to improve the identification of hyperthyroidism and hypothyroidism, ultimately contributing to better healthcare outcomes and patient well-being.",True
Biology,Assessing water quality of an ecologically critical urban canal incorporating machine learning approaches,"This study assessed water quality (WQ) in Tongi Canal, an ecologically critical and economically important urban canal in Bangladesh. The researchers employed the Root Mean Square Water Quality Index (RMS-WQI) model, utilizing seven WQ indicators, including temperature, dissolve oxygen, electrical conductivity, lead, cadmium, and iron to calculate the water quality index (WQI) score. The results showed that most of the water sampling locations showed poor WQ, with many indicators violating Bangladesh's environmental conservation regulations. This study employed eight machine learning algorithms, where the Gaussian process regression (GPR) model demonstrated superior performance (training RMSE = 1.77, testing RMSE = 0.0006) in predicting WQI scores. To validate the GPR model's performance, several performance measures, including the coefficient of determination (R2), the Nash-Sutcliffe efficiency (NSE), the model efficiency factor (MEF), Z statistics, and Taylor diagram analysis, were employed. The GPR model exhibited higher sensitivity (R2 = 1.0) and efficiency (NSE = 1.0, MEF = 0.0) in predicting WQ. The analysis of model uncertainty (standard uncertainty = 7.08 ± 0.9025; expanded uncertainty = 7.08 ± 1.846) indicates that the RMS-WQI model holds potential for assessing the WQ of inland waterbodies. These findings indicate that the RMS-WQI model could be an effective approach for assessing inland waters across Bangladesh. The study's results showed that most of the WQ indicators did not meet the recommended guidelines, indicating that the water in the Tongi Canal is unsafe and unsuitable for various purposes. The study's implications extend beyond the Tongi Canal and could contribute to WQ management initiatives across Bangladesh.",['Gaussian process regression (GPR)'],"The study addresses the critical issue of water quality deterioration in Tongi Canal, an ecologically important and economically significant urban waterbody in Bangladesh. The motivation stems from concerns that many water quality indicators in the canal violate environmental conservation regulations, rendering the water unsafe and unsuitable for various uses. The primary aim of the study is to assess the water quality of Tongi Canal by calculating a comprehensive water quality index based on multiple indicators such as temperature, dissolved oxygen, electrical conductivity, and heavy metals like lead, cadmium, and iron. The study seeks to evaluate the current status of the canal’s water quality and provide insights that could support water quality management efforts across Bangladesh.","The study addresses the critical issue of water quality deterioration in Tongi Canal, an ecologically important and economically significant urban waterbody in Bangladesh. The motivation stems from concerns that many water quality indicators in the canal violate environmental conservation regulations, rendering the water unsafe and unsuitable for various uses. The primary aim of the study is to assess the water quality of Tongi Canal by calculating a comprehensive water quality index based on multiple indicators such as temperature, dissolved oxygen, electrical conductivity, and heavy metals like lead, cadmium, and iron. The study seeks to evaluate the current status of the canal's water quality and provide insights that could support water quality management efforts across Bangladesh.",True
Biology,Comparing YOLOv8 and Mask R-CNN for instance segmentation in complex orchard environments,"Instance segmentation, an important image processing operation for automation in agriculture, is used to precisely delineate individual objects of interest within images, which provides foundational information for various automated or robotic tasks such as selective harvesting and precision pruning. This study compares the one-stage YOLOv8 and the two-stage Mask R-CNN machine learning models for instance segmentation under varying orchard conditions across two datasets. Dataset 1, collected in dormant season, includes images of dormant apple trees, which were used to train multi-object segmentation models delineating tree branches and trunks. Dataset 2, collected in the early growing season, includes images of apple tree canopies with green foliage and immature (green) apples (also called fruitlet), which were used to train single-object segmentation models delineating only immature green apples. The results showed that YOLOv8 performed better than Mask R-CNN, achieving good precision and near-perfect recall across both datasets at a confidence threshold of 0.5. Specifically, for Dataset 1, YOLOv8 achieved a precision of 0.90 and a recall of 0.95 for all classes. In comparison, Mask R-CNN demonstrated a precision of 0.81 and a recall of 0.81 for the same dataset. With Dataset 2, YOLOv8 achieved a precision of 0.93 and a recall of 0.97. Mask R-CNN, in this single-class scenario, achieved a precision of 0.85 and a recall of 0.88. Additionally, the inference times for YOLOv8 were 10.9 ms for multi-class segmentation (Dataset 1) and 7.8 ms for single-class segmentation (Dataset 2), compared to 15.6 ms and 12.8 ms achieved by Mask R-CNN's, respectively. These findings show YOLOv8's superior accuracy and efficiency in machine learning applications compared to two-stage models, specifically Mask-R-CNN, which suggests its suitability in developing smart and automated orchard operations, particularly when real-time applications are necessary in such cases as robotic harvesting and robotic immature green fruit thinning.","['YOLOv8', 'Mask R-CNN']","The research addresses the challenge of accurately identifying and delineating individual components of apple trees, such as branches, trunks, and immature green apples, under varying orchard conditions to support automation in agricultural practices like selective harvesting and precision pruning. Precise segmentation of these tree parts is essential for enabling effective and efficient orchard management through automated or robotic interventions. The primary objective of the study is to evaluate and compare the performance of different instance segmentation approaches in accurately detecting and segmenting multiple tree structures during the dormant season and immature green apples during the early growing season. This comparison aims to determine the most suitable method for enhancing smart and automated orchard operations, particularly for real-time applications such as robotic harvesting and fruit thinning.","The research addresses the challenge of accurately identifying and delineating individual components of apple trees, such as branches, trunks, and immature green apples, under varying orchard conditions to support automation in agricultural practices like selective harvesting and precision pruning. Precise segmentation of these tree parts is essential for enabling effective and efficient orchard management through automated or robotic interventions. The primary objective of the study is to evaluate and compare the performance of different segmentation methods in accurately detecting and separating multiple tree structures during the dormant season and immature green apples during the early growing season. This comparison aims to determine the most suitable approach for enhancing automated orchard operations, particularly for real-time applications such as robotic harvesting and fruit thinning.",True
Biology,Data-driven evolution of water quality models: An in-depth investigation of innovative outlier detection approaches-A case study of Irish Water Quality Index (IEWQI) model,"Recently, there has been a significant advancement in the water quality index (WQI) models utilizing data-driven approaches, especially those integrating machine learning and artificial intelligence (ML/AI) technology. Although, several recent studies have revealed that the data-driven model has produced inconsistent results due to the data outliers, which significantly impact model reliability and accuracy. The present study was carried out to assess the impact of data outliers on a recently developed Irish Water Quality Index (IEWQI) model, which relies on data-driven techniques. To the author's best knowledge, there has been no systematic framework for evaluating the influence of data outliers on such models. For the purposes of assessing the outlier impact of the data outliers on the water quality (WQ) model, this was the first initiative in research to introduce a comprehensive approach that combines machine learning with advanced statistical techniques. The proposed framework was implemented in Cork Harbour, Ireland, to evaluate the IEWQI model's sensitivity to outliers in input indicators to assess the water quality. In order to detect the data outlier, the study utilized two widely used ML techniques, including Isolation Forest (IF) and Kernel Density Estimation (KDE) within the dataset, for predicting WQ with and without these outliers. For validating the ML results, the study used five commonly used statistical measures. The performance metric (R2) indicates that the model performance improved slightly (R2 increased from 0.92 to 0.95) in predicting WQ after removing the data outlier from the input. But the IEWQI scores revealed that there were no statistically significant differences among the actual values, predictions with outliers, and predictions without outliers, with a 95% confidence interval at p < 0.05. The results of model uncertainty also revealed that the model contributed <1% uncertainty to the final assessment results for using both datasets (with and without outliers). In addition, all statistical measures indicated that the ML techniques provided reliable results that can be utilized for detecting outliers and their impacts on the IEWQI model. The findings of the research reveal that although the data outliers had no significant impact on the IEWQI model architecture, they had moderate impacts on the rating schemes' of the model. This finding indicated that detecting the data outliers could improve the accuracy of the IEWQI model in rating WQ as well as be helpful in mitigating the model eclipsing problem. In addition, the results of the research provide evidence of how the data outliers influenced the data-driven model in predicting WQ and reliability, particularly since the study confirmed that the IEWQI model's could be effective for accurately rating WQ despite the presence of the data outliers in the input. It could occur due to the spatio-temporal variability inherent in WQ indicators. However, the research assesses the influence of data input outliers on the IEWQI model and underscores important areas for future investigation. These areas include expanding temporal analysis using multi-year data, examining spatial outlier patterns, and evaluating detection methods. Moreover, it is essential to explore the real-world impacts of revised rating categories, involve stakeholders in outlier management, and fine-tune model parameters. Analysing model performance across varying temporal and spatial resolutions and incorporating additional environmental data can significantly enhance the accuracy of WQ assessment. Consequently, this study offers valuable insights to strengthen the IEWQI model's robustness and provides avenues for enhancing its utility in broader WQ assessment applications. Moreover, the study successfully adopted the framework for evaluating how data input outliers affect the data-driven model, such as the IEWQI model. The current study has been carried out in Cork Harbour for only a single year of WQ data. The framework should be tested across various domains for evaluating the response of the IEWQI model's in terms of the spatio-temporal resolution of the domain. Nevertheless, the study recommended that future research should be conducted to adjust or revise the IEWQI model's rating schemes and investigate the practical effects of data outliers on updated rating categories. However, the study provides potential recommendations for enhancing the IEWQI model's adaptability and reveals its effectiveness in expanding its applicability in more general WQ assessment scenarios.",['Isolation Forest (IF)'],"The research idea centers on addressing the challenges posed by data outliers in assessing water quality using the Irish Water Quality Index (IEWQI) model. Although recent advancements have improved water quality assessment, inconsistent results caused by outliers affect the reliability and accuracy of these evaluations. This study highlights the need for a systematic approach to understand how outliers influence water quality models and their rating schemes. The motivation is to improve the robustness and accuracy of water quality assessments despite the inherent variability in water quality indicators.

The primary objective of the study is to assess the impact of data outliers on the IEWQI model’s performance and rating schemes in evaluating water quality. The research aims to evaluate the sensitivity of the IEWQI model to outliers in input indicators by applying a comprehensive framework in Cork Harbour, Ireland. Additionally, the study seeks to provide insights into how outliers affect the model’s reliability and to offer recommendations for refining the model’s rating categories to enhance its applicability in broader water quality assessment scenarios.","The research idea centers on addressing the challenges posed by data outliers in assessing water quality using the Irish Water Quality Index (IEWQI) model. Although recent advancements have improved water quality assessment, inconsistent results caused by outliers affect the reliability and accuracy of these evaluations. This study highlights the need for a systematic approach to understand how outliers influence water quality models and their rating schemes. The motivation is to improve the robustness and accuracy of water quality assessments despite the inherent variability in water quality indicators.

The primary objective of the study is to assess the impact of data outliers on the IEWQI model's performance and rating schemes in evaluating water quality. The research aims to evaluate the sensitivity of the IEWQI model to outliers in input indicators by applying a comprehensive framework in Cork Harbour, Ireland. Additionally, the study seeks to provide insights into how outliers affect the model's reliability and to offer recommendations for refining the model's rating categories to enhance its applicability in broader water quality assessment scenarios.",True
Biology,Vision–language foundation model for echocardiogram interpretation,"Abstract The development of robust artificial intelligence models for echocardiography has been limited by the availability of annotated clinical data. Here, to address this challenge and improve the performance of cardiac imaging models, we developed EchoCLIP, a vision–language foundation model for echocardiography, that learns the relationship between cardiac ultrasound images and the interpretations of expert cardiologists across a wide range of patients and indications for imaging. After training on 1,032,975 cardiac ultrasound videos and corresponding expert text, EchoCLIP performs well on a diverse range of benchmarks for cardiac image interpretation, despite not having been explicitly trained for individual interpretation tasks. EchoCLIP can assess cardiac function (mean absolute error of 7.1% when predicting left ventricular ejection fraction in an external validation dataset) and identify implanted intracardiac devices (area under the curve (AUC) of 0.84, 0.92 and 0.97 for pacemakers, percutaneous mitral valve repair and artificial aortic valves, respectively). We also developed a long-context variant (EchoCLIP-R) using a custom tokenizer based on common echocardiography concepts. EchoCLIP-R accurately identified unique patients across multiple videos (AUC of 0.86), identified clinical transitions such as heart transplants (AUC of 0.79) and cardiac surgery (AUC 0.77) and enabled robust image-to-text search (mean cross-modal retrieval rank in the top 1% of candidate text reports). These capabilities represent a substantial step toward understanding and applying foundation models in cardiovascular imaging for preliminary interpretation of echocardiographic findings.",['vision–language foundation model'],"The research addresses the challenge of limited availability of annotated clinical data for echocardiography, which has hindered the development of robust methods for cardiac imaging interpretation. The study is motivated by the need to improve the understanding and assessment of cardiac ultrasound images across a wide range of patients and clinical indications. The primary objective of the study is to develop a comprehensive approach that learns the relationship between cardiac ultrasound images and expert cardiologists’ interpretations, enabling accurate assessment of cardiac function and identification of implanted intracardiac devices. Additionally, the study aims to enhance the ability to recognize unique patients, detect clinical transitions such as heart transplants and cardiac surgery, and facilitate effective retrieval of echocardiographic findings.","The research addresses the challenge of limited availability of annotated clinical data for echocardiography, which has hindered the development of robust methods for cardiac imaging interpretation. The study is motivated by the need to improve the understanding and assessment of cardiac ultrasound images across a wide range of patients and clinical indications. The primary objective of the study is to develop a comprehensive approach that captures the relationship between cardiac ultrasound images and expert cardiologists' interpretations, enabling accurate assessment of cardiac function and identification of implanted intracardiac devices. Additionally, the study aims to enhance the ability to recognize unique patients, detect clinical transitions such as heart transplants and cardiac surgery, and facilitate effective retrieval of echocardiographic findings.",True
Biology,Assessing ChatGPT’s Mastery of Bloom’s Taxonomy Using Psychosomatic Medicine Exam Questions: Mixed-Methods Study,"Background Large language models such as GPT-4 (Generative Pre-trained Transformer 4) are being increasingly used in medicine and medical education. However, these models are prone to “hallucinations” (ie, outputs that seem convincing while being factually incorrect). It is currently unknown how these errors by large language models relate to the different cognitive levels defined in Bloom’s taxonomy. Objective This study aims to explore how GPT-4 performs in terms of Bloom’s taxonomy using psychosomatic medicine exam questions. Methods We used a large data set of psychosomatic medicine multiple-choice questions (N=307) with real-world results derived from medical school exams. GPT-4 answered the multiple-choice questions using 2 distinct prompt versions: detailed and short. The answers were analyzed using a quantitative approach and a qualitative approach. Focusing on incorrectly answered questions, we categorized reasoning errors according to the hierarchical framework of Bloom’s taxonomy. Results GPT-4’s performance in answering exam questions yielded a high success rate: 93% (284/307) for the detailed prompt and 91% (278/307) for the short prompt. Questions answered correctly by GPT-4 had a statistically significant higher difficulty than questions answered incorrectly (P=.002 for the detailed prompt and P&lt;.001 for the short prompt). Independent of the prompt, GPT-4’s lowest exam performance was 78.9% (15/19), thereby always surpassing the “pass” threshold. Our qualitative analysis of incorrect answers, based on Bloom’s taxonomy, showed that errors were primarily in the “remember” (29/68) and “understand” (23/68) cognitive levels; specific issues arose in recalling details, understanding conceptual relationships, and adhering to standardized guidelines. Conclusions GPT-4 demonstrated a remarkable success rate when confronted with psychosomatic medicine multiple-choice exam questions, aligning with previous findings. When evaluated through Bloom’s taxonomy, our data revealed that GPT-4 occasionally ignored specific facts (remember), provided illogical reasoning (understand), or failed to apply concepts to a new situation (apply). These errors, which were confidently presented, could be attributed to inherent model biases and the tendency to generate outputs that maximize likelihood.",['GPT-4 (Generative Pre-trained Transformer 4)'],"The research idea addresses the challenge of understanding how errors made by large language models relate to different cognitive levels defined in Bloom’s taxonomy, particularly in the context of medical education and psychosomatic medicine exam questions. There is a need to investigate the nature of these errors, especially since such models can produce outputs that appear convincing but may be factually incorrect. The study aims to explore the performance of GPT-4 on psychosomatic medicine multiple-choice questions by examining its responses through the lens of Bloom’s taxonomy. The primary objective of the study is to evaluate GPT-4’s performance across various cognitive levels of Bloom’s taxonomy using a large set of psychosomatic medicine exam questions, with a focus on categorizing and understanding the reasoning errors in incorrectly answered questions according to this hierarchical framework.","The research idea addresses the challenge of understanding how errors made by automated text systems relate to different cognitive levels defined in Bloom's taxonomy, particularly in the context of medical education and psychosomatic medicine exam questions. There is a need to investigate the nature of these errors, especially since such systems can produce outputs that appear convincing but may be factually incorrect. The study aims to explore the performance of text-based response systems on psychosomatic medicine multiple-choice questions by examining responses through the lens of Bloom's taxonomy. The primary objective of the study is to evaluate these systems' performance across various cognitive levels of Bloom's taxonomy using a large set of psychosomatic medicine exam questions, with a focus on categorizing and understanding the reasoning errors in incorrectly answered questions according to this hierarchical framework.",True
Biology,Machine learning-assisted in-situ adaptive strategies for the control of defects and anomalies in metal additive manufacturing,"In metal additive manufacturing (AM), the material microstructure and part geometry are formed incrementally. Consequently, the resulting part could be defect- and anomaly-free if sufficient care is taken to deposit each layer under optimal process conditions. Conventional closed-loop control (CLC) engineering solutions which sought to achieve this were deterministic and rule-based, thus resulting in limited success in the stochastic environment experienced in the highly dynamic AM process. On the other hand, emerging machine learning (ML) based strategies are better suited to providing the robustness, scope, flexibility, and scalability required for process control in an uncertain environment. Offline ML models that help optimise AM process parameters before a build begins and online ML models that efficiently processed in-situ sensory data to detect and diagnose flaws in real-time (or near-real-time) have been developed. However, ML models that enable a process to take evasive or corrective actions in relation to flaws via on the fly decision-making are only emerging. These models must possess prognostic capabilities to provide context-sensitive recommendations for in-situ process control based on real-time diagnostics. In this article, we pinpoint the shortcomings in traditional CLC strategies, and provide a framework for defect and anomaly control through ML-assisted CLC in AM. We discuss flaws in terms of their causes, in-situ detectability, and controllability, and examine their management under three scenarios: avoidance, mitigation, and repair. Then, we summarise the research into ML models developed for offline optimisation and in-situ diagnosis before initiating a detailed conversation on the implementation of ML-assisted in-situ process control. We found that researchers favoured reinforcement learning approaches or inverse ML models for making rapid, situation-aware control decisions. We also observed that, to-date, the defects addressed were those that may be quantified relatively easily autonomously, and that mitigation (rather than avoidance or repair) was the aim of ML-assisted in-situ control strategies. Additionally, we highlight the various technologies that must seamlessly combine to advance the field of autonomous in-situ control so that it becomes a reality in industrial settings. Finally, we raise awareness of seldom discussed, yet highly pertinent, topics relevant to adaptive control. Our work closes a significant gap in the current AM literature by broaching wide-ranging discussions on matters relevant to in-situ adaptive control in AM.","['online ML models', 'reinforcement learning approaches']","The research idea centers on the challenge of producing defect- and anomaly-free parts in metal additive manufacturing, where material microstructure and part geometry are formed incrementally under highly dynamic and stochastic conditions. Traditional control methods have shown limited success in managing these complexities, highlighting the need for more robust and flexible approaches to ensure optimal process conditions during layer deposition. The study addresses the shortcomings of existing strategies in controlling defects and anomalies in real-time during the manufacturing process.

The primary objective of the study is to provide a comprehensive framework for defect and anomaly control in metal additive manufacturing through advanced in-situ process control strategies. It aims to examine flaws in terms of their causes, detectability, and controllability, and to explore their management via avoidance, mitigation, and repair. The study also seeks to advance the understanding and implementation of adaptive control mechanisms that enable real-time decision-making to improve part quality and process reliability in industrial settings.","The research idea centers on the challenge of producing defect- and anomaly-free parts in metal additive manufacturing, where material microstructure and part geometry are formed incrementally under highly dynamic and stochastic conditions. Traditional control methods have shown limited success in managing these complexities, highlighting the need for more robust and flexible approaches to ensure optimal process conditions during layer deposition. The study addresses the shortcomings of existing strategies in controlling defects and anomalies in real-time during the manufacturing process.

The primary objective of the study is to provide a comprehensive framework for defect and anomaly control in metal additive manufacturing through advanced in-situ process control strategies. It aims to examine flaws in terms of their causes, detectability, and controllability, and to explore their management via avoidance, mitigation, and repair. The study also seeks to advance the understanding and implementation of responsive control mechanisms that enable real-time decision-making to improve part quality and process reliability in industrial settings.",True
Biology,CCL-DTI: contributing the contrastive loss in drug–target interaction prediction,"Abstract Background The Drug–Target Interaction (DTI) prediction uses a drug molecule and a protein sequence as inputs to predict the binding affinity value. In recent years, deep learning-based models have gotten more attention. These methods have two modules: the feature extraction module and the task prediction module. In most deep learning-based approaches, a simple task prediction loss (i.e., categorical cross entropy for the classification task and mean squared error for the regression task) is used to learn the model. In machine learning, contrastive-based loss functions are developed to learn more discriminative feature space. In a deep learning-based model, extracting more discriminative feature space leads to performance improvement for the task prediction module. Results In this paper, we have used multimodal knowledge as input and proposed an attention-based fusion technique to combine this knowledge. Also, we investigate how utilizing contrastive loss function along the task prediction loss could help the approach to learn a more powerful model. Four contrastive loss functions are considered: (1) max-margin contrastive loss function, (2) triplet loss function, (3) Multi-class N-pair Loss Objective, and (4) NT-Xent loss function. The proposed model is evaluated using four well-known datasets: Wang et al. dataset, Luo's dataset, Davis, and KIBA datasets. Conclusions Accordingly, after reviewing the state-of-the-art methods, we developed a multimodal feature extraction network by combining protein sequences and drug molecules, along with protein–protein interaction networks and drug–drug interaction networks. The results show it performs significantly better than the comparable state-of-the-art approaches.","['contrastive loss function', 'max-margin contrastive loss function', 'triplet loss function', 'Multi-class N-pair Loss Objective', 'NT-Xent loss function']","The research idea centers on improving the prediction of drug–target interactions, which involves determining the binding affinity between drug molecules and protein sequences. Accurate prediction of these interactions is crucial for understanding drug efficacy and facilitating drug discovery. The study addresses the challenge of enhancing the discriminative power of features used to represent drugs and proteins to improve prediction performance. The primary objective of the study is to develop a method that effectively integrates multiple types of biological information, including protein sequences, drug molecules, protein–protein interaction networks, and drug–drug interaction networks, to improve the accuracy of drug–target interaction predictions. The study aims to demonstrate that combining these diverse biological data sources leads to significantly better predictive performance compared to existing approaches.","The research idea centers on improving the prediction of drug–target interactions, which involves determining the binding affinity between drug molecules and protein sequences. Accurate prediction of these interactions is crucial for understanding drug efficacy and facilitating drug discovery. The study addresses the challenge of enhancing the discriminative power of features used to represent drugs and proteins to improve prediction outcomes. The primary objective of the study is to develop a method that effectively integrates multiple types of biological information, including protein sequences, drug molecules, protein–protein interaction networks, and drug–drug interaction networks, to improve the accuracy of drug–target interaction predictions. The study aims to demonstrate that combining these diverse biological data sources leads to significantly better prediction results compared to existing approaches.",True
Biology,Pedestrian 3D Shape Understanding for Person Re-Identification via Multi-View Learning,"Recent development in computing power has resulted in performance improvements on holistic(none-occluded) person Re-Identification (ReID) tasks. Nevertheless, the precision of the recent research will diminish when a pedestrian is obstructed by obstacles. Within the realm of 2D space, the loss of information from obstructed objects continues to pose significant challenges in the context of person ReID. Person is a 3D non-grid object, and thus semantic representation learning in only 2D space limits the understanding of occluded person. In the present work, we propose a network based on 3D multi-view learning, allowing it to acquire geometric and shape details of an occluded pedestrian from 3D space. Simultaneously, it capitalizes on advancements in 2D-based networks to extract semantic representations from 3D multi-views. Specifically, the surface random selection strategy is proposed to convert images of 2D RGB into 3D multi-views. Using this strategy, we build four extensive 3D multi-view data collections for person ReID. After that, Pedestrian 3D Shape Understanding for Person Re-Identification via Multi-View Learning(MV-3DSReID), is proposed for identifying the person by learning person geometry and structure representation from the groups of multi-view images. In comparison to alternative data formats (e.g., 2D RGB, 3D point cloud), multi-view images complement each other's detailed features of the 3D object by adjusting rendering viewpoints, thus facilitating a more comprehensive understanding of the person for both holistic and occluded ReID situations. Experiments on occluded and holistic ReID tasks demonstrate performance levels comparable to state-of-the-art methods, validating the effectiveness of our proposed approach in tackling challenges related to occlusion. The code is available at https://github.com/hangjiaqi1/MV-TransReID.",['3D multi-view learning'],"The research idea addresses the challenge of accurately identifying pedestrians when they are partially obstructed by obstacles, which leads to a loss of information in traditional 2D representations. Since a person is a three-dimensional object, relying solely on 2D semantic representations limits the understanding of occluded individuals. This study recognizes the need for a more comprehensive approach that captures geometric and shape details in 3D space to improve identification under occlusion conditions. The research objective is to develop a method that leverages multiple views of a pedestrian in 3D space to learn their geometry and structural features, thereby enhancing person identification accuracy for both occluded and unobstructed cases. The study aims to build extensive multi-view data collections and utilize these to better represent and understand the 3D shape of pedestrians, ultimately improving performance in person re-identification tasks despite occlusions.","The research idea addresses the challenge of accurately identifying pedestrians when they are partially obstructed by obstacles, which leads to a loss of information in traditional 2D representations. Since a person is a three-dimensional object, relying solely on 2D semantic representations limits the understanding of occluded individuals. This study recognizes the need for a more comprehensive approach that captures geometric and shape details in 3D space to improve identification under occlusion conditions. The research objective is to develop a method that utilizes multiple views of a pedestrian in 3D space to understand their geometry and structural features, thereby enhancing person identification accuracy for both occluded and unobstructed cases. The study aims to build extensive multi-view data collections and use these to better represent and understand the 3D shape of pedestrians, ultimately improving performance in person re-identification tasks despite occlusions.",True
Biology,AlphaFold predictions of fold-switched conformations are driven by structure memorization,"Abstract Recent work suggests that AlphaFold (AF)–a deep learning-based model that can accurately infer protein structure from sequence–may discern important features of folded protein energy landscapes, defined by the diversity and frequency of different conformations in the folded state. Here, we test the limits of its predictive power on fold-switching proteins, which assume two structures with regions of distinct secondary and/or tertiary structure. We find that (1) AF is a weak predictor of fold switching and (2) some of its successes result from memorization of training-set structures rather than learned protein energetics. Combining &gt;280,000 models from several implementations of AF2 and AF3, a 35% success rate was achieved for fold switchers likely in AF’s training sets. AF2’s confidence metrics selected against models consistent with experimentally determined fold-switching structures and failed to discriminate between low and high energy conformations. Further, AF captured only one out of seven experimentally confirmed fold switchers outside of its training sets despite extensive sampling of an additional ~280,000 models. Several observations indicate that AF2 has memorized structural information during training, and AF3 misassigns coevolutionary restraints. These limitations constrain the scope of successful predictions, highlighting the need for physically based methods that readily predict multiple protein conformations.","['AlphaFold (AF)', 'AF2']","The research idea centers on understanding the ability to predict protein fold switching, a phenomenon where proteins adopt multiple distinct conformations with different secondary or tertiary structures, which is important for comprehending protein energy landscapes and functional diversity. The study addresses the challenge of accurately identifying fold-switching proteins and the limitations of current predictive approaches in capturing the full range of protein conformations. The primary objective of the study is to evaluate the effectiveness of existing predictive methods in identifying fold-switching proteins and to assess their ability to distinguish between different experimentally confirmed protein conformations. The study aims to highlight the constraints of these methods and emphasize the need for approaches that can reliably predict multiple protein structures based on physical principles.","The research idea centers on understanding the ability to predict protein fold switching, a phenomenon where proteins adopt multiple distinct conformations with different secondary or tertiary structures, which is important for comprehending protein energy landscapes and functional diversity. The study addresses the challenge of accurately identifying fold-switching proteins and the limitations of current analytical approaches in capturing the full range of protein conformations. The primary objective of the study is to evaluate the effectiveness of existing methods in identifying fold-switching proteins and to assess their ability to distinguish between different experimentally confirmed protein conformations. The study aims to highlight the constraints of these methods and emphasize the need for approaches that can reliably predict multiple protein structures based on physical principles.",True
Biology,Pattern recognition in the nucleation kinetics of non-equilibrium self-assembly,"Abstract Inspired by biology’s most sophisticated computer, the brain, neural networks constitute a profound reformulation of computational principles 1–3 . Analogous high-dimensional, highly interconnected computational architectures also arise within information-processing molecular systems inside living cells, such as signal transduction cascades and genetic regulatory networks 4–7 . Might collective modes analogous to neural computation be found more broadly in other physical and chemical processes, even those that ostensibly play non-information-processing roles? Here we examine nucleation during self-assembly of multicomponent structures, showing that high-dimensional patterns of concentrations can be discriminated and classified in a manner similar to neural network computation. Specifically, we design a set of 917 DNA tiles that can self-assemble in three alternative ways such that competitive nucleation depends sensitively on the extent of colocalization of high-concentration tiles within the three structures. The system was trained in silico to classify a set of 18 grayscale 30 × 30 pixel images into three categories. Experimentally, fluorescence and atomic force microscopy measurements during and after a 150 hour anneal established that all trained images were correctly classified, whereas a test set of image variations probed the robustness of the results. Although slow compared to previous biochemical neural networks, our approach is compact, robust and scalable. Our findings suggest that ubiquitous physical phenomena, such as nucleation, may hold powerful information-processing capabilities when they occur within high-dimensional multicomponent systems.",['neural networks'],"The research idea centers on exploring whether collective behaviors similar to neural computation, typically associated with the brain, can be found more broadly in physical and chemical processes that are not traditionally considered information-processing, such as nucleation during self-assembly of multicomponent structures. The study is motivated by the observation that complex, high-dimensional, and interconnected molecular systems inside living cells, like signal transduction cascades and genetic regulatory networks, exhibit computational-like properties. The research objective is to investigate how nucleation in self-assembling DNA tile systems can discriminate and classify high-dimensional concentration patterns, demonstrating that competitive nucleation depends on the spatial colocalization of specific components. The study aims to experimentally validate that these self-assembling structures can correctly classify different input patterns, thereby revealing that physical phenomena like nucleation may possess inherent information-processing capabilities within complex multicomponent systems.","The research idea centers on exploring whether collective behaviors similar to information processing, typically associated with the brain, can be found more broadly in physical and chemical processes that are not traditionally considered information-processing, such as nucleation during self-assembly of multicomponent structures. The study is motivated by the observation that complex, high-dimensional, and interconnected molecular systems inside living cells, like signal transduction cascades and genetic regulatory networks, exhibit information-processing properties. The research objective is to investigate how nucleation in self-assembling DNA tile systems can discriminate and categorize high-dimensional concentration patterns, demonstrating that competitive nucleation depends on the spatial colocalization of specific components. The study aims to experimentally validate that these self-assembling structures can correctly distinguish different input patterns, thereby revealing that physical phenomena like nucleation may possess inherent information-processing capabilities within complex multicomponent systems.",True
Biology,Automated Tool Support for Glaucoma Identification With Explainability Using Fundus Images,"Glaucoma is a progressive eye condition that causes irreversible vision loss due to damage to the optic nerve. Recent developments in deep learning and the accessibility of computing resources have provided tool support for automated glaucoma diagnosis. Despite deep learning's advances in disease diagnosis using medical images, generic convolutional neural networks are still not widely used in medical practices due to the limited trustworthiness of these models. Although deep learning-based glaucoma classification has gained popularity in recent years, only a few of them have addressed the explainability and interpretability of the models, which increases confidence in using such applications. This study presents state-of-the-art deep learning techniques to segment and classify fundus images to predict glaucoma conditions and applies visualization techniques to explain the results to ease understandability. Our predictions are based on U-Net with attention mechanisms with ResNet50 for the segmentation process and a modified Inception V3 architecture for the classification. Attention U-Net with modified ResNet50 backbone obtained 99.58% and 98.05% accuracies for optic disc segmentation and optic cup segmentation, respectively for the RIM-ONE dataset. Additionally, we generate heatmaps that highlight the regions that impacted the glaucoma diagnosis using both Gradient-weighted Class Activation Mapping (Grad-CAM) and Grad-CAM++. Our model that classifies the segmented images achieves accuracy, sensitivity, and specificity values of 98.97%, 99.42%, and 95.59%, respectively, with the RIM-ONE dataset. This model can be used as a support tool for automated glaucoma identification using fundus images.","['U-Net with attention mechanisms', 'ResNet50', 'modified Inception V3 architecture', 'Attention U-Net with modified ResNet50 backbone', 'Gradient-weighted Class Activation Mapping (Grad-CAM)', 'Grad-CAM++']","The research idea addresses the challenge of diagnosing glaucoma, a progressive eye disease that leads to irreversible vision loss due to optic nerve damage. Despite advances in automated glaucoma diagnosis using medical images, there remains limited trust and acceptance of these methods in clinical practice, partly because of a lack of explainability and interpretability in the diagnostic process. The study recognizes the need to improve confidence in automated glaucoma detection by making the diagnostic results more understandable to users. The primary objective of the study is to develop an approach for segmenting and classifying fundus images to predict glaucoma conditions accurately, while also providing visual explanations of the diagnostic results to enhance their interpretability and ease of understanding.","The research idea addresses the challenge of diagnosing glaucoma, a progressive eye disease that leads to irreversible vision loss due to optic nerve damage. Despite advances in medical image analysis for glaucoma diagnosis, there remains limited trust and acceptance of these methods in clinical practice, partly because of a lack of explainability and interpretability in the diagnostic process. The study recognizes the need to improve confidence in image-based glaucoma detection by making the diagnostic results more understandable to users. The primary objective of the study is to develop an approach for segmenting and classifying fundus images to predict glaucoma conditions accurately, while also providing visual explanations of the diagnostic results to enhance their interpretability and ease of understanding.",True
Biology,Improved random forest algorithms for increasing the accuracy of forest aboveground biomass estimation using Sentinel-2 imagery,"A simpler, unbiased, and comprehensive random forest (RF) model is needed to improve the accuracy of aboveground biomass (AGB) estimation. In this study, data were obtained from 128 sample plots of Pinus yunnanensis forest located in Chuxiong prefecture, Yunnan province, China. Sentinel-2 imagery data were applied to extract the important predictors of forest AGB, which were screened using the Boruta algorithm. We compared the fitting performance of two modified random forest models − regularized random forest (RRF) and quantile random forest (QRF) − with the random forest model. Moreover, we combined the smallest mean error of each quantile model as the best QRF (QRFb). The result showed: (1) Window sizes of 3 × 3 pixels and 5 × 5 pixels demonstrated greater sensitivity and suitability for estimating AGB than the 7 × 7 pixels window size. Enhanced vegetation indices derived from Red Edge 1 (B5) and Near-Infrared bands (B8A) were strongly correlated with AGB, indicating the heightened sensitivity of B5 and B8A bands to biomass and their potential in AGB estimation. (2) The RRF model outperformed both the standard RF and QRF in fitting performance, with an R2 of 0.56 and RMSE 57.14 Mg/ha. (3) The QRFb model exhibited the highest R2 of 0.88 and lowest RMSE of 29.56 Mg/ha, significantly reducing overestimation and underestimation issues. The modified RF regression supplies new insights into improving forest AGB estimation, which will be helpful for future research addressing carbon cycling.","['random forest (RF)', 'Boruta algorithm', 'regularized random forest (RRF)', 'quantile random forest (QRF)']","The study addresses the need for a simpler, unbiased, and comprehensive approach to improve the accuracy of estimating aboveground biomass (AGB) in forests. Accurate AGB estimation is crucial for understanding forest carbon storage and dynamics, which are important for ecological research and carbon cycling assessments. The primary aim of the study is to enhance the estimation of AGB in Pinus yunnanensis forests by identifying important predictors from satellite imagery and evaluating different modeling approaches to improve the precision and reliability of biomass estimates. This research seeks to provide improved methods for AGB estimation that can support future studies related to forest carbon cycling.","The study addresses the need for a simpler, unbiased, and comprehensive approach to improve the accuracy of estimating aboveground biomass (AGB) in forests. Accurate AGB estimation is crucial for understanding forest carbon storage and dynamics, which are important for ecological research and carbon cycling assessments. The primary aim of the study is to enhance the estimation of AGB in Pinus yunnanensis forests by identifying important predictors from satellite imagery and evaluating different analytical approaches to improve the precision and reliability of biomass estimates. This research seeks to provide improved methods for AGB estimation that can support future studies related to forest carbon cycling.",True
Biology,GAN based augmentation using a hybrid loss function for dermoscopy images,"Dermatology is the most appropriate field to utilize pattern recognition-based automated techniques for objective, accurate, and rapid diagnosis because diagnosis mainly relies on visual examinations of skin lesions. Recent approaches utilizing deep learning techniques have shown remarkable results in this field. However, they necessitate a substantial quantity of images and the availability of dermoscopy images is often limited. Also, even if enough images are available, their labeling requires expert knowledge and is time-consuming. To overcome these issues, an efficient augmentation approach is needed to expand training datasets from input images. Therefore, in this work, a generative adversarial network has been developed using a new hybrid loss function constructed with traditional loss functions to enhance the generation power of the architecture. Also, the effect of the proposed approach and different generative network-based augmentations, which have been used with dermoscopy images in the literature, on the classification of skin lesions has been investigated. Therefore, the main contributions of this work are: (i) introducing a new generative model for the augmentation of dermoscopy images; (ii) presenting the effect of the proposed model on the classification of the images; (iii) comparative evaluations of the effectiveness of different generative network-based augmentations in the classification of seven forms of skin lesions. The classification accuracy when the proposed augmentation is used is 93.12%, which is higher than its counterparts. Experimental results indicate the significance of augmentation techniques in the classification of skin lesions and the efficiency of the proposed structure in improving the classification accuracy.",['generative adversarial network'],"The research idea centers on the challenge of diagnosing skin lesions in dermatology, which primarily depends on visual examination and requires objective, accurate, and rapid methods. However, the limited availability of dermoscopy images and the time-consuming process of expert labeling hinder the development of effective diagnostic approaches. To address these limitations, there is a need to expand the available image datasets to improve the diagnosis of skin lesions. The primary objective of the study is to develop a new approach for augmenting dermoscopy images to enhance the classification of different types of skin lesions. The study aims to introduce a novel generative model for image augmentation, evaluate its impact on the classification accuracy of skin lesions, and compare its effectiveness with other augmentation methods used for seven forms of skin lesions.","The research idea centers on the challenge of diagnosing skin lesions in dermatology, which primarily depends on visual examination and requires objective, accurate, and rapid methods. However, the limited availability of dermoscopy images and the time-consuming process of expert labeling hinder the development of effective diagnostic approaches. To address these limitations, there is a need to expand the available image datasets to improve the diagnosis of skin lesions. The primary objective of the study is to develop a new approach for augmenting dermoscopy images to enhance the identification of different types of skin lesions. The study aims to introduce a new method for image augmentation, evaluate its impact on the diagnostic accuracy of skin lesions, and compare its effectiveness with other augmentation methods used for seven forms of skin lesions.",True
Biology,Predicting transient wind loads on tall buildings in three-dimensional spatial coordinates using machine learning,"Machine learning (ML) as a subset of artificial intelligence (AI), has gained significant attention in wind engineering applications over the past decade. Wind load predictions for tall buildings using ML studies presented in literature have always been limited to static pressure measurements or time history measurements without considering the spatial coordinates system. To design wind-sensitive tall buildings, ML models must be capable of estimating transient wind flow quantities along with its spatial distribution. Thus, in this study, for the first time, the authors used ML to model the transient wind pressure on a tall building using a three-dimensional (3D) spatial coordinates system. A series of Boundary Layer Wind Tunnel tests were performed to obtain the transient pressure readings on building surfaces, which were used to validate the Computational Fluid Dynamics (CFD) models. Turbulence was modelled using large eddy simulations and the data obtained through CFD simulations were utilised to generate the ML models. The popular Extreme Gradient Boosting (XGBoost) model was selected as the ML model due to its capability of efficient data handling. The trained XGBoost model accurately predicted the transient wind pressure throughout the flow time. The XGBoost model has captured the extreme values well, closely following the flow patterns. In addition, special flow features like flow separation, reattachment, and steep pressure gradients have been well captured over the corresponding surfaces. Therefore, this study showcases the ability to use ML to predict pressures on tall buildings, capturing all key flow features time-efficiently.",['Extreme Gradient Boosting (XGBoost)'],"The study addresses the challenge of accurately predicting transient wind pressures on tall buildings, emphasizing the importance of considering the spatial distribution of wind flow rather than relying solely on static or time history pressure measurements. Understanding these transient wind flow quantities and their spatial variation is crucial for designing wind-sensitive tall structures that can withstand complex aerodynamic forces. The primary objective of the study is to model the transient wind pressure on a tall building using a three-dimensional spatial coordinate system, aiming to capture key flow features such as flow separation, reattachment, and steep pressure gradients over building surfaces. This approach seeks to improve the prediction of wind pressures throughout the flow time to better inform the design and safety assessment of tall buildings.","The study addresses the challenge of accurately predicting transient wind pressures on tall buildings, emphasizing the importance of considering the spatial distribution of wind flow rather than relying solely on static or time history pressure measurements. Understanding these transient wind flow quantities and their spatial variation is crucial for designing wind-sensitive tall structures that can withstand complex aerodynamic forces. The primary objective of the study is to characterize the transient wind pressure on a tall building using a three-dimensional spatial coordinate system, aiming to capture key flow features such as flow separation, reattachment, and steep pressure gradients over building surfaces. This approach seeks to improve the estimation of wind pressures throughout the flow time to better inform the design and safety assessment of tall buildings.",True
Biology,Assessing ChatGPT 4.0’s test performance and clinical diagnostic accuracy on USMLE STEP 2 CK and clinical case reports,"Abstract While there is data assessing the test performance of artificial intelligence (AI) chatbots, including the Generative Pre-trained Transformer 4.0 (GPT 4) chatbot (ChatGPT 4.0), there is scarce data on its diagnostic accuracy of clinical cases. We assessed the large language model (LLM), ChatGPT 4.0, on its ability to answer questions from the United States Medical Licensing Exam (USMLE) Step 2, as well as its ability to generate a differential diagnosis based on corresponding clinical vignettes from published case reports. A total of 109 Step 2 Clinical Knowledge (CK) practice questions were inputted into both ChatGPT 3.5 and ChatGPT 4.0, asking ChatGPT to pick the correct answer. Compared to its previous version, ChatGPT 3.5, we found improved accuracy of ChatGPT 4.0 when answering these questions, from 47.7 to 87.2% ( p = 0.035) respectively. Utilizing the topics tested on Step 2 CK questions, we additionally found 63 corresponding published case report vignettes and asked ChatGPT 4.0 to come up with its top three differential diagnosis. ChatGPT 4.0 accurately created a shortlist of differential diagnoses in 74.6% of the 63 case reports (74.6%). We analyzed ChatGPT 4.0’s confidence in its diagnosis by asking it to rank its top three differentials from most to least likely. Out of the 47 correct diagnoses, 33 were the first (70.2%) on the differential diagnosis list, 11 were second (23.4%), and three were third (6.4%). Our study shows the continued iterative improvement in ChatGPT’s ability to answer standardized USMLE questions accurately and provides insights into ChatGPT’s clinical diagnostic accuracy.","['Generative Pre-trained Transformer 4.0 (GPT 4) chatbot (ChatGPT 4.0)', 'large language model (LLM)']","The research idea centers on the need to evaluate the diagnostic accuracy of clinical case assessments, as there is limited data on how well current tools perform in generating accurate clinical diagnoses. Specifically, the study addresses the gap in understanding the ability to correctly answer clinical knowledge questions and generate differential diagnoses based on clinical vignettes. The research objective is to assess the performance of a large language model in answering United States Medical Licensing Exam Step 2 Clinical Knowledge questions and its ability to generate accurate differential diagnoses from published clinical case vignettes. The study aims to measure the accuracy of the model’s responses and its confidence in ranking differential diagnoses to provide insights into its clinical diagnostic capabilities.","The research idea centers on the need to evaluate the diagnostic accuracy of clinical case assessments, as there is limited data on how well current tools perform in generating accurate clinical diagnoses. Specifically, the study addresses the gap in understanding the ability to correctly answer clinical knowledge questions and generate differential diagnoses based on clinical vignettes. The research objective is to assess the performance of a diagnostic system in answering United States Medical Licensing Exam Step 2 Clinical Knowledge questions and its ability to generate accurate differential diagnoses from published clinical case vignettes. The study aims to measure the accuracy of the system's responses and its confidence in ranking differential diagnoses to provide insights into its clinical diagnostic capabilities.",True
Biology,Improving River Routing Using a Differentiable Muskingum‐Cunge Model and Physics‐Informed Machine Learning,"Abstract Recently, rainfall‐runoff simulations in small headwater basins have been improved by methodological advances such as deep neural networks (NNs) and hybrid physics‐NN models—particularly, a genre called differentiable modeling that intermingles NNs with physics to learn relationships between variables. However, hydrologic routing simulations, necessary for simulating floods in stem rivers downstream of large heterogeneous basins, had not yet benefited from these advances and it was unclear if the routing process could be improved via coupled NNs. We present a novel differentiable routing method ( δ MC‐Juniata‐hydroDL2) that mimics the classical Muskingum‐Cunge routing model over a river network but embeds an NN to infer parameterizations for Manning's roughness ( n ) and channel geometries from raw reach‐scale attributes like catchment areas and sinuosity. The NN was trained solely on downstream hydrographs. Synthetic experiments show that while the channel geometry parameter was unidentifiable, n can be identified with moderate precision. With real‐world data, the trained differentiable routing model produced more accurate long‐term routing results for both the training gage and untrained inner gages for larger subbasins (&gt;2,000 km 2 ) than either a machine learning model assuming homogeneity, or simply using the sum of runoff from subbasins. The n parameterization trained on short periods gave high performance in other periods, despite significant errors in runoff inputs. The learned n pattern was consistent with literature expectations, demonstrating the framework's potential for knowledge discovery, but the absolute values can vary depending on training periods. The trained n parameterization can be coupled with traditional models to improve national‐scale hydrologic flood simulations.","['deep neural networks (NNs)', 'hybrid physics‐NN models', 'differentiable modeling']","The research idea addresses the challenge of improving hydrologic routing simulations necessary for simulating floods in stem rivers downstream of large heterogeneous basins, an area that had not yet benefited from recent advances in rainfall-runoff modeling. The study focuses on enhancing the representation of channel characteristics such as Manning's roughness and channel geometries to achieve more accurate flood routing over river networks. The primary objective of the study is to develop and evaluate a novel routing method that mimics classical hydrologic routing models while inferring parameterizations for channel roughness and geometry from physical attributes of river reaches. The aim is to produce more accurate long-term routing results for large subbasins and demonstrate the potential for improved flood simulations at national scales by coupling the inferred parameterizations with traditional hydrologic models.","The research idea addresses the challenge of improving hydrologic routing simulations necessary for simulating floods in stem rivers downstream of large heterogeneous basins, an area that had not yet benefited from recent advances in rainfall-runoff modeling. The study focuses on enhancing the representation of channel characteristics such as Manning's roughness and channel geometries to achieve more accurate flood routing over river networks. The primary objective of the study is to develop and evaluate a new routing method that resembles classical hydrologic routing models while inferring parameterizations for channel roughness and geometry from physical attributes of river reaches. The aim is to produce more accurate long-term routing results for large subbasins and demonstrate the potential for improved flood simulations at national scales by coupling the inferred parameterizations with traditional hydrologic models.",True
Biology,Traffic Sign Detection and Recognition Using YOLO Object Detection Algorithm: A Systematic Review,"Context: YOLO (You Look Only Once) is an algorithm based on deep neural networks with real-time object detection capabilities. This state-of-the-art technology is widely available, mainly due to its speed and precision. Since its conception, YOLO has been applied to detect and recognize traffic signs, pedestrians, traffic lights, vehicles, and so on. Objective: The goal of this research is to systematically analyze the YOLO object detection algorithm, applied to traffic sign detection and recognition systems, from five relevant aspects of this technology: applications, datasets, metrics, hardware, and challenges. Method: This study performs a systematic literature review (SLR) of studies on traffic sign detection and recognition using YOLO published in the years 2016–2022. Results: The search found 115 primary studies relevant to the goal of this research. After analyzing these investigations, the following relevant results were obtained. The most common applications of YOLO in this field are vehicular security and intelligent and autonomous vehicles. The majority of the sign datasets used to train, test, and validate YOLO-based systems are publicly available, with an emphasis on datasets from Germany and China. It has also been discovered that most works present sophisticated detection, classification, and processing speed metrics for traffic sign detection and recognition systems by using the different versions of YOLO. In addition, the most popular desktop data processing hardwares are Nvidia RTX 2080 and Titan Tesla V100 and, in the case of embedded or mobile GPU platforms, Jetson Xavier NX. Finally, seven relevant challenges that these systems face when operating in real road conditions have been identified. With this in mind, research has been reclassified to address these challenges in each case. Conclusions: This SLR is the most relevant and current work in the field of technology development applied to the detection and recognition of traffic signs using YOLO. In addition, insights are provided about future work that could be conducted to improve the field.",['YOLO (You Only Look Once)'],"The research idea centers on the need to improve the detection and recognition of traffic signs, which is crucial for enhancing vehicular security and the development of intelligent and autonomous vehicles. Accurate and efficient traffic sign recognition systems are essential for safe navigation and real-time decision-making in road environments. The study aims to systematically analyze existing approaches to traffic sign detection and recognition to better understand their applications, datasets, performance metrics, hardware requirements, and the challenges faced in real-world conditions. The primary objective of this research is to conduct a comprehensive review of studies focused on traffic sign detection and recognition, with the goal of identifying key aspects and challenges in the field to guide future improvements and developments.","The research idea centers on the need to improve the detection and recognition of traffic signs, which is crucial for enhancing vehicular security and the development of advanced and automated vehicles. Accurate and efficient traffic sign recognition systems are essential for safe navigation and real-time decision-making in road environments. The study aims to systematically analyze existing approaches to traffic sign detection and recognition to better understand their applications, datasets, performance metrics, hardware requirements, and the challenges faced in real-world conditions. The primary objective of this research is to conduct a comprehensive review of studies focused on traffic sign detection and recognition, with the goal of identifying key aspects and challenges in the field to guide future improvements and developments.",True
Biology,Advancing entity recognition in biomedicine via instruction tuning of large language models,"Abstract Motivation Large Language Models (LLMs) have the potential to revolutionize the field of Natural Language Processing, excelling not only in text generation and reasoning tasks but also in their ability for zero/few-shot learning, swiftly adapting to new tasks with minimal fine-tuning. LLMs have also demonstrated great promise in biomedical and healthcare applications. However, when it comes to Named Entity Recognition (NER), particularly within the biomedical domain, LLMs fall short of the effectiveness exhibited by fine-tuned domain-specific models. One key reason is that NER is typically conceptualized as a sequence labeling task, whereas LLMs are optimized for text generation and reasoning tasks. Results We developed an instruction-based learning paradigm that transforms biomedical NER from a sequence labeling task into a generation task. This paradigm is end-to-end and streamlines the training and evaluation process by automatically repurposing pre-existing biomedical NER datasets. We further developed BioNER-LLaMA using the proposed paradigm with LLaMA-7B as the foundational LLM. We conducted extensive testing on BioNER-LLaMA across three widely recognized biomedical NER datasets, consisting of entities related to diseases, chemicals, and genes. The results revealed that BioNER-LLaMA consistently achieved higher F1-scores ranging from 5% to 30% compared to the few-shot learning capabilities of GPT-4 on datasets with different biomedical entities. We show that a general-domain LLM can match the performance of rigorously fine-tuned PubMedBERT models and PMC-LLaMA, biomedical-specific language model. Our findings underscore the potential of our proposed paradigm in developing general-domain LLMs that can rival SOTA performances in multi-task, multi-domain scenarios in biomedical and health applications. Availability and implementation Datasets and other resources are available at https://github.com/BIDS-Xu-Lab/BioNER-LLaMA.","['few-shot learning', 'LLaMA-7B', 'GPT-4 few-shot learning', 'PubMedBERT fine-tuning']","The research addresses the challenge of effectively recognizing named entities within the biomedical domain, such as diseases, chemicals, and genes, which is crucial for advancing biomedical and healthcare applications. Traditional approaches conceptualize Named Entity Recognition (NER) as a sequence labeling task, but existing models optimized for text generation and reasoning do not perform as well in this context. The study aims to improve the performance of biomedical NER by transforming it into a generation task, thereby enhancing the ability to identify various biomedical entities across multiple datasets. The primary objective is to develop and evaluate a new paradigm that enables general-domain models to achieve competitive or superior results compared to specialized biomedical models in recognizing named entities related to diseases, chemicals, and genes.","The research addresses the challenge of effectively recognizing named entities within the biomedical domain, such as diseases, chemicals, and genes, which is crucial for advancing biomedical and healthcare applications. Traditional approaches conceptualize Named Entity Recognition (NER) as a sequence labeling task, but existing methodologies optimized for text generation and reasoning do not perform as well in this context. The study aims to improve the performance of biomedical NER by transforming it into a generation task, thereby enhancing the ability to identify various biomedical entities across multiple datasets. The primary objective is to develop and evaluate a new paradigm that enables general-domain approaches to achieve competitive or superior results compared to specialized biomedical methods in recognizing named entities related to diseases, chemicals, and genes.",True
Biology,Reliable water quality prediction and parametric analysis using explainable AI models,"Abstract The consumption of water constitutes the physical health of most of the living species and hence management of its purity and quality is extremely essential as contaminated water has to potential to create adverse health and environmental consequences. This creates the dire necessity to measure, control and monitor the quality of water. The primary contaminant present in water is Total Dissolved Solids (TDS), which is hard to filter out. There are various substances apart from mere solids such as potassium, sodium, chlorides, lead, nitrate, cadmium, arsenic and other pollutants. The proposed work aims to provide the automation of water quality estimation through Artificial Intelligence and uses Explainable Artificial Intelligence (XAI) for the explanation of the most significant parameters contributing towards the potability of water and the estimation of the impurities. XAI has the transparency and justifiability as a white-box model since the Machine Learning (ML) model is black-box and unable to describe the reasoning behind the ML classification. The proposed work uses various ML models such as Logistic Regression, Support Vector Machine (SVM), Gaussian Naive Bayes, Decision Tree (DT) and Random Forest (RF) to classify whether the water is drinkable. The various representations of XAI such as force plot, test patch, summary plot, dependency plot and decision plot generated in SHAPELY explainer explain the significant features, prediction score, feature importance and justification behind the water quality estimation. The RF classifier is selected for the explanation and yields optimum Accuracy and F1-Score of 0.9999, with Precision and Re-call of 0.9997 and 0.998 respectively. Thus, the work is an exploratory analysis of the estimation and management of water quality with indicators associated with their significance. This work is an emerging research at present with a vision of addressing the water quality for the future as well.","['Logistic Regression', 'Support Vector Machine (SVM)', 'Gaussian Naive Bayes', 'Decision Tree (DT)', 'Random Forest (RF)']","The consumption of water is vital for the physical health of most living species, making the management of its purity and quality extremely important due to the potential adverse health and environmental consequences caused by contaminated water. Total Dissolved Solids (TDS) and various other substances such as potassium, sodium, chlorides, lead, nitrate, cadmium, and arsenic are primary contaminants that affect water quality and are difficult to remove. The study aims to estimate and monitor water quality by identifying the most significant parameters contributing to water potability and impurity levels. The primary objective is to classify whether water is drinkable based on these key indicators and to provide an explanation of their significance in assessing water quality for better management and future sustainability.","The consumption of water is vital for the physical health of most living species, making the management of its purity and quality extremely important due to the potential adverse health and environmental consequences caused by contaminated water. Total Dissolved Solids (TDS) and various other substances such as potassium, sodium, chlorides, lead, nitrate, cadmium, and arsenic are primary contaminants that affect water quality and are difficult to remove. The study aims to estimate and monitor water quality by identifying the most significant parameters contributing to water potability and impurity levels. The primary objective is to determine whether water is drinkable based on these key indicators and to provide an explanation of their significance in assessing water quality for better management and future sustainability.",True
Psychology,Multiple Classification of Brain MRI Autism Spectrum Disorder by Age and Gender Using Deep Learning,"Abstract The fact that the rapid and definitive diagnosis of autism cannot be made today and that autism cannot be treated provides an impetus to look into novel technological solutions. To contribute to the resolution of this problem through multiple classifications by considering age and gender factors, in this study, two quadruple and one octal classifications were performed using a deep learning (DL) approach. Gender in one of the four classifications and age groups in the other were considered. In the octal classification, classes were created considering gender and age groups. In addition to the diagnosis of ASD (Autism Spectrum Disorders), another goal of this study is to find out the contribution of gender and age factors to the diagnosis of ASD by making multiple classifications based on age and gender for the first time. Brain structural MRI (sMRI) scans of participators with ASD and TD (Typical Development) were pre-processed in the system originally designed for this purpose. Using the Canny Edge Detection (CED) algorithm, the sMRI image data was cropped in the data pre-processing stage, and the data set was enlarged five times with the data augmentation (DA) techniques. The most optimal convolutional neural network (CNN) models were developed using the grid search optimization (GSO) algorism. The proposed DL prediction system was tested with the five-fold cross-validation technique. Three CNN models were designed to be used in the system. The first of these models is the quadruple classification model created by taking gender into account (model 1), the second is the quadruple classification model created by taking into account age (model 2), and the third is the eightfold classification model created by taking into account both gender and age (model 3). ). The accuracy rates obtained for all three designed models are 80.94, 85.42 and 67.94, respectively. These obtained accuracy rates were compared with pre-trained models by using the transfer learning approach. As a result, it was revealed that age and gender factors were effective in the diagnosis of ASD with the system developed for ASD multiple classifications, and higher accuracy rates were achieved compared to pre-trained models.","['convolutional neural network (CNN) models', 'transfer learning approach']","The research idea centers on the challenge that autism cannot currently be diagnosed rapidly and definitively, nor can it be treated effectively, which motivates the exploration of new approaches to improve diagnosis. This study addresses the need to understand how age and gender factors contribute to the diagnosis of Autism Spectrum Disorders (ASD), recognizing that these factors may influence diagnostic accuracy. The primary objective of the study is to investigate the contribution of gender and age to the diagnosis of ASD by performing multiple classifications based on these factors for the first time. Specifically, the study aims to assess how considering age and gender separately and together affects the accuracy of ASD diagnosis.","The research idea centers on the challenge that autism cannot currently be diagnosed rapidly and definitively, nor can it be treated effectively, which motivates the exploration of new approaches to improve diagnosis. This study addresses the need to understand how age and gender factors contribute to the diagnosis of Autism Spectrum Disorders (ASD), recognizing that these factors may influence diagnostic accuracy. The primary objective of the study is to investigate the contribution of gender and age to the diagnosis of ASD by examining these factors separately and in combination for the first time. Specifically, the study aims to assess how considering age and gender separately and together affects the accuracy of ASD diagnosis.",True
Psychology,An Explainable AI Paradigm for Alzheimer’s Diagnosis Using Deep Transfer Learning,"Alzheimer’s disease (AD) is a progressive neurodegenerative disorder that affects millions of individuals worldwide, causing severe cognitive decline and memory impairment. The early and accurate diagnosis of AD is crucial for effective intervention and disease management. In recent years, deep learning techniques have shown promising results in medical image analysis, including AD diagnosis from neuroimaging data. However, the lack of interpretability in deep learning models hinders their adoption in clinical settings, where explainability is essential for gaining trust and acceptance from healthcare professionals. In this study, we propose an explainable AI (XAI)-based approach for the diagnosis of Alzheimer’s disease, leveraging the power of deep transfer learning and ensemble modeling. The proposed framework aims to enhance the interpretability of deep learning models by incorporating XAI techniques, allowing clinicians to understand the decision-making process and providing valuable insights into disease diagnosis. By leveraging popular pre-trained convolutional neural networks (CNNs) such as VGG16, VGG19, DenseNet169, and DenseNet201, we conducted extensive experiments to evaluate their individual performances on a comprehensive dataset. The proposed ensembles, Ensemble-1 (VGG16 and VGG19) and Ensemble-2 (DenseNet169 and DenseNet201), demonstrated superior accuracy, precision, recall, and F1 scores compared to individual models, reaching up to 95%. In order to enhance interpretability and transparency in Alzheimer’s diagnosis, we introduced a novel model achieving an impressive accuracy of 96%. This model incorporates explainable AI techniques, including saliency maps and grad-CAM (gradient-weighted class activation mapping). The integration of these techniques not only contributes to the model’s exceptional accuracy but also provides clinicians and researchers with visual insights into the neural regions influencing the diagnosis. Our findings showcase the potential of combining deep transfer learning with explainable AI in the realm of Alzheimer’s disease diagnosis, paving the way for more interpretable and clinically relevant AI models in healthcare.","['deep learning', 'explainable AI (XAI)', 'deep transfer learning', 'ensemble modeling', 'pre-trained convolutional neural networks (CNNs)', 'VGG16', 'VGG19', 'DenseNet169', 'DenseNet201', 'saliency maps', 'grad-CAM (gradient-weighted class activation mapping)']","The research idea centers on addressing the challenge of early and accurate diagnosis of Alzheimer’s disease, a progressive neurodegenerative disorder characterized by severe cognitive decline and memory impairment. Despite advances in diagnostic approaches, there remains a critical need for methods that are not only accurate but also interpretable to gain trust and acceptance from healthcare professionals in clinical settings. The study highlights the importance of enhancing the transparency and explainability of diagnostic processes to improve disease management and intervention outcomes. The primary objective of the study is to develop an approach that improves the interpretability of Alzheimer’s disease diagnosis, enabling clinicians to better understand the decision-making process behind diagnostic outcomes. This objective aims to provide valuable insights into the neural regions influencing the diagnosis, thereby facilitating more informed clinical decisions and advancing the relevance of diagnostic tools in healthcare.","The research idea centers on addressing the challenge of early and accurate diagnosis of Alzheimer's disease, a progressive neurodegenerative disorder characterized by severe cognitive decline and memory impairment. Despite advances in diagnostic approaches, there remains a critical need for methods that are not only accurate but also interpretable to gain trust and acceptance from healthcare professionals in clinical settings. The study highlights the importance of enhancing the transparency and explainability of diagnostic processes to improve disease management and intervention outcomes. The primary objective of the study is to develop an approach that improves the interpretability of Alzheimer's disease diagnosis, enabling clinicians to better understand the reasoning behind diagnostic outcomes. This objective aims to provide valuable insights into the neural regions influencing the diagnosis, thereby facilitating more informed clinical decisions and advancing the relevance of diagnostic tools in healthcare.",True
Psychology,Larger and more instructable language models become less reliable,"Abstract The prevailing methods to make large language models more powerful and amenable have been based on continuous scaling up (that is, increasing their size, data volume and computational resources 1 ) and bespoke shaping up (including post-filtering 2,3 , fine tuning or use of human feedback 4,5 ). However, larger and more instructable large language models may have become less reliable. By studying the relationship between difficulty concordance, task avoidance and prompting stability of several language model families, here we show that easy instances for human participants are also easy for the models, but scaled-up, shaped-up models do not secure areas of low difficulty in which either the model does not err or human supervision can spot the errors. We also find that early models often avoid user questions but scaled-up, shaped-up models tend to give an apparently sensible yet wrong answer much more often, including errors on difficult questions that human supervisors frequently overlook. Moreover, we observe that stability to different natural phrasings of the same question is improved by scaling-up and shaping-up interventions, but pockets of variability persist across difficulty levels. These findings highlight the need for a fundamental shift in the design and development of general-purpose artificial intelligence, particularly in high-stakes areas for which a predictable distribution of errors is paramount.","['post-filtering', 'fine tuning', 'use of human feedback']","The research idea centers on understanding the reliability and error patterns of increasingly large and refined language models, particularly in relation to how these models handle tasks of varying difficulty compared to human participants. The study addresses concerns that while scaling and shaping these models improve certain aspects, they may also lead to more frequent and less detectable errors, especially on challenging questions. The primary objective of the study is to investigate the relationship between task difficulty, task avoidance, and response stability in different language model families, with a focus on identifying areas where models fail or succeed relative to human supervision. The aim is to highlight the limitations of current approaches and emphasize the need for a fundamental change in designing systems that require predictable and reliable performance, especially in high-stakes contexts.","The research idea centers on understanding the reliability and error patterns of increasingly large and refined language systems, particularly in relation to how these systems handle tasks of varying difficulty compared to human participants. The study addresses concerns that while improving and refining these systems enhances certain aspects, they may also lead to more frequent and less detectable errors, especially on challenging questions. The primary objective of the study is to investigate the relationship between task difficulty, task avoidance, and response stability in different system families, with a focus on identifying areas where these systems fail or succeed relative to human supervision. The aim is to highlight the limitations of current approaches and emphasize the need for a fundamental change in designing systems that require predictable and reliable performance, especially in high-stakes contexts.",True
Psychology,Evaluating the ChatGPT family of models for biomedical reasoning and classification,"Abstract Objective Large language models (LLMs) have shown impressive ability in biomedical question-answering, but have not been adequately investigated for more specific biomedical applications. This study investigates ChatGPT family of models (GPT-3.5, GPT-4) in biomedical tasks beyond question-answering. Materials and Methods We evaluated model performance with 11 122 samples for two fundamental tasks in the biomedical domain—classification (n = 8676) and reasoning (n = 2446). The first task involves classifying health advice in scientific literature, while the second task is detecting causal relations in biomedical literature. We used 20% of the dataset for prompt development, including zero- and few-shot settings with and without chain-of-thought (CoT). We then evaluated the best prompts from each setting on the remaining dataset, comparing them to models using simple features (BoW with logistic regression) and fine-tuned BioBERT models. Results Fine-tuning BioBERT produced the best classification (F1: 0.800-0.902) and reasoning (F1: 0.851) results. Among LLM approaches, few-shot CoT achieved the best classification (F1: 0.671-0.770) and reasoning (F1: 0.682) results, comparable to the BoW model (F1: 0.602-0.753 and 0.675 for classification and reasoning, respectively). It took 78 h to obtain the best LLM results, compared to 0.078 and 0.008 h for the top-performing BioBERT and BoW models, respectively. Discussion The simple BoW model performed similarly to the most complex LLM prompting. Prompt engineering required significant investment. Conclusion Despite the excitement around viral ChatGPT, fine-tuning for two fundamental biomedical natural language processing tasks remained the best strategy.","['ChatGPT family of models (GPT-3.5, GPT-4)', 'zero-shot prompting', 'few-shot prompting', 'chain-of-thought (CoT) prompting', 'Bag of Words (BoW) with logistic regression', 'fine-tuned BioBERT models']","The research idea centers on addressing the need to evaluate the effectiveness of advanced language models in specific biomedical applications beyond general question-answering, particularly focusing on tasks related to understanding and interpreting scientific health advice and causal relationships in biomedical literature. This study is motivated by the gap in knowledge regarding how well these models perform in fundamental biomedical tasks that are crucial for accurate information classification and reasoning within the psychological and health sciences context. The primary objective of the study is to investigate the performance of different approaches in classifying health advice and detecting causal relations in biomedical texts, aiming to determine which method yields the most accurate and reliable results for these essential tasks. The study seeks to compare various strategies to identify the best approach for supporting biomedical understanding and decision-making in psychological research and practice.","The research idea centers on addressing the need to evaluate the effectiveness of advanced language interpretation systems in specific biomedical applications beyond general question-answering, particularly focusing on tasks related to understanding and interpreting scientific health advice and causal relationships in biomedical literature. This study is motivated by the gap in knowledge regarding how well these systems perform in fundamental biomedical tasks that are crucial for accurate information classification and reasoning within the psychological and health sciences context. The primary objective of the study is to investigate the performance of different approaches in classifying health advice and detecting causal relations in biomedical texts, aiming to determine which method yields the most accurate and reliable results for these essential tasks. The study seeks to compare various strategies to identify the best approach for supporting biomedical understanding and decision-making in psychological research and practice.",True
Psychology,Machine Learning and Digital Biomarkers Can Detect Early Stages of Neurodegenerative Diseases,"Neurodegenerative diseases (NDs) such as Alzheimer’s Disease (AD) and Parkinson’s Disease (PD) are devastating conditions that can develop without noticeable symptoms, causing irreversible damage to neurons before any signs become clinically evident. NDs are a major cause of disability and mortality worldwide. Currently, there are no cures or treatments to halt their progression. Therefore, the development of early detection methods is urgently needed to delay neuronal loss as soon as possible. Despite advancements in Medtech, the early diagnosis of NDs remains a challenge at the intersection of medical, IT, and regulatory fields. Thus, this review explores “digital biomarkers” (tools designed for remote neurocognitive data collection and AI analysis) as a potential solution. The review summarizes that recent studies combining AI with digital biomarkers suggest the possibility of identifying pre-symptomatic indicators of NDs. For instance, research utilizing convolutional neural networks for eye tracking has achieved significant diagnostic accuracies. ROC-AUC scores reached up to 0.88, indicating high model performance in differentiating between PD patients and healthy controls. Similarly, advancements in facial expression analysis through tools have demonstrated significant potential in detecting emotional changes in ND patients, with some models reaching an accuracy of 0.89 and a precision of 0.85. This review follows a structured approach to article selection, starting with a comprehensive database search and culminating in a rigorous quality assessment and meaning for NDs of the different methods. The process is visualized in 10 tables with 54 parameters describing different approaches and their consequences for understanding various mechanisms in ND changes. However, these methods also face challenges related to data accuracy and privacy concerns. To address these issues, this review proposes strategies that emphasize the need for rigorous validation and rapid integration into clinical practice. Such integration could transform ND diagnostics, making early detection tools more cost-effective and globally accessible. In conclusion, this review underscores the urgent need to incorporate validated digital health tools into mainstream medical practice. This integration could indicate a new era in the early diagnosis of neurodegenerative diseases, potentially altering the trajectory of these conditions for millions worldwide. Thus, by highlighting specific and statistically significant findings, this review demonstrates the current progress in this field and the potential impact of these advancements on the global management of NDs.",['convolutional neural networks'],"The research idea centers on the urgent need for early detection methods for neurodegenerative diseases such as Alzheimer’s Disease and Parkinson’s Disease, which often develop without noticeable symptoms and cause irreversible neuronal damage before clinical signs appear. These diseases are a major cause of disability and mortality worldwide, and currently, there are no cures or treatments to halt their progression. Early diagnosis is crucial to delay neuronal loss and improve patient outcomes, yet it remains a significant challenge in the medical field. The study addresses the potential of emerging approaches to identify pre-symptomatic indicators and improve understanding of disease mechanisms.

The research objective of this review is to explore and summarize recent advancements in tools designed for remote neurocognitive data collection and their potential to facilitate early diagnosis of neurodegenerative diseases. It aims to evaluate the effectiveness and challenges of these approaches, highlight statistically significant findings, and propose strategies for rigorous validation and integration into clinical practice. Ultimately, the study seeks to demonstrate how these advancements could transform diagnostics, making early detection more accessible and cost-effective, thereby potentially altering the trajectory of neurodegenerative diseases on a global scale.","The research idea centers on the urgent need for early detection methods for neurodegenerative diseases such as Alzheimer's Disease and Parkinson's Disease, which often develop without noticeable symptoms and cause irreversible neuronal damage before clinical signs appear. These diseases are a major cause of disability and mortality worldwide, and currently, there are no cures or treatments to halt their progression. Early diagnosis is crucial to delay neuronal loss and improve patient outcomes, yet it remains a significant challenge in the medical field. The study addresses the potential of emerging approaches to identify pre-symptomatic indicators and improve understanding of disease mechanisms.

The research objective of this review is to explore and summarize recent advancements in tools designed for remote neurocognitive data collection and their potential to facilitate early diagnosis of neurodegenerative diseases. It aims to evaluate the effectiveness and challenges of these approaches, highlight statistically significant findings, and propose strategies for rigorous validation and integration into clinical practice. Ultimately, the study seeks to demonstrate how these advancements could transform diagnostics, making early detection more accessible and cost-effective, thereby potentially altering the trajectory of neurodegenerative diseases on a global scale.",True
Psychology,Reliability of ChatGPT for performing triage task in the emergency department using the Korean Triage and Acuity Scale,"Background Artificial intelligence (AI) technology can enable more efficient decision-making in healthcare settings. There is a growing interest in improving the speed and accuracy of AI systems in providing responses for given tasks in healthcare settings. Objective This study aimed to assess the reliability of ChatGPT in determining emergency department (ED) triage accuracy using the Korean Triage and Acuity Scale (KTAS). Methods Two hundred and two virtual patient cases were built. The gold standard triage classification for each case was established by an experienced ED physician. Three other human raters (ED paramedics) were involved and rated the virtual cases individually. The virtual cases were also rated by two different versions of the chat generative pre-trained transformer (ChatGPT, 3.5 and 4.0). Inter-rater reliability was examined using Fleiss’ kappa and intra-class correlation coefficient (ICC). Results The kappa values for the agreement between the four human raters and ChatGPTs were .523 (version 4.0) and .320 (version 3.5). Of the five levels, the performance was poor when rating patients at levels 1 and 5, as well as case scenarios with additional text descriptions. There were differences in the accuracy of the different versions of GPTs. The ICC between version 3.5 and the gold standard was .520, and that between version 4.0 and the gold standard was .802. Conclusions A substantial level of inter-rater reliability was revealed when GPTs were used as KTAS raters. The current study showed the potential of using GPT in emergency healthcare settings. Considering the shortage of experienced manpower, this AI method may help improve triaging accuracy.","['chat generative pre-trained transformer (ChatGPT, 3.5 and 4.0)']","The research idea centers on addressing the challenge of accurately and efficiently determining triage levels in emergency department settings, which is critical for patient care prioritization. There is a recognized need to improve the speed and accuracy of triage decisions to enhance healthcare outcomes, especially given the shortage of experienced personnel in emergency departments. The study’s primary objective is to assess the reliability of a specific approach in determining emergency department triage accuracy using the Korean Triage and Acuity Scale (KTAS). It aims to evaluate how well this approach agrees with expert human raters in classifying patient cases according to established triage levels.","The research idea centers on addressing the challenge of accurately and efficiently determining triage levels in emergency department settings, which is critical for patient care prioritization. There is a recognized need to improve the speed and accuracy of triage decisions to enhance healthcare outcomes, especially given the shortage of experienced personnel in emergency departments. The study's primary objective is to assess the reliability of a specific approach in determining emergency department triage accuracy using the Korean Triage and Acuity Scale (KTAS). It aims to evaluate how well this approach agrees with expert human raters in classifying patient cases according to established triage levels.",True
Psychology,Deep Reinforcement Learning Unleashing the Power of AI in Decision-Making,"Deep Reinforcement Learning (DRL) has emerged as a transformative paradigm in the field of artificial intelligence (AI), offering unprecedented capabilities in decision-making across diverse domains. This article explores the profound impact of DRL on enhancing the decision-making capabilities of AI systems, elucidating its underlying principles, applications, and implications.DRL represents a fusion of deep learning and reinforcement learning, enabling machines to learn complex behaviors and make decisions by interacting with their environment. The utilization of neural networks allows DRL algorithms to handle high-dimensional input spaces, making it well-suited for tasks that involve intricate decision-making processes.One of the key strengths of DRL lies in its ability to address problems with sparse and delayed rewards, common challenges in traditional reinforcement learning. Through a process of trial and error, DRL algorithms can learn optimal decision strategies by navigating through a vast decision space, adapting to dynamic environments, and maximizing cumulative rewards over time.The applications of DRL span various domains, including robotics, finance, healthcare, gaming, and autonomous systems. In robotics, DRL facilitates the development of intelligent agents capable of autonomously navigating complex environments, performing intricate tasks, and adapting to unforeseen circumstances. In finance, DRL is leveraged for portfolio optimization, algorithmic trading, and risk management, demonstrating its potential to revolutionize traditional financial strategies.","['Deep Reinforcement Learning (DRL)', 'deep learning', 'reinforcement learning']","The research idea centers on the challenge of enhancing decision-making capabilities in complex and dynamic environments, particularly where outcomes are influenced by sparse and delayed feedback. The study addresses the need for approaches that enable adaptive behavior and optimal strategy development through interaction with the environment. The primary objective of the study is to explore how decision-making processes can be improved by learning from experience in situations involving intricate and high-dimensional inputs. It aims to understand mechanisms that support the development of intelligent agents capable of navigating complex tasks and adapting to changing circumstances over time.","The research idea centers on the challenge of enhancing decision-making capabilities in complex and dynamic environments, particularly where outcomes are influenced by sparse and delayed feedback. The study addresses the need for approaches that enable adaptive behavior and optimal strategy development through interaction with the environment. The primary objective of the study is to explore how decision-making processes can be improved by gaining insights from experience in situations involving intricate and high-dimensional inputs. It aims to understand mechanisms that support the development of effective problem-solving methods capable of navigating complex tasks and adapting to changing circumstances over time.",True
Psychology,MentaLLaMA: Interpretable Mental Health Analysis on Social Media with Large Language Models,"As an integral part of people's daily lives, social media is becoming a rich source for automatic mental health analysis.As traditional discriminative methods bear poor generalization ability and low interpretability, the recent large language models (LLMs) have been explored for interpretable mental health analysis on social media, which aims to provide detailed explanations along with predictions in zero-shot or few-shot settings.The results show that LLMs still achieve unsatisfactory classification performance in a zero-shot/few-shot manner, which further significantly affects the quality of the generated explanations.Domain-specific finetuning is an effective solution, but faces two critical challenges: 1) lack of high-quality training data.2) no open-source foundation LLMs.To alleviate these problems, we formally model interpretable mental health analysis as a text generation task, and build the first multi-task and multi-source interpretable mental health instruction (IMHI) dataset with 105K data samples to support LLM instruction tuning and evaluation.The raw social media data are collected from 10 existing sources covering 8 mental health analysis tasks.We prompt ChatGPT with expert-designed few-shot prompts to obtain explanations.To ensure the reliability of the explanations, we perform strict automatic and human evaluations on the correctness, consistency, and quality of generated data.Based on the IMHI dataset and LLaMA2 foundation models, we train MentaLLaMA, the first open-source instruction-following LLM series for interpretable mental health analysis on social media.We evaluate Men-taLLaMA and other advanced methods on the IMHI benchmark, the first holistic evaluation benchmark for interpretable mental health analysis.The results show that MentaLLaMA approaches state-of-the-art discriminative methods in correctness and generates human-level explanations.MentaLLaMA models also show strong generalizability to unseen tasks.The project is available at https://github.com/SteveKGYang/MentaLLaMA.","['zero-shot learning', 'few-shot learning', 'domain-specific finetuning', 'instruction tuning']","The research idea centers on the increasing importance of social media as a source for understanding mental health, highlighting the challenges in achieving accurate and interpretable mental health analysis from social media content. Traditional methods have limitations in generalization and interpretability, which affects the quality of mental health assessments and explanations derived from social media data. The study addresses the need for more reliable and interpretable approaches to analyze mental health indicators in social media posts.

The primary objective of the study is to develop a comprehensive and interpretable framework for mental health analysis on social media by creating a large, multi-task, and multi-source dataset that supports detailed explanations alongside mental health predictions. The study aims to improve the reliability and quality of mental health assessments by providing human-level explanations and ensuring strong generalizability across different mental health tasks.","The research idea centers on the increasing importance of social media as a source for understanding mental health, highlighting the challenges in achieving accurate and interpretable mental health analysis from social media content. Traditional approaches have limitations in generalization and interpretability, which affects the quality of mental health assessments and explanations derived from social media data. The study addresses the need for more reliable and interpretable methodologies to analyze mental health indicators in social media posts.

The primary objective of the study is to develop a comprehensive and interpretable framework for mental health analysis on social media by creating a large, multi-task, and multi-source dataset that supports detailed explanations alongside mental health assessments. The study aims to improve the reliability and quality of mental health evaluations by providing human-level explanations and ensuring strong generalizability across different mental health tasks.",True
Psychology,Automated Classification of Cognitive Visual Objects Using Multivariate Swarm Sparse Decomposition From Multichannel EEG-MEG Signals,"In visual object decoding, magnetoencephalogram (MEG) and electroencephalogram (EEG) activation patterns demonstrate the utmost discriminative cognitive analysis due to their multivariate oscillatory nature. However, high noise in the recorded EEG-MEG signals and subject-specific variability make it extremely difficult to classify subject's cognitive responses to different visual stimuli. The proposed method is a multivariate extension of the swarm sparse decomposition method (MSSDM) for multivariate pattern analysis of EEG-MEG-based visual activation signals. In comparison, it is an advanced technique for decomposing nonstationary multicomponent signals into a finite number of channel-aligned oscillatory components that significantly enhance visual activation-related sub-bands. The MSSDM method adopts multivariate swarm filtering and sparse spectrum to automatically deliver optimal frequency bands in channel-specific sparse spectrums, resulting in improved filter banks. By combining the advantages of the multivariate SSDM and Riemann's correlation-assisted fusion feature (RCFF), the MSSDM-RCFF algorithm is investigated to improve the visual object recognition ability of EEG-MEG signals. We have also proposed time–frequency representation based on MSSDM to analyze discriminative cognitive patterns of different visual object classes from multichannel EEG-MEG signals. A proposed MSSDM is evaluated on multivariate synthetic signals and multivariate EEG-MEG signals using five classifiers. The proposed fusion feature and linear discriminant analysis classifier-based framework outperformed all existing state-of-the-art methods used for visual object detection and achieved the highest accuracy of 86.42% using tenfold cross-validation on EEG-MEG multichannel signals.","['sparse spectrum', 'linear discriminant analysis classifier']","The research idea centers on the challenge of accurately classifying cognitive responses to different visual stimuli using brain activation patterns recorded through EEG and MEG, which are often hindered by high noise levels and individual variability. This difficulty limits the understanding of how the brain processes and discriminates between various visual objects. The study’s primary objective is to enhance the ability to decode and recognize visual object-related cognitive patterns from EEG-MEG signals by improving the identification of discriminative neural activation associated with different visual stimuli. The aim is to achieve more precise classification of cognitive responses to visual objects despite the complexities introduced by signal noise and subject-specific differences.","The research idea centers on the challenge of accurately identifying cognitive responses to different visual stimuli using brain activation patterns recorded through EEG and MEG, which are often hindered by high noise levels and individual variability. This difficulty limits the understanding of how the brain processes and discriminates between various visual objects. The study's primary objective is to enhance the ability to decode and recognize visual object-related cognitive patterns from EEG-MEG signals by improving the identification of discriminative neural activation associated with different visual stimuli. The aim is to achieve more precise identification of cognitive responses to visual objects despite the complexities introduced by signal noise and subject-specific differences.",True
Psychology,CLARUS: An interactive explainable AI platform for manual counterfactuals in graph neural networks,"Lack of trust in artificial intelligence (AI) models in medicine is still the key blockage for the use of AI in clinical decision support systems (CDSS). Although AI models are already performing excellently in systems medicine, their black-box nature entails that patient-specific decisions are incomprehensible for the physician. Explainable AI (XAI) algorithms aim to ""explain"" to a human domain expert, which input features influenced a specific recommendation. However, in the clinical domain, these explanations must lead to some degree of causal understanding by a clinician. We developed the CLARUS platform, aiming to promote human understanding of graph neural network (GNN) predictions. CLARUS enables the visualisation of patient-specific networks, as well as, relevance values for genes and interactions, computed by XAI methods, such as GNNExplainer. This enables domain experts to gain deeper insights into the network and more importantly, the expert can interactively alter the patient-specific network based on the acquired understanding and initiate re-prediction or retraining. This interactivity allows us to ask manual counterfactual questions and analyse the effects on the GNN prediction. We present the first interactive XAI platform prototype, CLARUS, that allows not only the evaluation of specific human counterfactual questions based on user-defined alterations of patient networks and a re-prediction of the clinical outcome but also a retraining of the entire GNN after changing the underlying graph structures. The platform is currently hosted by the GWDG on https://rshiny.gwdg.de/apps/clarus/.","['graph neural network (GNN)', 'GNNExplainer']","The research idea addresses the challenge of lack of trust in clinical decision support systems due to the incomprehensibility of patient-specific decisions made by complex models in medicine. Although these models perform well, their opaque nature prevents physicians from understanding the rationale behind recommendations, which is crucial for clinical acceptance. The study emphasizes the need for explanations that lead to a causal understanding by clinicians to improve trust and usability in medical decision-making.

The primary objective of the study is to develop a platform that promotes human understanding of patient-specific predictions by enabling visualization and interactive exploration of relevant features influencing clinical outcomes. This platform aims to allow domain experts to gain deeper insights, modify patient-specific information based on their understanding, and observe the effects of these changes on clinical predictions. Ultimately, the goal is to facilitate a more transparent and interpretable decision-making process for clinicians.","The research idea addresses the challenge of lack of trust in clinical decision support systems due to the incomprehensibility of patient-specific decisions made by complex systems in medicine. Although these systems perform well, their opaque nature prevents physicians from understanding the rationale behind recommendations, which is crucial for clinical acceptance. The study emphasizes the need for explanations that lead to a causal understanding by clinicians to improve trust and usability in medical decision-making.

The primary objective of the study is to develop a platform that promotes human understanding of patient-specific predictions by enabling visualization and interactive exploration of relevant features influencing clinical outcomes. This platform aims to allow domain experts to gain deeper insights, modify patient-specific information based on their understanding, and observe the effects of these changes on clinical predictions. Ultimately, the goal is to facilitate a more transparent and interpretable decision-making process for clinicians.",True
Psychology,Role of machine learning and deep learning techniques in EEG-based BCI emotion recognition system: a review,"Abstract Emotion is a subjective psychophysiological reaction coming from external stimuli which impacts every aspect of our daily lives. Due to the continuing development of non-invasive and portable sensor technologies, such as brain-computer interfaces (BCI), intellectuals from several fields have been interested in emotion recognition techniques. Human emotions can be recognised using a variety of behavioural cues, including gestures and body language, voice, and physiological markers. The first three, however, might be ineffective because people sometimes conceal their genuine emotions either intentionally or unknowingly. More precise and objective emotion recognition can be accomplished using physiological signals. Among other physiological signals, Electroencephalogram (EEG) is more responsive and sensitive to variation in affective states. Various EEG-based emotion recognition methods have recently been introduced. This study reviews EEG-based BCIs for emotion identification and gives an outline of the progress made in this field. A summary of the datasets and techniques utilised to evoke human emotions and various emotion models is also given. We discuss several EEG feature extractions, feature selection/reduction, machine learning, and deep learning algorithms in accordance with standard emotional identification process. We provide an overview of the human brain's EEG rhythms, which are closely related to emotional states. We also go over a number of EEG-based emotion identification research and compare numerous machine learning and deep learning techniques. In conclusion, this study highlights the applications, challenges and potential areas for future research in identification and classification of human emotional states.",['feature selection/reduction'],"The research idea centers on the importance of accurately recognizing human emotions, which are subjective psychophysiological reactions influenced by external stimuli and affect many aspects of daily life. Traditional behavioral cues such as gestures, body language, and voice may be unreliable because individuals can consciously or unconsciously conceal their true emotions. Therefore, there is a need for more precise and objective methods of emotion recognition using physiological signals, with a particular focus on brain activity that is sensitive to changes in affective states. The study addresses the progress and challenges in identifying human emotions through these physiological markers.

The primary objective of the study is to review the current advancements in emotion identification using physiological signals, specifically focusing on brain activity related to emotional states. It aims to summarize the various approaches used to evoke and model human emotions and to provide an overview of research efforts in this area. Additionally, the study seeks to highlight the applications, challenges, and potential directions for future research in the identification and classification of human emotional states.","The research idea centers on the importance of accurately recognizing human emotions, which are subjective psychophysiological reactions influenced by external stimuli and affect many aspects of daily life. Traditional behavioral cues such as gestures, body language, and voice may be unreliable because individuals can consciously or unconsciously conceal their true emotions. Therefore, there is a need for more precise and objective methods of emotion recognition using physiological signals, with a particular focus on brain activity that is sensitive to changes in affective states. The study addresses the progress and challenges in identifying human emotions through these physiological markers.

The primary objective of the study is to review the current advancements in emotion identification using physiological signals, specifically focusing on brain activity related to emotional states. It aims to summarize the various approaches used to evoke and measure human emotions and to provide an overview of research efforts in this area. Additionally, the study seeks to highlight the applications, challenges, and potential directions for future research in the identification and analysis of human emotional states.",True
Psychology,Unlocking the Potential of XAI for Improved Alzheimer’s Disease Detection and Classification Using a ViT-GRU Model,"Alzheimer's Disease (AD) is a significant cause of dementia worldwide, and its progression from mild to severe affects an individual's ability to perform daily activities independently. The accurate and early diagnosis of AD is crucial for effective clinical intervention. However, interpreting AD from medical images can be challenging, even for experienced radiologists. Therefore, there is a need for an automatic diagnosis of AD, and researchers have investigated the potential of utilizing Artificial Intelligence (AI) techniques, particularly deep learning models, to address this challenge. This study proposes a framework that combines a Vision Transformer (ViT) and a Gated Recurrent Unit (GRU) to detect AD characteristics from Magnetic Resonance Imaging (MRI) images accurately and reliably. The ViT identifies crucial features from the input image, and the GRU establishes clear correlations between these features. The proposed model overcomes the class imbalance issue in the MRI image dataset and achieves superior accuracy and performance compared to existing methods. The model was trained on the Alzheimer's MRI Preprocessed Dataset obtained from Kaggle, achieving notable accuracies of 99.53% for 4-class and 99.69% for binary classification. It also demonstrated a high accuracy of 99.26% for 3-class on the AD Neuroimaging Initiative (ADNI) Baseline Database. These results were validated through a thorough 10-fold cross-validation process. Furthermore, Explainable AI (XAI) techniques were incorporated to make the model interpretable and explainable. This allows clinicians to understand the model's decision-making process and gain insights into the underlying factors driving the AD diagnosis.","['Vision Transformer (ViT)', 'Gated Recurrent Unit (GRU)']","The research idea centers on the significant impact of Alzheimer's Disease (AD) as a leading cause of dementia worldwide and the challenges associated with its progression from mild to severe stages, which impair an individual's ability to perform daily activities independently. Early and accurate diagnosis of AD is essential for effective clinical intervention, yet interpreting the disease from medical images remains difficult even for experienced radiologists. The study addresses the need for improved diagnostic approaches to facilitate timely and reliable identification of AD characteristics. The primary objective of the study is to develop a method that can accurately detect features indicative of Alzheimer's Disease from medical imaging, thereby enhancing the reliability and accuracy of diagnosis. This aims to support clinicians in making informed decisions by providing clearer insights into the disease’s progression and characteristics.","The research idea centers on the significant impact of Alzheimer's Disease (AD) as a leading cause of dementia worldwide and the challenges associated with its progression from mild to severe stages, which impair an individual's ability to perform daily activities independently. Early and accurate diagnosis of AD is essential for effective clinical intervention, yet interpreting the disease from medical images remains difficult even for experienced radiologists. The study addresses the need for improved diagnostic approaches to facilitate timely and reliable identification of AD characteristics. The primary objective of the study is to develop a method that can accurately detect features indicative of Alzheimer's Disease from medical imaging, thereby enhancing the reliability and accuracy of diagnosis. This aims to support clinicians in making informed decisions by providing clearer insights into the disease's progression and characteristics.",True
Psychology,Machine Learning–Based Prediction of Suicidality in Adolescents With Allergic Rhinitis: Derivation and Validation in 2 Independent Nationwide Cohorts,"Background Given the additional risk of suicide-related behaviors in adolescents with allergic rhinitis (AR), it is important to use the growing field of machine learning (ML) to evaluate this risk. Objective This study aims to evaluate the validity and usefulness of an ML model for predicting suicide risk in patients with AR. Methods We used data from 2 independent survey studies, Korea Youth Risk Behavior Web-based Survey (KYRBS; n=299,468) for the original data set and Korea National Health and Nutrition Examination Survey (KNHANES; n=833) for the external validation data set, to predict suicide risks of AR in adolescents aged 13 to 18 years, with 3.45% (10,341/299,468) and 1.4% (12/833) of the patients attempting suicide in the KYRBS and KNHANES studies, respectively. The outcome of interest was the suicide attempt risks. We selected various ML-based models with hyperparameter tuning in the discovery and performed an area under the receiver operating characteristic curve (AUROC) analysis in the train, test, and external validation data. Results The study data set included 299,468 (KYRBS; original data set) and 833 (KNHANES; external validation data set) patients with AR recruited between 2005 and 2022. The best-performing ML model was the random forest model with a mean AUROC of 84.12% (95% CI 83.98%-84.27%) in the original data set. Applying this result to the external validation data set revealed the best performance among the models, with an AUROC of 89.87% (sensitivity 83.33%, specificity 82.58%, accuracy 82.59%, and balanced accuracy 82.96%). While looking at feature importance, the 5 most important features in predicting suicide attempts in adolescent patients with AR are depression, stress status, academic achievement, age, and alcohol consumption. Conclusions This study emphasizes the potential of ML models in predicting suicide risks in patients with AR, encouraging further application of these models in other conditions to enhance adolescent health and decrease suicide rates.",['random forest'],"The research addresses the increased risk of suicide-related behaviors among adolescents with allergic rhinitis (AR), highlighting the importance of evaluating this risk to improve adolescent health outcomes. Given the serious implications of suicide attempts in this population, understanding the factors that contribute to suicide risk is crucial for prevention efforts. The primary objective of the study is to assess the validity and usefulness of approaches for predicting suicide risk in adolescents with AR. Specifically, the study aims to identify key factors associated with suicide attempts in this group to better inform strategies that could reduce suicide rates among affected adolescents.","The research addresses the increased risk of suicide-related behaviors among adolescents with allergic rhinitis (AR), highlighting the importance of evaluating this risk to improve adolescent health outcomes. Given the serious implications of suicide attempts in this population, understanding the factors that contribute to suicide risk is crucial for prevention efforts. The primary objective of the study is to assess the validity and usefulness of approaches for identifying suicide risk in adolescents with AR. Specifically, the study aims to identify key factors associated with suicide attempts in this group to better inform strategies that could reduce suicide rates among affected adolescents.",True
Psychology,Finding the Right XAI Method—A Guide for the Evaluation and Ranking of Explainable AI Methods in Climate Science,"Abstract Explainable artificial intelligence (XAI) methods shed light on the predictions of machine learning algorithms. Several different approaches exist and have already been applied in climate science. However, usually missing ground truth explanations complicate their evaluation and comparison, subsequently impeding the choice of the XAI method. Therefore, in this work, we introduce XAI evaluation in the climate context and discuss different desired explanation properties, namely, robustness, faithfulness, randomization, complexity, and localization. To this end, we chose previous work as a case study where the decade of annual-mean temperature maps is predicted. After training both a multilayer perceptron (MLP) and a convolutional neural network (CNN), multiple XAI methods are applied and their skill scores in reference to a random uniform explanation are calculated for each property. Independent of the network, we find that XAI methods such as Integrated Gradients, layerwise relevance propagation, and input times gradients exhibit considerable robustness, faithfulness, and complexity while sacrificing randomization performance. Sensitivity methods, gradient, SmoothGrad, NoiseGrad, and FusionGrad, match the robustness skill but sacrifice faithfulness and complexity for the randomization skill. We find architecture-dependent performance differences regarding robustness, complexity, and localization skills of different XAI methods, highlighting the necessity for research task-specific evaluation. Overall, our work offers an overview of different evaluation properties in the climate science context and shows how to compare and benchmark different explanation methods, assessing their suitability based on strengths and weaknesses, for the specific research problem at hand. By that, we aim to support climate researchers in the selection of a suitable XAI method. Significance Statement Explainable artificial intelligence (XAI) helps to understand the reasoning behind the prediction of a neural network. XAI methods have been applied in climate science to validate networks and provide new insight into physical processes. However, the increasing number of XAI methods can overwhelm practitioners, making it difficult to choose an explanation method. Since XAI methods’ results can vary, uninformed choices might cause misleading conclusions about the network decision. In this work, we introduce XAI evaluation to compare and assess the performance of explanation methods based on five desirable properties. We demonstrate that XAI evaluation reveals the strengths and weaknesses of different XAI methods. Thus, our work provides climate researchers with the tools to compare, analyze, and subsequently choose explanation methods.","['multilayer perceptron (MLP)', 'convolutional neural network (CNN)', 'Integrated Gradients', 'layerwise relevance propagation', 'SmoothGrad']","The research idea centers on the challenge of evaluating and comparing different explanation methods used to interpret complex predictive models in climate science, where the absence of definitive ground truth explanations complicates this process. This difficulty hinders researchers’ ability to select the most appropriate explanation approach, potentially leading to misleading interpretations of model decisions. The study aims to address this gap by introducing a framework for evaluating explanation methods based on key desirable properties such as robustness, faithfulness, and complexity. The primary objective of the study is to provide climate researchers with a systematic way to compare and assess the performance of various explanation methods, thereby supporting informed selection tailored to specific research problems and enhancing the understanding of model predictions in the climate context.","The research idea centers on the challenge of evaluating and comparing different explanation methods used to interpret complex systems in climate science, where the absence of definitive ground truth explanations complicates this process. This difficulty hinders researchers' ability to select the most appropriate explanation approach, potentially leading to misleading interpretations of findings. The study aims to address this gap by introducing a framework for evaluating explanation methods based on key desirable properties such as robustness, faithfulness, and complexity. The primary objective of the study is to provide climate researchers with a systematic way to compare and assess the performance of various explanation methods, thereby supporting informed selection tailored to specific research problems and enhancing the understanding of predictions in the climate context.",True
Psychology,Predictors for estimating subcortical EEG responses to continuous speech,"Perception of sounds and speech involves structures in the auditory brainstem that rapidly process ongoing auditory stimuli. The role of these structures in speech processing can be investigated by measuring their electrical activity using scalp-mounted electrodes. However, typical analysis methods involve averaging neural responses to many short repetitive stimuli that bear little relevance to daily listening environments. Recently, subcortical responses to more ecologically relevant continuous speech were detected using linear encoding models. These methods estimate the temporal response function (TRF), which is a regression model that minimises the error between the measured neural signal and a predictor derived from the stimulus. Using predictors that model the highly non-linear peripheral auditory system may improve linear TRF estimation accuracy and peak detection. Here, we compare predictors from both simple and complex peripheral auditory models for estimating brainstem TRFs on electroencephalography (EEG) data from 24 participants listening to continuous speech. We also investigate the data length required for estimating subcortical TRFs, and find that around 12 minutes of data is sufficient for clear wave V peaks (&gt;3 dB SNR) to be seen in nearly all participants. Interestingly, predictors derived from simple filterbank-based models of the peripheral auditory system yield TRF wave V peak SNRs that are not significantly different from those estimated using a complex model of the auditory nerve, provided that the nonlinear effects of adaptation in the auditory system are appropriately modelled. Crucially, computing predictors from these simpler models is more than 50 times faster compared to the complex model. This work paves the way for efficient modelling and detection of subcortical processing of continuous speech, which may lead to improved diagnosis metrics for hearing impairment and assistive hearing technology.","['linear encoding models', 'temporal response function (TRF)', 'regression model']","The research idea centers on understanding how structures in the auditory brainstem rapidly process ongoing auditory stimuli, particularly speech, and the challenge of studying these processes using typical methods that rely on repetitive stimuli not representative of everyday listening environments. There is a need to investigate subcortical responses to more ecologically relevant continuous speech to better reflect natural auditory processing. The primary objective of the study is to compare different predictors derived from simple and complex models of the peripheral auditory system for estimating brainstem responses to continuous speech, and to determine the amount of data required to reliably detect these responses. The study aims to identify efficient approaches for modeling and detecting subcortical processing of continuous speech, which could enhance diagnostic measures for hearing impairment and improve assistive hearing technologies.","The research idea centers on understanding how structures in the auditory brainstem rapidly process ongoing auditory stimuli, particularly speech, and the challenge of studying these processes using typical methods that rely on repetitive stimuli not representative of everyday listening environments. There is a need to investigate subcortical responses to more ecologically relevant continuous speech to better reflect natural auditory processing. The primary objective of the study is to compare different predictors derived from simple and complex representations of the peripheral auditory system for estimating brainstem responses to continuous speech, and to determine the amount of data required to reliably detect these responses. The study aims to identify efficient approaches for analyzing and detecting subcortical processing of continuous speech, which could enhance diagnostic measures for hearing impairment and improve assistive hearing technologies.",True
Psychology,"A Survey of Imitation Learning: Algorithms, Recent Developments, and Challenges","In recent years, the development of robotics and artificial intelligence (AI) systems has been nothing short of remarkable. As these systems continue to evolve, they are being utilized in increasingly complex and unstructured environments, such as autonomous driving, aerial robotics, and natural language processing. As a consequence, programming their behaviors manually or defining their behavior through the reward functions as done in reinforcement learning (RL) has become exceedingly difficult. This is because such environments require a high degree of flexibility and adaptability, making it challenging to specify an optimal set of rules or reward signals that can account for all the possible situations. In such environments, learning from an expert's behavior through imitation is often more appealing. This is where imitation learning (IL) comes into play -a process where desired behavior is learned by imitating an expert's behavior, which is provided through demonstrations.This article aims to provide an introduction to IL and an overview of its underlying assumptions and approaches. It also offers a detailed description of recent advances and emerging areas of research in the field. Additionally, this article discusses how researchers have addressed common challenges associated with IL and provides potential directions for future research. Overall, the goal of this article is to provide a comprehensive guide to the growing field of IL in robotics and AI.","['reinforcement learning (RL)', 'imitation learning (IL)']","The research idea centers on the challenge of enabling flexible and adaptable behavior in complex and unstructured environments, where manually programming or defining optimal rules for behavior is difficult. The study addresses the problem of how to effectively learn desired behaviors by observing and imitating expert behavior, which is considered a more appealing approach in such settings. The primary objective of the study is to provide an introduction and comprehensive overview of the process of learning behavior through imitation, including its underlying assumptions, recent advances, and emerging areas of research. Additionally, the study aims to discuss common challenges encountered in this approach and suggest potential directions for future research.","The research idea centers on the challenge of enabling flexible and adaptable behavior in complex and unstructured environments, where manually programming or defining optimal rules for behavior is difficult. The study addresses the problem of how to effectively develop desired behaviors by observing and imitating expert behavior, which is considered a more appealing approach in such settings. The primary objective of the study is to provide an introduction and comprehensive overview of the process of developing behavior through imitation, including its underlying assumptions, recent advances, and emerging areas of research. Additionally, the study aims to discuss common challenges encountered in this approach and suggest potential directions for future research.",True
Psychology,Seeking in Ride-on-Demand Service: A Reinforcement Learning Model With Dynamic Price Prediction,"Recent years witness the increasing popularity of ride-on-demand (RoD) services such as Uber and Didi. Compared with traditional taxi, RoD service is more ""data-driven"" and adopts dynamic pricing to manipulate the supply and demand in real time. Dynamic price could be viewed as an accurate and quantitative indicator of the supply and demand, and could provide clues to drivers, passengers, and the service providers, possibly reshaping the ways in which some problems are solved. In this paper, we focus on the seeking route recommendation problem that aims at increasing driver revenue by recommending highly profitable seeking routes to drivers of vacant cars with the help of dynamic prices. We first justify our motivation by showing the importance of route recommendation and answering why it is necessary to consider dynamic prices, based on the analysis of real service data. We then design a dynamic price prediction model to generate the dynamic prices at any given time and location based on multi-source urban data. After that, a reinforcement learning model is adopted to perform seeking route recommendation based on predicted dynamic prices. We conduct extensive experiments in different spatio-temporal combinations and make comparisons with multiple baselines. Results first show that our dynamic price prediction model achieves an accuracy ranging from 83.82% to 90.67% under different settings. It also proves that considering the real-time predicted dynamic prices significantly increases driver revenue by, for example, 12% and 47.5% during weekday evening rush hours, than merely using the average prices or completely ignoring dynamic prices.",['reinforcement learning model'],"The research idea centers on the growing use of ride-on-demand services and how dynamic pricing serves as a precise and quantitative indicator of supply and demand, influencing the behaviors of drivers, passengers, and service providers. This dynamic pricing has the potential to reshape problem-solving approaches within the context of ride services, particularly in relation to route selection for drivers. The study’s primary objective is to address the problem of recommending seeking routes that increase driver revenue by leveraging dynamic price information. It aims to demonstrate the importance of considering dynamic prices in route recommendations and to show how incorporating real-time price fluctuations can significantly enhance drivers’ earnings compared to relying on average prices or ignoring dynamic pricing altogether.","The research idea centers on the growing use of ride-on-demand services and how dynamic pricing serves as a precise and quantitative indicator of supply and demand, influencing the behaviors of drivers, passengers, and service providers. This dynamic pricing has the potential to reshape problem-solving approaches within the context of ride services, particularly in relation to route selection for drivers. The study's primary objective is to address the problem of recommending seeking routes that increase driver revenue by leveraging dynamic price information. It aims to demonstrate the importance of considering dynamic prices in route recommendations and to show how incorporating real-time price fluctuations can significantly enhance drivers' earnings compared to relying on average prices or ignoring dynamic pricing altogether.",True
Psychology,Deciphering Digital Social Dynamics: A Comparative Study of Logistic Regression and Random Forest in Predicting E-Commerce Customer Behavior,"This study compares Logistic Regression and Random Forest in predicting e-commerce customer churn. Utilizing the E-commerce Customer dataset, it navigates the complexities of customer interactions and behaviors, offering a rich context for analysis. The methodology focuses on meticulous data preprocessing to ensure data integrity, setting the stage for applying and evaluating Logistic Regression and Random Forest. Both models were assessed using accuracy, precision, recall, F1-Score, and AUC-ROC. Logistic Regression showed an accuracy of 90%, precision of 91% for class 0 and 82% for class 1, recall of 98% for class 0 and 50% for class 1, F1-Score of 94% for class 0 and 62% for class 1, and AUC-ROC of 0.88. Random Forest, with its ability to handle complex patterns, demonstrated higher overall performance with an accuracy of 95%, precision of 95% for class 0 and 93% for class 1, recall of 99% for class 0 and 74% for class 1, F1-Score of 97% for class 0 and 82% for class 1, and an AUC-ROC of 0.97. This comparative analysis offers insights into each model's strengths and suitability for predicting customer churn. The findings contribute to a deeper understanding of machine learning applications in e-commerce, guiding stakeholders in enhancing customer retention strategies. This research provides a foundation for further exploration into the digital social dynamics that shape customer behavior in the evolving digital marketplace.","['Logistic Regression', 'Random Forest']","The study addresses the challenge of understanding and predicting customer churn within the e-commerce context, highlighting the complexities of customer interactions and behaviors that influence retention. It emphasizes the importance of accurately identifying factors that lead to customers discontinuing their engagement with online platforms, which is critical for improving customer retention strategies. The primary aim of the study is to compare different approaches in predicting e-commerce customer churn to determine their effectiveness and suitability. By evaluating these approaches, the research seeks to provide insights that contribute to a deeper understanding of the social dynamics influencing customer behavior in the digital marketplace.","The study addresses the challenge of understanding and predicting customer churn within the e-commerce context, highlighting the complexities of customer interactions and behaviors that influence retention. It emphasizes the importance of accurately identifying factors that lead to customers discontinuing their engagement with online platforms, which is critical for improving customer retention strategies. The primary aim of the study is to compare different analytical methods in predicting e-commerce customer churn to determine their effectiveness and suitability. By evaluating these approaches, the research seeks to provide insights that contribute to a deeper understanding of the social dynamics influencing customer behavior in the digital marketplace.",True
Psychology,Artificial intelligence for oral squamous cell carcinoma detection based on oral photographs: A comprehensive literature review,"Abstract Introduction Oral squamous cell carcinoma (OSCC) presents a significant global health challenge. The integration of artificial intelligence (AI) and computer vision holds promise for the early detection of OSCC through the analysis of digitized oral photographs. This literature review explores the landscape of AI‐driven OSCC automatic detection, assessing both the performance and limitations of the current state of the art. Materials and Methods An electronic search using several data base was conducted, and a systematic review performed in accordance with PRISMA guidelines (CRD42023441416). Results Several studies have demonstrated remarkable results for this task, consistently achieving sensitivity rates exceeding 85% and accuracy rates surpassing 90%, often encompassing around 1000 images. The review scrutinizes these studies, shedding light on their methodologies, including the use of recent machine learning and pattern recognition approaches coupled with different supervision strategies. However, comparing the results from different papers is challenging due to variations in the datasets used. Discussion Considering these findings, this review underscores the urgent need for more robust and reliable datasets in the field of OSCC detection. Furthermore, it highlights the potential of advanced techniques such as multi‐task learning, attention mechanisms, and ensemble learning as crucial tools in enhancing the accuracy and sensitivity of OSCC detection through oral photographs. Conclusion These insights collectively emphasize the transformative impact of AI‐driven approaches on early OSCC diagnosis, with the potential to significantly improve patient outcomes and healthcare practices.","['machine learning', 'multi-task learning', 'attention mechanisms', 'ensemble learning']","The research idea centers on addressing the significant global health challenge posed by oral squamous cell carcinoma (OSCC) and the importance of early detection to improve patient outcomes. The study recognizes the current limitations in reliably identifying OSCC through oral photographs and the variability in existing datasets that complicate consistent detection efforts. The primary objective of the study is to explore and evaluate the current landscape of automatic OSCC detection methods, focusing on their performance and limitations. It aims to highlight the need for more robust and reliable datasets and to emphasize approaches that could enhance the accuracy and sensitivity of early OSCC diagnosis through oral photographic assessment.","The research idea centers on addressing the significant global health challenge posed by oral squamous cell carcinoma (OSCC) and the importance of early detection to improve patient outcomes. The study recognizes the current limitations in reliably identifying OSCC through oral photographs and the variability in existing datasets that complicate consistent detection efforts. The primary objective of the study is to explore and evaluate the current landscape of OSCC detection methods, focusing on their performance and limitations. It aims to highlight the need for more robust and reliable datasets and to emphasize approaches that could enhance the accuracy and sensitivity of early OSCC diagnosis through oral photographic assessment.",True
Psychology,A deep learning model for brain age prediction using minimally preprocessed T1w images as input,"Introduction In the last few years, several models trying to calculate the biological brain age have been proposed based on structural magnetic resonance imaging scans (T1-weighted MRIs, T1w) using multivariate methods and machine learning. We developed and validated a convolutional neural network (CNN)-based biological brain age prediction model that uses one T1w MRI preprocessing step when applying the model to external datasets to simplify implementation and increase accessibility in research settings. Our model only requires rigid image registration to the MNI space, which is an advantage compared to previous methods that require more preprocessing steps, such as feature extraction. Methods We used a multicohort dataset of cognitively healthy individuals (age range = 32.0–95.7 years) comprising 17,296 MRIs for training and evaluation. We compared our model using hold-out (CNN1) and cross-validation (CNN2–4) approaches. To verify generalisability, we used two external datasets with different populations and MRI scan characteristics to evaluate the model. To demonstrate its usability, we included the external dataset’s images in the cross-validation training (CNN3). To ensure that our model used only the brain signal on the image, we also predicted brain age using skull-stripped images (CNN4). Results: The trained models achieved a mean absolute error of 2.99, 2.67, 2.67, and 3.08 years for CNN1–4, respectively. The model’s performance in the external dataset was in the typical range of mean absolute error (MAE) found in the literature for testing sets. Adding the external dataset to the training set (CNN3), overall, MAE is unaffected, but individual cohort MAE improves (5.63–2.25 years). Salience maps of predictions reveal that periventricular, temporal, and insular regions are the most important for age prediction. Discussion We provide indicators for using biological (predicted) brain age as a metric for age correction in neuroimaging studies as an alternative to the traditional chronological age. In conclusion, using different approaches, our CNN-based model showed good performance using one T1w brain MRI preprocessing step. The proposed CNN model is made publicly available for the research community to be easily implemented and used to study ageing and age-related disorders.","['machine learning', 'convolutional neural network (CNN)', 'hold-out approach']","The research idea centers on the challenge of accurately estimating biological brain age as a meaningful metric that reflects brain health and aging beyond chronological age. Existing approaches often require complex preprocessing steps, which can limit accessibility and implementation in research settings. This study addresses the need for a simplified yet reliable method to predict biological brain age using structural brain imaging data. The primary objective of the study is to develop and validate a biological brain age prediction approach that requires minimal preprocessing of brain images, thereby enhancing usability and accessibility for studying brain aging and age-related disorders. The study aims to demonstrate the model’s performance and generalizability across diverse populations and imaging conditions, ultimately providing a practical tool for age correction in neuroimaging research.","The research idea centers on the challenge of accurately estimating biological brain age as a meaningful metric that reflects brain health and aging beyond chronological age. Existing approaches often require complex preprocessing steps, which can limit accessibility and implementation in research settings. This study addresses the need for a simplified yet reliable method to predict biological brain age using structural brain imaging data. The primary objective of the study is to develop and validate a biological brain age prediction approach that requires minimal preprocessing of brain images, thereby enhancing usability and accessibility for studying brain aging and age-related disorders. The study aims to demonstrate the method's performance and generalizability across diverse populations and imaging conditions, ultimately providing a practical tool for age correction in neuroimaging research.",True
Psychology,Collective Constitutional AI: Aligning a Language Model with Public Input,"There is growing consensus that language model (LM) developers should not be the sole deciders of LM behavior, creating a need for methods that enable the broader public to collectively shape the behavior of LM systems that affect them. To address this need, we present Collective Constitutional AI (CCAI): a multi-stage process for sourcing and integrating public input into LMs—from identifying a target population to sourcing principles to training and evaluating a model. We demonstrate the real-world practicality of this approach by creating what is, to our knowledge, the first LM fine-tuned with collectively sourced public input and evaluating this model against a baseline model trained with established principles from a LM developer. Our quantitative evaluations demonstrate several benefits of our approach: the CCAI-trained model shows lower bias across nine social dimensions compared to the baseline model, while maintaining equivalent performance on language, math, and helpful-harmless evaluations. Qualitative comparisons of the models suggest that the models differ on the basis of their respective constitutions, e.g., when prompted with contentious topics, the CCAI-trained model tends to generate responses that reframe the matter positively instead of a refusal. These results demonstrate a promising, tractable pathway toward publicly informed development of language models.",['fine-tuning'],"The research idea centers on the growing recognition that developers should not be the sole decision-makers regarding the behavior of language models, highlighting the need for approaches that allow the broader public to collectively influence systems that impact them. This reflects a broader motivation to democratize the shaping of such technologies to better align with diverse societal values and reduce biases. The primary objective of the study is to explore a process for incorporating public input into the development of language models, aiming to create a model that reflects collectively sourced principles rather than solely developer-established guidelines. The study seeks to evaluate whether this approach can reduce bias across various social dimensions while maintaining comparable performance on standard assessments.","The research idea centers on the growing recognition that developers should not be the sole decision-makers regarding the behavior of language systems, highlighting the need for approaches that allow the broader public to collectively influence systems that impact them. This reflects a broader motivation to democratize the shaping of such technologies to better align with diverse societal values and reduce biases. The primary objective of the study is to explore a process for incorporating public input into the development of language systems, aiming to create a framework that reflects collectively sourced principles rather than solely developer-established guidelines. The study seeks to evaluate whether this approach can reduce bias across various social dimensions while maintaining comparable performance on standard assessments.",True
Psychology,Graph Autoencoders for Embedding Learning in Brain Networks and Major Depressive Disorder Identification,"Brain functional connectivity (FC) networks inferred from functional magnetic resonance imaging (fMRI) have shown altered or aberrant brain functional connectome in various neuropsychiatric disorders. Recent application of deep neural networks to connectome-based classification mostly relies on traditional convolutional neural networks (CNNs) using input FCs on a regular Euclidean grid to learn spatial maps of brain networks neglecting the topological information of the brain networks, leading to potentially sub-optimal performance in brain disorder identification. We propose a novel graph deep learning framework that leverages non-Euclidean information inherent in the graph structure for classifying brain networks in major depressive disorder (MDD). We introduce a novel graph autoencoder (GAE) architecture, built upon graph convolutional networks (GCNs), to embed the topological structure and node content of large fMRI networks into low-dimensional representations. For constructing the brain networks, we employ the Ledoit-Wolf (LDW) shrinkage method to efficiently estimate high-dimensional FC metrics from fMRI data. We explore both supervised and unsupervised techniques for graph embedding learning. The resulting embeddings serve as feature inputs for a deep fully-connected neural network (FCNN) to distinguish MDD from healthy controls (HCs). Evaluating our model on resting-state fMRI MDD dataset, we observe that the GAE-FCNN outperforms several state-of-the-art methods for brain connectome classification, achieving the highest accuracy when using LDW-FC edges as node features. The graph embeddings of fMRI FC networks also reveal significant group differences between MDD and HCs. Our framework demonstrates the feasibility of learning graph embeddings from brain networks, providing valuable discriminative information for diagnosing brain disorders.","['deep neural networks', 'convolutional neural networks (CNNs)', 'graph autoencoder (GAE)', 'graph convolutional networks (GCNs)', 'supervised techniques for graph embedding learning', 'unsupervised techniques for graph embedding learning', 'deep fully-connected neural network (FCNN)']","The study addresses the problem of identifying altered brain functional connectivity patterns associated with major depressive disorder (MDD) using brain networks derived from functional magnetic resonance imaging (fMRI). It highlights the challenge that traditional approaches may overlook important topological information of brain networks, which could limit the effectiveness of distinguishing MDD from healthy controls. The primary aim of the study is to develop a framework that captures the topological structure and node content of brain networks to improve the classification of MDD. The objective is to explore representations of brain functional connectivity that reveal significant differences between individuals with MDD and healthy controls, thereby enhancing the ability to diagnose brain disorders based on brain network characteristics.","The study addresses the problem of identifying altered brain functional connectivity patterns associated with major depressive disorder (MDD) using brain networks derived from functional magnetic resonance imaging (fMRI). It highlights the challenge that traditional approaches may overlook important topological information of brain networks, which could limit the effectiveness of distinguishing MDD from healthy controls. The primary aim of the study is to develop a framework that captures the topological structure and node content of brain networks to improve the identification of MDD. The objective is to explore representations of brain functional connectivity that reveal significant differences between individuals with MDD and healthy controls, thereby enhancing the ability to diagnose brain disorders based on brain network characteristics.",True
Psychology,Do large language models show decision heuristics similar to humans? A case study using GPT-3.5.,"A Large Language Model (LLM) is an artificial intelligence system trained on vast amounts of natural language data, enabling it to generate human-like responses to written or spoken language input. Generative Pre-Trained Transformer (GPT)-3.5 is an example of an LLM that supports a conversational agent called ChatGPT. In this work, we used a series of novel prompts to determine whether ChatGPT shows heuristics and other context-sensitive responses. We also tested the same prompts on human participants. Across four studies, we found that ChatGPT was influenced by random anchors in making estimates (anchoring, Study 1); it judged the likelihood of two events occurring together to be higher than the likelihood of either event occurring alone, and it was influenced by anecdotal information (representativeness and availability heuristic, Study 2); it found an item to be more efficacious when its features were presented positively rather than negatively-even though both presentations contained statistically equivalent information (framing effect, Study 3); and it valued an owned item more than a newly found item even though the two items were objectively identical (endowment effect, Study 4). In each study, human participants showed similar effects. Heuristics and context-sensitive responses in humans are thought to be driven by cognitive and affective processes such as loss aversion and effort reduction. The fact that an LLM-which lacks these processes-also shows such responses invites consideration of the possibility that language is sufficiently rich to carry these effects and may play a role in generating these effects in humans. (PsycInfo Database Record (c) 2024 APA, all rights reserved).",['Generative Pre-Trained Transformer (GPT)-3.5'],"The research idea centers on understanding whether certain cognitive biases and heuristics, typically observed in human judgment and decision-making, can also be exhibited by a language-based system that lacks human cognitive and affective processes. This raises questions about the role of language itself in shaping these psychological effects, suggesting that language may be sufficiently rich to carry and generate such biases in humans. The primary objective of the study is to investigate whether a conversational agent demonstrates heuristics and context-sensitive responses similar to those found in human participants across various cognitive phenomena, including anchoring, representativeness, availability heuristic, framing effect, and the endowment effect. By comparing responses between the agent and humans, the study aims to explore the possibility that language contributes to the emergence of these cognitive biases in human psychology.","The research idea centers on understanding whether certain cognitive biases and heuristics, typically observed in human judgment and decision-making, can also be exhibited by a language-based system that lacks human cognitive and affective processes. This raises questions about the role of language itself in shaping these psychological effects, suggesting that language may be sufficiently rich to carry and generate such biases in humans. The primary objective of the study is to investigate whether a conversational system demonstrates heuristics and context-sensitive responses similar to those found in human participants across various cognitive phenomena, including anchoring, representativeness, availability heuristic, framing effect, and the endowment effect. By comparing responses between the system and humans, the study aims to explore the possibility that language contributes to the emergence of these cognitive biases in human psychology.",True
Psychology,Detecting ChatGPT-Generated Code Submissions in a CS1 Course Using Machine Learning Models,"The emergence of publicly accessible large language models (LLMs) such as ChatGPT poses unprecedented risks of new types of plagiarism and cheating where students use LLMs to solve exercises for them. Detecting this behavior will be a necessary component in introductory computer science (CS1) courses, and educators should be well-equipped with detection tools when the need arises. However, ChatGPT generates code non-deterministically, and thus, traditional similarity detectors might not suffice to detect AI-created code. In this work, we explore the affordances of Machine Learning (ML) models for the detection task. We used an openly available dataset of student programs for CS1 assignments and had ChatGPT generate code for the same assignments, and then evaluated the performance of both traditional machine learning models and Abstract Syntax Tree-based (AST-based) deep learning models in detecting ChatGPT code from student code submissions. Our results suggest that both traditional machine learning models and AST-based deep learning models are effective in identifying ChatGPT-generated code with accuracy above 90%. Since the deployment of such models requires ML knowledge and resources that are not always accessible to instructors, we also explore the patterns detected by deep learning models that indicate possible ChatGPT code signatures, which instructors could possibly use to detect LLM-based cheating manually. We also explore whether explicitly asking ChatGPT to impersonate a novice programmer affects the code produced. We further discuss the potential applications of our proposed models for enhancing introductory computer science instruction.",['Abstract Syntax Tree-based (AST-based) deep learning models'],"The research idea centers on the emerging challenge of plagiarism and cheating in educational settings due to students using advanced language models to complete assignments, which poses new risks that educators must address. This issue is particularly relevant in introductory courses where detecting such behavior is crucial to maintaining academic integrity. The study’s primary objective is to investigate ways to identify instances where students use these language models to generate assignment solutions, aiming to equip educators with effective means to recognize and address this form of academic dishonesty. Additionally, the research seeks to understand whether prompting the language model to mimic novice behavior influences the nature of the generated work, thereby informing detection strategies.","The research idea centers on the emerging challenge of plagiarism and cheating in educational settings due to students using technological tools to complete assignments, which poses new risks that educators must address. This issue is particularly relevant in introductory courses where detecting such behavior is crucial to maintaining academic integrity. The study's primary objective is to investigate ways to identify instances where students use these tools to generate assignment solutions, aiming to equip educators with effective means to recognize and address this form of academic dishonesty. Additionally, the research seeks to understand whether instructing these tools to mimic novice behavior influences the nature of the generated work, thereby informing detection strategies.",True
Psychology,Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic Motivation Reinforcement Learning Algorithms for Improved Training and Adaptability,"End-to-end multi-task dialogue systems are usually designed with separate modules for the dialogue pipeline. Among these, the policy module is essential for deciding what to do in response to user input. This policy is trained by reinforcement learning algorithms by taking advantage of an environment in which an agent receives feedback in the form of a reward signal. The current dialogue systems, however, only provide meagre and simplistic rewards. Investigating intrinsic motivation reinforcement learning algorithms is the goal of this study. Through this, the agent can quickly accelerate training and improve its capacity to judge the quality of its actions by teaching it an internal incentive system. In particular, we adapt techniques for random network distillation and curiosity-driven reinforcement learning to measure the frequency of state visits and encourage exploration by using semantic similarity between utterances. Experimental results on MultiWOZ, a heterogeneous dataset, show that intrinsic motivation-based debate systems outperform policies that depend on extrinsic incentives. By adopting random network distillation, for example, which is trained using semantic similarity between user-system dialogues, an astounding average success rate of 73% is achieved. This is a significant improvement over the baseline Proximal Policy optimization (PPO), which has an average success rate of 60%. In addition, performance indicators such as booking rates and completion rates show a 10% rise over the baseline. Furthermore, these intrinsic incentive models help improve the system's policy's resilience in an increasing amount of domains. This implies that they could be useful in scaling up to settings that cover a wider range of domains.","['reinforcement learning algorithms', 'intrinsic motivation reinforcement learning algorithms', 'random network distillation', 'curiosity-driven reinforcement learning', 'Proximal Policy Optimization (PPO)']","The research idea centers on addressing the limitations of current dialogue systems in providing only simplistic and insufficient rewards, which hinders their ability to effectively learn and respond to user input. The study is motivated by the need to enhance the decision-making capacity of dialogue policies by incorporating an internal incentive system that can better evaluate the quality of actions and accelerate learning. The primary objective of the study is to investigate intrinsic motivation approaches to improve the training and performance of dialogue policies, aiming to increase success rates, booking rates, and completion rates across multiple domains. Specifically, the study seeks to demonstrate that intrinsic incentives can lead to more resilient and scalable dialogue policies that perform better than those relying solely on external rewards.","The research idea centers on addressing the limitations of current dialogue systems in providing only simplistic and insufficient rewards, which hinders their ability to effectively respond to user input. The study is motivated by the need to enhance the decision-making capacity of dialogue policies by incorporating an internal incentive system that can better evaluate the quality of actions. The primary objective of the study is to investigate intrinsic motivation approaches to improve the training and performance of dialogue policies, aiming to increase success rates, booking rates, and completion rates across multiple domains. Specifically, the study seeks to demonstrate that intrinsic incentives can lead to more resilient and scalable dialogue policies that perform better than those relying solely on external rewards.",True
Psychology,Multimodal diagnosis model of Alzheimer’s disease based on improved Transformer,"Abstract Purpose Recent technological advancements in data acquisition tools allowed neuroscientists to acquire different modality data to diagnosis Alzheimer’s disease (AD). However, how to fuse these enormous amount different modality data to improve recognizing rate and find significance brain regions is still challenging. Methods The algorithm used multimodal medical images [structural magnetic resonance imaging (sMRI) and positron emission tomography (PET)] as experimental data. Deep feature representations of sMRI and PET images are extracted by 3D convolution neural network (3DCNN). An improved Transformer is then used to progressively learn global correlation information among features. Finally, the information from different modalities is fused for identification. A model-based visualization method is used to explain the decisions of the model and identify brain regions related to AD. Results The model attained a noteworthy classification accuracy of 98.1% for Alzheimer’s disease (AD) using the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset. Upon examining the visualization results, distinct brain regions associated with AD diagnosis were observed across different image modalities. Notably, the left parahippocampal region emerged consistently as a prominent and significant brain area. Conclusions A large number of comparative experiments have been carried out for the model, and the experimental results verify the reliability of the model. In addition, the model adopts a visualization analysis method based on the characteristics of the model, which improves the interpretability of the model. Some disease-related brain regions were found in the visualization results, which provides reliable information for AD clinical research.",['3D convolution neural network (3DCNN)'],"The research idea centers on the challenge of effectively integrating multiple types of brain imaging data to enhance the recognition of Alzheimer’s disease and to identify significant brain regions associated with the condition. Despite advancements in acquiring diverse neuroimaging data, combining these different modalities to improve diagnostic accuracy and understand the neural underpinnings of Alzheimer’s remains difficult. The primary objective of the study is to improve the identification of Alzheimer’s disease by utilizing multimodal brain imaging data and to pinpoint brain regions that are significantly related to the diagnosis. Additionally, the study aims to provide interpretable findings that can offer reliable information for clinical research on Alzheimer’s disease.","The research idea centers on the challenge of effectively integrating multiple types of brain imaging data to enhance the recognition of Alzheimer's disease and to identify significant brain regions associated with the condition. Despite advancements in acquiring diverse neuroimaging data, combining these different modalities to improve diagnostic accuracy and understand the neural underpinnings of Alzheimer's remains difficult. The primary objective of the study is to improve the identification of Alzheimer's disease by utilizing multimodal brain imaging data and to pinpoint brain regions that are significantly related to the diagnosis. Additionally, the study aims to provide interpretable findings that can offer reliable information for clinical research on Alzheimer's disease.",True
Psychology,An interpretable model based on graph learning for diagnosis of Parkinson’s disease with voice-related EEG,"Abstract Parkinson’s disease (PD) exhibits significant clinical heterogeneity, presenting challenges in the identification of reliable electroencephalogram (EEG) biomarkers. Machine learning techniques have been integrated with resting-state EEG for PD diagnosis, but their practicality is constrained by the interpretable features and the stochastic nature of resting-state EEG. The present study proposes a novel and interpretable deep learning model, graph signal processing-graph convolutional networks (GSP-GCNs), using event-related EEG data obtained from a specific task involving vocal pitch regulation for PD diagnosis. By incorporating both local and global information from single-hop and multi-hop networks, our proposed GSP-GCNs models achieved an averaged classification accuracy of 90.2%, exhibiting a significant improvement of 9.5% over other deep learning models. Moreover, the interpretability analysis revealed discriminative distributions of large-scale EEG networks and topographic map of microstate MS5 learned by our models, primarily located in the left ventral premotor cortex, superior temporal gyrus, and Broca’s area that are implicated in PD-related speech disorders, reflecting our GSP-GCN models’ ability to provide interpretable insights identifying distinctive EEG biomarkers from large-scale networks. These findings demonstrate the potential of interpretable deep learning models coupled with voice-related EEG signals for distinguishing PD patients from healthy controls with accuracy and elucidating the underlying neurobiological mechanisms.",['graph signal processing-graph convolutional networks (GSP-GCNs)'],"The research idea addresses the significant clinical heterogeneity of Parkinson’s disease (PD), which creates challenges in identifying reliable biomarkers for diagnosis. Specifically, the study focuses on the difficulty of finding interpretable and consistent features from brain activity related to PD, particularly in the context of speech and vocal pitch regulation. The motivation lies in improving the understanding and detection of PD-related neurobiological mechanisms through brain signals associated with vocal tasks. The primary objective of the study is to investigate event-related brain activity during a vocal pitch regulation task to identify distinctive neural patterns that can differentiate individuals with PD from healthy controls. The study aims to provide interpretable insights into the brain regions involved in PD-related speech disorders and to enhance the accuracy of PD diagnosis by examining large-scale brain network activity linked to vocal function.","The research idea addresses the significant clinical heterogeneity of Parkinson's disease (PD), which creates challenges in identifying reliable biomarkers for diagnosis. Specifically, the study focuses on the difficulty of finding interpretable and consistent features from brain activity related to PD, particularly in the context of speech and vocal pitch regulation. The motivation lies in improving the understanding and detection of PD-related neurobiological mechanisms through brain signals associated with vocal tasks. The primary objective of the study is to investigate event-related brain activity during a vocal pitch regulation task to identify distinctive neural patterns that can differentiate individuals with PD from healthy controls. The study aims to provide interpretable insights into the brain regions involved in PD-related speech disorders and to enhance the accuracy of PD diagnosis by examining large-scale brain network activity linked to vocal function.",True
Psychology,"Uncertainty Reduction in Flood Susceptibility Mapping Using Random Forest and eXtreme Gradient Boosting Algorithms in Two Tropical Desert Cities, Shibam and Marib, Yemen","Flooding is a natural disaster that coexists with human beings and causes severe loss of life and property worldwide. Although numerous studies for flood susceptibility modelling have been introduced, a notable gap has been the overlooked or reduced consideration of the uncertainty in the accuracy of the produced maps. Challenges such as limited data, uncertainty due to confidence bounds, and the overfitting problem are critical areas for improving accurate models. We focus on the uncertainty in susceptibility mapping, mainly when there is a significant variation in the predictive relevance of the predictor factors. It is also noted that the receiver operating characteristic (ROC) curve may not accurately depict the sensitivity of the resulting susceptibility map to overfitting. Therefore, reducing the overfitting problem was targeted to increase accuracy and improve processing time in flood prediction. This study created a spatial repository to test the models, containing data from historical flooding and twelve topographic and geo-environmental flood conditioning variables. Then, we applied random forest (RF) and extreme gradient boosting (XGB) algorithms to map flood susceptibility, incorporating a variable drop-off in the empirical loop function. The results showed that the drop-off loop function was a crucial method to resolve the model uncertainty associated with the conditioning factors of the susceptibility modelling and methods. The results showed that approximately 8.42% to 9.89% of Marib City and 9.93% to 15.69% of Shibam City areas were highly vulnerable to floods. Furthermore, this study significantly contributes to worldwide endeavors focused on reducing the hazards linked to natural disasters. The approaches used in this study can offer valuable insights and strategies for reducing natural disaster risks, particularly in Yemen.","['random forest (RF)', 'extreme gradient boosting (XGB)']","The research idea centers on addressing the significant challenge of uncertainty in flood susceptibility mapping, particularly the variability in the predictive relevance of factors influencing flood risk. Despite numerous studies on flood susceptibility, there has been limited attention to the accuracy and reliability of the resulting maps, especially concerning overfitting and confidence bounds. This gap highlights the need to improve the precision of flood risk assessments to better understand and mitigate the impact of flooding on vulnerable areas. The study aims to reduce the overfitting problem in flood susceptibility models to enhance their accuracy and reliability. The primary objective of the study is to improve flood susceptibility mapping by focusing on the uncertainty associated with predictor factors and to identify highly vulnerable areas to floods. By doing so, the research seeks to contribute valuable insights and strategies for reducing natural disaster risks, with a particular emphasis on flood hazards in specific regions such as Marib City and Shibam City in Yemen.","The research idea centers on addressing the significant challenge of uncertainty in flood susceptibility mapping, particularly the variability in the relevance of factors influencing flood risk. Despite numerous studies on flood susceptibility, there has been limited attention to the accuracy and reliability of the resulting maps, especially concerning estimation errors and confidence bounds. This gap highlights the need to improve the precision of flood risk assessments to better understand and mitigate the impact of flooding on vulnerable areas. The study aims to reduce estimation errors in flood susceptibility analyses to enhance their accuracy and reliability. The primary objective of the study is to improve flood susceptibility mapping by focusing on the uncertainty associated with predictor factors and to identify highly vulnerable areas to floods. By doing so, the research seeks to contribute valuable insights and strategies for reducing natural disaster risks, with a particular emphasis on flood hazards in specific regions such as Marib City and Shibam City in Yemen.",True
Psychology,Machine learning model (RG-DMML) and ensemble algorithm for prediction of students’ retention and graduation in education,"Automated prediction of students' retention and graduation in education using advanced analytical methods such as artificial intelligence (AI), has recently attracted the attention of educators, both in theory and in practice. Whereas invaluable insights and theories for measuring and testing the topic have been proposed, most of the existing methods do not technically highlight the non-trivial factors behind the renowned challenges and attrition. To this effect, by making use of two categories of data collected in a higher education setting about students (i) retention (n = 52262) and (ii) graduation (n = 53639); this study proposes a machine learning model - RG-DMML (retention and graduation data mining and machine learning) and ensemble algorithm for prediction of students' retention and graduation status in education. This was done by training and testing key features that are technically deemed suitable for measuring the constructs (retention and graduation), such as (i) the Average grade of the previous high school, and (ii) the Entry/admission score. The proposed model (RG-DMML) is designed based on the cross industry standard process for data mining (CRISP-DM) methodology, implemented using supervised machine learning technique such as K-Nearest Neighbor (KNN), and validated using the k-fold cross-validation method. The results show that the executed model and algorithm based on the Bagging method and 10-fold cross-validation are efficient and effective for predicting the student's retention and graduation status, with Precision (retention = 0.909, graduation = 0.822), Recall (retention = 1.000, graduation = 0.957), Accuracy (retention = 0.909, graduation = 0.817), F1-Score (retention = 0.952, graduation = 0.885) showing significant high accuracy levels or performance rate, and low Error-rate (retention = 0.090, graduation = 0.182), respectively. In addition, by considering the individual features selected through the Wrapper method in predicting the outputs, the proposed model proved more effective for predicting the students' retention status in comparison to the graduation data. The implications of the models' output and factors that impact the effective prediction or identification of at-risk students, e.g., for timely intervention, counselling, decision-making, and sustainable educational practice are empirically discussed in the study.","['ensemble algorithm', 'supervised machine learning technique', 'K-Nearest Neighbor (KNN)', 'Wrapper method']","The research idea centers on addressing the challenges related to student retention and graduation in higher education, highlighting that existing approaches often overlook the complex factors contributing to these issues. There is a recognized need to better understand and measure the key influences affecting students' continuation and completion of their studies. The primary objective of the study is to predict students' retention and graduation status by examining important features such as previous academic performance and admission scores. This aims to provide insights that can support timely intervention, counseling, and decision-making to improve educational outcomes and promote sustainable practices within educational settings.","The research idea centers on addressing the challenges related to student retention and graduation in higher education, highlighting that existing approaches often overlook the complex factors contributing to these issues. There is a recognized need to better understand and measure the key influences affecting students' continuation and completion of their studies. The primary objective of the study is to identify factors associated with students' retention and graduation status by examining important indicators such as previous academic performance and admission scores. This aims to provide insights that can support timely intervention, counseling, and decision-making to improve educational outcomes and promote sustainable practices within educational settings.",True
Psychology,Variation in social media sensitivity across people and contexts,"Abstract Social media impacts people’s wellbeing in different ways, but relatively little is known about why this is the case. Here we introduce the construct of “social media sensitivity” to understand how social media and wellbeing associations differ across people and the contexts in which these platforms are used. In a month-long large-scale intensive longitudinal study (total n = 1632; total number of observations = 120,599), we examined for whom and under which circumstances social media was associated with positive and negative changes in social and affective wellbeing. Applying a combination of frequentist and Bayesian multilevel models, we found a small negative average association between social media use AND subsequent wellbeing, but the associations were heterogenous across people. People with psychologically vulnerable dispositions (e.g., those who were depressed, lonely, not satisfied with life) tended to experience heightened negative social media sensitivity in comparison to people who were not psychologically vulnerable. People also experienced heightened negative social media sensitivity when in certain types of places (e.g., in social places, in nature) and while around certain types of people (e.g., around family members, close ties), as compared to using social media in other contexts. Our results suggest that an understanding of the effects of social media on wellbeing should account for the psychological dispositions of social media users, and the physical and social contexts surrounding their use. We discuss theoretical and practical implications of social media sensitivity for scholars, policymakers, and those in the technology industry.",['Bayesian multilevel models'],"The research idea centers on understanding why social media impacts people’s wellbeing in different ways, highlighting the need to explore individual differences and contextual factors that influence these effects. The study introduces the concept of “social media sensitivity” to explain how associations between social media use and wellbeing vary across individuals and the environments in which social media is used. The research objective is to examine for whom and under which circumstances social media use is associated with positive and negative changes in social and affective wellbeing. Specifically, the study aims to identify how psychological vulnerability and different physical and social contexts contribute to variations in social media sensitivity and its impact on wellbeing.","The research idea centers on understanding why social media impacts people's wellbeing in different ways, highlighting the need to explore individual differences and contextual factors that influence these effects. The study introduces the concept of ""social media sensitivity"" to explain how associations between social media use and wellbeing vary across individuals and the environments in which social media is used. The research objective is to examine for whom and under which circumstances social media use is associated with positive and negative changes in social and affective wellbeing. Specifically, the study aims to identify how psychological vulnerability and different physical and social contexts contribute to variations in social media sensitivity and its impact on wellbeing.",True
Psychology,Imagined Speech-EEG Detection Using Multivariate Swarm Sparse Decomposition-Based Joint Time-Frequency Analysis for Intuitive BCI,"In brain-computer interface (BCI) applications, imagined speech (IMS) decoding based on electroencephalography (EEG) has established a new neuro-paradigm that offers an intuitive communication tool for physically impaired patients.However, existing IMS-EEG-based BCI systems have introduced difficulties in feasible deployment due to nonstationary EEG signals, suboptimal feature extraction, and constrained multi-class scalability.To address these challenges, we have presented a novel approach using the multivariate swarm-sparse decomposition method (MSSDM) for joint time-frequency (JTF) analysis and further developed a feasible end-to-end framework from multichannel IMS-EEG signals for imagined speech detection.MSSDM employs improved multivariate swarm filtering and sparse spectrum techniques to design optimal filter banks for extracting an ensemble of channel-aligned oscillatory components (CAOCs), significantly enhancing IMS activation-related sub-bands.To enhance channelaligned information, multivariate JTF images have been constructed using joint instantaneous frequency and instantaneous amplitude across channels from the obtained CAOCs.Further, JTFbased deep features (JTFDF) were computed using different pretrained neural networks and mapped most discriminant features using two well-known feature correlation techniques: Canonical correlation analysis and Hellinger distance-based correlation.The proposed method has been tested on the 5-class BCI Competition DB and 6-class Coretto DB IMS datasets.The experimental findings on cross-subject reveal that the novel JTFDF feature-based classification model, MSSDM-SqueezeNet-JTFDF, achieved the highest classification performance against all other existing state-of-theart methods in imagined speech recognition.","['sparse spectrum techniques', 'pretrained neural networks', 'Canonical correlation analysis']","The research idea centers on the challenges faced in developing effective imagined speech decoding systems using brain signals, which hold promise as intuitive communication tools for individuals with physical impairments. Current approaches encounter difficulties due to the variability of brain signals, limitations in extracting meaningful features, and challenges in scaling to multiple speech categories. The study aims to overcome these obstacles to improve the feasibility and accuracy of imagined speech recognition. The primary objective of the study is to develop a novel approach that enhances the detection of imagined speech from brain signals by optimizing the extraction and representation of relevant neural components. This approach seeks to improve classification performance across multiple speech categories and demonstrate superior recognition accuracy compared to existing methods.","The research idea centers on the challenges faced in developing effective imagined speech decoding systems using brain signals, which hold promise as intuitive communication tools for individuals with physical impairments. Current approaches encounter difficulties due to the variability of brain signals, limitations in extracting meaningful features, and challenges in scaling to multiple speech categories. The study aims to overcome these obstacles to improve the feasibility and accuracy of imagined speech recognition. The primary objective of the study is to develop a novel approach that enhances the detection of imagined speech from brain signals by improving the extraction and representation of relevant neural components. This approach seeks to improve performance across multiple speech categories and demonstrate superior recognition accuracy compared to existing methods.",True
Psychology,A study on smart home use intention of elderly consumers based on technology acceptance models,"Purpose Smart home devices have great potential to improve the quality of life and independence of older people, positively impacting their health, safety, and comfort. However, Chinese research in this field is still in its early stages. Therefore, more comprehensive and in-depth studies are needed to comprehend the various aspects influencing the acceptance and use of smart homes by older users. Patients and methods This study adopted the Technology Acceptance Model (TAM) and included perceived usefulness, perceived ease of use, usage intention, intergenerational technology support, perceived value, and perceived risk as extension variables to delve deeper into the behavioral intentions of older users in smart home services. The study used a convenience sampling method to randomly distribute 236 questionnaires among older adults over the age of 60 in the school’s community and neighboring urban communities who have experience in smart home use and who can complete human-computer interactions either independently or with the help of others, mainly focusing on the four sections: user characteristics, family situation, experience of use, and usage intention. The study used structural equation modeling (SEM) and factor analysis to analyze the completion of questionnaires. Finally, we conducted a validation analysis of the rationality and scientificity of the model and derived the six dimensions of the model of the influencing factors on the use of smart home products by the elderly and the weight sizes of their corresponding 13 influencing factors. Results The results show that perceived usefulness and perceived ease of use have a positive effect on users’ intention to use smart homes. Perceived ease of use has a positive effect on the perceived usefulness of smart homes. In addition, intergenerational technology support, perceived value, and perceived risk impact users’ perceived usefulness and perceived ease of use of the smart home. Conclusion This research aims to describe the factors influencing older users’ willingness to use smart homes. The findings are not only significant for the elderly in China but also of broad value to other regions and countries facing similar demographic challenges. The development of smart homes not only involves the elderly but is also closely related to all segments of society. The government should increase policy support and guide more social forces to participate in the development of the smart home industry. Service providers and designers should fully understand the demand situation and user experience of target users to develop easy-to-use smart home solutions. At the same time, smart homes, as intelligent products for the elderly, need to focus not only on the basic needs of the elderly such as material life and home safety, but also on the spiritual needs of elderly users. Children or caregivers should always pay attention to the psychological state of the elderly and actively guide them to use smart homes to help them realize their self-worth. We look forward to more research focusing on this area in the future and further exploring the specific issues and solutions involved.",['factor analysis'],"The research idea centers on the potential of smart home devices to enhance the quality of life, independence, health, safety, and comfort of older adults, while recognizing that research on the acceptance and use of such technology among the elderly in China remains limited and underdeveloped. There is a need for more comprehensive and in-depth studies to understand the various factors influencing older users’ willingness to adopt smart home services. The primary objective of the study is to identify and describe the factors that affect older adults’ behavioral intentions to use smart home products, focusing on dimensions such as perceived usefulness, perceived ease of use, intergenerational support, perceived value, and perceived risk. This aims to provide insights that are valuable not only for the elderly population in China but also for other regions facing similar demographic challenges, ultimately guiding the development of smart home solutions that address both the practical and psychological needs of older users.","The research idea centers on the potential of smart home devices to enhance the quality of life, independence, health, safety, and comfort of older adults, while recognizing that research on the acceptance and use of such technology among the elderly in China remains limited and underdeveloped. There is a need for more comprehensive and in-depth studies to understand the various factors influencing older users' willingness to adopt smart home services. The primary objective of the study is to identify and describe the factors that affect older adults' behavioral intentions to use smart home products, focusing on dimensions such as perceived usefulness, perceived ease of use, intergenerational support, perceived value, and perceived risk. This aims to provide insights that are valuable not only for the elderly population in China but also for other regions facing similar demographic challenges, ultimately guiding the development of smart home solutions that address both the practical and psychological needs of older users.",True
Psychology,Enhancing Brain Tumor Classification by a Comprehensive Study on Transfer Learning Techniques and Model Efficiency Using MRI Datasets,"Brain tumors, a significant health concern, are a leading cause of mortality globally, with an annual projected increase of 5% by the World Health Organization. This work aims to comprehensively analyze the performance of transfer learning methods in identifying the types of brain tumors, with a particular emphasis on the necessity of prompt identification. The study demonstrates how useful it is to use pre-trained models, including models VGG-16, VGG-19, Inception-v3, ResNet-50, DenseNet, and MobileNet—on MRI datasets and used to obtain a precise classification. Using these methods model accuracy and efficiency have been enhanced. The research aims to contribute to improved treatment planning and patient outcomes by implementing optimal methodologies for precise and automated brain tumor analysis, evaluation framework encompasses vital metrics such as confusion matrices, ROC curves, and the achieved Area Under the Curve (AUC) for each approach. The comprehensive methodology outlined in this paper serves as a systematic guide for the implementation and evaluation of brain tumor classification models utilizing deep learning techniques. The integration of visual representations, code snippets, and performance metrics significantly enhances the clarity and understanding of the proposed approach. Among our proposed algorithms, VGG-16 attains the highest accuracy at 97% and consumes only 22% of time as compared to our previous proposed methodology.","['transfer learning', 'VGG-16', 'VGG-19', 'Inception-v3', 'ResNet-50', 'DenseNet', 'MobileNet']","The research idea addresses the significant health concern posed by brain tumors, which are a leading cause of mortality worldwide and are projected to increase annually. The study highlights the critical need for prompt and accurate identification of brain tumor types to improve patient outcomes. The research objective is to evaluate and enhance the accuracy and efficiency of methods for identifying different types of brain tumors, with the aim of contributing to improved treatment planning and patient care through precise classification. The study seeks to establish an effective approach for brain tumor analysis that supports timely and accurate diagnosis.","The research idea addresses the significant health concern posed by brain tumors, which are a leading cause of mortality worldwide and are projected to increase annually. The study highlights the critical need for prompt and accurate identification of brain tumor types to improve patient outcomes. The research objective is to evaluate and enhance the accuracy and efficiency of methods for identifying different types of brain tumors, with the aim of contributing to improved treatment planning and patient care through precise categorization. The study seeks to establish an effective approach for brain tumor analysis that supports timely and accurate diagnosis.",True
Psychology,"Machine learning in physical activity, sedentary, and sleep behavior research","Abstract The nature of human movement and non-movement behaviors is complex and multifaceted, making their study complicated and challenging. Thanks to the availability of wearable activity monitors, we can now monitor the full spectrum of physical activity, sedentary, and sleep behaviors better than ever before—whether the subjects are elite athletes, children, adults, or individuals with pre-existing medical conditions. The increasing volume of generated data, combined with the inherent complexities of human movement and non-movement behaviors, necessitates the development of new data analysis methods for the research of physical activity, sedentary, and sleep behaviors. The characteristics of machine learning (ML) methods, including their ability to deal with complicated data, make them suitable for such analysis and thus can be an alternative tool to deal with data of this nature. ML can potentially be an excellent tool for solving many traditional problems related to the research of physical activity, sedentary, and sleep behaviors such as activity recognition, posture detection, profile analysis, and correlates research. However, despite this potential, ML has not yet been widely utilized for analyzing and studying these behaviors. In this review, we aim to introduce experts in physical activity, sedentary behavior, and sleep research—individuals who may possess limited familiarity with ML—to the potential applications of these techniques for analyzing their data. We begin by explaining the underlying principles of the ML modeling pipeline, highlighting the challenges and issues that need to be considered when applying ML. We then present the types of ML: supervised and unsupervised learning, and introduce a few ML algorithms frequently used in supervised and unsupervised learning. Finally, we highlight three research areas where ML methodologies have already been used in physical activity, sedentary behavior, and sleep behavior research, emphasizing their successes and challenges. This paper serves as a resource for ML in physical activity, sedentary, and sleep behavior research, offering guidance and resources to facilitate its utilization.","['supervised learning', 'unsupervised learning']","The study addresses the complexity and multifaceted nature of human movement and non-movement behaviors, including physical activity, sedentary behavior, and sleep, which makes their study challenging. With the availability of wearable activity monitors, there is now an unprecedented ability to monitor these behaviors across diverse populations such as elite athletes, children, adults, and individuals with pre-existing medical conditions. However, the increasing volume and complexity of behavioral data require new approaches to better understand these behaviors.

The primary aim of the study is to introduce experts in physical activity, sedentary behavior, and sleep research to novel approaches that can enhance the analysis of their data. The study seeks to highlight the potential applications, successes, and challenges of these approaches in advancing research on physical activity, sedentary behavior, and sleep, thereby providing guidance and resources to facilitate their utilization in this field.","The study addresses the complexity and multifaceted nature of human movement and non-movement behaviors, including physical activity, sedentary behavior, and sleep, which makes their study challenging. With the availability of wearable activity monitors, there is now an unprecedented ability to monitor these behaviors across diverse populations such as elite athletes, children, adults, and individuals with pre-existing medical conditions. However, the increasing volume and complexity of behavioral data require new analytical methods to better understand these behaviors.

The primary aim of the study is to introduce experts in physical activity, sedentary behavior, and sleep research to novel analytical approaches that can enhance the analysis of their data. The study seeks to highlight the potential applications, successes, and challenges of these approaches in advancing research on physical activity, sedentary behavior, and sleep, thereby providing guidance and resources to facilitate their utilization in this field.",True
Psychology,Exploring the frontier: Transformer-based models in EEG signal analysis for brain-computer interfaces,"This review systematically explores the application of transformer-based models in EEG signal processing and brain-computer interface (BCI) development, with a distinct focus on ensuring methodological rigour and adhering to empirical validations within the existing literature. By examining various transformer architectures, such as the Temporal Spatial Transformer Network (TSTN) and EEG Conformer, this review delineates their capabilities in mitigating challenges intrinsic to EEG data, such as noise and artifacts, and their subsequent implications on decoding and classification accuracies across disparate mental tasks. The analytical scope extends to a meticulous examination of attention mechanisms within transformer models, delineating their role in illuminating critical temporal and spatial EEG features and facilitating interpretability in model decision-making processes. The discourse additionally encapsulates emerging works that substantiate the efficacy of transformer models in noise reduction of EEG signals and diversifying applications beyond the conventional motor imagery paradigm. Furthermore, this review elucidates evident gaps and propounds exploratory avenues in the applications of pre-trained transformers in EEG analysis and the potential expansion into real-time and multi-task BCI applications. Collectively, this review distils extant knowledge, navigates through the empirical findings, and puts forward a structured synthesis, thereby serving as a conduit for informed future research endeavours in transformer-enhanced, EEG-based BCI systems.","['transformer-based models', 'attention mechanisms within transformer models']","The research idea centers on addressing the challenges inherent in processing EEG signals, such as noise and artifacts, which affect the accuracy of decoding and classification across various mental tasks. There is a need to ensure methodological rigor and empirical validation in the study of EEG signal processing to improve the interpretability and reliability of findings. The study aims to systematically explore existing approaches that enhance the understanding and application of EEG data in brain-computer interface development. The primary objective of the study is to review and synthesize current knowledge on advanced methods used to mitigate EEG signal challenges, highlight their implications for mental task classification, and identify gaps and future directions for expanding EEG applications beyond traditional paradigms.","The research idea centers on addressing the challenges inherent in processing EEG signals, such as noise and artifacts, which affect the accuracy of decoding and classification across various mental tasks. There is a need to ensure methodological rigor and empirical validation in the study of EEG signal processing to improve the interpretability and reliability of findings. The study aims to systematically explore existing approaches that enhance the understanding and application of EEG data in brain-computer interface development. The primary objective of the study is to review and synthesize current knowledge on effective methods used to mitigate EEG signal challenges, highlight their implications for mental task classification, and identify gaps and future directions for expanding EEG applications beyond traditional paradigms.",True
Psychology,A Novel EEG-Based Parkinson’s Disease Detection Model Using Multiscale Convolutional Prototype Networks,"Objective and accurate detection of Parkinson's disease (PD) is crucial for timely intervention and treatment. Electroencephalography (EEG) has been proven to characterize PD by measuring brain activity. In recent years, deep learning methods have gained great attention in automated PD detection, but their performance is limited by insufficient data samples. In this article, we propose a novel PD automated detection model named the multiscale convolutional prototype network (MCPNet), which integrates and improves upon multiscale convolutional neural networks (CNNs) and prototype learning. On the one hand, it employs multiscale CNNs to extract brain features from different scales, enhancing feature diversity and utilization. On the other hand, a prototype calibration strategy is introduced to mitigate the effect of data noise on prototype generation, improving the generalization performance of model. Multiple within-dataset and cross-dataset experiments on three different datasets demonstrate the effectiveness of our model in PD detection. The leave-one-subject-out (LOSO) results of within-dataset experiments show that MCPNet achieves an accuracy of 92.5%, a sensitivity of 93.1%, a specificity of 91.9%, and an AUC of 92.4% in cross-subject classification between PD patients and healthy controls. In the cross-dataset classification, the performance of MCPNet is somewhat weakened due to dataset variations. However, this weakening is partially compensated by introducing the prototype calibration strategy. With the introduction of the calibration strategy, the accuracy of cross-dataset classification increases to 90.2%, a 4.0% improvement compared to when it is not used. These results indicate that the proposed model may be a promising tool for automated PD diagnosis.","['multiscale convolutional neural networks (CNNs)', 'prototype learning']","The research idea centers on the critical need for objective and accurate detection of Parkinson's disease (PD) to enable timely intervention and treatment. Since brain activity measured through electroencephalography (EEG) has been shown to characterize PD, improving detection methods based on these measurements is essential. The study addresses challenges related to limited data samples that affect the performance of current automated PD detection approaches. The primary objective of the study is to develop and evaluate a novel approach that enhances the detection of Parkinson's disease by effectively capturing brain features across different scales and improving the reliability of classification despite data variability. This aims to provide a more accurate and generalizable tool for distinguishing PD patients from healthy individuals.","The research idea centers on the critical need for objective and accurate detection of Parkinson's disease (PD) to enable timely intervention and treatment. Since brain activity measured through electroencephalography (EEG) has been shown to characterize PD, improving detection methods based on these measurements is essential. The study addresses challenges related to limited data samples that affect the performance of current PD detection approaches. The primary objective of the study is to develop and evaluate a novel approach that enhances the detection of Parkinson's disease by effectively capturing brain features across different scales and improving the reliability of analysis despite data variability. This aims to provide a more accurate and generalizable tool for distinguishing PD patients from healthy individuals.",True
Psychology,Predicting University Student Graduation Using Academic Performance and Machine Learning: A Systematic Literature Review,"Predicting university student graduation is a beneficial tool for both students and institutions. With the help of this predictive capacity, students may make well-informed decisions about their academic and career paths, and institutions can proactively identify students who may not graduate and offer tailored support to ensure their success. The use of machine learning for predicting university student graduation has drawn more attention in recent years. Large datasets of student academic performance data can be used to train machine learning algorithms to identify patterns that are applicable in predicting future outcomes. In accordance with some studies, this approach predicts student graduation with an accuracy rate as high as 90%. Many SLRs have been conducted in this field, but there are still limitations, including not discussing the predictive models and algorithms used, a lack of coverage of the machine learning algorithms applied, small database coverage, keyword selection that does not cover all synonyms relevant to the investigation, and less specific data collection transparency. By delving into the limitations of existing SLRs on this topic, this research not only enhances the understanding of machine learning applications in forecasting student graduation but also fills a crucial gap in the literature. The inclusion of weaknesses in current SLRs provides a foundation for justifying the need for this study, emphasizing the necessity of a more nuanced and comprehensive review to advance the field and guide future research efforts in smart learning environments. This research conducts a thorough systematic review of the existing literature on machine learning-based student graduation prediction models from 70 journal articles from 2018 through 2023 that are pertinent. This review includes the various machine learning algorithms that have been implemented, the various academic performance data that was obtained from students, and the effectiveness of the models that have been developed. It also discusses the difficulties and potential advantages of utilizing machine learning to predict student graduation. The review indicates that the most common approach employed is the prediction of students' academic performance, which relies on data obtained from the Learning Management System and Student Information System. The primary data utilized for prediction purposes consists Student retention and time of academic and behavioral information. Among the various algorithms employed, Support Vector Machine and Random Forest are the most commonly utilized. This study makes a significant contribution to the advancement of learner modules within the smart learning environment.","['Support Vector Machine', 'Random Forest']","The research idea centers on the importance of predicting university student graduation to benefit both students and educational institutions. Accurate prediction enables students to make informed decisions about their academic and career paths, while institutions can identify at-risk students early and provide tailored support to improve graduation outcomes. Despite existing studies, there remain limitations in the current literature reviews, such as insufficient discussion of predictive approaches and incomplete data coverage, highlighting a need for a more comprehensive understanding of this area. The study addresses these gaps to enhance knowledge about factors influencing student graduation and to support efforts aimed at improving student success.

The primary objective of this study is to conduct a thorough systematic review of the existing literature on student graduation prediction, focusing on academic performance and related factors influencing graduation outcomes. It aims to evaluate the effectiveness of various approaches used to forecast student graduation, discuss challenges and potential benefits associated with these predictions, and provide a clearer understanding of how academic and behavioral information contributes to student retention and timely graduation. Through this comprehensive review, the study seeks to advance knowledge that can inform future research and support interventions designed to improve student success in higher education.","The research idea centers on the importance of predicting university student graduation to benefit both students and educational institutions. Accurate prediction enables students to make informed decisions about their academic and career paths, while institutions can identify at-risk students early and provide tailored support to improve graduation outcomes. Despite existing studies, there remain limitations in the current literature reviews, such as insufficient discussion of methodological approaches and incomplete data coverage, highlighting a need for a more comprehensive understanding of this area. The study addresses these gaps to enhance knowledge about factors influencing student graduation and to support efforts aimed at improving student success.

The primary objective of this study is to conduct a thorough systematic review of the existing literature on student graduation prediction, focusing on academic performance and related factors influencing graduation outcomes. It aims to evaluate the effectiveness of various approaches used to forecast student graduation, discuss challenges and potential benefits associated with these predictions, and provide a clearer understanding of how academic and behavioral information contributes to student retention and timely graduation. Through this comprehensive review, the study seeks to advance knowledge that can inform future research and support interventions designed to improve student success in higher education.",True
Psychology,Chatbots and Large Language Models in Radiology: A Practical Primer for Clinical and Research Applications,"Although chatbots have existed for decades, the emergence of transformer-based large language models (LLMs) has captivated the world through the most recent wave of artificial intelligence chatbots, including ChatGPT. Transformers are a type of neural network architecture that enables better contextual understanding of language and efficient training on massive amounts of unlabeled data, such as unstructured text from the internet. As LLMs have increased in size, their improved performance and emergent abilities have revolutionized natural language processing. Since language is integral to human thought, applications based on LLMs have transformative potential in many industries. In fact, LLM-based chatbots have demonstrated human-level performance on many professional benchmarks, including in radiology. LLMs offer numerous clinical and research applications in radiology, several of which have been explored in the literature with encouraging results. Multimodal LLMs can simultaneously interpret text and images to generate reports, closely mimicking current diagnostic pathways in radiology. Thus, from requisition to report, LLMs have the opportunity to positively impact nearly every step of the radiology journey. Yet, these impressive models are not without limitations. This article reviews the limitations of LLMs and mitigation strategies, as well as potential uses of LLMs, including multimodal models. Also reviewed are existing LLM-based applications that can enhance efficiency in supervised settings.","['transformer-based large language models (LLMs)', 'Transformers', 'Multimodal LLMs']","The research idea centers on the transformative potential of advanced language-based technologies in understanding and processing human language, which is fundamental to human thought and communication. These technologies have shown promising applications in professional fields such as radiology, where they can mimic diagnostic processes and improve various stages of the clinical workflow. However, despite their impressive capabilities, there are notable limitations that need to be addressed to fully realize their benefits. The primary objective of the study is to review the limitations and mitigation strategies of these language-based technologies, explore their potential uses including multimodal applications, and examine existing implementations that enhance efficiency in supervised clinical settings.","The research idea centers on the transformative potential of advanced language-based technologies in understanding and processing human language, which is fundamental to human thought and communication. These technologies have shown promising applications in professional fields such as radiology, where they can support diagnostic processes and improve various stages of the clinical workflow. However, despite their impressive capabilities, there are notable limitations that need to be addressed to fully realize their benefits. The primary objective of the study is to review the limitations and mitigation strategies of these language-based technologies, explore their potential uses including multimodal applications, and examine existing implementations that enhance efficiency in supervised clinical settings.",True
Psychology,Detecting hallucinations in large language models using semantic entropy,"Abstract Large language model (LLM) systems, such as ChatGPT 1 or Gemini 2 , can show impressive reasoning and question-answering capabilities but often ‘hallucinate’ false outputs and unsubstantiated answers 3,4 . Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents 5 or untrue facts in news articles 6 and even posing a risk to human life in medical domains such as radiology 7 . Encouraging truthfulness through supervision or reinforcement has been only partially successful 8 . Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. Here we develop new methods grounded in statistics, proposing entropy-based uncertainty estimators for LLMs to detect a subset of hallucinations—confabulations—which are arbitrary and incorrect generations. Our method addresses the fact that one idea can be expressed in many ways by computing uncertainty at the level of meaning rather than specific sequences of words. Our method works across datasets and tasks without a priori knowledge of the task, requires no task-specific data and robustly generalizes to new tasks not seen before. By detecting when a prompt is likely to produce a confabulation, our method helps users understand when they must take extra care with LLMs and opens up new possibilities for using LLMs that are otherwise prevented by their unreliability.",['entropy-based uncertainty estimators'],"The research idea centers on the problem of unreliable and false outputs produced by large language models, which hinder their adoption in critical fields such as law, journalism, and medicine due to the risk of fabricating untrue information. This issue of hallucination, where models generate arbitrary and incorrect content, poses significant challenges for ensuring truthfulness and safety in various applications. The study is motivated by the need for a general approach to detect these hallucinations, especially in situations involving new or unseen questions that even humans might find difficult to answer. The primary objective of the study is to develop a method that can identify when such false or confabulated responses are likely to occur, thereby helping users recognize when extra caution is necessary. This approach aims to improve the reliability of responses across different tasks and domains without requiring prior knowledge or task-specific information, ultimately facilitating safer and more trustworthy use of language-based systems.","The research idea centers on the problem of unreliable and false outputs produced by advanced text generation systems, which hinder their adoption in critical fields such as law, journalism, and medicine due to the risk of fabricating untrue information. This issue of hallucination, where these systems generate arbitrary and incorrect content, poses significant challenges for ensuring truthfulness and safety in various applications. The study is motivated by the need for a general approach to detect these hallucinations, especially in situations involving new or unseen questions that even humans might find difficult to answer. The primary objective of the study is to develop a method that can identify when such false or confabulated responses are likely to occur, thereby helping users recognize when extra caution is necessary. This approach aims to improve the reliability of responses across different tasks and domains without requiring prior knowledge or task-specific information, ultimately facilitating safer and more trustworthy use of text generation systems.",True
Psychology,Comparing the quality of human and ChatGPT feedback of students’ writing,"Offering students formative feedback on their writing is an effective way to facilitate writing development. Recent advances in AI (i.e., ChatGPT) may function as an automated writing evaluation tool, increasing the amount of feedback students receive and diminishing the burden on teachers to provide frequent feedback to large classes. We examined the ability of generative AI (ChatGPT) to provide formative feedback. We compared the quality of human and AI feedback by scoring the feedback each provided on secondary student essays. We scored the degree to which feedback (a) was criteria-based, (b) provided clear directions for improvement, (c) was accurate, (d) prioritized essential features, and (e) used a supportive tone. 200 pieces of human-generated formative feedback and 200 pieces of AI-generated formative feedback for the same essays. We examined whether ChatGPT and human feedback differed in quality for the whole sample, for compositions that differed in overall quality, and for native English speakers and English learners by comparing descriptive statistics and effect sizes. Human raters were better at providing high-quality feedback to students in all categories other than criteria-based. AI and humans showed differences in feedback quality based on essay quality. Feedback did not vary by language status for humans or AI. Well-trained evaluators provided higher quality feedback than ChatGPT. Considering the ease of generating feedback through ChatGPT and its overall quality, generative AI may still be useful in some contexts, particularly in formative early drafts or instances where a well-trained educator is unavailable.",['generative AI (ChatGPT)'],"The research idea centers on the importance of providing students with formative feedback to support their writing development and the challenge teachers face in delivering frequent, high-quality feedback to large classes. The study addresses the potential role of automated tools in increasing the amount of feedback students receive while reducing the burden on educators. The primary objective of the study is to evaluate and compare the quality of formative feedback provided by humans and an automated source on secondary student essays. Specifically, the study aims to assess differences in feedback quality across various criteria, essay quality levels, and language backgrounds to determine the effectiveness and potential utility of automated feedback in educational contexts.","The research idea centers on the importance of providing students with formative feedback to support their writing development and the challenge teachers face in delivering frequent, high-quality feedback to large classes. The study addresses the potential role of assessment tools in increasing the amount of feedback students receive while reducing the burden on educators. The primary objective of the study is to evaluate and compare the quality of formative feedback provided by humans and an alternative source on secondary student essays. Specifically, the study aims to assess differences in feedback quality across various criteria, essay quality levels, and language backgrounds to determine the effectiveness and potential utility of this alternative feedback approach in educational contexts.",True
Psychology,Exploring AI-mediated informal digital learning of English (AI-IDLE): a mixed-method investigation of Chinese EFL learners’ AI adoption and experiences,"Recent advancements in natural language processing and large language models have ushered language learning into the age of artificial intelligence (AI). Recognizing the affordances of generative AI tools, this paper aims to examine the degree to which L2 learners accepted and leveraged large language model platforms (e.g. ChatGPT, Bing Chat) for the informal digital learning of English (IDLE) purposes. Employing an explanatory sequential mixed-method design, this study draws on the technology acceptance model (TAM) and collects data via an adapted TAM questionnaire and an interview guide. A total of 867 Chinese EFL (English as a foreign language) learners answered the online survey, while 20 attended the post-survey interviews. Drawing on a validated structural model that elucidates the inter-factor relationships of perceived ease of use, perceived usefulness, intention to use, and actual use, the quantitative analysis provides statistical accounts for EFL learners' adoption of Generative Pre-trained Transformer (GPT) technologies. The qualitative findings, derived from the interview data, reveal three key themes: (1) how perceived usefulness of chatbots for IDLE emerges from hands-on experimentation with these tools; (2) how intention to use increases as learners negotiate chatbot affordances and constraints; and (3) how actual use of chatbots for IDLE involves using these tools as tutors or conversation partners. Connections between quantitative and qualitative findings enhance our understanding of how EFL learners negotiate the affordances and constraints of highly capable AI technologies to participate in creative IDLE practices. By understanding these practices, this study draws attention to the attitudes and practices that constitute AI literacies, ultimately offering implications for future classroom practices and research.",['Generative Pre-trained Transformer (GPT)'],"The research idea centers on understanding how second language (L2) learners engage with and accept new digital tools for informal English learning outside the classroom. It addresses the motivation to explore learners’ attitudes and behaviors toward using advanced language platforms to support their English as a foreign language (EFL) development in informal settings. The study recognizes the importance of examining learners’ perceptions and actual use of these tools to better comprehend their role in language learning practices. The primary objective of the study is to investigate the extent to which Chinese EFL learners accept and utilize large language model platforms for informal digital English learning purposes. It aims to elucidate the relationships between learners’ perceived ease of use, perceived usefulness, intention to use, and actual use of these platforms, as well as to explore learners’ experiences and attitudes toward integrating these tools as part of their language learning activities.","The research idea centers on understanding how second language (L2) learners engage with and accept new digital tools for informal English learning outside the classroom. It addresses the motivation to explore learners' attitudes and behaviors toward using advanced language platforms to support their English as a foreign language (EFL) development in informal settings. The study recognizes the importance of examining learners' perceptions and actual use of these tools to better comprehend their role in language learning practices. The primary objective of the study is to investigate the extent to which Chinese EFL learners accept and utilize innovative language platforms for informal digital English learning purposes. It aims to elucidate the relationships between learners' perceived ease of use, perceived usefulness, intention to use, and actual use of these platforms, as well as to explore learners' experiences and attitudes toward integrating these tools as part of their language learning activities.",True
Psychology,Mental-LLM,"Advances in large language models (LLMs) have empowered a variety of applications. However, there is still a significant gap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health. In this work, we present a comprehensive evaluation of multiple LLMs on various mental health prediction tasks via online text data, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4. We conduct a broad range of experiments, covering zero-shot prompting, few-shot prompting, and instruction fine-tuning. The results indicate a promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9% on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8%. They further perform on par with the state-of-the-art task-specific language model. We also conduct an exploratory case study on LLMs' capability on mental health reasoning tasks, illustrating the promising capability of certain models such as GPT-4. We summarize our findings into a set of action guidelines for potential methods to enhance LLMs' capability for mental health tasks. Meanwhile, we also emphasize the important limitations before achieving deployability in real-world mental health settings, such as known racial and gender bias. We highlight the important ethical risks accompanying this line of research.","['zero-shot prompting', 'few-shot prompting', 'instruction fine-tuning']","The research idea centers on addressing the significant gap in understanding and improving the capabilities related to mental health within current approaches. Despite advances in related technologies, there remains a need to evaluate and enhance performance specifically for mental health prediction and reasoning tasks. The study is motivated by the potential to better support mental health applications while acknowledging existing limitations and ethical concerns such as bias. The primary objective of the study is to comprehensively evaluate various approaches on multiple mental health prediction tasks using online text data, aiming to identify methods that can significantly improve performance across these tasks. Additionally, the study seeks to explore the reasoning capabilities related to mental health and to provide guidelines for enhancing effectiveness while highlighting important ethical considerations before real-world deployment.","The research idea centers on addressing the significant gap in understanding and improving the capabilities related to mental health within current approaches. Despite advances in related technologies, there remains a need to evaluate and enhance performance specifically for mental health assessment and reasoning tasks. The study is motivated by the potential to better support mental health applications while acknowledging existing limitations and ethical concerns such as bias. The primary objective of the study is to comprehensively evaluate various approaches on multiple mental health assessment tasks using online text data, aiming to identify methods that can significantly improve performance across these tasks. Additionally, the study seeks to explore the reasoning capabilities related to mental health and to provide guidelines for enhancing effectiveness while highlighting important ethical considerations before real-world deployment.",True
Psychology,Human-AI collaboration patterns in AI-assisted academic writing,"Artificial Intelligence (AI) has increasingly influenced higher education, notably in academic writing where AI-powered assisting tools offer both opportunities and challenges. Recently, the rapid growth of generative AI (GAI) has brought its impacts into sharper focus, yet the dynamics of its utilisation in academic writing remain largely unexplored. This paper focuses on examining the nature of human-AI interactions in academic writing, specifically investigating the strategies doctoral students employ when collaborating with a GAI-powered assisting tool. This study involves 626 recorded activities on how ten doctoral students interact with GAI-powered assisting tool during academic writing. AI-driven learning analytics approach was adopted for three layered analyses: (1) data pre-processing and analysis with quantitative content analysis, (2) sequence analysis with Hidden Markov Model (HMM) and hierarchical sequence clustering, and (3) pattern analysis with process mining. Findings indicate that doctoral students engaging in iterative, highly interactive processes with the GAI-powered assisting tool generally achieve better performance in the writing task. In contrast, those who use GAI merely as a supplementary information source, maintaining a linear writing approach, tend to get lower writing performance. This study points to the need for further investigations into human-AI collaboration in learning in higher education, with implications for tailored educational strategies and solutions.",['Hidden Markov Model (HMM)'],"The study addresses the growing influence of generative AI on academic writing in higher education, highlighting both the opportunities and challenges it presents. Despite the increasing use of AI-assisted tools, the dynamics of how doctoral students engage with these tools during academic writing remain largely unexplored. The primary aim of the study is to examine the nature of interactions between doctoral students and AI-assisted tools in academic writing, specifically investigating the strategies these students employ when collaborating with such tools. The research seeks to understand how different interaction patterns relate to writing performance, with the goal of informing tailored educational strategies in higher education.","The study addresses the growing influence of emerging text generation technologies on academic writing in higher education, highlighting both the opportunities and challenges they present. Despite the increasing use of these writing assistance tools, the dynamics of how doctoral students engage with these tools during academic writing remain largely unexplored. The primary aim of the study is to examine the nature of interactions between doctoral students and these writing assistance tools in academic writing, specifically investigating the strategies these students employ when collaborating with such tools. The research seeks to understand how different interaction patterns relate to writing performance, with the goal of informing tailored educational strategies in higher education.",True
Psychology,Recent advancements and challenges of NLP-based sentiment analysis: A state-of-the-art review,"Sentiment analysis is a method within natural language processing that evaluates and identifies the emotional tone or mood conveyed in textual data. Scrutinizing words and phrases categorizes them into positive, negative, or neutral sentiments. The significance of sentiment analysis lies in its capacity to derive valuable insights from extensive textual data, empowering businesses to grasp customer sentiments, make informed choices, and enhance their offerings. For the further advancement of sentiment analysis, gaining a deep understanding of its algorithms, applications, current performance, and challenges is imperative. Therefore, in this extensive survey, we began exploring the vast array of application domains for sentiment analysis, scrutinizing them within the context of existing research. We then delved into prevalent pre-processing techniques, datasets, and evaluation metrics to enhance comprehension. We also explored Machine Learning, Deep Learning, Large Language Models and Pre-trained models in sentiment analysis, providing insights into their advantages and drawbacks. Subsequently, we precisely reviewed the experimental results and limitations of recent state-of-the-art articles. Finally, we discussed the diverse challenges encountered in sentiment analysis and proposed future research directions to mitigate these concerns. This extensive review provides a complete understanding of sentiment analysis, covering its models, application domains, results analysis, challenges, and research directions.","['Machine Learning', 'Deep Learning']","The research idea centers on the importance of understanding the emotional tone or mood conveyed in textual data to gain valuable insights into human sentiments. This understanding is crucial for interpreting positive, negative, or neutral feelings expressed in language, which can inform decision-making and improve various applications that rely on sentiment interpretation. The study recognizes the need to thoroughly examine the current state, challenges, and applications of sentiment evaluation to advance knowledge in this area. The primary objective of the study is to provide a comprehensive review of the existing research on sentiment evaluation, including its application domains, current performance, and challenges. It aims to enhance understanding by scrutinizing relevant techniques, datasets, and evaluation criteria, as well as discussing limitations and proposing future directions to address ongoing issues in the field.","The research idea centers on the importance of understanding the emotional tone or mood conveyed in textual data to gain valuable insights into human sentiments. This understanding is crucial for interpreting positive, negative, or neutral feelings expressed in language, which can inform decision-making and improve various applications that rely on sentiment interpretation. The study recognizes the need to thoroughly examine the current state, challenges, and applications of sentiment evaluation to advance knowledge in this area. The primary objective of the study is to provide a comprehensive review of the existing research on sentiment evaluation, including its application domains, current performance, and challenges. It aims to enhance understanding by scrutinizing relevant approaches, datasets, and evaluation criteria, as well as discussing limitations and proposing future directions to address ongoing issues in the field.",True
Psychology,Artificial intelligence (AI) learning tools in K-12 education: A scoping review,"Abstract Artificial intelligence (AI) literacy is a global strategic objective in education. However, little is known about how AI should be taught. In this paper, 46 studies in academic conferences and journals are reviewed to investigate pedagogical strategies, learning tools, assessment methods in AI literacy education in K-12 contexts, and students’ learning outcomes. The investigation reveals that the promotion of AI literacy education has seen significant progress in the past two decades. This highlights that intelligent agents, including Google’s Teachable Machine, Learning ML, and Machine Learning for Kids, are age-appropriate tools for AI literacy education in K-12 contexts. Kindergarten students can benefit from learning tools such as PopBots, while software devices, such as Scratch and Python, which help to develop the computational thinking of AI algorithms, can be introduced to both primary and secondary schools. The research shows that project-based, human–computer collaborative learning and play- and game-based approaches, with constructivist methodologies, have been applied frequently in AI literacy education. Cognitive, affective, and behavioral learning outcomes, course satisfaction and soft skills acquisition have been reported. The paper informs educators of appropriate learning tools, pedagogical strategies, assessment methodologies in AI literacy education, and students’ learning outcomes. Research implications and future research directions within the K-12 context are also discussed.",['Teachable Machine'],"The research idea centers on the growing importance of literacy in a specific emerging field as a global strategic objective in education, while highlighting the current lack of understanding regarding effective teaching methods for this subject in K-12 contexts. The study addresses the need to explore how educational strategies, learning tools, and assessment methods contribute to students’ learning outcomes in this area. The primary objective of the study is to investigate and review existing pedagogical strategies, learning tools, and assessment methods used in K-12 education to promote literacy in this field, as well as to examine the cognitive, affective, and behavioral learning outcomes, course satisfaction, and soft skills acquisition reported in the literature. The study aims to provide educators with informed guidance on appropriate educational approaches and to discuss implications and future directions for literacy education within the K-12 context.","The research idea centers on the growing importance of literacy in a specific emerging field as a global strategic objective in education, while highlighting the current lack of understanding regarding effective teaching methods for this subject in K-12 contexts. The study addresses the need to explore how educational strategies, learning tools, and assessment methods contribute to students' learning outcomes in this area. The primary objective of the study is to investigate and review existing pedagogical strategies, learning tools, and assessment methods used in K-12 education to promote literacy in this field, as well as to examine the cognitive, affective, and behavioral learning outcomes, course satisfaction, and soft skills acquisition reported in the literature. The study aims to provide educators with informed guidance on appropriate educational approaches and to discuss implications and future directions for literacy education within the K-12 context.",True
Psychology,AI-Driven Clinical Decision Support Systems: An Ongoing Pursuit of Potential,"Clinical Decision Support Systems (CDSS) are essential tools in contemporary healthcare, enhancing clinicians' decisions and patient outcomes. The integration of artificial intelligence (AI) is now revolutionizing CDSS even further. This review delves into AI technologies transforming CDSS, their applications in healthcare decision-making, associated challenges, and the potential trajectory toward fully realizing AI-CDSS's potential. The review begins by laying the groundwork with a definition of CDSS and its function within the healthcare field. It then highlights the increasingly significant role that AI is playing in enhancing CDSS effectiveness and efficiency, underlining its evolving prominence in shaping healthcare practices. It examines the integration of AI technologies into CDSS, including machine learning algorithms like neural networks and decision trees, natural language processing, and deep learning. It also addresses the challenges associated with AI integration, such as interpretability and bias. We then shift to AI applications within CDSS, with real-life examples of AI-driven diagnostics, personalized treatment recommendations, risk prediction, early intervention, and AI-assisted clinical documentation. The review emphasizes user-centered design in AI-CDSS integration, addressing usability, trust, workflow, and ethical and legal considerations. It acknowledges prevailing obstacles and suggests strategies for successful AI-CDSS adoption, highlighting the need for workflow alignment and interdisciplinary collaboration. The review concludes by summarizing key findings, underscoring AI's transformative potential in CDSS, and advocating for continued research and innovation. It emphasizes the need for collaborative efforts to realize a future where AI-powered CDSS optimizes healthcare delivery and improves patient outcomes.","['neural networks', 'decision trees', 'deep learning']","The study addresses the critical role of clinical decision support systems (CDSS) in improving healthcare delivery by enhancing clinicians' decisions and patient outcomes. It highlights the evolving challenges and opportunities in integrating advanced technologies into healthcare decision-making processes, emphasizing the importance of usability, trust, workflow alignment, and ethical considerations. The primary objective of the study is to review the current state and applications of these technologies within CDSS, identify associated challenges such as interpretability and bias, and propose strategies for successful adoption. The study aims to underscore the transformative potential of these systems in optimizing healthcare practices and improving patient outcomes through continued research and interdisciplinary collaboration.","The study addresses the critical role of clinical decision support systems (CDSS) in improving healthcare delivery by enhancing clinicians' decisions and patient outcomes. It highlights the evolving challenges and opportunities in integrating technologies into healthcare decision-making processes, emphasizing the importance of usability, trust, workflow alignment, and ethical considerations. The primary objective of the study is to review the current state and applications of these technologies within CDSS, identify associated challenges such as interpretability and bias, and propose strategies for successful adoption. The study aims to underscore the transformative potential of these systems in optimizing healthcare practices and improving patient outcomes through continued research and interdisciplinary collaboration.",True
Psychology,Hallucination Rates and Reference Accuracy of ChatGPT and Bard for Systematic Reviews: Comparative Analysis,"Background Large language models (LLMs) have raised both interest and concern in the academic community. They offer the potential for automating literature search and synthesis for systematic reviews but raise concerns regarding their reliability, as the tendency to generate unsupported (hallucinated) content persist. Objective The aim of the study is to assess the performance of LLMs such as ChatGPT and Bard (subsequently rebranded Gemini) to produce references in the context of scientific writing. Methods The performance of ChatGPT and Bard in replicating the results of human-conducted systematic reviews was assessed. Using systematic reviews pertaining to shoulder rotator cuff pathology, these LLMs were tested by providing the same inclusion criteria and comparing the results with original systematic review references, serving as gold standards. The study used 3 key performance metrics: recall, precision, and F1-score, alongside the hallucination rate. Papers were considered “hallucinated” if any 2 of the following information were wrong: title, first author, or year of publication. Results In total, 11 systematic reviews across 4 fields yielded 33 prompts to LLMs (3 LLMs×11 reviews), with 471 references analyzed. Precision rates for GPT-3.5, GPT-4, and Bard were 9.4% (13/139), 13.4% (16/119), and 0% (0/104) respectively (P&lt;.001). Recall rates were 11.9% (13/109) for GPT-3.5 and 13.7% (15/109) for GPT-4, with Bard failing to retrieve any relevant papers (P&lt;.001). Hallucination rates stood at 39.6% (55/139) for GPT-3.5, 28.6% (34/119) for GPT-4, and 91.4% (95/104) for Bard (P&lt;.001). Further analysis of nonhallucinated papers retrieved by GPT models revealed significant differences in identifying various criteria, such as randomized studies, participant criteria, and intervention criteria. The study also noted the geographical and open-access biases in the papers retrieved by the LLMs. Conclusions Given their current performance, it is not recommended for LLMs to be deployed as the primary or exclusive tool for conducting systematic reviews. Any references generated by such models warrant thorough validation by researchers. The high occurrence of hallucinations in LLMs highlights the necessity for refining their training and functionality before confidently using them for rigorous academic purposes.","['GPT-3.5', 'GPT-4']","The research idea centers on the growing interest and concern within the academic community regarding the reliability of automated tools in supporting scientific literature synthesis, particularly for systematic reviews. There is a motivation to explore whether these tools can effectively replicate human-conducted systematic reviews without generating unsupported or inaccurate content. The study’s primary objective is to evaluate the ability of these tools to produce accurate and reliable references in the context of scientific writing, specifically assessing their performance in replicating the results of systematic reviews related to shoulder rotator cuff pathology. The aim is to determine the extent to which these tools can retrieve relevant papers accurately and to identify the prevalence of errors or hallucinated content in their outputs.","The research idea centers on the growing interest and concern within the academic community regarding the reliability of tools in supporting scientific literature synthesis, particularly for systematic reviews. There is a motivation to explore whether these tools can effectively replicate human-conducted systematic reviews without generating unsupported or inaccurate content. The study's primary objective is to evaluate the ability of these tools to produce accurate and reliable references in the context of scientific writing, specifically assessing their performance in replicating the results of systematic reviews related to shoulder rotator cuff pathology. The aim is to determine the extent to which these tools can retrieve relevant papers accurately and to identify the prevalence of errors or fabricated content in their outputs.",True
Psychology,Detecting and Preventing Hallucinations in Large Vision Language Models,"Instruction tuned Large Vision Language Models (LVLMs) have significantly advanced in generalizing across a diverse set of multi-modal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of the hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address this, we introduce M-HalDetect, a Multimodal Hallucination Detection Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained annotations on VQA examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions and relationships that are unfaithful. To demonstrate the potential of this dataset for hallucination prevention, we optimize InstructBLIP through our novel Fine-grained Direct Preference Optimization (FDPO). We also train fine-grained multi-modal reward models from InstructBLIP and evaluate their effectiveness with best-of-n rejection sampling (RS). We perform human evaluation on both FDPO and rejection sampling, and find that they reduce hallucination rates in InstructBLIP by 41% and 55% respectively. We also find that our reward model generalizes to other multi-modal models, reducing hallucinations in LLaVA and mPLUG-OWL by 15% and 57% respectively, and has strong correlation with human evaluated accuracy scores. The dataset is available at https://github.com/hendryx-scale/mhal-detect.",['Instruction tuned Large Vision Language Models (LVLMs)'],"The research idea centers on the challenge of generating detailed and visually accurate responses in visual question answering tasks, highlighting that even advanced models frequently produce hallucinated content such as non-existent objects, unfaithful descriptions, and inaccurate relationships. This issue of hallucination undermines the reliability and fidelity of image-based descriptions, indicating a significant problem in ensuring that responses are truly grounded in the visual input. The primary objective of the study is to create a comprehensive dataset with fine-grained annotations that identify hallucinations in visual question answering, including both entity descriptions and relationships, to facilitate the detection and reduction of these inaccuracies. The study aims to use this dataset to improve the accuracy and faithfulness of detailed image descriptions by reducing hallucination rates, thereby enhancing the quality of responses in visual question answering tasks.","The research idea centers on the challenge of generating detailed and visually accurate responses in visual question answering tasks, highlighting that even current systems frequently produce hallucinated content such as non-existent objects, unfaithful descriptions, and inaccurate relationships. This issue of hallucination undermines the reliability and fidelity of image-based descriptions, indicating a significant problem in ensuring that responses are truly grounded in the visual input. The primary objective of the study is to create a comprehensive dataset with fine-grained annotations that identify hallucinations in visual question answering, including both entity descriptions and relationships, to facilitate the detection and reduction of these inaccuracies. The study aims to use this dataset to improve the accuracy and faithfulness of detailed image descriptions by reducing hallucination rates, thereby enhancing the quality of responses in visual question answering tasks.",True
Psychology,Sentiment Analysis in the Age of Generative AI,"Abstract In the rapidly advancing age of Generative AI, Large Language Models (LLMs) such as ChatGPT stand at the forefront of disrupting marketing practice and research. This paper presents a comprehensive exploration of LLMs’ proficiency in sentiment analysis, a core task in marketing research for understanding consumer emotions, opinions, and perceptions. We benchmark the performance of three state-of-the-art LLMs, i.e., GPT-3.5, GPT-4, and Llama 2, against established, high-performing transfer learning models. Despite their zero-shot nature, our research reveals that LLMs can not only compete with but in some cases also surpass traditional transfer learning methods in terms of sentiment classification accuracy. We investigate the influence of textual data characteristics and analytical procedures on classification accuracy, shedding light on how data origin, text complexity, and prompting techniques impact LLM performance. We find that linguistic features such as the presence of lengthy, content-laden words improve classification performance, while other features such as single-sentence reviews and less structured social media text documents reduce performance. Further, we explore the explainability of sentiment classifications generated by LLMs. The findings indicate that LLMs, especially Llama 2, offer remarkable classification explanations, highlighting their advanced human-like reasoning capabilities. Collectively, this paper enriches the current understanding of sentiment analysis, providing valuable insights and guidance for the selection of suitable methods by marketing researchers and practitioners in the age of Generative AI.","['GPT-3.5', 'GPT-4', 'Llama 2', 'transfer learning models']","The research idea centers on understanding how advancements in language processing technologies impact the ability to analyze consumer emotions, opinions, and perceptions through sentiment analysis, which is a fundamental task in marketing research. The study addresses the challenge of accurately interpreting diverse textual data from various sources, considering factors such as text complexity and linguistic features that influence classification performance. The primary objective of the study is to evaluate the effectiveness of different approaches in sentiment classification accuracy and to examine how characteristics of textual data affect this accuracy. Additionally, the study aims to explore the clarity and quality of explanations provided for sentiment classifications, thereby enhancing the understanding of how these methods can support marketing researchers and practitioners in selecting appropriate tools for sentiment analysis.","The research idea centers on understanding how advancements in language processing technologies impact the ability to analyze consumer emotions, opinions, and perceptions through sentiment analysis, which is a fundamental task in marketing research. The study addresses the challenge of accurately interpreting diverse textual data from various sources, considering factors such as text complexity and linguistic features that influence interpretation performance. The primary objective of the study is to evaluate the effectiveness of different approaches in sentiment categorization accuracy and to examine how characteristics of textual data affect this accuracy. Additionally, the study aims to explore the clarity and quality of explanations provided for sentiment interpretations, thereby enhancing the understanding of how these methods can support marketing researchers and practitioners in selecting appropriate tools for sentiment analysis.",True
Psychology,Investigating Spatial Effects through Machine Learning and Leveraging Explainable AI for Child Malnutrition in Pakistan,"While socioeconomic gradients in regional health inequalities are firmly established, the synergistic interactions between socioeconomic deprivation and climate vulnerability within convenient proximity and neighbourhood locations with health disparities remain poorly explored and thus require deep understanding within a regional context. Furthermore, disregarding the importance of spatial spillover effects and nonlinear effects of covariates on childhood stunting are inevitable in dealing with an enduring issue of regional health inequalities. The present study aims to investigate the spatial inequalities in childhood stunting at the district level in Pakistan and validate the importance of spatial lag in predicting childhood stunting. Furthermore, it examines the presence of any nonlinear relationships among the selected independent features with childhood stunting. The study utilized data related to socioeconomic features from MICS 2017–2018 and climatic data from Integrated Contextual Analysis. A multi-model approach was employed to address the research questions, which included Ordinary Least Squares Regression (OLS), various Spatial Models, Machine Learning Algorithms and Explainable Artificial Intelligence methods. Firstly, OLS was used to analyse and test the linear relationships among selected variables. Secondly, Spatial Durbin Error Model (SDEM) was used to detect and capture the impact of spatial spillover on childhood stunting. Third, XGBoost and Random Forest machine learning algorithms were employed to examine and validate the importance of the spatial lag component. Finally, EXAI methods such as SHapley were utilized to identify potential nonlinear relationships. The study found a clear pattern of spatial clustering and geographical disparities in childhood stunting, with multidimensional poverty, high climate vulnerability and early marriage worsening childhood stunting. In contrast, low climate vulnerability, high exposure to mass media and high women’s literacy were found to reduce childhood stunting. The use of machine learning algorithms, specifically XGBoost and Random Forest, highlighted the significant role played by the average value in the neighbourhood in predicting childhood stunting in nearby districts, confirming that the spatial spillover effect is not bounded by geographical boundaries. Furthermore, EXAI methods such as partial dependency plot reveal the existence of a nonlinear relationship between multidimensional poverty and childhood stunting. The study’s findings provide valuable insights into the spatial distribution of childhood stunting in Pakistan, emphasizing the importance of considering spatial effects in predicting childhood stunting. Individual and household-level factors such as exposure to mass media and women’s literacy have shown positive implications for childhood stunting. It further provides a justification for the usage of EXAI methods to draw better insights and propose customised intervention policies accordingly.","['XGBoost', 'Random Forest', 'partial dependency plot']","The research idea centers on addressing the insufficient understanding of how socioeconomic deprivation and climate vulnerability interact within local neighborhoods to influence regional health disparities, specifically childhood stunting. Despite established socioeconomic gradients in health inequalities, the combined effects of these factors and the role of spatial spillover and nonlinear relationships remain underexplored in a regional context. The study aims to investigate spatial inequalities in childhood stunting at the district level in Pakistan, emphasizing the importance of spatial influences and complex relationships among contributing factors. The primary objective of the study is to examine the spatial distribution and clustering of childhood stunting, validate the significance of spatial spillover effects, and explore nonlinear associations between socioeconomic and climatic variables with childhood stunting. It seeks to provide insights into how multidimensional poverty, climate vulnerability, early marriage, exposure to mass media, and women’s literacy relate to childhood stunting, thereby informing targeted intervention policies.","The research idea centers on addressing the insufficient understanding of how socioeconomic deprivation and climate vulnerability interact within local neighborhoods to influence regional health disparities, specifically childhood stunting. Despite established socioeconomic gradients in health inequalities, the combined effects of these factors and the role of spatial spillover and nonlinear relationships remain underexplored in a regional context. The study aims to investigate spatial inequalities in childhood stunting at the district level in Pakistan, emphasizing the importance of spatial influences and complex relationships among contributing factors. The primary objective of the study is to examine the spatial distribution and clustering of childhood stunting, validate the significance of spatial spillover effects, and explore nonlinear associations between socioeconomic and climatic variables with childhood stunting. It seeks to provide insights into how multidimensional poverty, climate vulnerability, early marriage, exposure to mass media, and women's literacy relate to childhood stunting, thereby informing targeted intervention policies.",True
Psychology,Multiple Classification of Brain MRI Autism Spectrum Disorder by Age and Gender Using Deep Learning,"Abstract The fact that the rapid and definitive diagnosis of autism cannot be made today and that autism cannot be treated provides an impetus to look into novel technological solutions. To contribute to the resolution of this problem through multiple classifications by considering age and gender factors, in this study, two quadruple and one octal classifications were performed using a deep learning (DL) approach. Gender in one of the four classifications and age groups in the other were considered. In the octal classification, classes were created considering gender and age groups. In addition to the diagnosis of ASD (Autism Spectrum Disorders), another goal of this study is to find out the contribution of gender and age factors to the diagnosis of ASD by making multiple classifications based on age and gender for the first time. Brain structural MRI (sMRI) scans of participators with ASD and TD (Typical Development) were pre-processed in the system originally designed for this purpose. Using the Canny Edge Detection (CED) algorithm, the sMRI image data was cropped in the data pre-processing stage, and the data set was enlarged five times with the data augmentation (DA) techniques. The most optimal convolutional neural network (CNN) models were developed using the grid search optimization (GSO) algorism. The proposed DL prediction system was tested with the five-fold cross-validation technique. Three CNN models were designed to be used in the system. The first of these models is the quadruple classification model created by taking gender into account (model 1), the second is the quadruple classification model created by taking into account age (model 2), and the third is the eightfold classification model created by taking into account both gender and age (model 3). ). The accuracy rates obtained for all three designed models are 80.94, 85.42 and 67.94, respectively. These obtained accuracy rates were compared with pre-trained models by using the transfer learning approach. As a result, it was revealed that age and gender factors were effective in the diagnosis of ASD with the system developed for ASD multiple classifications, and higher accuracy rates were achieved compared to pre-trained models.","['convolutional neural network (CNN) models', 'transfer learning approach']","The research idea centers on the challenge that autism cannot currently be diagnosed rapidly and definitively, nor can it be treated effectively, which motivates the exploration of new approaches to improve diagnosis. The study addresses the need to understand how age and gender factors contribute to the diagnosis of Autism Spectrum Disorders (ASD), recognizing that these factors may influence diagnostic accuracy. The primary objective of the study is to investigate the contribution of gender and age to the diagnosis of ASD by performing multiple classifications based on these factors for the first time. Additionally, the study aims to enhance the accuracy of ASD diagnosis by considering age and gender differences in the classification process.","The research idea centers on the challenge that autism cannot currently be diagnosed rapidly and definitively, nor can it be treated effectively, which motivates the exploration of new approaches to improve diagnosis. The study addresses the need to understand how age and gender factors contribute to the diagnosis of Autism Spectrum Disorders (ASD), recognizing that these factors may influence diagnostic accuracy. The primary objective of the study is to investigate the contribution of gender and age to the diagnosis of ASD by examining these factors in relation to diagnostic outcomes for the first time. Additionally, the study aims to enhance the accuracy of ASD diagnosis by considering age and gender differences in the diagnostic assessment process.",True
Psychology,Assessing ChatGPT’s Mastery of Bloom’s Taxonomy Using Psychosomatic Medicine Exam Questions: Mixed-Methods Study,"Background Large language models such as GPT-4 (Generative Pre-trained Transformer 4) are being increasingly used in medicine and medical education. However, these models are prone to “hallucinations” (ie, outputs that seem convincing while being factually incorrect). It is currently unknown how these errors by large language models relate to the different cognitive levels defined in Bloom’s taxonomy. Objective This study aims to explore how GPT-4 performs in terms of Bloom’s taxonomy using psychosomatic medicine exam questions. Methods We used a large data set of psychosomatic medicine multiple-choice questions (N=307) with real-world results derived from medical school exams. GPT-4 answered the multiple-choice questions using 2 distinct prompt versions: detailed and short. The answers were analyzed using a quantitative approach and a qualitative approach. Focusing on incorrectly answered questions, we categorized reasoning errors according to the hierarchical framework of Bloom’s taxonomy. Results GPT-4’s performance in answering exam questions yielded a high success rate: 93% (284/307) for the detailed prompt and 91% (278/307) for the short prompt. Questions answered correctly by GPT-4 had a statistically significant higher difficulty than questions answered incorrectly (P=.002 for the detailed prompt and P&lt;.001 for the short prompt). Independent of the prompt, GPT-4’s lowest exam performance was 78.9% (15/19), thereby always surpassing the “pass” threshold. Our qualitative analysis of incorrect answers, based on Bloom’s taxonomy, showed that errors were primarily in the “remember” (29/68) and “understand” (23/68) cognitive levels; specific issues arose in recalling details, understanding conceptual relationships, and adhering to standardized guidelines. Conclusions GPT-4 demonstrated a remarkable success rate when confronted with psychosomatic medicine multiple-choice exam questions, aligning with previous findings. When evaluated through Bloom’s taxonomy, our data revealed that GPT-4 occasionally ignored specific facts (remember), provided illogical reasoning (understand), or failed to apply concepts to a new situation (apply). These errors, which were confidently presented, could be attributed to inherent model biases and the tendency to generate outputs that maximize likelihood.",['GPT-4 (Generative Pre-trained Transformer 4)'],"The research idea centers on understanding the nature of errors made by a language-based system when responding to psychosomatic medicine exam questions, particularly how these errors correspond to different cognitive levels defined in Bloom’s taxonomy. There is a need to investigate the relationship between the accuracy of responses and the cognitive demands of exam questions, as well as to identify specific types of reasoning mistakes in the context of medical education. The primary objective of the study is to explore the performance of the system in answering psychosomatic medicine multiple-choice questions according to Bloom’s taxonomy, with a focus on categorizing the reasoning errors associated with incorrectly answered questions. This aims to shed light on how cognitive processes such as remembering, understanding, and applying knowledge are reflected in the errors made during exam question responses.","The research idea centers on understanding the nature of errors made by a knowledge-based system when responding to psychosomatic medicine exam questions, particularly how these errors correspond to different cognitive levels defined in Bloom's taxonomy. There is a need to investigate the relationship between the accuracy of responses and the cognitive demands of exam questions, as well as to identify specific types of reasoning mistakes in the context of medical education. The primary objective of the study is to explore the performance of the system in answering psychosomatic medicine multiple-choice questions according to Bloom's taxonomy, with a focus on categorizing the reasoning errors associated with incorrectly answered questions. This aims to shed light on how cognitive processes such as remembering, understanding, and applying knowledge are reflected in the errors made during exam question responses.",True
Psychology,An Explainable AI Paradigm for Alzheimer’s Diagnosis Using Deep Transfer Learning,"Alzheimer’s disease (AD) is a progressive neurodegenerative disorder that affects millions of individuals worldwide, causing severe cognitive decline and memory impairment. The early and accurate diagnosis of AD is crucial for effective intervention and disease management. In recent years, deep learning techniques have shown promising results in medical image analysis, including AD diagnosis from neuroimaging data. However, the lack of interpretability in deep learning models hinders their adoption in clinical settings, where explainability is essential for gaining trust and acceptance from healthcare professionals. In this study, we propose an explainable AI (XAI)-based approach for the diagnosis of Alzheimer’s disease, leveraging the power of deep transfer learning and ensemble modeling. The proposed framework aims to enhance the interpretability of deep learning models by incorporating XAI techniques, allowing clinicians to understand the decision-making process and providing valuable insights into disease diagnosis. By leveraging popular pre-trained convolutional neural networks (CNNs) such as VGG16, VGG19, DenseNet169, and DenseNet201, we conducted extensive experiments to evaluate their individual performances on a comprehensive dataset. The proposed ensembles, Ensemble-1 (VGG16 and VGG19) and Ensemble-2 (DenseNet169 and DenseNet201), demonstrated superior accuracy, precision, recall, and F1 scores compared to individual models, reaching up to 95%. In order to enhance interpretability and transparency in Alzheimer’s diagnosis, we introduced a novel model achieving an impressive accuracy of 96%. This model incorporates explainable AI techniques, including saliency maps and grad-CAM (gradient-weighted class activation mapping). The integration of these techniques not only contributes to the model’s exceptional accuracy but also provides clinicians and researchers with visual insights into the neural regions influencing the diagnosis. Our findings showcase the potential of combining deep transfer learning with explainable AI in the realm of Alzheimer’s disease diagnosis, paving the way for more interpretable and clinically relevant AI models in healthcare.","['deep learning', 'explainable AI (XAI)', 'deep transfer learning', 'ensemble modeling', 'pre-trained convolutional neural networks (CNNs)', 'VGG16', 'VGG19', 'DenseNet169', 'DenseNet201', 'saliency maps', 'grad-CAM (gradient-weighted class activation mapping)']","The research idea centers on addressing the challenge of early and accurate diagnosis of Alzheimer’s disease, a progressive neurodegenerative disorder that leads to severe cognitive decline and memory impairment. Despite advances in diagnostic approaches, there remains a critical need for methods that are not only accurate but also interpretable and transparent to gain trust and acceptance from healthcare professionals. The study highlights the importance of enhancing the explainability of diagnostic tools to support effective intervention and disease management in clinical settings. The primary objective of the study is to develop an approach that improves the interpretability of Alzheimer’s disease diagnosis, enabling clinicians to understand the decision-making process behind diagnostic outcomes. This objective aims to provide valuable insights into the neural regions influencing the diagnosis, thereby facilitating more transparent and clinically relevant assessments of the disease.","The research idea centers on addressing the challenge of early and accurate diagnosis of Alzheimer's disease, a progressive neurodegenerative disorder that leads to severe cognitive decline and memory impairment. Despite advances in diagnostic approaches, there remains a critical need for methods that are not only accurate but also interpretable and transparent to gain trust and acceptance from healthcare professionals. The study highlights the importance of enhancing the explainability of diagnostic tools to support effective intervention and disease management in clinical settings. The primary objective of the study is to develop an approach that improves the interpretability of Alzheimer's disease diagnosis, enabling clinicians to understand the reasoning process behind diagnostic outcomes. This objective aims to provide valuable insights into the neural regions influencing the diagnosis, thereby facilitating more transparent and clinically relevant assessments of the disease.",True
Psychology,REVIEWING THE TRANSFORMATIONAL IMPACT OF EDGE COMPUTING ON REAL-TIME DATA PROCESSING AND ANALYTICS,"Edge computing has emerged as a pivotal paradigm shift in the realm of data processing and analytics, revolutionizing the way organizations handle real-time data. This review presents a comprehensive review of the transformational impact of edge computing on real-time data processing and analytics. Firstly, the review delves into the fundamental concepts of edge computing, elucidating its architectural framework and highlighting its distinct advantages over traditional cloud-centric approaches. By distributing computational resources closer to data sources, edge computing mitigates latency issues and enhances responsiveness, thereby enabling real-time data processing at the edge. Furthermore, this review explores how edge computing facilitates the seamless integration of analytics capabilities into edge devices, empowering organizations to derive actionable insights at the source of data generation. Leveraging advanced analytics algorithms, such as machine learning and artificial intelligence, edge computing enables autonomous decision-making and predictive analytics in real time, fostering innovation across diverse industry verticals. Moreover, the review examines the transformative implications of edge computing on various sectors, including healthcare, manufacturing, transportation, and smart cities. By enabling localized data processing and analytics, edge computing enhances operational efficiency, ensures data privacy and security, and unlocks new opportunities for business optimization and value creation. This review underscores the profound impact of edge computing on real-time data processing and analytics, revolutionizing the way organizations harness data to drive informed decision-making and gain competitive advantage in today's dynamic business landscape. As edge computing continues to evolve, its transformative potential is poised to redefine the future of data-driven innovation and digital transformation.&#x0D; Keywords: Edge, Computing, Analytics, Data, Impact, Review.",['machine learning'],"The research idea centers on understanding the significant changes brought about by a new approach to processing information closer to the source of data generation, which addresses challenges related to delays and responsiveness in real-time data handling. This shift has important implications for various fields, including healthcare, manufacturing, transportation, and urban development, by improving efficiency, privacy, and opportunities for optimization. The primary objective of the study is to comprehensively review the impact of this approach on real-time information processing and its ability to integrate analytical capabilities directly into the environment where data is produced. The study aims to highlight how these advancements enable more immediate and informed decision-making, thereby transforming operational practices across multiple sectors.","The research idea centers on understanding the significant changes brought about by a new approach to processing information closer to the source of data generation, which addresses challenges related to delays and responsiveness in real-time data handling. This shift has important implications for various fields, including healthcare, manufacturing, transportation, and urban development, by improving efficiency, privacy, and opportunities for optimization. The primary objective of the study is to comprehensively review the impact of this approach on real-time information processing and its ability to integrate analysis capabilities directly into the environment where data is produced. The study aims to highlight how these advancements enable more immediate and informed decision-making, thereby transforming operational practices across multiple sectors.",True
Psychology,"Human-in-the-Loop Reinforcement Learning: A Survey and Position on Requirements, Challenges, and Opportunities","Artificial intelligence (AI) and especially reinforcement learning (RL) have the potential to enable agents to learn and perform tasks autonomously with superhuman performance. However, we consider RL as fundamentally a Human-in-the-Loop (HITL) paradigm, even when an agent eventually performs its task autonomously. In cases where the reward function is challenging or impossible to define, HITL approaches are considered particularly advantageous. The application of Reinforcement Learning from Human Feedback (RLHF) in systems such as ChatGPT demonstrates the effectiveness of optimizing for user experience and integrating their feedback into the training loop. In HITL RL, human input is integrated during the agent’s learning process, allowing iterative updates and fine-tuning based on human feedback, thus enhancing the agent’s performance. Since the human is an essential part of this process, we argue that human-centric approaches are the key to successful RL, a fact that has not been adequately considered in the existing literature. This paper aims to inform readers about current explainability methods in HITL RL. It also shows how the application of explainable AI (xAI) and specific improvements to existing explainability approaches can enable a better human-agent interaction in HITL RL for all types of users, whether for lay people, domain experts, or machine learning specialists. Accounting for the workflow in HITL RL and based on software and machine learning methodologies, this article identifies four phases for human involvement for creating HITL RL systems: (1) Agent Development, (2) Agent Learning, (3) Agent Evaluation, and (4) Agent Deployment. We highlight human involvement, explanation requirements, new challenges, and goals for each phase. We furthermore identify low-risk, high-return opportunities for explainability research in HITL RL and present long-term research goals to advance the field. Finally, we propose a vision of human-robot collaboration that allows both parties to reach their full potential and cooperate effectively.","['reinforcement learning (RL)', 'Reinforcement Learning from Human Feedback (RLHF)', 'explainable AI (xAI)']","The research idea centers on the importance of human involvement in learning processes where autonomous agents perform tasks, emphasizing that human-centric approaches are crucial for success but have been insufficiently addressed in existing studies. It highlights the challenge of defining reward functions and the advantage of integrating human feedback to improve task performance, underscoring the need for better understanding and support of human-agent interaction. The primary objective of the study is to inform readers about current methods that enhance explainability in human-involved learning paradigms and to demonstrate how improvements in these approaches can facilitate more effective interaction between humans and agents across different user groups. Additionally, the study aims to identify key phases of human involvement, outline explanation requirements and challenges, and propose long-term goals to advance collaboration between humans and autonomous systems.","The research idea centers on the importance of human involvement in processes where autonomous agents perform tasks, emphasizing that human-centric approaches are crucial for success but have been insufficiently addressed in existing studies. It highlights the challenge of defining reward functions and the advantage of integrating human feedback to improve task performance, underscoring the need for better understanding and support of human-agent interaction. The primary objective of the study is to inform readers about current methods that enhance explainability in human-involved paradigms and to demonstrate how improvements in these approaches can facilitate more effective interaction between humans and agents across different user groups. Additionally, the study aims to identify key phases of human involvement, outline explanation requirements and challenges, and propose long-term goals to advance collaboration between humans and autonomous systems.",True
Psychology,AlphaFold predictions of fold-switched conformations are driven by structure memorization,"Abstract Recent work suggests that AlphaFold (AF)–a deep learning-based model that can accurately infer protein structure from sequence–may discern important features of folded protein energy landscapes, defined by the diversity and frequency of different conformations in the folded state. Here, we test the limits of its predictive power on fold-switching proteins, which assume two structures with regions of distinct secondary and/or tertiary structure. We find that (1) AF is a weak predictor of fold switching and (2) some of its successes result from memorization of training-set structures rather than learned protein energetics. Combining &gt;280,000 models from several implementations of AF2 and AF3, a 35% success rate was achieved for fold switchers likely in AF’s training sets. AF2’s confidence metrics selected against models consistent with experimentally determined fold-switching structures and failed to discriminate between low and high energy conformations. Further, AF captured only one out of seven experimentally confirmed fold switchers outside of its training sets despite extensive sampling of an additional ~280,000 models. Several observations indicate that AF2 has memorized structural information during training, and AF3 misassigns coevolutionary restraints. These limitations constrain the scope of successful predictions, highlighting the need for physically based methods that readily predict multiple protein conformations.","['AlphaFold (AF)', 'AF2']","The research idea centers on understanding the challenges in accurately predicting the structural behavior of fold-switching proteins, which can adopt multiple distinct conformations. This study addresses the problem of discerning important features of protein energy landscapes, particularly the diversity and frequency of different folded states, and the limitations in current predictive approaches for these complex proteins. The research objective is to evaluate the effectiveness of existing predictive methods in identifying fold-switching proteins and to assess their ability to distinguish between different conformations based on experimentally determined structures. The study aims to highlight the constraints of current approaches and emphasize the need for methods that can reliably predict multiple protein conformations.","The research idea centers on understanding the challenges in accurately characterizing the structural behavior of fold-switching proteins, which can adopt multiple distinct conformations. This study addresses the problem of discerning important features of protein energy landscapes, particularly the diversity and frequency of different folded states, and the limitations in current analytical approaches for these complex proteins. The research objective is to evaluate the effectiveness of existing structural analysis methods in identifying fold-switching proteins and to assess their ability to distinguish between different conformations based on experimentally determined structures. The study aims to highlight the constraints of current approaches and emphasize the need for methods that can reliably identify and characterize multiple protein conformations.",True
Psychology,Larger and more instructable language models become less reliable,"Abstract The prevailing methods to make large language models more powerful and amenable have been based on continuous scaling up (that is, increasing their size, data volume and computational resources 1 ) and bespoke shaping up (including post-filtering 2,3 , fine tuning or use of human feedback 4,5 ). However, larger and more instructable large language models may have become less reliable. By studying the relationship between difficulty concordance, task avoidance and prompting stability of several language model families, here we show that easy instances for human participants are also easy for the models, but scaled-up, shaped-up models do not secure areas of low difficulty in which either the model does not err or human supervision can spot the errors. We also find that early models often avoid user questions but scaled-up, shaped-up models tend to give an apparently sensible yet wrong answer much more often, including errors on difficult questions that human supervisors frequently overlook. Moreover, we observe that stability to different natural phrasings of the same question is improved by scaling-up and shaping-up interventions, but pockets of variability persist across difficulty levels. These findings highlight the need for a fundamental shift in the design and development of general-purpose artificial intelligence, particularly in high-stakes areas for which a predictable distribution of errors is paramount.","['post-filtering', 'fine tuning', 'use of human feedback']","The research idea centers on the challenge that increasing the size and refinement of language models, while intended to enhance their performance, may actually reduce their reliability, especially in areas where errors are difficult to detect by human supervisors. The study addresses the problem that although easier tasks for humans are also easier for these models, more advanced models tend to produce plausible but incorrect answers more frequently, including on difficult questions, which raises concerns about error predictability and oversight. The primary objective of the study is to investigate the relationship between task difficulty, task avoidance, and response consistency across different language model versions, with a focus on understanding how scaling and refinement impact the models’ error patterns and stability in responding to varied question phrasings. The aim is to highlight the limitations of current approaches and emphasize the need for a fundamental change in the development of reliable and predictable systems, particularly in contexts where error management is critical.","The research idea centers on the challenge that increasing the size and refinement of information processing systems, while intended to enhance their performance, may actually reduce their reliability, especially in areas where errors are difficult to detect by human supervisors. The study addresses the problem that although easier tasks for humans are also easier for these systems, more advanced systems tend to produce plausible but incorrect answers more frequently, including on difficult questions, which raises concerns about error predictability and oversight. The primary objective of the study is to investigate the relationship between task difficulty, task avoidance, and response consistency across different system versions, with a focus on understanding how scaling and refinement impact the systems' error patterns and stability in responding to varied question phrasings. The aim is to highlight the limitations of current approaches and emphasize the need for a fundamental change in the development of reliable and predictable systems, particularly in contexts where error management is critical.",True
Psychology,Semantic and Instance Segmentation in Coastal Urban Spatial Perception: A Multi-Task Learning Framework with an Attention Mechanism,"With the continuous acceleration of urbanization, urban planning and design require more in-depth research and development. Street view images can express rich urban features and guide residents’ emotions toward a city, thereby providing the most intuitive reflection of their perception of the city’s spatial quality. However, current researchers mainly conduct research on urban spatial quality through subjective experiential judgment, which includes problems such as a high cost and a low judgment accuracy. In response to these problems, this study proposes a multi-task learning urban spatial attribute perception model that integrates an attention mechanism. Via this model, the existing attributes of urban street scenes are analyzed. Then, the model is improved by introducing semantic segmentation and instance segmentation to identify and match the qualities of the urban space. The experimental results show that the multi-task learning urban spatial attribute perception model with an integrated attention mechanism has prediction accuracies of 79.54%, 78.62%, 79.68%, 77.42%, 78.45%, and 76.98% for the urban spatial attributes of beauty, boredom, depression, liveliness, safety, and richness, respectively. The accuracy of the multi-task learning urban spatial scene feature image segmentation model with an integrated attention mechanism is 95.4, 94.8, 96.2, 92.1, and 96.7 for roads, walls, sky, vehicles, and buildings, respectively. The multi-task learning urban spatial scene feature image segmentation model with an integrated attention mechanism has a higher recognition accuracy for urban spatial buildings than other models. These research results indicate the model’s effectiveness in matching urban spatial quality with public perception.","['multi-task learning', 'attention mechanism', 'semantic segmentation', 'instance segmentation']","The research idea centers on the need for more accurate and cost-effective methods to assess urban spatial quality, as current approaches relying on subjective experiential judgment face challenges such as high costs and low accuracy. Urban street scenes convey rich features that influence residents' emotional responses and perceptions of a city's spatial quality, highlighting the importance of understanding these perceptions in urban planning and design. The primary objective of the study is to analyze and improve the identification and matching of urban spatial attributes related to residents' perceptions, such as beauty, boredom, depression, liveliness, safety, and richness, by enhancing the recognition of urban street scene qualities. This aims to provide a more effective way to reflect public perception of urban spatial quality to support urban planning efforts.","The research idea centers on the need for more accurate and cost-effective methods to assess urban spatial quality, as current approaches relying on subjective experiential judgment face challenges such as high costs and low accuracy. Urban street scenes convey rich features that influence residents' emotional responses and perceptions of a city's spatial quality, highlighting the importance of understanding these perceptions in urban planning and design. The primary objective of the study is to analyze and improve the identification and matching of urban spatial attributes related to residents' perceptions, such as beauty, boredom, depression, liveliness, safety, and richness, by enhancing the evaluation of urban street scene qualities. This aims to provide a more effective way to reflect public perception of urban spatial quality to support urban planning efforts.",True
Psychology,Evaluating the ChatGPT family of models for biomedical reasoning and classification,"Abstract Objective Large language models (LLMs) have shown impressive ability in biomedical question-answering, but have not been adequately investigated for more specific biomedical applications. This study investigates ChatGPT family of models (GPT-3.5, GPT-4) in biomedical tasks beyond question-answering. Materials and Methods We evaluated model performance with 11 122 samples for two fundamental tasks in the biomedical domain—classification (n = 8676) and reasoning (n = 2446). The first task involves classifying health advice in scientific literature, while the second task is detecting causal relations in biomedical literature. We used 20% of the dataset for prompt development, including zero- and few-shot settings with and without chain-of-thought (CoT). We then evaluated the best prompts from each setting on the remaining dataset, comparing them to models using simple features (BoW with logistic regression) and fine-tuned BioBERT models. Results Fine-tuning BioBERT produced the best classification (F1: 0.800-0.902) and reasoning (F1: 0.851) results. Among LLM approaches, few-shot CoT achieved the best classification (F1: 0.671-0.770) and reasoning (F1: 0.682) results, comparable to the BoW model (F1: 0.602-0.753 and 0.675 for classification and reasoning, respectively). It took 78 h to obtain the best LLM results, compared to 0.078 and 0.008 h for the top-performing BioBERT and BoW models, respectively. Discussion The simple BoW model performed similarly to the most complex LLM prompting. Prompt engineering required significant investment. Conclusion Despite the excitement around viral ChatGPT, fine-tuning for two fundamental biomedical natural language processing tasks remained the best strategy.","['ChatGPT family of models (GPT-3.5, GPT-4)', 'zero-shot prompting', 'few-shot prompting', 'chain-of-thought (CoT) prompting', 'Bag of Words (BoW) with logistic regression', 'fine-tuned BioBERT']","The research idea centers on addressing the need to evaluate the effectiveness of advanced language models in specific biomedical tasks beyond general question-answering, particularly focusing on classification of health advice and detection of causal relations in biomedical literature. This study is motivated by the gap in understanding how these models perform in fundamental biomedical applications that require nuanced interpretation and reasoning. The primary objective of the study is to investigate the performance of different approaches in classifying health advice and detecting causal relationships within biomedical texts, aiming to determine which strategies yield the most accurate and efficient results for these critical tasks. The study seeks to compare various methods to identify the best approach for improving biomedical text understanding and reasoning.","The research idea centers on addressing the need to evaluate the effectiveness of advanced language interpretation methods in specific biomedical tasks beyond general question-answering, particularly focusing on classification of health advice and detection of causal relations in biomedical literature. This study is motivated by the gap in understanding how these interpretation approaches perform in fundamental biomedical applications that require nuanced interpretation and reasoning. The primary objective of the study is to investigate the performance of different methodologies in classifying health advice and detecting causal relationships within biomedical texts, aiming to determine which strategies yield the most accurate and efficient results for these critical tasks. The study seeks to compare various analytical methods to identify the best approach for improving biomedical text understanding and reasoning.",True
Psychology,Machine Learning and Digital Biomarkers Can Detect Early Stages of Neurodegenerative Diseases,"Neurodegenerative diseases (NDs) such as Alzheimer’s Disease (AD) and Parkinson’s Disease (PD) are devastating conditions that can develop without noticeable symptoms, causing irreversible damage to neurons before any signs become clinically evident. NDs are a major cause of disability and mortality worldwide. Currently, there are no cures or treatments to halt their progression. Therefore, the development of early detection methods is urgently needed to delay neuronal loss as soon as possible. Despite advancements in Medtech, the early diagnosis of NDs remains a challenge at the intersection of medical, IT, and regulatory fields. Thus, this review explores “digital biomarkers” (tools designed for remote neurocognitive data collection and AI analysis) as a potential solution. The review summarizes that recent studies combining AI with digital biomarkers suggest the possibility of identifying pre-symptomatic indicators of NDs. For instance, research utilizing convolutional neural networks for eye tracking has achieved significant diagnostic accuracies. ROC-AUC scores reached up to 0.88, indicating high model performance in differentiating between PD patients and healthy controls. Similarly, advancements in facial expression analysis through tools have demonstrated significant potential in detecting emotional changes in ND patients, with some models reaching an accuracy of 0.89 and a precision of 0.85. This review follows a structured approach to article selection, starting with a comprehensive database search and culminating in a rigorous quality assessment and meaning for NDs of the different methods. The process is visualized in 10 tables with 54 parameters describing different approaches and their consequences for understanding various mechanisms in ND changes. However, these methods also face challenges related to data accuracy and privacy concerns. To address these issues, this review proposes strategies that emphasize the need for rigorous validation and rapid integration into clinical practice. Such integration could transform ND diagnostics, making early detection tools more cost-effective and globally accessible. In conclusion, this review underscores the urgent need to incorporate validated digital health tools into mainstream medical practice. This integration could indicate a new era in the early diagnosis of neurodegenerative diseases, potentially altering the trajectory of these conditions for millions worldwide. Thus, by highlighting specific and statistically significant findings, this review demonstrates the current progress in this field and the potential impact of these advancements on the global management of NDs.",['convolutional neural networks'],"The research idea centers on the urgent need for early detection methods for neurodegenerative diseases such as Alzheimer’s Disease and Parkinson’s Disease, which often develop without noticeable symptoms and cause irreversible neuronal damage before clinical signs appear. These diseases are a major cause of disability and mortality worldwide, and currently, no cures or treatments exist to halt their progression. Early diagnosis is critical to delay neuronal loss and improve patient outcomes, yet it remains a significant challenge in the medical field. The study addresses the potential of emerging approaches to identify pre-symptomatic indicators that could transform the management of these conditions.

The primary objective of the study is to explore and evaluate the potential of novel tools designed for remote neurocognitive data collection as early detection methods for neurodegenerative diseases. The review aims to summarize recent findings that suggest these tools can identify early, pre-symptomatic changes in patients, thereby facilitating earlier diagnosis. Additionally, the study seeks to highlight the challenges and propose strategies for the validation and integration of these tools into clinical practice to make early detection more accessible and effective worldwide.","The research idea centers on the urgent need for early detection methods for neurodegenerative diseases such as Alzheimer's Disease and Parkinson's Disease, which often develop without noticeable symptoms and cause irreversible neuronal damage before clinical signs appear. These diseases are a major cause of disability and mortality worldwide, and currently, no cures or treatments exist to halt their progression. Early diagnosis is critical to delay neuronal loss and improve patient outcomes, yet it remains a significant challenge in the medical field. The study addresses the potential of emerging approaches to identify pre-symptomatic indicators that could transform the management of these conditions.

The primary objective of the study is to explore and evaluate the potential of novel tools designed for remote neurocognitive data collection as early detection methods for neurodegenerative diseases. The review aims to summarize recent findings that suggest these tools can identify early, pre-symptomatic changes in patients, thereby facilitating earlier diagnosis. Additionally, the study seeks to highlight the challenges and propose strategies for the validation and integration of these tools into clinical practice to make early detection more accessible and effective worldwide.",True
Psychology,Evaluating LLM-generated Worked Examples in an Introductory Programming Course,"Worked examples, which illustrate the process for solving a problem step-by-step, are a well-established pedagogical technique that has been widely studied in computing classrooms. However, creating high-quality worked examples is very time-intensive for educators, and thus learners tend not to have access to a broad range of such examples. The recent emergence of powerful large language models (LLMs), which appear capable of generating high-quality human-like content, may offer a solution. Separate strands of recent work have shown that LLMs can accurately generate code suitable for a novice audience, and that they can generate high-quality explanations of code. Therefore, LLMs may be well suited to creating a broad range of worked examples, overcoming the bottleneck of manual effort that is currently required. In this work, we present a novel tool, 'WorkedGen', which uses an LLM to generate interactive worked examples. We evaluate this tool with both an expert assessment of the content, and a user study involving students in a first-year Python programming course (n = ~400). We find that prompt chaining and one-shot learning are useful strategies for optimising the output of an LLM when producing worked examples. Our expert analysis suggests that LLMs generate clear explanations, and our classroom deployment revealed that students find the LLM-generated worked examples useful for their learning. We propose several avenues for future work, including investigating WorkedGen's value in a range of programming languages, and with more complex questions suitable for more advanced courses.","['prompt chaining', 'one-shot learning']","The research idea addresses the challenge that creating high-quality worked examples, which are important for illustrating problem-solving processes step-by-step, is very time-intensive for educators, limiting learners' access to a broad range of such examples. This bottleneck in manual effort restricts the availability of effective instructional materials that support student learning. The study’s primary objective is to explore the potential of a novel approach to generate interactive worked examples that can provide clear explanations and support novice learners effectively. It aims to evaluate the quality and usefulness of these generated worked examples through expert assessment and student feedback in an educational setting.","The research idea addresses the challenge that creating high-quality worked examples, which are important for illustrating problem-solving processes step-by-step, is very time-intensive for educators, limiting learners' access to a broad range of such examples. This bottleneck in manual effort restricts the availability of effective instructional materials that support student learning. The study's primary objective is to explore the potential of a novel approach to generate interactive worked examples that can provide clear explanations and support novice learners effectively. It aims to evaluate the quality and usefulness of these generated worked examples through expert assessment and student feedback in an educational setting.",True
Psychology,Firefighter Skill Advancement through IoT-Enabled Virtual Reality and CNN-Based Training,"To maintain the safety and efficacy of firefighters in various circumstances, modern firefighting necessitates constantly improving skills and training techniques. Utilizing the Internet of Things (IoT), virtual reality (VR), and convolutional neural networks (CNN), this paper details a novel method for training firefighters. The proposed system collects real-time data on ambient variables, equipment state, and firefighter biometrics via integrating IoT sensors into firefighting equipment and training settings. Using this information, it can develop lifelike VR training simulations of difficult and potentially dangerous scenarios. To make the training settings more realistic and malleable, CNN-based algorithms are used to assess the data. The capacity to simulate a wide variety of firefighting situations, customize training difficulty depending on individual and team performance, and provide instant feedback and performance metrics to trainees are all major benefits of this method. The method also allows teachers to check in and evaluate their learners remotely, improving instruction quality. An IoT-enabled VR and CNN-based training technique has shown promising preliminary results in pilot trials, suggesting it might greatly enhance firefighter competence, situational awareness, and decision-making ability. Because of this, it has the potential to completely alter the way firefighters are informed and prepared for the ever-changing dangers users may encounter on the job.",['convolutional neural networks (CNN)'],"The research idea centers on the need to enhance the safety and effectiveness of firefighters by continuously improving their skills and training methods to better prepare them for various challenging and hazardous situations. The study addresses the importance of creating realistic and adaptable training environments that can simulate diverse firefighting scenarios to improve situational awareness and decision-making abilities. The primary objective of the study is to develop and evaluate a novel approach to firefighter training that can simulate difficult and dangerous situations, customize training difficulty based on individual and team performance, and provide immediate feedback to improve competence and instructional quality. This approach aims to significantly enhance firefighters' preparedness for the dynamic risks they face in their work environment.","The research idea centers on the need to enhance the safety and effectiveness of firefighters by continuously improving their skills and training methods to better prepare them for various challenging and hazardous situations. The study addresses the importance of creating realistic and adaptable training environments that can simulate diverse firefighting scenarios to improve situational awareness and decision-making abilities. The primary objective of the study is to develop and evaluate a new approach to firefighter training that can simulate difficult and dangerous situations, customize training difficulty based on individual and team performance, and provide immediate feedback to improve competence and instructional quality. This approach aims to significantly enhance firefighters' preparedness for the dynamic risks they face in their work environment.",True
Psychology,Flood Detection with SAR: A Review of Techniques and Datasets,"Floods are among the most severe and impacting natural disasters. Their occurrence rate and intensity have been significantly increasing worldwide in the last years due to climate change and urbanization, bringing unprecedented effects on human lives and activities. Hence, providing a prompt response to flooding events is of crucial relevance for humanitarian, social and economic reasons. Satellite remote sensing using synthetic aperture radar (SAR) offers a great deal of support in facing flood events and mitigating their effects on a global scale. As opposed to multi-spectral sensors, SAR offers important advantages, as it enables Earth’s surface imaging regardless of weather and sunlight illumination conditions. In the last decade, the increasing availability of SAR data, even at no cost, thanks to the efforts of international and national space agencies, has been deeply stimulating research activities in every Earth observation field, including flood mapping and monitoring, where advanced processing paradigms, e.g., fuzzy logic, machine learning, data fusion, have been applied, demonstrating their superiority with respect to traditional classification strategies. However, a fair assessment of the performance and reliability of flood mapping techniques is of key importance for an efficient disasters response and, hence, should be addressed carefully and on a quantitative basis trough synthetic quality metrics and high-quality reference data. To this end, the recent development of open SAR datasets specifically covering flood events with related ground-truth reference data can support thorough and objective validation as well as reproducibility of results. Notwithstanding, SAR-based flood monitoring still suffers from severe limitations, especially in vegetated and urban areas, where complex scattering mechanisms can impair an accurate extraction of water regions. All such aspects, including classification methodologies, SAR datasets, validation strategies, challenges and future perspectives for SAR-based flood mapping are described and discussed.",['machine learning'],"The research idea centers on the increasing severity and frequency of floods due to climate change and urbanization, which have profound impacts on human lives and activities, making prompt and effective responses to flooding events critically important for humanitarian, social, and economic reasons. The study highlights the challenges in accurately monitoring floods, especially in vegetated and urban areas, where complex environmental factors complicate the identification of water-affected regions. The primary objective of the study is to evaluate and improve the reliability and performance of flood mapping techniques to support efficient disaster response. It aims to address the limitations in current flood monitoring approaches by discussing classification methods, datasets, validation strategies, and the challenges involved in achieving accurate flood detection and mapping.","The research idea centers on the increasing severity and frequency of floods due to climate change and urbanization, which have profound impacts on human lives and activities, making prompt and effective responses to flooding events critically important for humanitarian, social, and economic reasons. The study highlights the challenges in accurately monitoring floods, especially in vegetated and urban areas, where complex environmental factors complicate the identification of water-affected regions. The primary objective of the study is to evaluate and improve the reliability and performance of flood mapping techniques to support efficient disaster response. It aims to address the limitations in current flood monitoring approaches by discussing various analysis methods, datasets, validation strategies, and the challenges involved in achieving accurate flood detection and mapping.",True
Psychology,Investigating the impact of motion in the scanner on brain age predictions,"Abstract Brain Age Gap (BAG) is defined as the difference between the brain’s predicted age and the chronological age of an individual. Magnetic resonance imaging (MRI)-based BAG can quantify acceleration of brain aging, and is used to infer brain health as aging and disease interact. Motion in the scanner is a common occurrence that can affect the acquired MRI data and act as a major confound in the derived models. As such, age-related changes in head motion may impact the observed age-related differences. However, the relationship between head motion and BAG as estimated by structural MRI has not been systematically examined. The aim of this study is to assess the impact of motion on voxel-based morphometry (VBM) based BAG. Data were obtained from two sources: i) T1-weighted (T1w) MRIs from the Cambridge Centre for Ageing and Neuroscience (CamCAN) were used to train the brain age prediction model, and ii) T1w MRIs from the Movement-related artifacts (MR-ART) dataset were used to assess the impact of motion on BAG. MR-ART includes one motion-free and two motion-affected (one low and one high) 3D T1w MRIs. We also visually rated the motion levels of the MR-ART MRIs from 0 to 5, with 0 meaning no motion and 5 high motion levels. All images were pre-processed through a standard VBM pipeline. GM density across cortical and subcortical regions were then used to train the brain age prediction model and assess the relationship between BAG and MRI motion. Principal component analysis was used to perform dimension reduction and extract the VBM-based features. BAG was estimated by regressing out the portion of delta age explained by chronological age. Linear mixed-effects models were used to investigate the relationship between BAG and motion session as well as motion severity, including participant IDs as random effects. We repeated the same analysis using cortical thickness based on FreeSurfer 7.4.1 and to compare the results for volumetric versus surface-based measures of brain morphometry. In contrast with the session with no induced motion, predicted delta age was significantly higher for high motion sessions 2.35 years (t = 5.17, p &amp;lt; 0.0001), with marginal effect for low motion sessions 0.95 years (t = 2.11, p = 0.035) for VBM analysis as well as 3.46 years (t = 11.45, p &amp;lt; 0.0001) for high motion and 2.28 years (t = 7.54, p &amp;lt; 0.0001) for low motion based on cortical thickness. In addition, delta age was significantly associated with motion severity as evaluated by visual rating 0.45 years per rating level (t = 4.59, p &amp;lt; 0.0001) for VBM analysis and 0.83 years per motion level (t = 12.89, p &amp;lt; 0.0001) for cortical thickness analysis. Motion in the scanner can significantly impact brain age estimates, and needs to be accounted for as a confound, particularly when studying populations that are known to have higher levels of motion in the scanner. These results have significant implications for brain age studies in aging and neurodegeneration. Based on these findings, we recommend assessment and inclusion of visual motion ratings in such studies. In cases that the visual rating proves prohibitive, we recommend the inclusion of normalized Euler number from FreeSurfer as defined in the manuscript as a covariate in the models.","['principal component analysis', 'regression']","The research idea centers on understanding how motion during brain imaging scans may influence the measurement of brain aging, specifically the difference between predicted brain age and chronological age, known as the Brain Age Gap (BAG). Since motion is common during scanning and can affect the quality of imaging data, it may confound assessments of brain health and aging, yet the relationship between head motion and BAG has not been thoroughly investigated. The study addresses the potential impact of age-related changes in head motion on observed differences in brain aging measures. The primary objective of the study is to assess the impact of motion on brain age estimates derived from structural brain imaging, examining how different levels of motion affect the accuracy of brain age predictions. The study aims to determine the extent to which motion influences brain age measurements and to provide recommendations for accounting for motion as a confounding factor in brain aging research, particularly in populations prone to increased motion during scanning.","The research idea centers on understanding how motion during brain imaging scans may influence the measurement of brain aging, specifically the difference between predicted brain age and chronological age, known as the Brain Age Gap (BAG). Since motion is common during scanning and can affect the quality of imaging data, it may confound assessments of brain health and aging, yet the relationship between head motion and BAG has not been thoroughly investigated. The study addresses the potential impact of age-related changes in head motion on observed differences in brain aging measures. The primary objective of the study is to assess the impact of motion on brain age estimates derived from structural brain imaging, examining how different levels of motion affect the accuracy of brain age determination. The study aims to determine the extent to which motion influences brain age measurements and to provide recommendations for accounting for motion as a confounding factor in brain aging research, particularly in populations prone to increased motion during scanning.",True
Psychology,Reliability of ChatGPT for performing triage task in the emergency department using the Korean Triage and Acuity Scale,"Background Artificial intelligence (AI) technology can enable more efficient decision-making in healthcare settings. There is a growing interest in improving the speed and accuracy of AI systems in providing responses for given tasks in healthcare settings. Objective This study aimed to assess the reliability of ChatGPT in determining emergency department (ED) triage accuracy using the Korean Triage and Acuity Scale (KTAS). Methods Two hundred and two virtual patient cases were built. The gold standard triage classification for each case was established by an experienced ED physician. Three other human raters (ED paramedics) were involved and rated the virtual cases individually. The virtual cases were also rated by two different versions of the chat generative pre-trained transformer (ChatGPT, 3.5 and 4.0). Inter-rater reliability was examined using Fleiss’ kappa and intra-class correlation coefficient (ICC). Results The kappa values for the agreement between the four human raters and ChatGPTs were .523 (version 4.0) and .320 (version 3.5). Of the five levels, the performance was poor when rating patients at levels 1 and 5, as well as case scenarios with additional text descriptions. There were differences in the accuracy of the different versions of GPTs. The ICC between version 3.5 and the gold standard was .520, and that between version 4.0 and the gold standard was .802. Conclusions A substantial level of inter-rater reliability was revealed when GPTs were used as KTAS raters. The current study showed the potential of using GPT in emergency healthcare settings. Considering the shortage of experienced manpower, this AI method may help improve triaging accuracy.","['chat generative pre-trained transformer (ChatGPT, 3.5 and 4.0)']","The research idea centers on addressing the challenge of accurately and efficiently determining patient triage levels in emergency department settings, which is critical for prioritizing care and managing limited healthcare resources. There is a recognized need to improve the speed and reliability of triage decisions to enhance patient outcomes and optimize emergency healthcare delivery. The study’s primary objective is to assess the reliability of a novel approach in determining emergency department triage accuracy using the Korean Triage and Acuity Scale (KTAS). Specifically, the study aims to evaluate how well this approach agrees with established triage classifications made by experienced emergency department physicians and paramedics.","The research idea centers on addressing the challenge of accurately and efficiently determining patient triage levels in emergency department settings, which is critical for prioritizing care and managing limited healthcare resources. There is a recognized need to improve the speed and reliability of triage decisions to enhance patient outcomes and optimize emergency healthcare delivery. The study's primary objective is to assess the reliability of a novel approach in determining emergency department triage accuracy using the Korean Triage and Acuity Scale (KTAS). Specifically, the study aims to evaluate how well this approach agrees with established triage classifications made by experienced emergency department physicians and paramedics.",True
Psychology,Deep Reinforcement Learning Unleashing the Power of AI in Decision-Making,"Deep Reinforcement Learning (DRL) has emerged as a transformative paradigm in the field of artificial intelligence (AI), offering unprecedented capabilities in decision-making across diverse domains. This article explores the profound impact of DRL on enhancing the decision-making capabilities of AI systems, elucidating its underlying principles, applications, and implications.DRL represents a fusion of deep learning and reinforcement learning, enabling machines to learn complex behaviors and make decisions by interacting with their environment. The utilization of neural networks allows DRL algorithms to handle high-dimensional input spaces, making it well-suited for tasks that involve intricate decision-making processes.One of the key strengths of DRL lies in its ability to address problems with sparse and delayed rewards, common challenges in traditional reinforcement learning. Through a process of trial and error, DRL algorithms can learn optimal decision strategies by navigating through a vast decision space, adapting to dynamic environments, and maximizing cumulative rewards over time.The applications of DRL span various domains, including robotics, finance, healthcare, gaming, and autonomous systems. In robotics, DRL facilitates the development of intelligent agents capable of autonomously navigating complex environments, performing intricate tasks, and adapting to unforeseen circumstances. In finance, DRL is leveraged for portfolio optimization, algorithmic trading, and risk management, demonstrating its potential to revolutionize traditional financial strategies.","['Deep Reinforcement Learning (DRL)', 'deep learning', 'reinforcement learning']","The research idea centers on the challenge of enhancing decision-making capabilities in complex and dynamic environments, particularly where outcomes are influenced by sparse and delayed feedback. The study addresses the need to understand how agents can learn optimal strategies through interaction and adaptation over time, which is a significant problem in fields requiring intricate decision processes. The primary objective of the study is to explore the mechanisms that enable effective decision-making by agents navigating complex tasks and environments, with a focus on how these agents can improve their performance by learning from experience and adapting to changing conditions. The study aims to elucidate the principles and implications of such learning processes to advance understanding of adaptive behavior in challenging contexts.","The research idea centers on the challenge of enhancing decision-making capabilities in complex and dynamic environments, particularly where outcomes are influenced by sparse and delayed feedback. The study addresses the need to understand how agents can develop optimal strategies through interaction and adaptation over time, which is a significant problem in fields requiring intricate decision processes. The primary objective of the study is to explore the mechanisms that enable effective decision-making by agents navigating complex tasks and environments, with a focus on how these agents can improve their performance by gaining from experience and adapting to changing conditions. The study aims to elucidate the principles and implications of such adaptive processes to advance understanding of adaptive behavior in challenging contexts.",True
Psychology,MentaLLaMA: Interpretable Mental Health Analysis on Social Media with Large Language Models,"As an integral part of people's daily lives, social media is becoming a rich source for automatic mental health analysis.As traditional discriminative methods bear poor generalization ability and low interpretability, the recent large language models (LLMs) have been explored for interpretable mental health analysis on social media, which aims to provide detailed explanations along with predictions in zero-shot or few-shot settings.The results show that LLMs still achieve unsatisfactory classification performance in a zero-shot/few-shot manner, which further significantly affects the quality of the generated explanations.Domain-specific finetuning is an effective solution, but faces two critical challenges: 1) lack of high-quality training data.2) no open-source foundation LLMs.To alleviate these problems, we formally model interpretable mental health analysis as a text generation task, and build the first multi-task and multi-source interpretable mental health instruction (IMHI) dataset with 105K data samples to support LLM instruction tuning and evaluation.The raw social media data are collected from 10 existing sources covering 8 mental health analysis tasks.We prompt ChatGPT with expert-designed few-shot prompts to obtain explanations.To ensure the reliability of the explanations, we perform strict automatic and human evaluations on the correctness, consistency, and quality of generated data.Based on the IMHI dataset and LLaMA2 foundation models, we train MentaLLaMA, the first open-source instruction-following LLM series for interpretable mental health analysis on social media.We evaluate Men-taLLaMA and other advanced methods on the IMHI benchmark, the first holistic evaluation benchmark for interpretable mental health analysis.The results show that MentaLLaMA approaches state-of-the-art discriminative methods in correctness and generates human-level explanations.MentaLLaMA models also show strong generalizability to unseen tasks.The project is available at https://github.com/SteveKGYang/MentaLLaMA.","['zero-shot learning', 'few-shot learning', 'domain-specific finetuning', 'instruction tuning']","The research idea centers on the growing importance of social media as a source for understanding mental health, highlighting challenges in achieving accurate and interpretable mental health assessments from social media content. Traditional methods struggle with generalization and interpretability, which limits their effectiveness in providing meaningful explanations alongside mental health evaluations. The study addresses the need for more reliable and interpretable approaches to analyze mental health indicators in social media data.

The primary objective of the study is to develop a comprehensive and interpretable framework for mental health analysis on social media by creating a large, multi-task, and multi-source dataset that supports detailed explanations in mental health assessments. The study aims to improve the quality and reliability of mental health interpretations by ensuring correctness, consistency, and human-level explanation quality, ultimately enhancing the generalizability of mental health analysis across various tasks.","The research idea centers on the growing importance of social media as a source for understanding mental health, highlighting challenges in achieving accurate and interpretable mental health assessments from social media content. Current approaches struggle with generalization and interpretability, which limits their effectiveness in providing meaningful explanations alongside mental health evaluations. The study addresses the need for more reliable and interpretable approaches to analyze mental health indicators in social media data.

The primary objective of the study is to develop a comprehensive and interpretable framework for mental health analysis on social media by creating a large, multi-task, and multi-source dataset that supports detailed explanations in mental health assessments. The study aims to improve the quality and reliability of mental health interpretations by ensuring correctness, consistency, and human-level explanation quality, ultimately enhancing the generalizability of mental health analysis across various tasks.",True
Psychology,MixFormer: End-to-End Tracking With Iterative Mixed Attention,"Visual object tracking often employs a multi-stage pipeline of feature extraction, target information integration, and bounding box estimation. To simplify this pipeline and unify the process of feature extraction and target information integration, in this paper, we present a compact tracking framework, termed as MixFormer, built upon transformers. Our core design is to utilize the flexibility of attention operations, and we propose a Mixed Attention Module (MAM) for simultaneous feature extraction and target information integration. This synchronous modeling scheme allows us to extract target-specific discriminative features and perform extensive communication between target and search area. Based on MAM, we build our MixFormer trackers simply by stacking multiple MAMs and placing a localization head on top. Specifically, we instantiate two types of MixFormer trackers, a hierarchical tracker MixCvT, and a non-hierarchical simple tracker MixViT. For these two trackers, we investigate a series of pre-training methods and uncover the different behaviors between supervised pre-training and self-supervised pre-training in our MixFormer trackers. We also extend the masked autoencoder pre-training to our MixFormer trackers and design the new competitive TrackMAE pre-training technique. Finally, to handle multiple target templates during online tracking, we devise an asymmetric attention scheme in MAM to reduce computational cost, and propose an effective score prediction module to select high-quality templates. Our MixFormer trackers set a new state-of-the-art performance on seven tracking benchmarks, including LaSOT, TrackingNet, VOT2020, GOT-10 k, OTB100, TOTB and UAV123. In particular, our MixViT-L achieves AUC scores of 73.3% on LaSOT, 86.1% on TrackingNet and 82.8% on TOTB.","['transformers', 'MixFormer trackers', 'supervised pre-training', 'self-supervised pre-training', 'masked autoencoder pre-training']","The research idea centers on improving the process of visual object tracking by addressing the complexity involved in multiple stages such as feature extraction, target information integration, and bounding box estimation. The study is motivated by the need to simplify and unify these processes to enhance the effectiveness of tracking performance. The primary objective of the study is to develop a streamlined framework that integrates feature extraction and target information simultaneously, allowing for more precise identification and communication between the target and its surrounding area. This approach aims to improve tracking accuracy and efficiency across various challenging benchmarks.","The research idea centers on improving the process of visual object tracking by addressing the complexity involved in multiple stages such as feature characterization, target information integration, and bounding box estimation. The study is motivated by the need to simplify and unify these processes to enhance the effectiveness of tracking performance. The primary objective of the study is to develop a streamlined framework that integrates feature characterization and target information simultaneously, allowing for more precise identification and communication between the target and its surrounding area. This approach aims to improve tracking accuracy and efficiency across various challenging benchmarks.",True
Psychology,Automated Classification of Cognitive Visual Objects Using Multivariate Swarm Sparse Decomposition From Multichannel EEG-MEG Signals,"In visual object decoding, magnetoencephalogram (MEG) and electroencephalogram (EEG) activation patterns demonstrate the utmost discriminative cognitive analysis due to their multivariate oscillatory nature. However, high noise in the recorded EEG-MEG signals and subject-specific variability make it extremely difficult to classify subject's cognitive responses to different visual stimuli. The proposed method is a multivariate extension of the swarm sparse decomposition method (MSSDM) for multivariate pattern analysis of EEG-MEG-based visual activation signals. In comparison, it is an advanced technique for decomposing nonstationary multicomponent signals into a finite number of channel-aligned oscillatory components that significantly enhance visual activation-related sub-bands. The MSSDM method adopts multivariate swarm filtering and sparse spectrum to automatically deliver optimal frequency bands in channel-specific sparse spectrums, resulting in improved filter banks. By combining the advantages of the multivariate SSDM and Riemann's correlation-assisted fusion feature (RCFF), the MSSDM-RCFF algorithm is investigated to improve the visual object recognition ability of EEG-MEG signals. We have also proposed time–frequency representation based on MSSDM to analyze discriminative cognitive patterns of different visual object classes from multichannel EEG-MEG signals. A proposed MSSDM is evaluated on multivariate synthetic signals and multivariate EEG-MEG signals using five classifiers. The proposed fusion feature and linear discriminant analysis classifier-based framework outperformed all existing state-of-the-art methods used for visual object detection and achieved the highest accuracy of 86.42% using tenfold cross-validation on EEG-MEG multichannel signals.","['sparse spectrum', 'linear discriminant analysis classifier']","The research idea centers on the challenge of accurately classifying cognitive responses to different visual stimuli using brain activation patterns recorded through EEG and MEG, which are often hindered by high noise levels and individual variability. This difficulty limits the understanding of how the brain discriminates between various visual objects based on neural signals. The study’s primary objective is to enhance the ability to decode and recognize visual object-related cognitive patterns from EEG-MEG signals by improving the identification of relevant frequency bands and oscillatory components associated with visual activation. Ultimately, the research aims to achieve more precise discrimination of cognitive responses to different visual stimuli, thereby advancing knowledge of visual object recognition processes in the brain.","The research idea centers on the challenge of accurately characterizing cognitive responses to different visual stimuli using brain activation patterns recorded through EEG and MEG, which are often hindered by high noise levels and individual variability. This difficulty limits the understanding of how the brain discriminates between various visual objects based on neural signals. The study's primary objective is to enhance the ability to interpret and recognize visual object-related cognitive patterns from EEG-MEG signals by improving the identification of relevant frequency bands and oscillatory components associated with visual activation. Ultimately, the research aims to achieve more precise discrimination of cognitive responses to different visual stimuli, thereby advancing knowledge of visual object recognition processes in the brain.",True
Psychology,CLARUS: An interactive explainable AI platform for manual counterfactuals in graph neural networks,"Lack of trust in artificial intelligence (AI) models in medicine is still the key blockage for the use of AI in clinical decision support systems (CDSS). Although AI models are already performing excellently in systems medicine, their black-box nature entails that patient-specific decisions are incomprehensible for the physician. Explainable AI (XAI) algorithms aim to ""explain"" to a human domain expert, which input features influenced a specific recommendation. However, in the clinical domain, these explanations must lead to some degree of causal understanding by a clinician. We developed the CLARUS platform, aiming to promote human understanding of graph neural network (GNN) predictions. CLARUS enables the visualisation of patient-specific networks, as well as, relevance values for genes and interactions, computed by XAI methods, such as GNNExplainer. This enables domain experts to gain deeper insights into the network and more importantly, the expert can interactively alter the patient-specific network based on the acquired understanding and initiate re-prediction or retraining. This interactivity allows us to ask manual counterfactual questions and analyse the effects on the GNN prediction. We present the first interactive XAI platform prototype, CLARUS, that allows not only the evaluation of specific human counterfactual questions based on user-defined alterations of patient networks and a re-prediction of the clinical outcome but also a retraining of the entire GNN after changing the underlying graph structures. The platform is currently hosted by the GWDG on https://rshiny.gwdg.de/apps/clarus/.","['graph neural network (GNN)', 'GNNExplainer']","The research idea addresses the challenge of lack of trust in clinical decision support systems due to the incomprehensibility of patient-specific decisions made by complex models in medicine. Although these models perform well, their opaque nature prevents physicians from understanding the rationale behind recommendations, which is crucial for clinical acceptance. The study emphasizes the need for explanations that lead to a causal understanding by clinicians to improve trust and usability in medical decision-making. The research objective is to develop a platform that promotes human understanding of patient-specific predictions by enabling visualization and interactive exploration of relevant features influencing clinical outcomes. This platform aims to allow domain experts to gain deeper insights, modify patient-specific information based on their understanding, and observe the effects of these changes on clinical predictions, thereby facilitating a more transparent and interpretable decision-making process in medicine.","The research idea addresses the challenge of lack of trust in clinical decision support systems due to the incomprehensibility of patient-specific decisions made by complex analytical frameworks in medicine. Although these frameworks perform well, their opaque nature prevents physicians from understanding the rationale behind recommendations, which is crucial for clinical acceptance. The study emphasizes the need for explanations that lead to a causal understanding by clinicians to improve trust and usability in medical decision-making. The research objective is to develop a platform that promotes human understanding of patient-specific assessments by enabling visualization and interactive exploration of relevant features influencing clinical outcomes. This platform aims to allow domain experts to gain deeper insights, modify patient-specific information based on their understanding, and observe the effects of these changes on clinical assessments, thereby facilitating a more transparent and interpretable decision-making process in medicine.",True
Psychology,Visual Adversarial Examples Jailbreak Aligned Large Language Models,"Warning: this paper contains data, prompts, and model outputs that are offensive in nature. Recently, there has been a surge of interest in integrating vision into Large Language Models (LLMs), exemplified by Visual Language Models (VLMs) such as Flamingo and GPT-4. This paper sheds light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated LLMs. Second, we highlight that the versatility of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. As an illustration, we present a case study in which we exploit visual adversarial examples to circumvent the safety guardrail of aligned LLMs with integrated vision. Intriguingly, we discover that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions (that it otherwise would not) and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example. Our study underscores the escalating adversarial risks associated with the pursuit of multimodality. Our findings also connect the long-studied adversarial vulnerabilities of neural networks to the nascent field of AI alignment. The presented attack suggests a fundamental adversarial challenge for AI alignment, especially in light of the emerging trend toward multimodality in frontier foundation models.","['Visual Language Models (VLMs)', 'Flamingo', 'GPT-4']","The research idea centers on the increasing integration of visual information into language-based models and the resulting security and safety concerns that arise from this development. Specifically, the study addresses how the complex nature of visual input creates vulnerabilities that can be exploited to bypass safety measures, leading to harmful outcomes. This highlights broader psychological and ethical implications related to the control and alignment of advanced cognitive systems when exposed to diverse and potentially malicious stimuli. The primary objective of the study is to investigate the risks associated with adversarial visual inputs that can undermine safety protocols, demonstrating how a single visual stimulus can provoke harmful responses that extend beyond initially targeted content. The research aims to illuminate the challenges in maintaining aligned and safe behavior in systems that process multimodal information, emphasizing the psychological significance of these vulnerabilities in the context of emerging technologies.","The research idea centers on the increasing integration of visual information into language-based systems and the resulting security and safety concerns that arise from this development. Specifically, the study addresses how the complex nature of visual input creates vulnerabilities that can be exploited to bypass safety measures, leading to harmful outcomes. This highlights broader psychological and ethical implications related to the control and alignment of advanced cognitive frameworks when exposed to diverse and potentially malicious stimuli. The primary objective of the study is to investigate the risks associated with adversarial visual inputs that can undermine safety protocols, demonstrating how a single visual stimulus can provoke harmful responses that extend beyond initially targeted content. The research aims to illuminate the challenges in maintaining aligned and safe behavior in systems that process multimodal information, emphasizing the psychological significance of these vulnerabilities in the context of emerging technologies.",True
Psychology,Multimodal data integration for oncology in the era of deep neural networks: a review,"Cancer research encompasses data across various scales, modalities, and resolutions, from screening and diagnostic imaging to digitized histopathology slides to various types of molecular data and clinical records. The integration of these diverse data types for personalized cancer care and predictive modeling holds the promise of enhancing the accuracy and reliability of cancer screening, diagnosis, and treatment. Traditional analytical methods, which often focus on isolated or unimodal information, fall short of capturing the complex and heterogeneous nature of cancer data. The advent of deep neural networks has spurred the development of sophisticated multimodal data fusion techniques capable of extracting and synthesizing information from disparate sources. Among these, Graph Neural Networks (GNNs) and Transformers have emerged as powerful tools for multimodal learning, demonstrating significant success. This review presents the foundational principles of multimodal learning including oncology data modalities, taxonomy of multimodal learning, and fusion strategies. We delve into the recent advancements in GNNs and Transformers for the fusion of multimodal data in oncology, spotlighting key studies and their pivotal findings. We discuss the unique challenges of multimodal learning, such as data heterogeneity and integration complexities, alongside the opportunities it presents for a more nuanced and comprehensive understanding of cancer. Finally, we present some of the latest comprehensive multimodal pan-cancer data sources. By surveying the landscape of multimodal data integration in oncology, our goal is to underline the transformative potential of multimodal GNNs and Transformers. Through technological advancements and the methodological innovations presented in this review, we aim to chart a course for future research in this promising field. This review may be the first that highlights the current state of multimodal modeling applications in cancer using GNNs and transformers, presents comprehensive multimodal oncology data sources, and sets the stage for multimodal evolution, encouraging further exploration and development in personalized cancer care.","['deep neural networks', 'Graph Neural Networks (GNNs)', 'Transformers']","The research idea centers on addressing the complexity and heterogeneity of cancer by integrating diverse types of data, such as screening, diagnostic imaging, histopathology, molecular data, and clinical records, to improve personalized cancer care. Traditional approaches that focus on isolated data types are insufficient for capturing the multifaceted nature of cancer, highlighting the need for more comprehensive methods to enhance the accuracy and reliability of cancer screening, diagnosis, and treatment. The research objective is to review and synthesize the current advancements in multimodal data integration within oncology, emphasizing the potential for combining various cancer-related data sources to achieve a more nuanced and comprehensive understanding of the disease. This study aims to outline the foundational principles, challenges, and opportunities in multimodal learning for cancer research, ultimately guiding future investigations toward improving personalized cancer care.","The research idea centers on addressing the complexity and heterogeneity of cancer by integrating diverse types of data, such as screening, diagnostic imaging, histopathology, molecular data, and clinical records, to improve personalized cancer care. Traditional approaches that focus on isolated data types are insufficient for capturing the multifaceted nature of cancer, highlighting the need for more comprehensive methods to enhance the accuracy and reliability of cancer screening, diagnosis, and treatment. The research objective is to review and synthesize the current advancements in multimodal data integration within oncology, emphasizing the potential for combining various cancer-related data sources to achieve a more nuanced and comprehensive understanding of the disease. This study aims to outline the foundational principles, challenges, and opportunities in integrating multiple data modalities for cancer research, ultimately guiding future investigations toward improving personalized cancer care.",True
Psychology,Role of machine learning and deep learning techniques in EEG-based BCI emotion recognition system: a review,"Abstract Emotion is a subjective psychophysiological reaction coming from external stimuli which impacts every aspect of our daily lives. Due to the continuing development of non-invasive and portable sensor technologies, such as brain-computer interfaces (BCI), intellectuals from several fields have been interested in emotion recognition techniques. Human emotions can be recognised using a variety of behavioural cues, including gestures and body language, voice, and physiological markers. The first three, however, might be ineffective because people sometimes conceal their genuine emotions either intentionally or unknowingly. More precise and objective emotion recognition can be accomplished using physiological signals. Among other physiological signals, Electroencephalogram (EEG) is more responsive and sensitive to variation in affective states. Various EEG-based emotion recognition methods have recently been introduced. This study reviews EEG-based BCIs for emotion identification and gives an outline of the progress made in this field. A summary of the datasets and techniques utilised to evoke human emotions and various emotion models is also given. We discuss several EEG feature extractions, feature selection/reduction, machine learning, and deep learning algorithms in accordance with standard emotional identification process. We provide an overview of the human brain's EEG rhythms, which are closely related to emotional states. We also go over a number of EEG-based emotion identification research and compare numerous machine learning and deep learning techniques. In conclusion, this study highlights the applications, challenges and potential areas for future research in identification and classification of human emotional states.",['feature selection/reduction'],"The research idea centers on the importance of accurately recognizing human emotions, which are subjective psychophysiological reactions influenced by external stimuli and affect many aspects of daily life. Traditional behavioral cues such as gestures, body language, and voice may be unreliable because individuals can consciously or unconsciously conceal their true emotions. Therefore, there is a need for more precise and objective methods of emotion recognition using physiological signals, with a particular focus on brain activity that is sensitive to changes in affective states. The study addresses the ongoing development and progress in understanding how these physiological markers can improve emotion identification.

The primary objective of the study is to review the current state of research on emotion identification using physiological signals, specifically focusing on brain activity related to emotional states. It aims to summarize the various approaches used to evoke and model human emotions, outline the progress made in this area, and highlight the applications, challenges, and potential directions for future research in the identification and classification of human emotional states.","The research idea centers on the importance of accurately recognizing human emotions, which are subjective psychophysiological reactions influenced by external stimuli and affect many aspects of daily life. Traditional behavioral cues such as gestures, body language, and voice may be unreliable because individuals can consciously or unconsciously conceal their true emotions. Therefore, there is a need for more precise and objective methods of emotion recognition using physiological signals, with a particular focus on brain activity that is sensitive to changes in affective states. The study addresses the ongoing development and progress in understanding how these physiological markers can improve emotion identification.

The primary objective of the study is to review the current state of research on emotion identification using physiological signals, specifically focusing on brain activity related to emotional states. It aims to summarize the various approaches used to evoke and analyze human emotions, outline the progress made in this area, and highlight the applications, challenges, and potential directions for future research in the identification and assessment of human emotional states.",True
Medicine,Discovering biomarkers associated and predicting cardiovascular disease with high accuracy using a novel nexus of machine learning techniques for precision medicine,"Abstract Personalized interventions are deemed vital given the intricate characteristics, advancement, inherent genetic composition, and diversity of cardiovascular diseases (CVDs). The appropriate utilization of artificial intelligence (AI) and machine learning (ML) methodologies can yield novel understandings of CVDs, enabling improved personalized treatments through predictive analysis and deep phenotyping. In this study, we proposed and employed a novel approach combining traditional statistics and a nexus of cutting-edge AI/ML techniques to identify significant biomarkers for our predictive engine by analyzing the complete transcriptome of CVD patients. After robust gene expression data pre-processing, we utilized three statistical tests (Pearson correlation, Chi-square test, and ANOVA) to assess the differences in transcriptomic expression and clinical characteristics between healthy individuals and CVD patients. Next, the recursive feature elimination classifier assigned rankings to transcriptomic features based on their relation to the case–control variable. The top ten percent of commonly observed significant biomarkers were evaluated using four unique ML classifiers (Random Forest, Support Vector Machine, Xtreme Gradient Boosting Decision Trees, and k-Nearest Neighbors). After optimizing hyperparameters, the ensembled models, which were implemented using a soft voting classifier, accurately differentiated between patients and healthy individuals. We have uncovered 18 transcriptomic biomarkers that are highly significant in the CVD population that were used to predict disease with up to 96% accuracy. Additionally, we cross-validated our results with clinical records collected from patients in our cohort. The identified biomarkers served as potential indicators for early detection of CVDs. With its successful implementation, our newly developed predictive engine provides a valuable framework for identifying patients with CVDs based on their biomarker profiles.","['Random Forest', 'Support Vector Machine', 'Xtreme Gradient Boosting Decision Trees', 'k-Nearest Neighbors', 'soft voting classifier']","The research idea centers on the need for personalized interventions in cardiovascular diseases (CVDs) due to their complex characteristics, progression, genetic makeup, and diversity. Understanding these intricacies is crucial for improving treatment outcomes and early detection. The study aims to identify significant biomarkers that can serve as potential indicators for early detection and better management of CVDs. The primary objective of the study is to identify and validate transcriptomic biomarkers that distinguish between healthy individuals and CVD patients, thereby enabling accurate prediction and early diagnosis of cardiovascular diseases. This objective is pursued through the analysis of gene expression data and clinical characteristics to uncover biomarkers highly significant in the CVD population.","The research idea centers on the need for personalized interventions in cardiovascular diseases (CVDs) due to their complex characteristics, progression, genetic makeup, and diversity. Understanding these intricacies is crucial for improving treatment outcomes and early detection. The study aims to identify significant biomarkers that can serve as potential indicators for early detection and better management of CVDs. The primary objective of the study is to identify and validate transcriptomic biomarkers that distinguish between healthy individuals and CVD patients, thereby enabling accurate identification and early diagnosis of cardiovascular diseases. This objective is pursued through the examination of gene expression data and clinical characteristics to uncover biomarkers highly significant in the CVD population.",True
Medicine,Improving large language models for clinical named entity recognition via prompt engineering,"Abstract Importance The study highlights the potential of large language models, specifically GPT-3.5 and GPT-4, in processing complex clinical data and extracting meaningful information with minimal training data. By developing and refining prompt-based strategies, we can significantly enhance the models’ performance, making them viable tools for clinical NER tasks and possibly reducing the reliance on extensive annotated datasets. Objectives This study quantifies the capabilities of GPT-3.5 and GPT-4 for clinical named entity recognition (NER) tasks and proposes task-specific prompts to improve their performance. Materials and Methods We evaluated these models on 2 clinical NER tasks: (1) to extract medical problems, treatments, and tests from clinical notes in the MTSamples corpus, following the 2010 i2b2 concept extraction shared task, and (2) to identify nervous system disorder-related adverse events from safety reports in the vaccine adverse event reporting system (VAERS). To improve the GPT models' performance, we developed a clinical task-specific prompt framework that includes (1) baseline prompts with task description and format specification, (2) annotation guideline-based prompts, (3) error analysis-based instructions, and (4) annotated samples for few-shot learning. We assessed each prompt's effectiveness and compared the models to BioClinicalBERT. Results Using baseline prompts, GPT-3.5 and GPT-4 achieved relaxed F1 scores of 0.634, 0.804 for MTSamples and 0.301, 0.593 for VAERS. Additional prompt components consistently improved model performance. When all 4 components were used, GPT-3.5 and GPT-4 achieved relaxed F1 socres of 0.794, 0.861 for MTSamples and 0.676, 0.736 for VAERS, demonstrating the effectiveness of our prompt framework. Although these results trail BioClinicalBERT (F1 of 0.901 for the MTSamples dataset and 0.802 for the VAERS), it is very promising considering few training samples are needed. Discussion The study’s findings suggest a promising direction in leveraging LLMs for clinical NER tasks. However, while the performance of GPT models improved with task-specific prompts, there's a need for further development and refinement. LLMs like GPT-4 show potential in achieving close performance to state-of-the-art models like BioClinicalBERT, but they still require careful prompt engineering and understanding of task-specific knowledge. The study also underscores the importance of evaluation schemas that accurately reflect the capabilities and performance of LLMs in clinical settings. Conclusion While direct application of GPT models to clinical NER tasks falls short of optimal performance, our task-specific prompt framework, incorporating medical knowledge and training samples, significantly enhances GPT models' feasibility for potential clinical applications.","['large language models (GPT-3.5)', 'large language models (GPT-4)', 'prompt-based strategies', 'few-shot learning', 'BioClinicalBERT']","The study addresses the challenge of accurately extracting meaningful clinical information, such as medical problems, treatments, and adverse events, from complex clinical texts with limited annotated data. It highlights the need for effective strategies to improve the performance of tools used for clinical named entity recognition (NER) tasks, which are essential for processing clinical notes and safety reports. The primary aim of the study is to quantify the capabilities of specific language models in performing clinical NER tasks and to propose task-specific strategies that enhance their performance in extracting relevant clinical concepts. This objective focuses on improving the feasibility of these tools for clinical applications by incorporating medical knowledge and limited training samples.","The study addresses the challenge of accurately extracting meaningful clinical information, such as medical problems, treatments, and adverse events, from complex clinical texts with limited annotated data. It highlights the need for effective strategies to improve the performance of tools used for clinical named entity recognition (NER) tasks, which are essential for processing clinical notes and safety reports. The primary aim of the study is to quantify the capabilities of specific text analysis approaches in performing clinical NER tasks and to propose task-specific strategies that enhance their performance in extracting relevant clinical concepts. This objective focuses on improving the feasibility of these tools for clinical applications by incorporating medical knowledge and limited training samples.",True
Medicine,Can large language models replace humans in systematic reviews? Evaluating <scp>GPT</scp>‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages,"Systematic reviews are vital for guiding practice, research and policy, although they are often slow and labour-intensive. Large language models (LLMs) could speed up and automate systematic reviews, but their performance in such tasks has yet to be comprehensively evaluated against humans, and no study has tested Generative Pre-Trained Transformer (GPT)-4, the biggest LLM so far. This pre-registered study uses a ""human-out-of-the-loop"" approach to evaluate GPT-4's capability in title/abstract screening, full-text review and data extraction across various literature types and languages. Although GPT-4 had accuracy on par with human performance in some tasks, results were skewed by chance agreement and dataset imbalance. Adjusting for these caused performance scores to drop across all stages: for data extraction, performance was moderate, and for screening, it ranged from none in highly balanced literature datasets (~1:1) to moderate in those datasets where the ratio of inclusion to exclusion in studies was imbalanced (~1:3). When screening full-text literature using highly reliable prompts, GPT-4's performance was more robust, reaching ""human-like"" levels. Although our findings indicate that, currently, substantial caution should be exercised if LLMs are being used to conduct systematic reviews, they also offer preliminary evidence that, for certain review tasks delivered under specific conditions, LLMs can rival human performance.",['Generative Pre-Trained Transformer (GPT)-4'],"The research idea centers on the importance of systematic reviews for guiding clinical practice, research, and policy, while highlighting the challenges posed by their slow and labor-intensive nature. There is a need to explore ways to improve the efficiency of conducting systematic reviews without compromising accuracy. The study’s primary objective is to evaluate the capability of a large language model, specifically GPT-4, in performing key tasks involved in systematic reviews such as title and abstract screening, full-text review, and data extraction across diverse types of literature and languages. The aim is to determine how well GPT-4 performs these tasks compared to human reviewers and to assess its potential role in supporting or automating systematic review processes.","The research idea centers on the importance of systematic reviews for guiding clinical practice, research, and policy, while highlighting the challenges posed by their slow and labor-intensive nature. There is a need to explore ways to improve the efficiency of conducting systematic reviews without compromising accuracy. The study's primary objective is to evaluate alternative methods for performing key tasks involved in systematic reviews such as title and abstract screening, full-text review, and data extraction across diverse types of literature and languages. The aim is to determine how well these alternative approaches perform these tasks compared to human reviewers and to assess their potential role in supporting or streamlining systematic review processes.",True
Medicine,Investigating Spatial Effects through Machine Learning and Leveraging Explainable AI for Child Malnutrition in Pakistan,"While socioeconomic gradients in regional health inequalities are firmly established, the synergistic interactions between socioeconomic deprivation and climate vulnerability within convenient proximity and neighbourhood locations with health disparities remain poorly explored and thus require deep understanding within a regional context. Furthermore, disregarding the importance of spatial spillover effects and nonlinear effects of covariates on childhood stunting are inevitable in dealing with an enduring issue of regional health inequalities. The present study aims to investigate the spatial inequalities in childhood stunting at the district level in Pakistan and validate the importance of spatial lag in predicting childhood stunting. Furthermore, it examines the presence of any nonlinear relationships among the selected independent features with childhood stunting. The study utilized data related to socioeconomic features from MICS 2017–2018 and climatic data from Integrated Contextual Analysis. A multi-model approach was employed to address the research questions, which included Ordinary Least Squares Regression (OLS), various Spatial Models, Machine Learning Algorithms and Explainable Artificial Intelligence methods. Firstly, OLS was used to analyse and test the linear relationships among selected variables. Secondly, Spatial Durbin Error Model (SDEM) was used to detect and capture the impact of spatial spillover on childhood stunting. Third, XGBoost and Random Forest machine learning algorithms were employed to examine and validate the importance of the spatial lag component. Finally, EXAI methods such as SHapley were utilized to identify potential nonlinear relationships. The study found a clear pattern of spatial clustering and geographical disparities in childhood stunting, with multidimensional poverty, high climate vulnerability and early marriage worsening childhood stunting. In contrast, low climate vulnerability, high exposure to mass media and high women’s literacy were found to reduce childhood stunting. The use of machine learning algorithms, specifically XGBoost and Random Forest, highlighted the significant role played by the average value in the neighbourhood in predicting childhood stunting in nearby districts, confirming that the spatial spillover effect is not bounded by geographical boundaries. Furthermore, EXAI methods such as partial dependency plot reveal the existence of a nonlinear relationship between multidimensional poverty and childhood stunting. The study’s findings provide valuable insights into the spatial distribution of childhood stunting in Pakistan, emphasizing the importance of considering spatial effects in predicting childhood stunting. Individual and household-level factors such as exposure to mass media and women’s literacy have shown positive implications for childhood stunting. It further provides a justification for the usage of EXAI methods to draw better insights and propose customised intervention policies accordingly.","['XGBoost', 'Random Forest', 'partial dependency plot']","The research idea addresses the complex interplay between socioeconomic deprivation and climate vulnerability in contributing to regional health inequalities, specifically focusing on childhood stunting. Despite established knowledge of socioeconomic gradients in health disparities, the combined effects of these factors within local neighborhoods and their spatial interactions remain insufficiently understood, particularly in the context of Pakistan. This gap highlights the need to explore how spatial spillover effects and nonlinear relationships among various determinants influence childhood stunting across different districts. The study aims to investigate spatial inequalities in childhood stunting at the district level in Pakistan, emphasizing the importance of spatial factors in understanding health disparities. It seeks to validate the role of spatial proximity in predicting childhood stunting and to examine potential nonlinear relationships between selected socioeconomic and climatic variables and childhood stunting, thereby providing insights for targeted public health interventions.","The research idea addresses the complex interplay between socioeconomic deprivation and climate vulnerability in contributing to regional health inequalities, specifically focusing on childhood stunting. Despite established knowledge of socioeconomic gradients in health disparities, the combined effects of these factors within local neighborhoods and their spatial interactions remain insufficiently understood, particularly in the context of Pakistan. This gap highlights the need to explore how spatial spillover effects and relationships among various determinants influence childhood stunting across different districts. The study aims to investigate spatial inequalities in childhood stunting at the district level in Pakistan, emphasizing the importance of spatial factors in understanding health disparities. It seeks to validate the role of spatial proximity in predicting childhood stunting and to examine relationships between selected socioeconomic and climatic variables and childhood stunting, thereby providing insights for targeted public health interventions.",True
Medicine,Unmasking bias in artificial intelligence: a systematic review of bias detection and mitigation strategies in electronic health record-based models,"Abstract Objectives Leveraging artificial intelligence (AI) in conjunction with electronic health records (EHRs) holds transformative potential to improve healthcare. However, addressing bias in AI, which risks worsening healthcare disparities, cannot be overlooked. This study reviews methods to handle various biases in AI models developed using EHR data. Materials and Methods We conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and Meta-analyses guidelines, analyzing articles from PubMed, Web of Science, and IEEE published between January 01, 2010 and December 17, 2023. The review identified key biases, outlined strategies for detecting and mitigating bias throughout the AI model development, and analyzed metrics for bias assessment. Results Of the 450 articles retrieved, 20 met our criteria, revealing 6 major bias types: algorithmic, confounding, implicit, measurement, selection, and temporal. The AI models were primarily developed for predictive tasks, yet none have been deployed in real-world healthcare settings. Five studies concentrated on the detection of implicit and algorithmic biases employing fairness metrics like statistical parity, equal opportunity, and predictive equity. Fifteen studies proposed strategies for mitigating biases, especially targeting implicit and selection biases. These strategies, evaluated through both performance and fairness metrics, predominantly involved data collection and preprocessing techniques like resampling and reweighting. Discussion This review highlights evolving strategies to mitigate bias in EHR-based AI models, emphasizing the urgent need for both standardized and detailed reporting of the methodologies and systematic real-world testing and evaluation. Such measures are essential for gauging models’ practical impact and fostering ethical AI that ensures fairness and equity in healthcare.","['resampling', 'reweighting']","The research idea centers on the critical issue of bias in healthcare applications developed using electronic health records, which poses a risk of exacerbating healthcare disparities. Addressing these biases is essential to ensure fairness and equity in healthcare delivery. The study recognizes the transformative potential of improving healthcare outcomes but emphasizes that bias cannot be overlooked in this context. The primary objective of the study is to review existing methods for identifying and mitigating various types of bias in healthcare models developed from electronic health record data. It aims to outline strategies for detecting and reducing bias and to analyze metrics used for bias assessment, with the goal of promoting ethical practices and ensuring equitable healthcare outcomes.","The research idea centers on the critical issue of bias in healthcare applications developed using electronic health records, which poses a risk of exacerbating healthcare disparities. Addressing these biases is essential to ensure fairness and equity in healthcare delivery. The study recognizes the transformative potential of improving healthcare outcomes but emphasizes that bias cannot be overlooked in this context. The primary objective of the study is to review existing methods for identifying and mitigating various types of bias in healthcare analyses developed from electronic health record data. It aims to outline strategies for detecting and reducing bias and to analyze metrics used for bias assessment, with the goal of promoting ethical practices and ensuring equitable healthcare outcomes.",True
Medicine,Vision–language foundation model for echocardiogram interpretation,"Abstract The development of robust artificial intelligence models for echocardiography has been limited by the availability of annotated clinical data. Here, to address this challenge and improve the performance of cardiac imaging models, we developed EchoCLIP, a vision–language foundation model for echocardiography, that learns the relationship between cardiac ultrasound images and the interpretations of expert cardiologists across a wide range of patients and indications for imaging. After training on 1,032,975 cardiac ultrasound videos and corresponding expert text, EchoCLIP performs well on a diverse range of benchmarks for cardiac image interpretation, despite not having been explicitly trained for individual interpretation tasks. EchoCLIP can assess cardiac function (mean absolute error of 7.1% when predicting left ventricular ejection fraction in an external validation dataset) and identify implanted intracardiac devices (area under the curve (AUC) of 0.84, 0.92 and 0.97 for pacemakers, percutaneous mitral valve repair and artificial aortic valves, respectively). We also developed a long-context variant (EchoCLIP-R) using a custom tokenizer based on common echocardiography concepts. EchoCLIP-R accurately identified unique patients across multiple videos (AUC of 0.86), identified clinical transitions such as heart transplants (AUC of 0.79) and cardiac surgery (AUC 0.77) and enabled robust image-to-text search (mean cross-modal retrieval rank in the top 1% of candidate text reports). These capabilities represent a substantial step toward understanding and applying foundation models in cardiovascular imaging for preliminary interpretation of echocardiographic findings.",['vision–language foundation model'],"The research addresses the challenge of limited availability of annotated clinical data for echocardiography, which has hindered the development of robust methods for cardiac imaging interpretation. Improving the understanding and performance of cardiac ultrasound image analysis is essential for better assessment of cardiac function and identification of intracardiac devices across diverse patient populations and clinical indications. The primary aim of the study is to develop a comprehensive approach that learns the relationship between cardiac ultrasound images and expert cardiologists’ interpretations to enhance the accuracy and applicability of echocardiographic evaluation. This includes assessing cardiac function, identifying implanted devices, recognizing clinical transitions such as heart transplants and cardiac surgery, and enabling effective retrieval of relevant clinical information from imaging data.","The research addresses the challenge of limited availability of annotated clinical data for echocardiography, which has hindered the development of robust methods for cardiac imaging interpretation. Improving the understanding and performance of cardiac ultrasound image analysis is essential for better assessment of cardiac function and identification of intracardiac devices across diverse patient populations and clinical indications. The primary aim of the study is to develop a comprehensive approach that captures the relationship between cardiac ultrasound images and expert cardiologists' interpretations to enhance the accuracy and applicability of echocardiographic evaluation. This includes assessing cardiac function, identifying implanted devices, recognizing clinical transitions such as heart transplants and cardiac surgery, and enabling effective retrieval of relevant clinical information from imaging data.",True
Medicine,Enhancing heart disease prediction using a self-attention-based transformer model,"Abstract Cardiovascular diseases (CVDs) continue to be the leading cause of more than 17 million mortalities worldwide. The early detection of heart failure with high accuracy is crucial for clinical trials and therapy. Patients will be categorized into various types of heart disease based on characteristics like blood pressure, cholesterol levels, heart rate, and other characteristics. With the use of an automatic system, we can provide early diagnoses for those who are prone to heart failure by analyzing their characteristics. In this work, we deploy a novel self-attention-based transformer model, that combines self-attention mechanisms and transformer networks to predict CVD risk. The self-attention layers capture contextual information and generate representations that effectively model complex patterns in the data. Self-attention mechanisms provide interpretability by giving each component of the input sequence a certain amount of attention weight. This includes adjusting the input and output layers, incorporating more layers, and modifying the attention processes to collect relevant information. This also makes it possible for physicians to comprehend which features of the data contributed to the model's predictions. The proposed model is tested on the Cleveland dataset, a benchmark dataset of the University of California Irvine (UCI) machine learning (ML) repository. Comparing the proposed model to several baseline approaches, we achieved the highest accuracy of 96.51%. Furthermore, the outcomes of our experiments demonstrate that the prediction rate of our model is higher than that of other cutting-edge approaches used for heart disease prediction.","['self-attention-based transformer model', 'self-attention mechanisms', 'transformer networks']","Cardiovascular diseases (CVDs) remain the leading cause of over 17 million deaths globally, highlighting the critical need for early and accurate detection of heart failure to improve clinical outcomes and guide therapy. Identifying patients at risk by categorizing them based on clinical characteristics such as blood pressure, cholesterol levels, and heart rate is essential for timely intervention. The primary aim of this study is to enhance the early diagnosis of heart failure by effectively utilizing patient characteristics to predict the risk of cardiovascular disease. This approach seeks to support physicians in understanding which clinical features contribute most significantly to the risk, thereby facilitating better-informed medical decisions.","Cardiovascular diseases (CVDs) remain the leading cause of over 17 million deaths globally, highlighting the critical need for early and accurate detection of heart failure to improve clinical outcomes and guide therapy. Identifying patients at risk by categorizing them based on clinical characteristics such as blood pressure, cholesterol levels, and heart rate is essential for timely intervention. The primary aim of this study is to enhance the early diagnosis of heart failure by effectively utilizing patient characteristics to determine the risk of cardiovascular disease. This approach seeks to support physicians in understanding which clinical features contribute most significantly to the risk, thereby facilitating better-informed medical decisions.",True
Medicine,The diagnostic and triage accuracy of the GPT-3 artificial intelligence model: an observational study,"BackgroundArtificial intelligence (AI) applications in health care have been effective in many areas of medicine, but they are often trained for a single task using labelled data, making deployment and generalisability challenging. How well a general-purpose AI language model performs diagnosis and triage relative to physicians and laypeople is not well understood.MethodsWe compared the predictive accuracy of Generative Pre-trained Transformer 3 (GPT-3)'s diagnostic and triage ability for 48 validated synthetic case vignettes (<50 words; sixth-grade reading level or below) of both common (eg, viral illness) and severe (eg, heart attack) conditions to a nationally representative sample of 5000 lay people from the USA who could use the internet to find the correct options and 21 practising physicians at Harvard Medical School. There were 12 vignettes for each of four triage categories: emergent, within one day, within 1 week, and self-care. The correct diagnosis and triage category (ie, ground truth) for each vignette was determined by two general internists at Harvard Medical School. For each vignette, human respondents and GPT-3 were prompted to list diagnoses in order of likelihood, and the vignette was marked as correct if the ground-truth diagnosis was in the top three of the listed diagnoses. For triage accuracy, we examined whether the human respondents' and GPT-3's selected triage was exactly correct according to the four triage categories, or matched a dichotomised triage variable (emergent or within 1 day vs within 1 week or self-care). We estimated GPT-3's diagnostic and triage confidence on a given vignette using a modified bootstrap resampling procedure, and examined how well calibrated GPT-3's confidence was by computing calibration curves and Brier scores. We also performed subgroup analysis by case acuity, and an error analysis for triage advice to characterise how its advice might affect patients using this tool to decide if they should seek medical care immediately.FindingsAmong all cases, GPT-3 replied with the correct diagnosis in its top three for 88% (42/48, 95% CI 75–94) of cases, compared with 54% (2700/5000, 53–55) for lay individuals (p<0.0001) and 96% (637/666, 94–97) for physicians (p=0·012). GPT-3 triaged 70% correct (34/48, 57–82) versus 74% (3706/5000, 73–75; p=0.60) for lay individuals and 91% (608/666, 89–93%; p<0.0001) for physicians. As measured by the Brier score, GPT-3 confidence in its top prediction was reasonably well calibrated for diagnosis (Brier score=0·18) and triage (Brier score=0·22). We observed an inverse relationship between case acuity and GPT-3 accuracy (p<0·0001) with a fitted trend line of –8·33% decrease in accuracy for every level of increase in case acuity. For triage error analysis, GPT-3 deprioritised truly emergent cases in seven instances.InterpretationA general-purpose AI language model without any content-specific training could perform diagnosis at levels close to, but below, physicians and better than lay individuals. We found that GPT-3's performance was inferior to physicians for triage, sometimes by a large margin, and its performance was closer to that of lay individuals. Although the diagnostic performance of GPT-3 was comparable to physicians, it was significantly better than a typical person using a search engine.FundingThe National Heart, Lung, and Blood Institute.",['Generative Pre-trained Transformer 3 (GPT-3)'],"The study addresses the challenge of understanding how well a general-purpose diagnostic and triage tool performs compared to physicians and laypeople, particularly given the difficulty in deploying tools that are typically trained for single tasks. There is limited knowledge about the accuracy and reliability of such a tool in diagnosing and triaging a range of medical conditions, from common illnesses to severe emergencies. The primary aim of the study is to evaluate the diagnostic and triage accuracy of a general-purpose language-based tool relative to practicing physicians and lay individuals using validated clinical case vignettes. The study seeks to determine how closely the tool’s performance approaches that of physicians and whether it surpasses the abilities of laypeople in providing correct diagnoses and appropriate triage recommendations.","The study addresses the challenge of understanding how well a general-purpose diagnostic and triage tool performs compared to physicians and laypeople, particularly given the difficulty in deploying tools that are typically designed for single tasks. There is limited knowledge about the accuracy and reliability of such a tool in diagnosing and triaging a range of medical conditions, from common illnesses to severe emergencies. The primary aim of the study is to evaluate the diagnostic and triage accuracy of a general-purpose text analysis tool relative to practicing physicians and lay individuals using validated clinical case vignettes. The study seeks to determine how closely the tool's performance approaches that of physicians and whether it surpasses the abilities of laypeople in providing correct diagnoses and appropriate triage recommendations.",True
Medicine,Machine Learning and Digital Biomarkers Can Detect Early Stages of Neurodegenerative Diseases,"Neurodegenerative diseases (NDs) such as Alzheimer’s Disease (AD) and Parkinson’s Disease (PD) are devastating conditions that can develop without noticeable symptoms, causing irreversible damage to neurons before any signs become clinically evident. NDs are a major cause of disability and mortality worldwide. Currently, there are no cures or treatments to halt their progression. Therefore, the development of early detection methods is urgently needed to delay neuronal loss as soon as possible. Despite advancements in Medtech, the early diagnosis of NDs remains a challenge at the intersection of medical, IT, and regulatory fields. Thus, this review explores “digital biomarkers” (tools designed for remote neurocognitive data collection and AI analysis) as a potential solution. The review summarizes that recent studies combining AI with digital biomarkers suggest the possibility of identifying pre-symptomatic indicators of NDs. For instance, research utilizing convolutional neural networks for eye tracking has achieved significant diagnostic accuracies. ROC-AUC scores reached up to 0.88, indicating high model performance in differentiating between PD patients and healthy controls. Similarly, advancements in facial expression analysis through tools have demonstrated significant potential in detecting emotional changes in ND patients, with some models reaching an accuracy of 0.89 and a precision of 0.85. This review follows a structured approach to article selection, starting with a comprehensive database search and culminating in a rigorous quality assessment and meaning for NDs of the different methods. The process is visualized in 10 tables with 54 parameters describing different approaches and their consequences for understanding various mechanisms in ND changes. However, these methods also face challenges related to data accuracy and privacy concerns. To address these issues, this review proposes strategies that emphasize the need for rigorous validation and rapid integration into clinical practice. Such integration could transform ND diagnostics, making early detection tools more cost-effective and globally accessible. In conclusion, this review underscores the urgent need to incorporate validated digital health tools into mainstream medical practice. This integration could indicate a new era in the early diagnosis of neurodegenerative diseases, potentially altering the trajectory of these conditions for millions worldwide. Thus, by highlighting specific and statistically significant findings, this review demonstrates the current progress in this field and the potential impact of these advancements on the global management of NDs.",['convolutional neural networks'],"Neurodegenerative diseases such as Alzheimer’s Disease and Parkinson’s Disease are devastating conditions that often develop without noticeable symptoms, leading to irreversible neuronal damage before clinical signs appear. These diseases are a major cause of disability and mortality worldwide, and currently, there are no cures or treatments to halt their progression. Therefore, there is an urgent need for early detection methods to delay neuronal loss and improve patient outcomes. The study aims to explore and summarize recent advancements in early diagnostic approaches for neurodegenerative diseases, focusing on the potential of novel tools to identify pre-symptomatic indicators. It seeks to evaluate the effectiveness and challenges of these approaches, propose strategies for their validation, and emphasize the importance of integrating validated early detection tools into clinical practice to transform diagnostics and improve global management of neurodegenerative diseases.","Neurodegenerative diseases such as Alzheimer's Disease and Parkinson's Disease are devastating conditions that often develop without noticeable symptoms, leading to irreversible neuronal damage before clinical signs appear. These diseases are a major cause of disability and mortality worldwide, and currently, there are no cures or treatments to halt their progression. Therefore, there is an urgent need for early detection methods to delay neuronal loss and improve patient outcomes. The study aims to explore and summarize recent advancements in early diagnostic approaches for neurodegenerative diseases, focusing on the potential of novel tools to identify pre-symptomatic indicators. It seeks to evaluate the effectiveness and challenges of these approaches, propose strategies for their validation, and emphasize the importance of integrating validated early detection tools into clinical practice to transform diagnostics and improve global management of neurodegenerative diseases.",True
Medicine,RanMerFormer: Randomized vision transformer with token merging for brain tumor classification,"Brains are the control center of the nervous system in human bodies, and brain tumor is one of the most deadly diseases. Currently, magnetic resonance imaging (MRI) is the most effective way to brain tumors early detection in clinical diagnoses due to its superior imaging quality for soft tissues. Manual analysis of brain MRI is error-prone which depends on empirical experience and the fatigue state of the radiologists to a large extent. Computer-aided diagnosis (CAD) systems are becoming more and more impactful because they can provide accurate prediction results based on medical images with advanced techniques from computer vision. Therefore, a novel CAD method for brain tumor classification named RanMerFormer is presented in this paper. A pre-trained vision transformer is used as the backbone model. Then, a merging mechanism is proposed to remove the redundant tokens in the vision transformer, which improves computing efficiency substantially. Finally, a randomized vector functional-link serves as the head in the proposed RanMerFormer, which can be trained swiftly. All the simulation results are obtained from two public benchmark datasets, which reveal that the proposed RanMerFormer can achieve state-of-the-art performance for brain tumor classification. The trained RanMerFormer can be applied in real-world scenarios to assist in brain tumor diagnosis.","['vision transformer', 'randomized vector functional-link']","The study addresses the critical challenge of early and accurate detection of brain tumors, which are among the most deadly diseases affecting the nervous system. Magnetic resonance imaging (MRI) is currently the most effective clinical tool for detecting brain tumors due to its superior imaging quality for soft tissues. However, manual analysis of brain MRI scans is prone to errors and heavily reliant on the radiologists' experience and fatigue levels, which can impact diagnostic accuracy. The primary objective of this study is to develop a method that improves the classification of brain tumors using MRI images to assist in more accurate and efficient diagnosis. This method aims to enhance diagnostic performance and can be applied in real-world clinical scenarios to support radiologists in brain tumor diagnosis.","The study addresses the critical challenge of early and accurate detection of brain tumors, which are among the most deadly diseases affecting the nervous system. Magnetic resonance imaging (MRI) is currently the most effective clinical tool for detecting brain tumors due to its superior imaging quality for soft tissues. However, manual analysis of brain MRI scans is prone to errors and heavily reliant on the radiologists' experience and fatigue levels, which can impact diagnostic accuracy. The primary objective of this study is to develop a method that improves the identification of brain tumors using MRI images to assist in more accurate and efficient diagnosis. This method aims to enhance diagnostic performance and can be applied in real-world clinical scenarios to support radiologists in brain tumor diagnosis.",True
Medicine,A hybrid deep CNN model for brain tumor image multi-classification,"Abstract The current approach to diagnosing and classifying brain tumors relies on the histological evaluation of biopsy samples, which is invasive, time-consuming, and susceptible to manual errors. These limitations underscore the pressing need for a fully automated, deep-learning-based multi-classification system for brain malignancies. This article aims to leverage a deep convolutional neural network (CNN) to enhance early detection and presents three distinct CNN models designed for different types of classification tasks. The first CNN model achieves an impressive detection accuracy of 99.53% for brain tumors. The second CNN model, with an accuracy of 93.81%, proficiently categorizes brain tumors into five distinct types: normal, glioma, meningioma, pituitary, and metastatic. Furthermore, the third CNN model demonstrates an accuracy of 98.56% in accurately classifying brain tumors into their different grades. To ensure optimal performance, a grid search optimization approach is employed to automatically fine-tune all the relevant hyperparameters of the CNN models. The utilization of large, publicly accessible clinical datasets results in robust and reliable classification outcomes. This article conducts a comprehensive comparison of the proposed models against classical models, such as AlexNet, DenseNet121, ResNet-101, VGG-19, and GoogleNet, reaffirming the superiority of the deep CNN-based approach in advancing the field of brain tumor classification and early detection.","['deep convolutional neural network (CNN)', 'AlexNet', 'DenseNet121', 'ResNet-101', 'VGG-19', 'GoogleNet']","The current approach to diagnosing and classifying brain tumors relies on the histological evaluation of biopsy samples, which is invasive, time-consuming, and susceptible to manual errors. These limitations highlight the need for improved methods that can facilitate early detection and accurate classification of brain malignancies. The primary aim of this study is to enhance early detection of brain tumors and to accurately classify them into different types and grades. This is achieved by developing and evaluating models designed for multi-classification tasks to improve the diagnosis and categorization of brain tumors.","The current approach to diagnosing and classifying brain tumors relies on the histological evaluation of biopsy samples, which is invasive, time-consuming, and susceptible to manual errors. These limitations highlight the need for improved methods that can facilitate early detection and accurate classification of brain malignancies. The primary aim of this study is to enhance early detection of brain tumors and to accurately classify them into different types and grades. This is achieved by developing and evaluating new analytical approaches designed for multi-classification tasks to improve the diagnosis and categorization of brain tumors.",True
Medicine,Artificial intelligence-driven virtual rehabilitation for people living in the community: A scoping review,"Abstract Virtual Rehabilitation (VRehab) is a promising approach to improving the physical and mental functioning of patients living in the community. The use of VRehab technology results in the generation of multi-modal datasets collected through various devices. This presents opportunities for the development of Artificial Intelligence (AI) techniques in VRehab, namely the measurement, detection, and prediction of various patients’ health outcomes. The objective of this scoping review was to explore the applications and effectiveness of incorporating AI into home-based VRehab programs. PubMed/MEDLINE, Embase, IEEE Xplore, Web of Science databases, and Google Scholar were searched from inception until June 2023 for studies that applied AI for the delivery of VRehab programs to the homes of adult patients. After screening 2172 unique titles and abstracts and 51 full-text studies, 13 studies were included in the review. A variety of AI algorithms were applied to analyze data collected from various sensors and make inferences about patients’ health outcomes, most involving evaluating patients’ exercise quality and providing feedback to patients. The AI algorithms used in the studies were mostly fuzzy rule-based methods, template matching, and deep neural networks. Despite the growing body of literature on the use of AI in VRehab, very few studies have examined its use in patients’ homes. Current research suggests that integrating AI with home-based VRehab can lead to improved rehabilitation outcomes for patients. However, further research is required to fully assess the effectiveness of various forms of AI-driven home-based VRehab, taking into account its unique challenges and using standardized metrics.","['fuzzy rule-based methods', 'deep neural networks']","The research idea centers on the potential of virtual rehabilitation (VRehab) to enhance the physical and mental functioning of patients living in the community, particularly through home-based programs. While VRehab generates diverse health-related data, there is limited exploration of its application directly in patients’ homes, despite its promise for improving rehabilitation outcomes. The study aims to address the gap in understanding the effectiveness of home-based VRehab interventions and their impact on patient health outcomes. The primary objective of this scoping review was to explore the applications and effectiveness of incorporating advanced techniques into home-based VRehab programs for adult patients. It sought to evaluate existing studies that delivered VRehab in home settings and to assess how these interventions influence patients’ exercise quality and overall rehabilitation progress.","The research idea centers on the potential of virtual rehabilitation (VRehab) to enhance the physical and mental functioning of patients living in the community, particularly through home-based programs. While VRehab generates diverse health-related data, there is limited exploration of its application directly in patients' homes, despite its promise for improving rehabilitation outcomes. The study aims to address the gap in understanding the effectiveness of home-based VRehab interventions and their impact on patient health outcomes. The primary objective of this scoping review was to explore the applications and effectiveness of incorporating innovative approaches into home-based VRehab programs for adult patients. It sought to evaluate existing studies that delivered VRehab in home settings and to assess how these interventions influence patients' exercise quality and overall rehabilitation progress.",True
Medicine,Assessing ChatGPT 4.0’s test performance and clinical diagnostic accuracy on USMLE STEP 2 CK and clinical case reports,"Abstract While there is data assessing the test performance of artificial intelligence (AI) chatbots, including the Generative Pre-trained Transformer 4.0 (GPT 4) chatbot (ChatGPT 4.0), there is scarce data on its diagnostic accuracy of clinical cases. We assessed the large language model (LLM), ChatGPT 4.0, on its ability to answer questions from the United States Medical Licensing Exam (USMLE) Step 2, as well as its ability to generate a differential diagnosis based on corresponding clinical vignettes from published case reports. A total of 109 Step 2 Clinical Knowledge (CK) practice questions were inputted into both ChatGPT 3.5 and ChatGPT 4.0, asking ChatGPT to pick the correct answer. Compared to its previous version, ChatGPT 3.5, we found improved accuracy of ChatGPT 4.0 when answering these questions, from 47.7 to 87.2% ( p = 0.035) respectively. Utilizing the topics tested on Step 2 CK questions, we additionally found 63 corresponding published case report vignettes and asked ChatGPT 4.0 to come up with its top three differential diagnosis. ChatGPT 4.0 accurately created a shortlist of differential diagnoses in 74.6% of the 63 case reports (74.6%). We analyzed ChatGPT 4.0’s confidence in its diagnosis by asking it to rank its top three differentials from most to least likely. Out of the 47 correct diagnoses, 33 were the first (70.2%) on the differential diagnosis list, 11 were second (23.4%), and three were third (6.4%). Our study shows the continued iterative improvement in ChatGPT’s ability to answer standardized USMLE questions accurately and provides insights into ChatGPT’s clinical diagnostic accuracy.","['Generative Pre-trained Transformer 4.0 (GPT 4) chatbot (ChatGPT 4.0)', 'large language model (LLM)']","The research idea centers on the limited information available regarding the diagnostic accuracy of clinical cases when assessed by advanced language-based tools, despite existing data on their test performance in medical examinations. This study addresses the need to evaluate how well such tools can generate accurate differential diagnoses based on clinical vignettes and answer medical licensing exam questions. The primary objective of the study is to assess the ability of the latest version of a language-based tool to correctly answer questions from the United States Medical Licensing Exam Step 2 Clinical Knowledge and to generate accurate differential diagnoses from published clinical case vignettes. The study aims to compare the diagnostic accuracy and performance improvements of this tool relative to its previous version in the context of clinical reasoning and medical knowledge application.","The research idea centers on the limited information available regarding the diagnostic accuracy of clinical cases when assessed by contemporary clinical reasoning systems, despite existing data on their test performance in medical examinations. This study addresses the need to evaluate how well such systems can generate accurate differential diagnoses based on clinical vignettes and answer medical licensing exam questions. The primary objective of the study is to assess the ability of the latest version of a diagnostic reasoning system to correctly answer questions from the United States Medical Licensing Exam Step 2 Clinical Knowledge and to generate accurate differential diagnoses from published clinical case vignettes. The study aims to compare the diagnostic accuracy and performance improvements of this system relative to its previous version in the context of clinical reasoning and medical knowledge application.",True
Medicine,Advanced Ensemble Machine Learning Techniques for Optimizing Diabetes Mellitus Prognostication: A Detailed Examination of Hospital Data,"Diabetes is a chronic disease that affects millions of people worldwide. Early diagnosis and effective management are crucial for reducing its complications. Diabetes is the fourth-highest cause of mortality due to its association with various comorbidities, including heart disease, nerve damage, blood vessel damage, and blindness. The potential of machine learning algorithms in predicting Diabetes and related conditions is significant, and mining diabetes data is an efficient method for extracting new insights.The primary objective of this study is to develop an enhanced ensemble model to predict Diabetes with improved accuracy by leveraging various machine learning algorithms.This study tested several popular machine learning algorithms commonly used in diabetes prediction, including Naive Bayes (NB), Generalized Linear Model (GLM), Logistic Regression (LR), Fast Large Margin (FLM), Deep Learning (DL), Decision Tree (DT), Random Forest (RF), Gradient Boosted Trees (GBT), and Support Vector Machine (SVM). The performance of these algorithms was compared, and two different ensemble techniques—stacking and voting—were used to build a more accurate predictive model.The top three algorithms based on accuracy were Deep Learning, Naive Bayes, and Gradient Boosted Trees. The machine learning algorithms revealed that individuals with Diabetes are significantly affected by the number of chronic conditions they have, as well as their gender and age. The ensemble models, particularly the stacking method, provided higher accuracy than individual algorithms. The stacking ensemble model achieved a slightly better accuracy of 99.94% compared to 99.34% for the voting method.Building an ensemble model significantly increased the accuracy of predicting Diabetes and related conditions. The stacking ensemble model, in particular, demonstrated superior performance, highlighting the importance of combining multiple machine learning approaches to enhance predictive accuracy","['Naive Bayes (NB)', 'Logistic Regression (LR)', 'Deep Learning (DL)', 'Decision Tree (DT)', 'Random Forest (RF)', 'Gradient Boosted Trees (GBT)', 'Support Vector Machine (SVM)', 'stacking ensemble', 'voting ensemble']","Diabetes is a chronic disease affecting millions worldwide and is a leading cause of mortality due to its association with various comorbidities such as heart disease, nerve damage, blood vessel damage, and blindness. Early diagnosis and effective management are essential to reduce the complications arising from diabetes. The primary objective of this study is to improve the accuracy of diabetes prediction by developing an enhanced approach that combines multiple methods. This study aims to identify the most effective strategy for predicting diabetes and related conditions to support better clinical outcomes.","Diabetes is a chronic disease affecting millions worldwide and is a leading cause of mortality due to its association with various comorbidities such as heart disease, nerve damage, blood vessel damage, and blindness. Early diagnosis and effective management are essential to reduce the complications arising from diabetes. The primary objective of this study is to improve the accuracy of diabetes prediction by developing an enhanced analytical approach that combines multiple methodologies. This study aims to identify the most effective strategy for predicting diabetes and related conditions to support better clinical outcomes.",True
Medicine,Reliability of ChatGPT for performing triage task in the emergency department using the Korean Triage and Acuity Scale,"Background Artificial intelligence (AI) technology can enable more efficient decision-making in healthcare settings. There is a growing interest in improving the speed and accuracy of AI systems in providing responses for given tasks in healthcare settings. Objective This study aimed to assess the reliability of ChatGPT in determining emergency department (ED) triage accuracy using the Korean Triage and Acuity Scale (KTAS). Methods Two hundred and two virtual patient cases were built. The gold standard triage classification for each case was established by an experienced ED physician. Three other human raters (ED paramedics) were involved and rated the virtual cases individually. The virtual cases were also rated by two different versions of the chat generative pre-trained transformer (ChatGPT, 3.5 and 4.0). Inter-rater reliability was examined using Fleiss’ kappa and intra-class correlation coefficient (ICC). Results The kappa values for the agreement between the four human raters and ChatGPTs were .523 (version 4.0) and .320 (version 3.5). Of the five levels, the performance was poor when rating patients at levels 1 and 5, as well as case scenarios with additional text descriptions. There were differences in the accuracy of the different versions of GPTs. The ICC between version 3.5 and the gold standard was .520, and that between version 4.0 and the gold standard was .802. Conclusions A substantial level of inter-rater reliability was revealed when GPTs were used as KTAS raters. The current study showed the potential of using GPT in emergency healthcare settings. Considering the shortage of experienced manpower, this AI method may help improve triaging accuracy.","['chat generative pre-trained transformer (ChatGPT, 3.5 and 4.0)']","The research addresses the challenge of accurately and efficiently determining triage levels in emergency departments, which is critical for prioritizing patient care and managing limited healthcare resources. There is a need to improve the speed and reliability of triage decisions to enhance patient outcomes and support healthcare providers, especially given the shortage of experienced personnel. The primary aim of the study was to assess the reliability of ChatGPT in determining emergency department triage accuracy using the Korean Triage and Acuity Scale (KTAS). Specifically, the study sought to compare the triage classifications made by ChatGPT with those made by experienced emergency department physicians and paramedics to evaluate its potential role in emergency healthcare settings.","The research addresses the challenge of accurately and efficiently determining triage levels in emergency departments, which is critical for prioritizing patient care and managing limited healthcare resources. There is a need to improve the speed and reliability of triage decisions to enhance patient outcomes and support healthcare providers, especially given the shortage of experienced personnel. The primary aim of the study was to assess the reliability of alternative assessment methods in determining emergency department triage accuracy using the Korean Triage and Acuity Scale (KTAS). Specifically, the study sought to compare the triage classifications made through these new approaches with those made by experienced emergency department physicians and paramedics to evaluate their potential role in emergency healthcare settings.",True
Medicine,Assessing generalisability of deep learning-based polyp detection and segmentation methods through a computer vision challenge,"Abstract Polyps are well-known cancer precursors identified by colonoscopy. However, variability in their size, appearance, and location makes the detection of polyps challenging. Moreover, colonoscopy surveillance and removal of polyps are highly operator-dependent procedures and occur in a highly complex organ topology. There exists a high missed detection rate and incomplete removal of colonic polyps. To assist in clinical procedures and reduce missed rates, automated methods for detecting and segmenting polyps using machine learning have been achieved in past years. However, the major drawback in most of these methods is their ability to generalise to out-of-sample unseen datasets from different centres, populations, modalities, and acquisition systems. To test this hypothesis rigorously, we, together with expert gastroenterologists, curated a multi-centre and multi-population dataset acquired from six different colonoscopy systems and challenged the computational expert teams to develop robust automated detection and segmentation methods in a crowd-sourcing Endoscopic computer vision challenge. This work put forward rigorous generalisability tests and assesses the usability of devised deep learning methods in dynamic and actual clinical colonoscopy procedures. We analyse the results of four top performing teams for the detection task and five top performing teams for the segmentation task. Our analyses demonstrate that the top-ranking teams concentrated mainly on accuracy over the real-time performance required for clinical applicability. We further dissect the devised methods and provide an experiment-based hypothesis that reveals the need for improved generalisability to tackle diversity present in multi-centre datasets and routine clinical procedures.","['machine learning', 'deep learning']","The research idea addresses the challenge of detecting colonic polyps during colonoscopy, which is complicated by variability in polyp size, appearance, and location, as well as the highly operator-dependent nature of colonoscopy procedures within a complex organ topology. There is a significant issue with high missed detection rates and incomplete removal of polyps, which are precursors to cancer, highlighting the need for improved detection methods that can perform reliably across diverse clinical settings. The primary objective of the study is to rigorously evaluate the generalisability and clinical applicability of automated polyp detection and segmentation methods across multi-centre and multi-population datasets obtained from different colonoscopy systems. This evaluation aims to identify the limitations of current approaches in real-time clinical procedures and to provide insights for enhancing the robustness of polyp detection techniques in routine colonoscopy practice.","The research idea addresses the challenge of detecting colonic polyps during colonoscopy, which is complicated by variability in polyp size, appearance, and location, as well as the highly operator-dependent nature of colonoscopy procedures within a complex organ topology. There is a significant issue with high missed detection rates and incomplete removal of polyps, which are precursors to cancer, highlighting the need for improved detection methods that can perform reliably across diverse clinical settings. The primary objective of the study is to rigorously evaluate the generalisability and clinical applicability of medical imaging analysis for polyp detection and segmentation across multi-centre and multi-population datasets obtained from different colonoscopy systems. This evaluation aims to identify the limitations of current approaches in real-time clinical procedures and to provide insights for enhancing the robustness of polyp detection techniques in routine colonoscopy practice.",True
Medicine,A comparative study of explainable ensemble learning and logistic regression for predicting in-hospital mortality in the emergency department,"Abstract This study addresses the challenges associated with emergency department (ED) overcrowding and emphasizes the need for efficient risk stratification tools to identify high-risk patients for early intervention. While several scoring systems, often based on logistic regression (LR) models, have been proposed to indicate patient illness severity, this study aims to compare the predictive performance of ensemble learning (EL) models with LR for in-hospital mortality in the ED. A cross-sectional single-center study was conducted at the ED of Imam Reza Hospital in northeast Iran from March 2016 to March 2017. The study included adult patients with one to three levels of emergency severity index. EL models using Bagging, AdaBoost, random forests (RF), Stacking and extreme gradient boosting (XGB) algorithms, along with an LR model, were constructed. The training and validation visits from the ED were randomly divided into 80% and 20%, respectively. After training the proposed models using tenfold cross-validation, their predictive performance was evaluated. Model performance was compared using the Brier score (BS), The area under the receiver operating characteristics curve (AUROC), The area and precision–recall curve (AUCPR), Hosmer–Lemeshow (H–L) goodness-of-fit test, precision, sensitivity, accuracy, F1-score, and Matthews correlation coefficient (MCC). The study included 2025 unique patients admitted to the hospital’s ED, with a total percentage of hospital deaths at approximately 19%. In the training group and the validation group, 274 of 1476 (18.6%) and 152 of 728 (20.8%) patients died during hospitalization, respectively. According to the evaluation of the presented framework, EL models, particularly Bagging, predicted in-hospital mortality with the highest AUROC (0.839, CI (0.802–0.875)) and AUCPR = 0.64 comparable in terms of discrimination power with LR (AUROC (0.826, CI (0.787–0.864)) and AUCPR = 0.61). XGB achieved the highest precision (0.83), sensitivity (0.831), accuracy (0.842), F1-score (0.833), and the highest MCC (0.48). Additionally, the most accurate models in the unbalanced dataset belonged to RF with the lowest BS (0.128). Although all studied models overestimate mortality risk and have insufficient calibration ( P &gt; 0.05), stacking demonstrated relatively good agreement between predicted and actual mortality. EL models are not superior to LR in predicting in-hospital mortality in the ED. Both EL and LR models can be considered as screening tools to identify patients at risk of mortality.","['logistic regression (LR)', 'ensemble learning (EL)', 'Bagging', 'AdaBoost', 'random forests (RF)', 'Stacking', 'extreme gradient boosting (XGB)']","The study addresses the challenges of overcrowding in emergency departments and highlights the necessity for effective tools to stratify risk and identify high-risk patients for timely intervention. Existing scoring systems based on logistic regression have been used to indicate patient illness severity, but there is a need to evaluate alternative approaches for predicting in-hospital mortality in the emergency department. The primary objective of the study is to compare the predictive performance of different models for in-hospital mortality among adult patients admitted to the emergency department. The study aims to determine whether alternative approaches offer superior accuracy compared to logistic regression in identifying patients at risk of mortality during hospitalization.","The study addresses the challenges of overcrowding in emergency departments and highlights the necessity for effective tools to stratify risk and identify high-risk patients for timely intervention. Existing scoring systems based on statistical methods have been used to indicate patient illness severity, but there is a need to evaluate alternative approaches for predicting in-hospital mortality in the emergency department. The primary objective of the study is to compare the predictive performance of different analytical methods for in-hospital mortality among adult patients admitted to the emergency department. The study aims to determine whether alternative approaches offer superior accuracy compared to traditional statistical analysis in identifying patients at risk of mortality during hospitalization.",True
Medicine,"Prediction of atmospheric PM2.5 level by machine learning techniques in Isfahan, Iran","Abstract With increasing levels of air pollution, air quality prediction has attracted more attention. Mathematical models are being developed by researchers to achieve precise predictions. Monitoring and prediction of atmospheric PM 2.5 levels, as a predominant pollutant, is essential in emission mitigation programs. In this study, meteorological datasets from 9 years in Isfahan city, a large metropolis of Iran, were applied to predict the PM 2.5 levels, using four machine learning algorithms including Artificial Neural |Networks (ANNs), K-Nearest-Neighbors (KNN), Support Vector |Machines (SVMs) and ensembles of classification trees Random Forest (RF). The data from 7 air quality monitoring stations located in Isfahan City were taken into consideration. The Confusion Matrix and Cross-Entropy Loss were used to analyze the performance of classification models. Several parameters, including sensitivity, specificity, accuracy, F1 score, precision, and the area under the curve (AUC), are computed to assess model performance. Finally, by introducing the predicted data for 2020 into ArcGIS software and using the IDW (Inverse Distance Weighting) method, interpolation was conducted for the area of Isfahan city and the pollution map was illustrated for each month of the year. The results showed that, based on the accuracy percentage, the ANN model has a better performance (90.1%) in predicting PM 2.5 grades compared to the other models for the applied meteorological dataset, followed by RF (86.1%), SVM (84.6%) and KNN (82.2%) models, respectively. Therefore, ANN modelling provides a feasible procedure for the managerial planning of air pollution control.","['Artificial Neural Networks (ANNs)', 'K-Nearest-Neighbors (KNN)', 'Support Vector Machines (SVMs)', 'Random Forest (RF)']","The study addresses the growing concern of air pollution and the critical need for accurate prediction of atmospheric PM 2.5 levels, which are a predominant pollutant affecting public health and environmental quality. Monitoring and predicting these pollutant levels are essential components of emission mitigation programs, especially in large urban areas like Isfahan city, Iran. The primary aim of the study is to predict PM 2.5 levels in Isfahan city using meteorological data collected over nine years from multiple air quality monitoring stations. This prediction is intended to support effective managerial planning and control of air pollution in the region.","The study addresses the growing concern of air pollution and the critical need for accurate prediction of atmospheric PM 2.5 levels, which are a predominant pollutant affecting public health and environmental quality. Monitoring and predicting these pollutant levels are essential components of emission mitigation programs, especially in large urban areas like Isfahan city, Iran. The primary aim of the study is to forecast PM 2.5 levels in Isfahan city using meteorological data collected over nine years from multiple air quality monitoring stations. This forecasting work is intended to support effective managerial planning and control of air pollution in the region.",True
Medicine,Machine Learning–Based Prediction of Suicidality in Adolescents With Allergic Rhinitis: Derivation and Validation in 2 Independent Nationwide Cohorts,"Background Given the additional risk of suicide-related behaviors in adolescents with allergic rhinitis (AR), it is important to use the growing field of machine learning (ML) to evaluate this risk. Objective This study aims to evaluate the validity and usefulness of an ML model for predicting suicide risk in patients with AR. Methods We used data from 2 independent survey studies, Korea Youth Risk Behavior Web-based Survey (KYRBS; n=299,468) for the original data set and Korea National Health and Nutrition Examination Survey (KNHANES; n=833) for the external validation data set, to predict suicide risks of AR in adolescents aged 13 to 18 years, with 3.45% (10,341/299,468) and 1.4% (12/833) of the patients attempting suicide in the KYRBS and KNHANES studies, respectively. The outcome of interest was the suicide attempt risks. We selected various ML-based models with hyperparameter tuning in the discovery and performed an area under the receiver operating characteristic curve (AUROC) analysis in the train, test, and external validation data. Results The study data set included 299,468 (KYRBS; original data set) and 833 (KNHANES; external validation data set) patients with AR recruited between 2005 and 2022. The best-performing ML model was the random forest model with a mean AUROC of 84.12% (95% CI 83.98%-84.27%) in the original data set. Applying this result to the external validation data set revealed the best performance among the models, with an AUROC of 89.87% (sensitivity 83.33%, specificity 82.58%, accuracy 82.59%, and balanced accuracy 82.96%). While looking at feature importance, the 5 most important features in predicting suicide attempts in adolescent patients with AR are depression, stress status, academic achievement, age, and alcohol consumption. Conclusions This study emphasizes the potential of ML models in predicting suicide risks in patients with AR, encouraging further application of these models in other conditions to enhance adolescent health and decrease suicide rates.",['random forest'],"The research idea addresses the increased risk of suicide-related behaviors in adolescents with allergic rhinitis (AR), highlighting the importance of evaluating this risk to improve adolescent health outcomes. Given the significant impact of suicide attempts among this population, there is a critical need to identify factors that contribute to suicide risk in patients with AR. The study’s primary objective is to evaluate the validity and usefulness of a predictive approach for assessing suicide risk in adolescents aged 13 to 18 years with AR. Specifically, the study aims to determine how well this approach can identify individuals at risk of suicide attempts, thereby supporting efforts to reduce suicide rates in this vulnerable group.","The research idea addresses the increased risk of suicide-related behaviors in adolescents with allergic rhinitis (AR), highlighting the importance of evaluating this risk to improve adolescent health outcomes. Given the significant impact of suicide attempts among this population, there is a critical need to identify factors that contribute to suicide risk in patients with AR. The study's primary objective is to evaluate the validity and usefulness of a risk assessment method for identifying suicide risk in adolescents aged 13 to 18 years with AR. Specifically, the study aims to determine how effectively this approach can identify individuals at risk of suicide attempts, thereby supporting efforts to reduce suicide rates in this vulnerable group.",True
Medicine,"Developing Deep LSTMs With Later Temporal Attention for Predicting COVID-19 Severity, Clinical Outcome, and Antibody Level by Screening Serological Indicators Over Time","Objective: The clinical course of COVID-19, as well as the immunological reaction, is notable for its extreme variability. Identifying the main associated factors might help understand the disease progression and physiological status of COVID-19 patients. The dynamic changes of the antibody against Spike protein are crucial for understanding the immune response. This work explores a temporal attention (TA) mechanism of deep learning to predict COVID-19 disease severity, clinical outcomes, and Spike antibody levels by screening serological indicators over time. Methods: We use feature selection techniques to filter feature subsets that are highly correlated with the target. The specific deep Long Short-Term Memory (LSTM) models are employed to capture the dynamic changes of disease severity, clinical outcome, and Spike antibody level. We also propose deep LSTMs with a TA mechanism to emphasize the later blood test records because later records often attract more attention from doctors. Results: Risk factors highly correlated with COVID-19 are revealed. LSTM achieves the highest classification accuracy for disease severity prediction. Temporal Attention Long Short-Term Memory (TA-LSTM) achieves the best performance for clinical outcome prediction. For Spike antibody level prediction, LSTM achieves the best permanence. Conclusion: The experimental results demonstrate the effectiveness of the proposed models. The proposed models can provide a computer-aided medical diagnostics system by simply using time series of serological indicators.","['deep Long Short-Term Memory (LSTM) models', 'deep LSTMs with a temporal attention (TA) mechanism', 'Temporal Attention Long Short-Term Memory (TA-LSTM)']","The clinical course of COVID-19 and the immunological response exhibit significant variability among patients, making it challenging to understand disease progression and physiological status. Identifying the main factors associated with COVID-19 severity and immune response is essential for improving knowledge of the disease. The dynamic changes in antibodies against the Spike protein are particularly important for understanding the immune response in COVID-19 patients. This study aims to predict COVID-19 disease severity, clinical outcomes, and Spike antibody levels by examining temporal changes in serological indicators, with the goal of better understanding the progression and immune status of affected individuals.","The clinical course of COVID-19 and the immunological response exhibit significant variability among patients, making it challenging to understand disease progression and physiological status. Identifying the main factors associated with COVID-19 severity and immune response is essential for improving knowledge of the disease. The dynamic changes in antibodies against the Spike protein are particularly important for understanding the immune response in COVID-19 patients. This study aims to examine COVID-19 disease severity, clinical outcomes, and Spike antibody levels by investigating temporal changes in serological indicators, with the goal of better understanding the progression and immune status of affected individuals.",True
Medicine,Development and Validation of a Machine Learning Model to Predict Weekly Risk of Hypoglycemia in Patients with Type 1 Diabetes Based on Continuous Glucose Monitoring,"Aim: The aim of this study was to develop and validate a prediction model based on CGM data to identify a week-to-week risk profile of excessive hypoglycemia. Methods: We analyzed, trained, and internally tested two prediction models using CGM data from 205 type 1 diabetes patients with long-term CGM monitoring. A binary classification approach (XGBoost) combined with feature engineering deployed on the CGM signals was utilized to predict excessive hypoglycemia risk defined by two targets (TBR > 4% and the upper TBR 90th percentile limit) of time below range (TBR) the following week. The models were validated in two independent cohorts with a total of 253 additional patients. Results: A total of 61,470 weeks of CGM data were included in the analysis. The XGBoost models had a ROC-AUC of 0.83-0.87 (95% confidence interval [CI]; 0.83-0.88) in the test dataset. The external validation showed ROC-AUCs of 0.81-0.90. The most discriminative features included the low blood glucose index (LBGI), the glycemic risk assessment diabetes equation (GRADE), hypoglycemia, the TBR, waveform length, the CV and mean glucose during the previous week. This highlights that the pattern of hypoglycemia combined with glucose variability during the past week contains information on the risk of future hypoglycemia. Conclusion: Prediction models based on real-world CGM data can be used to predict the risk of hypoglycemia in the forthcoming week. The models showed good performance in both the internal and external validation cohorts.",['XGBoost'],"The research idea addresses the challenge of identifying patients with type 1 diabetes who are at risk of experiencing excessive hypoglycemia in the upcoming week. Hypoglycemia poses significant health risks, and understanding the week-to-week risk profile is crucial for improving patient safety and management. The study focuses on utilizing continuous glucose monitoring data to capture patterns that may indicate future hypoglycemia events. The primary objective of the study was to develop and validate a method to predict the risk of excessive hypoglycemia in the following week among type 1 diabetes patients. This involved assessing specific targets related to time spent below the glucose range and evaluating the predictive performance in both internal and external patient cohorts to ensure reliability and applicability.","The research idea addresses the challenge of identifying patients with type 1 diabetes who are at risk of experiencing excessive hypoglycemia in the upcoming week. Hypoglycemia poses significant health risks, and understanding the week-to-week risk profile is crucial for improving patient safety and management. The study focuses on utilizing continuous glucose monitoring data to capture patterns that may indicate future hypoglycemia events. The primary objective of the study was to develop and validate a method to forecast the risk of excessive hypoglycemia in the following week among type 1 diabetes patients. This involved assessing specific targets related to time spent below the glucose range and evaluating the forecast performance in both internal and external patient cohorts to ensure reliability and applicability.",True
Medicine,Performance of convolutional neural networks for the classification of brain tumors using magnetic resonance imaging,"Brain tumors are a diverse group of neoplasms that are challenging to detect and classify due to their varying characteristics. Deep learning techniques have proven to be effective in tumor classification. However, there is a lack of studies that compare these techniques using a common methodology. This work aims to analyze the performance of convolutional neural networks in the classification of brain tumors. We propose a network consisting of a few convolutional layers, batch normalization, and max-pooling. Then, we explore recent deep architectures, such as VGG, ResNet, EfficientNet, or ConvNeXt. The study relies on two magnetic resonance imaging datasets with over 3000 images of three types of tumors –gliomas, meningiomas, and pituitary tumors–, as well as images without tumors. We determine the optimal hyperparameters of the networks using the training and validation sets. The training and test sets are used to assess the performance of the models from different perspectives, including training from scratch, data augmentation, transfer learning, and fine-tuning. The experiments are performed using the TensorFlow and Keras libraries in Python. We compare the accuracy of the models and analyze their complexity based on the capacity of the networks, their training times, and image throughput. Several networks achieve high accuracy rates on both datasets, with the best model achieving 98.7% accuracy, which is on par with state-of-the-art methods. The average precision for each type of tumor is 94.3% for gliomas, 93.8% for meningiomas, 97.9% for pituitary tumors, and 95.3% for images without tumors. VGG is the largest model with over 171 million parameters, whereas MobileNet and EfficientNetB0 are the smallest ones with 3.2 and 5.9 million parameters, respectively. These two neural networks are also the fastest to train with 23.7 and 25.4 seconds per epoch, respectively. On the other hand, ConvNext is the slowest model with 58.2 seconds per epoch. Our custom model obtained the highest image throughput with 234.37 images per second, followed by MobileNet with 226 images per second. ConvNext obtained the smallest throughput with 97.35 images per second. ResNet, MobileNet, and EfficientNet are the most accurate networks, with MobileNet and EfficientNet demonstrating superior performance in terms of complexity. Most models achieve the best accuracy using transfer learning followed by a fine-tuning step. However, data augmentation does not contribute to increasing the accuracy of the models in general.","['convolutional neural networks', 'batch normalization', 'max-pooling', 'VGG', 'ResNet', 'EfficientNet', 'ConvNeXt', 'transfer learning', 'fine-tuning']","The study addresses the challenge of detecting and classifying brain tumors, which are a diverse group of neoplasms with varying characteristics that complicate accurate diagnosis. There is a need for a comprehensive comparison of different approaches to brain tumor classification using a consistent methodology. The primary aim of the study is to evaluate and compare the performance of various classification methods on magnetic resonance imaging datasets containing images of gliomas, meningiomas, pituitary tumors, and non-tumor cases. The objective is to determine the effectiveness and accuracy of these methods in correctly identifying and classifying different types of brain tumors.","The study addresses the challenge of detecting and classifying brain tumors, which are a diverse group of neoplasms with varying characteristics that complicate accurate diagnosis. There is a need for a comprehensive comparison of different approaches to brain tumor classification using a consistent methodology. The primary aim of the study is to evaluate and compare the performance of various analytical methods on magnetic resonance imaging datasets containing images of gliomas, meningiomas, pituitary tumors, and non-tumor cases. The objective is to determine the effectiveness and accuracy of these methods in correctly identifying and classifying different types of brain tumors.",True
Medicine,Predictors for estimating subcortical EEG responses to continuous speech,"Perception of sounds and speech involves structures in the auditory brainstem that rapidly process ongoing auditory stimuli. The role of these structures in speech processing can be investigated by measuring their electrical activity using scalp-mounted electrodes. However, typical analysis methods involve averaging neural responses to many short repetitive stimuli that bear little relevance to daily listening environments. Recently, subcortical responses to more ecologically relevant continuous speech were detected using linear encoding models. These methods estimate the temporal response function (TRF), which is a regression model that minimises the error between the measured neural signal and a predictor derived from the stimulus. Using predictors that model the highly non-linear peripheral auditory system may improve linear TRF estimation accuracy and peak detection. Here, we compare predictors from both simple and complex peripheral auditory models for estimating brainstem TRFs on electroencephalography (EEG) data from 24 participants listening to continuous speech. We also investigate the data length required for estimating subcortical TRFs, and find that around 12 minutes of data is sufficient for clear wave V peaks (&gt;3 dB SNR) to be seen in nearly all participants. Interestingly, predictors derived from simple filterbank-based models of the peripheral auditory system yield TRF wave V peak SNRs that are not significantly different from those estimated using a complex model of the auditory nerve, provided that the nonlinear effects of adaptation in the auditory system are appropriately modelled. Crucially, computing predictors from these simpler models is more than 50 times faster compared to the complex model. This work paves the way for efficient modelling and detection of subcortical processing of continuous speech, which may lead to improved diagnosis metrics for hearing impairment and assistive hearing technology.","['linear encoding models', 'temporal response function (TRF)', 'regression model']","The perception of sounds and speech relies on structures in the auditory brainstem that rapidly process ongoing auditory stimuli, yet typical methods to study these responses use repetitive stimuli that do not reflect everyday listening environments. Understanding how these brainstem structures respond to continuous, ecologically relevant speech is important for advancing knowledge of auditory processing. The primary aim of this study is to compare different models of the peripheral auditory system in estimating brainstem responses to continuous speech, and to determine the amount of data needed to reliably detect key neural response peaks. This research seeks to improve the accuracy and efficiency of measuring subcortical processing of speech, which could enhance diagnostic approaches for hearing impairment and the development of assistive hearing technologies.","The perception of sounds and speech relies on structures in the auditory brainstem that rapidly process ongoing auditory stimuli, yet typical methods to study these responses use repetitive stimuli that do not reflect everyday listening environments. Understanding how these brainstem structures respond to continuous, ecologically relevant speech is important for advancing knowledge of auditory processing. The primary aim of this study is to compare different theoretical frameworks of the peripheral auditory system in estimating brainstem responses to continuous speech, and to determine the amount of data needed to reliably detect key neural response peaks. This research seeks to improve the accuracy and efficiency of measuring subcortical processing of speech, which could enhance diagnostic approaches for hearing impairment and the development of assistive hearing technologies.",True
Medicine,"Comparative performance analysis of Boruta, SHAP, and Borutashap for disease diagnosis: A study with multiple machine learning algorithms","Interpretable machine learning models are instrumental in disease diagnosis and clinical decision-making, shedding light on relevant features. Notably, Boruta, SHAP (SHapley Additive exPlanations), and BorutaShap were employed for feature selection, each contributing to the identification of crucial features. These selected features were then utilized to train six machine learning algorithms, including LR, SVM, ETC, AdaBoost, RF, and LR, using diverse medical datasets obtained from public sources after rigorous preprocessing. The performance of each feature selection technique was evaluated across multiple ML models, assessing accuracy, precision, recall, and F1-score metrics. Among these, SHAP showcased superior performance, achieving average accuracies of 80.17%, 85.13%, 90.00%, and 99.55% across diabetes, cardiovascular, statlog, and thyroid disease datasets, respectively. Notably, the LGBM emerged as the most effective algorithm, boasting an average accuracy of 91.00% for most disease states. Moreover, SHAP enhanced the interpretability of the models, providing valuable insights into the underlying mechanisms driving disease diagnosis. This comprehensive study contributes significant insights into feature selection techniques and machine learning algorithms for disease diagnosis, benefiting researchers and practitioners in the medical field. Further exploration of feature selection methods and algorithms holds promise for advancing disease diagnosis methodologies, paving the way for more accurate and interpretable diagnostic models.","['Boruta', 'SHAP (SHapley Additive exPlanations)', 'LR', 'SVM', 'AdaBoost', 'RF', 'LGBM']","The research idea centers on improving disease diagnosis and clinical decision-making by identifying the most relevant features that contribute to accurate diagnosis across various medical conditions. The study addresses the need for enhanced interpretability and effectiveness in selecting crucial features from medical datasets to better understand the underlying mechanisms of diseases such as diabetes, cardiovascular disease, statlog, and thyroid disorders. The primary objective of the study is to evaluate and compare different feature selection techniques in their ability to identify important features that improve diagnostic accuracy across multiple disease datasets. Additionally, the study aims to determine which approach provides the most reliable and interpretable insights to support more accurate disease diagnosis and benefit medical researchers and practitioners.","The research idea centers on improving disease diagnosis and clinical decision-making by identifying the most relevant factors that contribute to accurate diagnosis across various medical conditions. The study addresses the need for enhanced interpretability and effectiveness in selecting crucial indicators from medical datasets to better understand the underlying mechanisms of diseases such as diabetes, cardiovascular disease, statlog, and thyroid disorders. The primary objective of the study is to evaluate and compare different variable prioritization methods in their ability to identify important indicators that improve diagnostic accuracy across multiple disease datasets. Additionally, the study aims to determine which approach provides the most reliable and interpretable insights to support more accurate disease diagnosis and benefit medical researchers and practitioners.",True
Medicine,Distilling large language models for matching patients to clinical trials,"Abstract Objective The objective of this study is to systematically examine the efficacy of both proprietary (GPT-3.5, GPT-4) and open-source large language models (LLMs) (LLAMA 7B, 13B, 70B) in the context of matching patients to clinical trials in healthcare. Materials and methods The study employs a multifaceted evaluation framework, incorporating extensive automated and human-centric assessments along with a detailed error analysis for each model, and assesses LLMs’ capabilities in analyzing patient eligibility against clinical trial’s inclusion and exclusion criteria. To improve the adaptability of open-source LLMs, a specialized synthetic dataset was created using GPT-4, facilitating effective fine-tuning under constrained data conditions. Results The findings indicate that open-source LLMs, when fine-tuned on this limited and synthetic dataset, achieve performance parity with their proprietary counterparts, such as GPT-3.5. Discussion This study highlights the recent success of LLMs in the high-stakes domain of healthcare, specifically in patient-trial matching. The research demonstrates the potential of open-source models to match the performance of proprietary models when fine-tuned appropriately, addressing challenges like cost, privacy, and reproducibility concerns associated with closed-source proprietary LLMs. Conclusion The study underscores the opportunity for open-source LLMs in patient-trial matching. To encourage further research and applications in this field, the annotated evaluation dataset and the fine-tuned LLM, Trial-LLAMA, are released for public use.","['GPT-3.5', 'GPT-4', 'LLAMA 7B', 'LLAMA 13B', 'LLAMA 70B', 'fine-tuning']","The research idea centers on addressing the challenge of effectively matching patients to appropriate clinical trials, which is a critical step in advancing personalized healthcare and improving patient outcomes. This process requires accurate assessment of patient eligibility based on clinical trial inclusion and exclusion criteria, a task that is complex and resource-intensive. The study is motivated by the need to explore solutions that can enhance the efficiency and accuracy of patient-trial matching while considering issues such as cost, privacy, and reproducibility in healthcare settings. The primary objective of the study is to systematically examine the efficacy of various language models in matching patients to clinical trials by evaluating their ability to analyze patient eligibility criteria. The study aims to determine whether open-source models can achieve comparable performance to proprietary models in this context, thereby offering viable alternatives for patient-trial matching in healthcare.","The research idea centers on addressing the challenge of effectively matching patients to appropriate clinical trials, which is a critical step in advancing personalized healthcare and improving patient outcomes. This process requires accurate assessment of patient eligibility based on clinical trial inclusion and exclusion criteria, a task that is complex and resource-intensive. The study is motivated by the need to explore solutions that can enhance the efficiency and accuracy of patient-trial matching while considering issues such as cost, privacy, and reproducibility in healthcare settings. The primary objective of the study is to systematically examine different approaches for matching patients to clinical trials by evaluating their ability to analyze patient eligibility criteria. The study aims to determine whether open-source tools can achieve comparable performance to proprietary solutions in this context, thereby offering viable alternatives for patient-trial matching in healthcare.",True
Medicine,Auto-detection of the coronavirus disease by using deep convolutional neural networks and X-ray photographs,"Abstract The most widely used method for detecting Coronavirus Disease 2019 (COVID-19) is real-time polymerase chain reaction. However, this method has several drawbacks, including high cost, lengthy turnaround time for results, and the potential for false-negative results due to limited sensitivity. To address these issues, additional technologies such as computed tomography (CT) or X-rays have been employed for diagnosing the disease. Chest X-rays are more commonly used than CT scans due to the widespread availability of X-ray machines, lower ionizing radiation, and lower cost of equipment. COVID-19 presents certain radiological biomarkers that can be observed through chest X-rays, making it necessary for radiologists to manually search for these biomarkers. However, this process is time-consuming and prone to errors. Therefore, there is a critical need to develop an automated system for evaluating chest X-rays. Deep learning techniques can be employed to expedite this process. In this study, a deep learning-based method called Custom Convolutional Neural Network (Custom-CNN) is proposed for identifying COVID-19 infection in chest X-rays. The Custom-CNN model consists of eight weighted layers and utilizes strategies like dropout and batch normalization to enhance performance and reduce overfitting. The proposed approach achieved a classification accuracy of 98.19% and aims to accurately classify COVID-19, normal, and pneumonia samples.","['Deep learning', 'dropout', 'batch normalization']","The research addresses the limitations of the current standard method for detecting COVID-19, real-time polymerase chain reaction, which includes high cost, long turnaround times, and the possibility of false-negative results due to limited sensitivity. To overcome these challenges, chest X-rays have been utilized as a diagnostic tool because they are more accessible, involve lower radiation exposure, and are less expensive compared to CT scans. However, identifying COVID-19-related radiological biomarkers on chest X-rays requires manual examination by radiologists, which is time-consuming and susceptible to errors. The study’s primary objective is to develop an approach that can accurately classify COVID-19, normal, and pneumonia cases using chest X-rays, thereby improving the efficiency and reliability of diagnosis.","The research addresses the limitations of the current standard method for detecting COVID-19, real-time polymerase chain reaction, which includes high cost, long turnaround times, and the possibility of false-negative results due to limited sensitivity. To overcome these challenges, chest X-rays have been utilized as a diagnostic tool because they are more accessible, involve lower radiation exposure, and are less expensive compared to CT scans. However, identifying COVID-19-related radiological biomarkers on chest X-rays requires manual examination by radiologists, which is time-consuming and susceptible to errors. The study's primary objective is to develop a method that can accurately distinguish between COVID-19, normal, and pneumonia cases using chest X-rays, thereby improving the efficiency and reliability of diagnosis.",True
Medicine,GAN-based generation of realistic 3D volumetric data: A systematic review and taxonomy,"With the massive proliferation of data-driven algorithms, such as deep learning-based approaches, the availability of high-quality data is of great interest. Volumetric data is very important in medicine, as it ranges from disease diagnoses to therapy monitoring. When the dataset is sufficient, models can be trained to help doctors with these tasks. Unfortunately, there are scenarios where large amounts of data is unavailable. For example, rare diseases and privacy issues can lead to restricted data availability. In non-medical fields, the high cost of obtaining enough high-quality data can also be a concern. A solution to these problems can be the generation of realistic synthetic data using Generative Adversarial Networks (GANs). The existence of these mechanisms is a good asset, especially in healthcare, as the data must be of good quality, realistic, and without privacy issues. Therefore, most of the publications on volumetric GANs are within the medical domain. In this review, we provide a summary of works that generate realistic volumetric synthetic data using GANs. We therefore outline GAN-based methods in these areas with common architectures, loss functions and evaluation metrics, including their advantages and disadvantages. We present a novel taxonomy, evaluations, challenges, and research opportunities to provide a holistic overview of the current state of volumetric GANs.","['Generative Adversarial Networks (GANs)', 'GAN-based methods']","The research idea centers on the critical importance of high-quality volumetric data in medicine, which is essential for tasks ranging from disease diagnosis to therapy monitoring. However, the availability of sufficient medical data is often limited due to factors such as the rarity of certain diseases and privacy concerns. This scarcity poses significant challenges for medical research and clinical applications that rely on comprehensive datasets. The study aims to address these challenges by exploring approaches to generate realistic synthetic volumetric data that can supplement limited real-world data while maintaining quality and privacy standards.","The research idea centers on the critical importance of high-quality volumetric data in medicine, which is essential for tasks ranging from disease diagnosis to therapy monitoring. However, the availability of sufficient medical data is often limited due to factors such as the rarity of certain diseases and privacy concerns. This scarcity poses significant challenges for medical research and clinical applications that rely on comprehensive datasets. The study aims to address these challenges by exploring approaches to create realistic synthetic volumetric data that can supplement limited real-world data while maintaining quality and privacy standards.",True
Medicine,"Artificial intelligence in lung cancer screening: Detection, classification, prediction, and prognosis","Abstract Background The exceptional capabilities of artificial intelligence (AI) in extracting image information and processing complex models have led to its recognition across various medical fields. With the continuous evolution of AI technologies based on deep learning, particularly the advent of convolutional neural networks (CNNs), AI presents an expanded horizon of applications in lung cancer screening, including lung segmentation, nodule detection, false‐positive reduction, nodule classification, and prognosis. Methodology This review initially analyzes the current status of AI technologies. It then explores the applications of AI in lung cancer screening, including lung segmentation, nodule detection, and classification, and assesses the potential of AI in enhancing the sensitivity of nodule detection and reducing false‐positive rates. Finally, it addresses the challenges and future directions of AI in lung cancer screening. Results AI holds substantial prospects in lung cancer screening. It demonstrates significant potential in improving nodule detection sensitivity, reducing false‐positive rates, and classifying nodules, while also showing value in predicting nodule growth and pathological/genetic typing. Conclusions AI offers a promising supportive approach to lung cancer screening, presenting considerable potential in enhancing nodule detection sensitivity, reducing false‐positive rates, and classifying nodules. However, the universality and interpretability of AI results need further enhancement. Future research should focus on the large‐scale validation of new deep learning‐based algorithms and multi‐center studies to improve the efficacy of AI in lung cancer screening.","['deep learning', 'convolutional neural networks (CNNs)']","The research idea centers on the need to improve lung cancer screening by enhancing the detection and classification of lung nodules, as well as reducing false-positive rates, which are critical challenges in early diagnosis and prognosis. Lung cancer screening currently faces limitations in sensitivity and accuracy, and there is a growing interest in methods that can better identify and characterize nodules to support clinical decision-making. The study’s primary objective is to evaluate the current advancements in lung cancer screening techniques aimed at improving nodule detection sensitivity, reducing false-positive findings, and accurately classifying nodules. Additionally, the study seeks to explore the potential for predicting nodule growth and pathological or genetic characteristics to enhance screening outcomes and guide future research directions.","The research idea centers on the need to improve lung cancer screening by enhancing the detection and classification of lung nodules, as well as reducing false-positive rates, which are critical challenges in early diagnosis and prognosis. Lung cancer screening currently faces limitations in sensitivity and accuracy, and there is a growing interest in methods that can better identify and characterize nodules to support clinical decision-making. The study's primary objective is to evaluate the current advancements in lung cancer screening techniques aimed at improving nodule detection sensitivity, reducing false-positive findings, and accurately classifying nodules. Additionally, the study seeks to explore the potential for predicting nodule growth and pathological or genetic characteristics to enhance screening outcomes and guide future research directions.",True
Medicine,Enhancing EfficientNetv2 with global and efficient channel attention mechanisms for accurate MRI-Based brain tumor classification,"Abstract The early and accurate diagnosis of brain tumors is critical for effective treatment planning, with Magnetic Resonance Imaging (MRI) serving as a key tool in the non-invasive examination of such conditions. Despite the advancements in Computer-Aided Diagnosis (CADx) systems powered by deep learning, the challenge of accurately classifying brain tumors from MRI scans persists due to the high variability of tumor appearances and the subtlety of early-stage manifestations. This work introduces a novel adaptation of the EfficientNetv2 architecture, enhanced with Global Attention Mechanism (GAM) and Efficient Channel Attention (ECA), aimed at overcoming these hurdles. This enhancement not only amplifies the model’s ability to focus on salient features within complex MRI images but also significantly improves the classification accuracy of brain tumors. Our approach distinguishes itself by meticulously integrating attention mechanisms that systematically enhance feature extraction, thereby achieving superior performance in detecting a broad spectrum of brain tumors. Demonstrated through extensive experiments on a large public dataset, our model achieves an exceptional high-test accuracy of 99.76%, setting a new benchmark in MRI-based brain tumor classification. Moreover, the incorporation of Grad-CAM visualization techniques sheds light on the model’s decision-making process, offering transparent and interpretable insights that are invaluable for clinical assessment. By addressing the limitations inherent in previous models, this study not only advances the field of medical imaging analysis but also highlights the pivotal role of attention mechanisms in enhancing the interpretability and accuracy of deep learning models for brain tumor diagnosis. This research sets the stage for advanced CADx systems, enhancing patient care and treatment outcomes.","['EfficientNetv2 architecture', 'Global Attention Mechanism (GAM)', 'Efficient Channel Attention (ECA)']","The early and accurate diagnosis of brain tumors is critical for effective treatment planning, with Magnetic Resonance Imaging (MRI) serving as a key tool in the non-invasive examination of such conditions. However, accurately classifying brain tumors from MRI scans remains challenging due to the high variability of tumor appearances and the subtlety of early-stage manifestations. The primary aim of this study is to improve the classification accuracy of brain tumors using MRI images by enhancing the ability to focus on salient features within complex scans. This work seeks to overcome existing limitations in brain tumor diagnosis to ultimately support better clinical assessment and treatment outcomes.","The early and accurate diagnosis of brain tumors is critical for effective treatment planning, with Magnetic Resonance Imaging (MRI) serving as a key tool in the non-invasive examination of such conditions. However, accurately classifying brain tumors from MRI scans remains challenging due to the high variability of tumor appearances and the subtlety of early-stage manifestations. The primary aim of this study is to improve the classification accuracy of brain tumors using MRI images by enhancing the ability to identify and analyze salient features within complex scans. This work seeks to overcome existing limitations in brain tumor diagnosis to ultimately support better clinical assessment and treatment outcomes.",True
Medicine,Automated model discovery for human cardiac tissue: Discovering the best model and parameters,"For more than half a century, scientists have developed mathematical models to understand the behavior of the human heart. Today, we have dozens of heart tissue models to choose from, but selecting the best model is limited to expert professionals, prone to user bias, and vulnerable to human error. Here we take the human out of the loop and automate the process of model discovery. Towards this goal, we establish a novel incompressible orthotropic constitutive neural network to simultaneously discover both, model and parameters, that best explain human cardiac tissue. Notably, our network features 32 individual terms, 8 isotropic and 24 anisotropic, and fully autonomously selects the best model, out of more than 4 billion possible combinations of terms. We demonstrate that we can successfully train the network with triaxial shear and biaxial extension tests and systematically sparsify the parameter vector with L1-regularization. Strikingly, we robustly discover a four-term model that features a quadratic term in the second invariant I2, and exponential quadratic terms in the fourth and eighth invariants I4f, I4n, and I8fs. Importantly, our discovered model is interpretable by design and has parameters with well-defined physical units. We show that it outperforms popular existing myocardium models and generalizes well, from homogeneous laboratory tests to heterogeneous whole heart simulations. This is made possible by a new universal material subroutine that directly takes the discovered network weights as input. Automating the process of model discovery has the potential to democratize cardiac modeling, broaden participation in scientific discovery, and accelerate the development of innovative treatments for cardiovascular disease. Our source code, data, and examples are available at https://github.com/LivingMatterLab/CANN.",['L1-regularization'],"The study addresses the challenge of selecting the most appropriate mathematical model to understand the behavior of human cardiac tissue, a process currently limited to expert professionals and susceptible to user bias and human error. Despite the availability of numerous heart tissue models, there is a need for a more objective and reliable approach to identify the best model that accurately represents human myocardium. The primary aim of the study is to develop a method that can autonomously discover both the optimal model and its parameters to best explain human cardiac tissue behavior. This approach seeks to improve the interpretability and physical relevance of the model while enhancing its performance and generalizability across different cardiac testing conditions.","The study addresses the challenge of selecting the most appropriate mathematical model to understand the behavior of human cardiac tissue, a process currently limited to expert professionals and susceptible to user bias and human error. Despite the availability of numerous heart tissue models, there is a need for a more objective and reliable approach to identify the best model that accurately represents human myocardium. The primary aim of the study is to develop a systematic method that can independently determine both the optimal model and its parameters to best explain human cardiac tissue behavior. This approach seeks to improve the interpretability and physical relevance of the model while enhancing its performance and generalizability across different cardiac testing conditions.",True
Medicine,Improving diabetes disease patients classification using stacking ensemble method with PIMA and local healthcare data,"Diabetes mellitus, a chronic metabolic disorder, continues to be a major public health issue around the world. It is estimated that one in every two diabetics is undiagnosed. Early diagnosis and management of diabetes can also prevent or delay the onset of complications. With the help of a variety of machine learning and deep learning models, stacking algorithms, and other techniques, our study's goal is to detect diseases early. In this study, we propose two stacking-based models for diabetes disease classification using a combination of the PIMA Indian diabetes dataset, simulated data, and additional data collected from a local healthcare facility. We use both the classical and deep neural network stacking ensemble methods to combine the predictions of multiple classification models and improve classification accuracy and robustness. In the evaluation protocol, we used both the train-test and cross-validation (CV) techniques to validate our proposed model. The highest accuracy is obtained by stacking ensemble with three NN architectures, resulting in an accuracy of 95.50 %, precision of 94 %, recall of 97 %, and f1-score of 96 % using 5-fold CV on simulation study. The stacked accuracy obtained from ML algorithms for the Pima Indian Diabetes dataset is 75.03 % using the train-test split protocol, while the accuracy obtained from the CV protocol is 77.10 % on the stacked model. The range of performance scores that outperformed the CV protocol 2.23 %–12 %. Our proposed method achieves a high accuracy range from 92 % to 95 %, precision, recall, and F1-score ranges from 88 % to 96 % using classical and deep neural network (NN)-based stacking method on the primary dataset. The proposed dataset and ensemble method could be useful in the early detection and treatment of diabetes, as well as in the advancement of machine learning and data analysis techniques in the healthcare industry.","['stacking algorithms', 'stacking-based models', 'classical stacking ensemble methods', 'deep neural network stacking ensemble methods']","Diabetes mellitus, a chronic metabolic disorder, remains a significant public health concern worldwide, with an estimated one in every two diabetics remaining undiagnosed. Early diagnosis and management of diabetes are crucial to prevent or delay the onset of complications associated with the disease. The primary aim of this study is to improve the early detection of diabetes by developing effective classification approaches using a combination of existing datasets and locally collected healthcare data. This study seeks to enhance the accuracy and robustness of diabetes diagnosis to support timely treatment and better health outcomes.","Diabetes mellitus, a chronic metabolic disorder, remains a significant public health concern worldwide, with an estimated one in every two diabetics remaining undiagnosed. Early diagnosis and management of diabetes are crucial to prevent or delay the onset of complications associated with the disease. The primary aim of this study is to improve the early detection of diabetes by developing effective diagnostic methods using a combination of existing datasets and locally collected healthcare data. This study seeks to enhance the accuracy and robustness of diabetes diagnosis to support timely treatment and better health outcomes.",True
Medicine,Federated Learning for Decentralized Artificial Intelligence in Melanoma Diagnostics,"Importance The development of artificial intelligence (AI)–based melanoma classifiers typically calls for large, centralized datasets, requiring hospitals to give away their patient data, which raises serious privacy concerns. To address this concern, decentralized federated learning has been proposed, where classifier development is distributed across hospitals. Objective To investigate whether a more privacy-preserving federated learning approach can achieve comparable diagnostic performance to a classical centralized (ie, single-model) and ensemble learning approach for AI-based melanoma diagnostics. Design, Setting, and Participants This multicentric, single-arm diagnostic study developed a federated model for melanoma-nevus classification using histopathological whole-slide images prospectively acquired at 6 German university hospitals between April 2021 and February 2023 and benchmarked it using both a holdout and an external test dataset. Data analysis was performed from February to April 2023. Exposures All whole-slide images were retrospectively analyzed by an AI-based classifier without influencing routine clinical care. Main Outcomes and Measures The area under the receiver operating characteristic curve (AUROC) served as the primary end point for evaluating the diagnostic performance. Secondary end points included balanced accuracy, sensitivity, and specificity. Results The study included 1025 whole-slide images of clinically melanoma-suspicious skin lesions from 923 patients, consisting of 388 histopathologically confirmed invasive melanomas and 637 nevi. The median (range) age at diagnosis was 58 (18-95) years for the training set, 57 (18-93) years for the holdout test dataset, and 61 (18-95) years for the external test dataset; the median (range) Breslow thickness was 0.70 (0.10-34.00) mm, 0.70 (0.20-14.40) mm, and 0.80 (0.30-20.00) mm, respectively. The federated approach (0.8579; 95% CI, 0.7693-0.9299) performed significantly worse than the classical centralized approach (0.9024; 95% CI, 0.8379-0.9565) in terms of AUROC on a holdout test dataset (pairwise Wilcoxon signed-rank, P &amp;amp;lt; .001) but performed significantly better (0.9126; 95% CI, 0.8810-0.9412) than the classical centralized approach (0.9045; 95% CI, 0.8701-0.9331) on an external test dataset (pairwise Wilcoxon signed-rank, P &amp;amp;lt; .001). Notably, the federated approach performed significantly worse than the ensemble approach on both the holdout (0.8867; 95% CI, 0.8103-0.9481) and external test dataset (0.9227; 95% CI, 0.8941-0.9479). Conclusions and Relevance The findings of this diagnostic study suggest that federated learning is a viable approach for the binary classification of invasive melanomas and nevi on a clinically representative distributed dataset. Federated learning can improve privacy protection in AI-based melanoma diagnostics while simultaneously promoting collaboration across institutions and countries. Moreover, it may have the potential to be extended to other image classification tasks in digital cancer histopathology and beyond.","['federated learning', 'ensemble learning approach']","The study addresses the challenge of developing effective melanoma diagnostic tools while preserving patient privacy, as traditional approaches require centralized collection of sensitive patient data from multiple hospitals. This raises significant privacy concerns and limits collaboration across institutions. The research explores whether a more privacy-preserving approach to melanoma classification can maintain diagnostic accuracy comparable to conventional centralized methods.

The primary aim of the study is to investigate whether a privacy-preserving federated approach can achieve diagnostic performance comparable to classical centralized and ensemble learning approaches for melanoma diagnostics. The study focuses on evaluating the effectiveness of this approach in classifying invasive melanomas and nevi using histopathological whole-slide images collected from multiple university hospitals.","The study addresses the challenge of developing effective melanoma diagnostic tools while preserving patient privacy, as traditional approaches require centralized collection of sensitive patient data from multiple hospitals. This raises significant privacy concerns and limits collaboration across institutions. The research explores whether a more privacy-preserving approach to melanoma classification can maintain diagnostic accuracy comparable to conventional centralized methods.

The primary aim of the study is to investigate whether a privacy-preserving federated approach can achieve diagnostic performance comparable to classical centralized methods for melanoma diagnostics. The study focuses on evaluating the effectiveness of this approach in classifying invasive melanomas and nevi using histopathological whole-slide images collected from multiple university hospitals.",True
Medicine,Enhancing cervical cancer detection and robust classification through a fusion of deep learning models,"Abstract Cervical cancer, the second most prevalent cancer affecting women, arises from abnormal cell growth in the cervix, a crucial anatomical structure within the uterus. The significance of early detection cannot be overstated, prompting the use of various screening methods such as Pap smears, colposcopy, and Human Papillomavirus (HPV) testing to identify potential risks and initiate timely intervention. These screening procedures encompass visual inspections, Pap smears, colposcopies, biopsies, and HPV-DNA testing, each demanding the specialized knowledge and skills of experienced physicians and pathologists due to the inherently subjective nature of cancer diagnosis. In response to the imperative for efficient and intelligent screening, this article introduces a groundbreaking methodology that leverages pre-trained deep neural network models, including Alexnet, Resnet-101, Resnet-152, and InceptionV3, for feature extraction. The fine-tuning of these models is accompanied by the integration of diverse machine learning algorithms, with ResNet152 showcasing exceptional performance, achieving an impressive accuracy rate of 98.08%. It is noteworthy that the SIPaKMeD dataset, publicly accessible and utilized in this study, contributes to the transparency and reproducibility of our findings. The proposed hybrid methodology combines aspects of DL and ML for cervical cancer classification. Most intricate and complicated features from images can be extracted through DL. Further various ML algorithms can be implemented on extracted features. This innovative approach not only holds promise for significantly improving cervical cancer detection but also underscores the transformative potential of intelligent automation within the realm of medical diagnostics, paving the way for more accurate and timely interventions.","['Alexnet', 'Resnet-101', 'Resnet-152', 'InceptionV3', 'deep neural network models', 'fine-tuning']","The research idea centers on the critical importance of early detection of cervical cancer, which is the second most prevalent cancer affecting women and arises from abnormal cell growth in the cervix. Current screening methods such as Pap smears, colposcopy, and HPV testing require specialized knowledge due to the subjective nature of cancer diagnosis, highlighting the need for more efficient and accurate screening approaches. The study addresses the challenge of improving cervical cancer detection to enable timely intervention and better patient outcomes. The primary objective of the study is to develop and evaluate a novel approach for cervical cancer classification that enhances the accuracy of detection by utilizing advanced techniques for feature extraction and classification. This approach aims to improve the reliability and effectiveness of screening procedures, ultimately facilitating earlier diagnosis and treatment of cervical cancer.","The research idea centers on the critical importance of early detection of cervical cancer, which is the second most prevalent cancer affecting women and arises from abnormal cell growth in the cervix. Current screening methods such as Pap smears, colposcopy, and HPV testing require specialized knowledge due to the subjective nature of cancer diagnosis, highlighting the need for more efficient and accurate screening approaches. The study addresses the challenge of improving cervical cancer detection to enable timely intervention and better patient outcomes. The primary objective of the study is to develop and evaluate a novel approach for cervical cancer identification that enhances the accuracy of detection through improved analysis methods. This approach aims to improve the reliability and effectiveness of screening procedures, ultimately facilitating earlier diagnosis and treatment of cervical cancer.",True
Medicine,Artificial intelligence for oral squamous cell carcinoma detection based on oral photographs: A comprehensive literature review,"Abstract Introduction Oral squamous cell carcinoma (OSCC) presents a significant global health challenge. The integration of artificial intelligence (AI) and computer vision holds promise for the early detection of OSCC through the analysis of digitized oral photographs. This literature review explores the landscape of AI‐driven OSCC automatic detection, assessing both the performance and limitations of the current state of the art. Materials and Methods An electronic search using several data base was conducted, and a systematic review performed in accordance with PRISMA guidelines (CRD42023441416). Results Several studies have demonstrated remarkable results for this task, consistently achieving sensitivity rates exceeding 85% and accuracy rates surpassing 90%, often encompassing around 1000 images. The review scrutinizes these studies, shedding light on their methodologies, including the use of recent machine learning and pattern recognition approaches coupled with different supervision strategies. However, comparing the results from different papers is challenging due to variations in the datasets used. Discussion Considering these findings, this review underscores the urgent need for more robust and reliable datasets in the field of OSCC detection. Furthermore, it highlights the potential of advanced techniques such as multi‐task learning, attention mechanisms, and ensemble learning as crucial tools in enhancing the accuracy and sensitivity of OSCC detection through oral photographs. Conclusion These insights collectively emphasize the transformative impact of AI‐driven approaches on early OSCC diagnosis, with the potential to significantly improve patient outcomes and healthcare practices.","['machine learning', 'multi-task learning', 'attention mechanisms', 'ensemble learning']","Oral squamous cell carcinoma (OSCC) presents a significant global health challenge due to its prevalence and the critical importance of early detection for improving patient outcomes. Early diagnosis of OSCC through the analysis of oral photographs could greatly enhance treatment effectiveness and reduce mortality rates. The study addresses the need to evaluate current approaches for detecting OSCC to understand their performance and limitations. The primary aim of this study is to systematically review the existing literature on automatic detection of OSCC using digitized oral photographs, assessing the sensitivity and accuracy reported in various studies. Additionally, the study seeks to highlight the challenges posed by variability in datasets and emphasize the necessity for more robust and reliable data to improve early OSCC diagnosis.","Oral squamous cell carcinoma (OSCC) presents a significant global health challenge due to its prevalence and the critical importance of early detection for improving patient outcomes. Early diagnosis of OSCC through the analysis of oral photographs could greatly enhance treatment effectiveness and reduce mortality rates. The study addresses the need to evaluate current approaches for detecting OSCC to understand their performance and limitations. The primary aim of this study is to systematically review the existing literature on detection of OSCC using digitized oral photographs, assessing the sensitivity and accuracy reported in various studies. Additionally, the study seeks to highlight the challenges posed by variability in datasets and emphasize the necessity for more robust and reliable data to improve early OSCC diagnosis.",True
Medicine,Investigation on explainable machine learning models to predict chronic kidney diseases,"Chronic kidney disease (CKD) is a major worldwide health problem, affecting a large proportion of the world's population and leading to higher morbidity and death rates. The early stages of CKD sometimes present without visible symptoms, causing patients to be unaware. Early detection and treatments are critical in reducing complications and improving the overall quality of life for people afflicted. In this work, we investigate the use of an explainable artificial intelligence (XAI)-based strategy, leveraging clinical characteristics, to predict CKD. This study collected clinical data from 491 patients, comprising 56 with CKD and 435 without CKD, encompassing clinical, laboratory, and demographic variables. To develop the predictive model, five machine learning (ML) methods, namely logistic regression (LR), random forest (RF), decision tree (DT), Naïve Bayes (NB), and extreme gradient boosting (XGBoost), were employed. The optimal model was selected based on accuracy and area under the curve (AUC). Additionally, the SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) algorithms were utilized to demonstrate the influence of the features on the optimal model. Among the five models developed, the XGBoost model achieved the best performance with an AUC of 0.9689 and an accuracy of 93.29%. The analysis of feature importance revealed that creatinine, glycosylated hemoglobin type A1C (HgbA1C), and age were the three most influential features in the XGBoost model. The SHAP force analysis further illustrated the model's visualization of individualized CKD predictions. For further insights into individual predictions, we also utilized the LIME algorithm. This study presents an interpretable ML-based approach for the early prediction of CKD. The SHAP and LIME methods enhance the interpretability of ML models and help clinicians better understand the rationale behind the predicted outcomes more effectively.","['logistic regression (LR)', 'random forest (RF)', 'decision tree (DT)', 'Naïve Bayes (NB)', 'extreme gradient boosting (XGBoost)', 'SHAP (SHapley Additive exPlanations)', 'LIME (Local Interpretable Model-agnostic Explanations)']","Chronic kidney disease (CKD) is a major worldwide health problem that affects a large proportion of the global population and leads to increased morbidity and mortality. Early stages of CKD often present without visible symptoms, causing many patients to remain unaware of their condition. Early detection and treatment are essential to reduce complications and improve the quality of life for those affected. The primary aim of this study is to investigate clinical characteristics to enable early prediction of CKD, thereby facilitating timely intervention and better patient outcomes.","Chronic kidney disease (CKD) is a major worldwide health problem that affects a large proportion of the global population and leads to increased morbidity and mortality. Early stages of CKD often present without visible symptoms, causing many patients to remain unaware of their condition. Early detection and treatment are essential to reduce complications and improve the quality of life for those affected. The primary aim of this study is to investigate clinical characteristics to enable early identification of CKD, thereby facilitating timely intervention and better patient outcomes.",True
Medicine,Prostate cancer grading framework based on deep transfer learning and Aquila optimizer,"Abstract Prostate cancer is the one of the most dominant cancer among males. It represents one of the leading cancer death causes worldwide. Due to the current evolution of artificial intelligence in medical imaging, deep learning has been successfully applied in diseases diagnosis. However, most of the recent studies in prostate cancer classification suffers from either low accuracy or lack of data. Therefore, the present work introduces a hybrid framework for early and accurate classification and segmentation of prostate cancer using deep learning. The proposed framework consists of two stages, namely classification stage and segmentation stage. In the classification stage, 8 pretrained convolutional neural networks were fine-tuned using Aquila optimizer and used to classify patients of prostate cancer from normal ones. If the patient is diagnosed with prostate cancer, segmenting the cancerous spot from the overall image using U-Net can help in accurate diagnosis, and here comes the importance of the segmentation stage. The proposed framework is trained on 3 different datasets in order to generalize the framework. The best reported classification accuracies of the proposed framework are 88.91% using MobileNet for the “ISUP Grade-wise Prostate Cancer” dataset and 100% using MobileNet and ResNet152 for the “Transverse Plane Prostate Dataset” dataset with precisions 89.22% and 100%, respectively. U-Net model gives an average segmentation accuracy and AUC of 98.46% and 0.9778, respectively, using the “PANDA: Resized Train Data (512 × 512)” dataset. The results give an indicator of the acceptable performance of the proposed framework.","['pretrained convolutional neural networks', 'MobileNet', 'ResNet152', 'U-Net']","Prostate cancer is one of the most prevalent cancers among males and is a leading cause of cancer-related deaths worldwide. Accurate and early diagnosis of prostate cancer is crucial for effective treatment and improved patient outcomes. The study aims to develop a reliable approach for the early and precise classification and segmentation of prostate cancer to enhance diagnostic accuracy. Specifically, the primary objective is to classify patients with prostate cancer from normal individuals and to accurately identify and segment cancerous regions within medical images to support better diagnosis.","Prostate cancer is one of the most prevalent cancers among males and is a leading cause of cancer-related deaths worldwide. Accurate and early diagnosis of prostate cancer is crucial for effective treatment and improved patient outcomes. The study aims to develop a reliable approach for the early and precise identification and delineation of prostate cancer to enhance diagnostic accuracy. Specifically, the primary objective is to distinguish patients with prostate cancer from normal individuals and to accurately identify and outline cancerous regions within medical images to support better diagnosis.",True
Medicine,Methodological insights into ChatGPT’s screening performance in systematic reviews,"Abstract Background The screening process for systematic reviews and meta-analyses in medical research is a labor-intensive and time-consuming task. While machine learning and deep learning have been applied to facilitate this process, these methods often require training data and user annotation. This study aims to assess the efficacy of ChatGPT, a large language model based on the Generative Pretrained Transformers (GPT) architecture, in automating the screening process for systematic reviews in radiology without the need for training data. Methods A prospective simulation study was conducted between May 2nd and 24th, 2023, comparing ChatGPT’s performance in screening abstracts against that of general physicians (GPs). A total of 1198 abstracts across three subfields of radiology were evaluated. Metrics such as sensitivity, specificity, positive and negative predictive values (PPV and NPV), workload saving, and others were employed. Statistical analyses included the Kappa coefficient for inter-rater agreement, ROC curve plotting, AUC calculation, and bootstrapping for p-values and confidence intervals. Results ChatGPT completed the screening process within an hour, while GPs took an average of 7–10 days. The AI model achieved a sensitivity of 95% and an NPV of 99%, slightly outperforming the GPs’ sensitive consensus (i.e., including records if at least one person includes them). It also exhibited remarkably low false negative counts and high workload savings, ranging from 40 to 83%. However, ChatGPT had lower specificity and PPV compared to human raters. The average Kappa agreement between ChatGPT and other raters was 0.27. Conclusions ChatGPT shows promise in automating the article screening phase of systematic reviews, achieving high sensitivity and workload savings. While not entirely replacing human expertise, it could serve as an efficient first-line screening tool, particularly in reducing the burden on human resources. Further studies are needed to fine-tune its capabilities and validate its utility across different medical subfields.","['machine learning', 'deep learning', 'Generative Pretrained Transformers (GPT) architecture']","The research idea addresses the challenge that the screening process for systematic reviews and meta-analyses in medical research is labor-intensive and time-consuming, particularly in the field of radiology. This process requires significant human effort to evaluate large volumes of abstracts, which can delay the synthesis of medical evidence. The study is motivated by the need to find more efficient ways to conduct this screening without compromising accuracy. The research objective is to assess the efficacy of an automated approach in performing the screening of abstracts for systematic reviews in radiology, specifically evaluating its sensitivity, specificity, predictive values, and potential to reduce workload compared to general physicians. The study aims to determine whether this approach can serve as an efficient first-line screening tool that maintains high sensitivity while significantly saving human resources during the article screening phase.","The research idea addresses the challenge that the screening process for systematic reviews and meta-analyses in medical research is labor-intensive and time-consuming, particularly in the field of radiology. This process requires significant human effort to evaluate large volumes of abstracts, which can delay the synthesis of medical evidence. The study is motivated by the need to find more efficient ways to conduct this screening without compromising accuracy. The research objective is to assess the efficacy of a systematic approach in performing the screening of abstracts for systematic reviews in radiology, specifically evaluating its reliability, accuracy, and potential to reduce workload compared to general physicians. The study aims to determine whether this approach can serve as an efficient first-line screening tool that maintains high sensitivity while significantly saving human resources during the article screening phase.",True
Medicine,Advancing Ligand Docking through Deep Learning: Challenges and Prospects in Virtual Screening,"ConspectusMolecular docking, also termed ligand docking (LD), is a pivotal element of structure-based virtual screening (SBVS) used to predict the binding conformations and affinities of protein–ligand complexes. Traditional LD methodologies rely on a search and scoring framework, utilizing heuristic algorithms to explore binding conformations and scoring functions to evaluate binding strengths. However, to meet the efficiency demands of SBVS, these algorithms and functions are often simplified, prioritizing speed over accuracy.The emergence of deep learning (DL) has exerted a profound impact on diverse fields, ranging from natural language processing to computer vision and drug discovery. DeepMind's AlphaFold2 has impressively exhibited its ability to accurately predict protein structures solely from amino acid sequences, highlighting the remarkable potential of DL in conformation prediction. This groundbreaking advancement circumvents the traditional search-scoring frameworks in LD, enhancing both accuracy and processing speed and thereby catalyzing a broader adoption of DL algorithms in binding pose prediction. Nevertheless, a consensus on certain aspects remains elusive.In this Account, we delineate the current status of employing DL to augment LD within the VS paradigm, highlighting our contributions to this domain. Furthermore, we discuss the challenges and future prospects, drawing insights from our scholarly investigations. Initially, we present an overview of VS and LD, followed by an introduction to DL paradigms, which deviate significantly from traditional search-scoring frameworks. Subsequently, we delve into the challenges associated with the development of DL-based LD (DLLD), encompassing evaluation metrics, application scenarios, and physical plausibility of the predicted conformations. In the evaluation of LD algorithms, it is essential to recognize the multifaceted nature of the metrics. While the accuracy of binding pose prediction, often measured by the success rate, is a pivotal aspect, the scoring/screening power and computational speed of these algorithms are equally important given the pivotal role of LD tools in VS. Regarding application scenarios, early methods focused on blind docking, where the binding site is unknown. However, recent studies suggest a shift toward identifying binding sites rather than solely predicting binding poses within these models. In contrast, LD with a known pocket in VS has been shown to be more practical. Physical plausibility poses another significant challenge. Although DLLD models often achieve higher success rates compared to traditional methods, they may generate poses with implausible local structures, such as incorrect bond angles or lengths, which are disadvantageous for postprocessing tasks like visualization. Finally, we discuss the future perspectives for DLLD, emphasizing the need to improve generalization ability, strike a balance between speed and accuracy, account for protein conformation flexibility, and enhance physical plausibility. Additionally, we delve into the comparison between generative and regression algorithms in this context, exploring their respective strengths and potential.","['deep learning (DL)', ""DeepMind's AlphaFold2"", 'regression algorithms']","The research idea centers on the critical role of molecular docking in structure-based virtual screening for predicting protein–ligand binding conformations and affinities, highlighting the limitations of traditional methods that prioritize speed over accuracy. There is a growing need to improve the accuracy and efficiency of binding pose prediction while addressing challenges such as physical plausibility and the practical application of docking when binding sites are known or unknown. The study’s primary objective is to evaluate and enhance the current approaches to molecular docking within virtual screening, focusing on improving prediction accuracy, balancing computational speed, and ensuring the physical plausibility of predicted conformations. Additionally, the research aims to explore future directions to better account for protein flexibility and improve the generalization of docking predictions in practical scenarios.","The research idea centers on the critical role of molecular docking in structure-based virtual screening for predicting protein–ligand binding conformations and affinities, highlighting the limitations of traditional methods that prioritize speed over accuracy. There is a growing need to improve the accuracy and efficiency of binding pose prediction while addressing challenges such as physical plausibility and the practical application of docking when binding sites are known or unknown. The study's primary objective is to evaluate and enhance the current approaches to molecular docking within virtual screening, focusing on improving prediction accuracy, balancing computational speed, and ensuring the physical plausibility of predicted conformations. Additionally, the research aims to explore future directions to better account for protein flexibility and improve the generalization of docking predictions in practical scenarios.",True
Medicine,AI-POWERED FRAUD DETECTION IN BANKING: SAFEGUARDING FINANCIAL TRANSACTIONS,"The banking industry's metamorphosis through digitalization has unquestionably revolutionized accessibility and convenience for customers worldwide. However, this paradigm shift has ushered in a new era of challenges, most notably in the realm of cybersecurity. Conventional rule-based fraud detection strategies have struggled to keep pace with the rapid evolution of cyber threats, prompting a surge of interest in more adaptive approaches like unsupervised learning. Furthermore, the COVID-19 pandemic has exacerbated the issue of bank fraud due to the widespread transition to online platforms and the proliferation of charitable funds, which present ripe opportunities for exploitation by cybercriminals. In response to these pressing concerns, this study delves into the realm of machine learning algorithms for the analysis and identification of fraudulent banking transactions. Notably, it contributes scientific novelty by developing models specifically tailored to this purpose and implementing innovative preprocessing techniques to enhance detection accuracy. Utilizing a diverse array of algorithms, including Random Forest, K-Nearest Neighbor (KNN), Naïve Bayes, Decision Trees, and Logistic Regression, the study showcases promising results. In particular, logistic regression and decision tree models exhibit impressive accuracy and Area Under the Curve (AUC) values of approximately 0.98, 0.97 and 0.95, 0.94, respectively. Given the pervasive nature of banking fraud in our digital society, the utilization of artificial intelligence algorithms for fraud detection stands as a critical and timely endeavor, promising enhanced security and trust in the financial ecosystem.","['unsupervised learning', 'Random Forest', 'K-Nearest Neighbor (KNN)', 'Naïve Bayes', 'Decision Trees', 'Logistic Regression']","The research idea addresses the increasing challenges posed by the rapid evolution of cyber threats in the banking industry, especially as digitalization has expanded accessibility and convenience for customers worldwide. The COVID-19 pandemic has further intensified the problem of bank fraud due to the widespread shift to online platforms and the increased flow of charitable funds, which have become targets for exploitation by criminals. The study is motivated by the need to improve the detection of fraudulent banking transactions in this changing landscape. The primary objective of the study is to develop and evaluate approaches specifically designed to identify fraudulent banking transactions with enhanced accuracy, aiming to contribute to improved security and trust within the financial ecosystem.","The research idea addresses the increasing challenges posed by the rapid evolution of cyber threats in the banking industry, especially as digitalization has expanded accessibility and convenience for customers worldwide. The COVID-19 pandemic has further intensified the problem of bank fraud due to the widespread shift to online platforms and the increased flow of charitable funds, which have become targets for exploitation by criminals. The study is motivated by the need to improve the detection of fraudulent banking transactions in this changing landscape. The primary objective of the study is to develop and evaluate methods specifically designed to identify fraudulent banking transactions with enhanced accuracy, aiming to contribute to improved security and trust within the financial ecosystem.",True
Medicine,Detecting COVID-19 in chest CT images based on several pre-trained models,"Abstract This paper explores the use of chest CT scans for early detection of COVID-19 and improved patient outcomes. The proposed method employs advanced techniques, including binary cross-entropy, transfer learning, and deep convolutional neural networks, to achieve accurate results. The COVIDx dataset, which contains 104,009 chest CT images from 1,489 patients, is used for a comprehensive analysis of the virus. A sample of 13,413 images from this dataset is categorised into two groups: 7,395 CT scans of individuals with confirmed COVID-19 and 6,018 images of normal cases. The study presents pre-trained transfer learning models such as ResNet (50), VGG (19), VGG (16), and Inception V3 to enhance the DCNN for classifying the input CT images. The binary cross-entropy metric is used to compare COVID-19 cases with normal cases based on predicted probabilities for each class. Stochastic Gradient Descent and Adam optimizers are employed to address overfitting issues. The study shows that the proposed pre-trained transfer learning models achieve accuracies of 99.07%, 98.70%, 98.55%, and 96.23%, respectively, in the validation set using the Adam optimizer. Therefore, the proposed work demonstrates the effectiveness of pre-trained transfer learning models in enhancing the accuracy of DCNNs for image classification. Furthermore, this paper provides valuable insights for the development of more accurate and efficient diagnostic tools for COVID-19.","['transfer learning', 'deep convolutional neural networks', 'pre-trained transfer learning models', 'ResNet (50)', 'VGG (19)', 'VGG (16)', 'Inception V3', 'Stochastic Gradient Descent optimizer', 'Adam optimizer']",The research idea centers on the importance of early detection of COVID-19 through chest CT scans to improve patient outcomes. Accurate identification of COVID-19 cases is critical for timely treatment and controlling the spread of the virus. The study addresses the challenge of distinguishing COVID-19 infections from normal cases using imaging techniques. The primary objective of the study is to enhance the accuracy of classifying chest CT images into COVID-19 positive and normal cases. This aims to provide more reliable diagnostic tools that can support healthcare professionals in the early and precise detection of COVID-19.,The research idea centers on the importance of early detection of COVID-19 through chest CT scans to improve patient outcomes. Accurate identification of COVID-19 cases is critical for timely treatment and controlling the spread of the virus. The study addresses the challenge of distinguishing COVID-19 infections from normal cases using imaging techniques. The primary objective of the study is to enhance the accuracy of identifying chest CT images as COVID-19 positive or normal cases. This aims to provide more reliable diagnostic tools that can support healthcare professionals in the early and precise detection of COVID-19.,True
Medicine,A Novel Early Detection and Prevention of Coronary Heart Disease Framework Using Hybrid Deep Learning Model and Neural Fuzzy Inference System,"Diabetes is the ""mother of all diseases"" as it affects multiple organs of body of an individual in some way. Its timely detection and management are critically important. Otherwise, the long run, it can cause several complications in a diabetic. Heart disease is one of the major complications of diabetes.This work proposed an Optimal Scrutiny Boosted Graph Convolutional LSTM (O-SBGC-LSTM), SBGC-LSTM enhanced by Eurygaster Optimization Algorithm (EOA) to tune hyperparameters for early prevention and detection of diabetes disease. This work proposed an Optimal Scrutiny Boosted Graph Convolutional LSTM (O-SBGC-LSTM), SBGC-LSTM enhanced by Eurygaster Optimization Algorithm (EOA) to tune hyperparameters for early prevention and detection of diabetes disease. This method not only captures discriminative features in spatial configuration and temporal dynamics but also explore the co-occurrence relationship between spatial and temporal domains. This method also presents a temporal hierarchical architecture to increase temporal receptive fields of top SBGC-LSTM layer, which boosts the ability to learn high-level semantic representation and significantly reduces computation cost. The performance of O-SBGC-LSTM was found overall to be satisfactory, reaching >98% accuracy in most studies. In comparison with classic machine learning approaches, proposed hybrid DL was found to achieve better performance in almost all studies that reported such comparison outcomes. Furthermore, prevention is better than cure. Additionally, employed fuzzy based inference techniques to enhance the prevention procedure using suggestion table.",['fuzzy based inference techniques'],"The research idea centers on the critical importance of timely detection and management of diabetes, a disease that affects multiple organs and can lead to serious complications, including heart disease. Early prevention and detection are essential to reduce the long-term adverse effects associated with diabetes. The study’s primary objective is to develop an approach for the early prevention and detection of diabetes disease, aiming to improve the identification of discriminative features related to the condition and enhance the ability to learn meaningful representations that support better disease management. Additionally, the study seeks to improve prevention procedures by incorporating techniques that provide suggestions to aid in managing diabetes effectively.","The research idea centers on the critical importance of timely detection and management of diabetes, a disease that affects multiple organs and can lead to serious complications, including heart disease. Early prevention and detection are essential to reduce the long-term adverse effects associated with diabetes. The study's primary objective is to develop an approach for the early prevention and detection of diabetes disease, aiming to improve the identification of distinctive indicators related to the condition and enhance the understanding of meaningful relationships that support better disease management. Additionally, the study seeks to improve prevention procedures by incorporating methods that provide suggestions to aid in managing diabetes effectively.",True
Medicine,Unified deep learning models for enhanced lung cancer prediction with ResNet-50–101 and EfficientNet-B3 using DICOM images,"Abstract Significant advancements in machine learning algorithms have the potential to aid in the early detection and prevention of cancer, a devastating disease. However, traditional research methods face obstacles, and the amount of cancer-related information is rapidly expanding. The authors have developed a helpful support system using three distinct deep-learning models, ResNet-50, EfficientNet-B3, and ResNet-101, along with transfer learning, to predict lung cancer, thereby contributing to health and reducing the mortality rate associated with this condition. This offer aims to address the issue effectively. Using a dataset of 1,000 DICOM lung cancer images from the LIDC-IDRI repository, each image is classified into four different categories. Although deep learning is still making progress in its ability to analyze and understand cancer data, this research marks a significant step forward in the fight against cancer, promoting better health outcomes and potentially lowering the mortality rate. The Fusion Model, like all other models, achieved 100% precision in classifying Squamous Cells. The Fusion Model and ResNet-50 achieved a precision of 90%, closely followed by EfficientNet-B3 and ResNet-101 with slightly lower precision. To prevent overfitting and improve data collection and planning, the authors implemented a data extension strategy. The relationship between acquiring knowledge and reaching specific scores was also connected to advancing and addressing the issue of imprecise accuracy, ultimately contributing to advancements in health and a reduction in the mortality rate associated with lung cancer.","['ResNet-50', 'EfficientNet-B3', 'ResNet-101', 'transfer learning']","The research idea centers on the urgent need for improved early detection and prevention of lung cancer, a disease with a high mortality rate, amid the challenges posed by rapidly expanding cancer-related information and limitations of traditional research methods. The study addresses the critical problem of accurately classifying lung cancer types to contribute to better health outcomes and reduce mortality associated with this condition. The primary objective of the study is to develop an effective approach for predicting lung cancer by classifying lung cancer images into distinct categories, thereby enhancing diagnostic precision. This aims to promote advancements in health and contribute to lowering the mortality rate linked to lung cancer.","The research idea centers on the urgent need for improved early detection and prevention of lung cancer, a disease with a high mortality rate, amid the challenges posed by rapidly expanding cancer-related information and limitations of traditional research methods. The study addresses the critical problem of accurately classifying lung cancer types to contribute to better health outcomes and reduce mortality associated with this condition. The primary objective of the study is to develop an effective approach for identifying lung cancer by categorizing lung cancer images into distinct types, thereby enhancing diagnostic precision. This aims to promote advancements in health and contribute to lowering the mortality rate linked to lung cancer.",True
Medicine,More Robots are Coming: Large Multimodal Models (ChatGPT) can Solve Visually Diverse Images of Parsons Problems,"Large language models are reshaping computing education. Based on recent research, these models explain code better than students, answer multiple choice questions at or above the class average, and generate code that can pass automated tests in introductory courses. In response to these capabilities, instructors have quickly adjusted their courses and assessment methods to align with shifting learning goals and the increased risk of academic integrity issues. While some scholars have advocated for the integration of visual problems as a safeguard against the capabilities of language models, new multimodal models now have vision and language capabilities that may allow them to analyze and solve visual problems. In this paper, we compare the large multimodal model (LMMs) GPT-4V with Bard, an LLM that uses Google Lens for text recognition. We find that LMMs, which have learned both pixel features (from images) and text features (from prompts) in the same embedding space, performed substantially better than Bard which uses a piecemeal approach. With a specific focus on Parsons problems presented across diverse visual representations, our results show that GPT-4V solved 96.7% these visual problems, struggling minimally with a single Parsons problem. Conversely, Bard performed poorly by only solving 69.2% of problems, struggling with common issues like hallucinations and refusals. These findings suggest that merely transitioning to visual programming problems might not be a panacea to issues of academic integrity in the generative AI era.",['large multimodal models (LMMs)'],"The study addresses the challenge of maintaining academic integrity in computing education amid the emergence of advanced language models capable of understanding and solving visual programming problems. As instructors adapt their courses and assessments to counteract the increased risk of academic dishonesty, there is a concern that traditional safeguards, such as using visual problems, may no longer be effective. The research aims to evaluate the effectiveness of large multimodal models in solving visual programming problems, specifically Parsons problems, to determine whether transitioning to visual problem formats can mitigate academic integrity issues. The primary objective is to compare the performance of different multimodal models on these visual problems to assess their capability and the implications for educational assessment strategies.","The study addresses the challenge of maintaining academic integrity in computing education amid the emergence of advanced text generation tools capable of understanding and solving visual programming problems. As instructors adapt their courses and assessments to counteract the increased risk of academic dishonesty, there is a concern that traditional safeguards, such as using visual problems, may no longer be effective. The research aims to evaluate the effectiveness of recent technological tools in solving visual programming problems, specifically Parsons problems, to determine whether transitioning to visual problem formats can mitigate academic integrity issues. The primary objective is to compare the performance of different tools on these visual problems to assess their capability and the implications for educational assessment strategies.",True
Medicine,The Utility of AI in Writing a Scientific Review Article on the Impacts of COVID-19 on Musculoskeletal Health,"Abstract Purpose of Review There were two primary purposes to our reviews. First, to provide an update to the scientific community about the impacts of COVID-19 on musculoskeletal health. Second, was to determine the value of using a large language model, ChatGPT 4.0, in the process of writing a scientific review article. To accomplish these objectives, we originally set out to write three review articles on the topic using different methods to produce the initial drafts of the review articles. The first review article was written in the traditional manner by humans, the second was to be written exclusively using ChatGPT (AI-only or AIO), and the third approach was to input the outline and references selected by humans from approach 1 into ChatGPT, using the AI to assist in completing the writing (AI-assisted or AIA). All review articles were extensively fact-checked and edited by all co-authors leading to the final drafts of the manuscripts, which were significantly different from the initial drafts. Recent Findings Unfortunately, during this process, it became clear that approach 2 was not feasible for a very recent topic like COVID-19 as at the time, ChatGPT 4.0 had a cutoff date of September 2021 and all articles published after this date had to be provided to ChatGPT, making approaches 2 and 3 virtually identical. Therefore, only two approaches and two review articles were written (human and AI-assisted). Here we found that the human-only approach took less time to complete than the AI-assisted approach. This was largely due to the number of hours required to fact-check and edit the AI-assisted manuscript. Of note, the AI-assisted approach resulted in inaccurate attributions of references (about 20%) and had a higher similarity index suggesting an increased risk of plagiarism. Summary The main aim of this project was to determine whether the use of AI could improve the process of writing a scientific review article. Based on our experience, with the current state of technology, it would not be advised to solely use AI to write a scientific review article, especially on a recent topic.","['large language model, ChatGPT 4.0']","The research idea centers on addressing the impacts of COVID-19 on musculoskeletal health and exploring the effectiveness of different approaches to writing scientific review articles on this recent and evolving topic. The study highlights challenges in producing accurate and reliable scientific reviews, especially given the rapidly changing nature of COVID-19-related research. The primary objective of the study was to evaluate whether the use of an AI-assisted approach could improve the process of writing a scientific review article compared to the traditional human-only method. The study aimed to assess the feasibility, accuracy, and efficiency of these approaches in producing high-quality scientific reviews on the musculoskeletal effects of COVID-19.","The research idea centers on addressing the impacts of COVID-19 on musculoskeletal health and exploring the effectiveness of different approaches to writing scientific review articles on this recent and evolving topic. The study highlights challenges in producing accurate and reliable scientific reviews, especially given the rapidly changing nature of COVID-19-related research. The primary objective of the study was to evaluate whether alternative collaborative writing methods could improve the process of writing a scientific review article compared to the traditional approach. The study aimed to assess the feasibility, accuracy, and efficiency of these approaches in producing high-quality scientific reviews on the musculoskeletal effects of COVID-19.",True
Medicine,Machine learning model (RG-DMML) and ensemble algorithm for prediction of students’ retention and graduation in education,"Automated prediction of students' retention and graduation in education using advanced analytical methods such as artificial intelligence (AI), has recently attracted the attention of educators, both in theory and in practice. Whereas invaluable insights and theories for measuring and testing the topic have been proposed, most of the existing methods do not technically highlight the non-trivial factors behind the renowned challenges and attrition. To this effect, by making use of two categories of data collected in a higher education setting about students (i) retention (n = 52262) and (ii) graduation (n = 53639); this study proposes a machine learning model - RG-DMML (retention and graduation data mining and machine learning) and ensemble algorithm for prediction of students' retention and graduation status in education. This was done by training and testing key features that are technically deemed suitable for measuring the constructs (retention and graduation), such as (i) the Average grade of the previous high school, and (ii) the Entry/admission score. The proposed model (RG-DMML) is designed based on the cross industry standard process for data mining (CRISP-DM) methodology, implemented using supervised machine learning technique such as K-Nearest Neighbor (KNN), and validated using the k-fold cross-validation method. The results show that the executed model and algorithm based on the Bagging method and 10-fold cross-validation are efficient and effective for predicting the student's retention and graduation status, with Precision (retention = 0.909, graduation = 0.822), Recall (retention = 1.000, graduation = 0.957), Accuracy (retention = 0.909, graduation = 0.817), F1-Score (retention = 0.952, graduation = 0.885) showing significant high accuracy levels or performance rate, and low Error-rate (retention = 0.090, graduation = 0.182), respectively. In addition, by considering the individual features selected through the Wrapper method in predicting the outputs, the proposed model proved more effective for predicting the students' retention status in comparison to the graduation data. The implications of the models' output and factors that impact the effective prediction or identification of at-risk students, e.g., for timely intervention, counselling, decision-making, and sustainable educational practice are empirically discussed in the study.","['ensemble algorithm', 'supervised machine learning technique', 'K-Nearest Neighbor (KNN)', 'Wrapper method']","The research idea addresses the challenge of understanding and predicting student retention and graduation in higher education, highlighting that existing approaches often fail to reveal the complex factors contributing to these issues. There is a need to identify key features that influence students' likelihood to continue their studies or successfully graduate, which is critical for timely intervention and support. The study aims to improve insights into the factors behind student attrition and graduation outcomes to enhance educational practices and decision-making. The primary objective of the study is to predict students' retention and graduation status by utilizing key academic indicators such as average high school grades and admission scores. The study seeks to evaluate the effectiveness of these indicators in identifying students at risk of dropping out or not graduating, thereby supporting timely counseling and sustainable educational strategies.","The research idea addresses the challenge of understanding and predicting student retention and graduation in higher education, highlighting that existing approaches often fail to reveal the complex factors contributing to these issues. There is a need to identify key features that influence students' likelihood to continue their studies or successfully graduate, which is critical for timely intervention and support. The study aims to improve insights into the factors behind student attrition and graduation outcomes to enhance educational practices and decision-making. The primary objective of the study is to analyze students' retention and graduation status by examining key academic indicators such as average high school grades and admission scores. The study seeks to evaluate the effectiveness of these indicators in identifying students at risk of dropping out or not graduating, thereby supporting timely counseling and sustainable educational strategies.",True
Medicine,Deep learning algorithm-based multimodal MRI radiomics and pathomics data improve prediction of bone metastases in primary prostate cancer,"Abstract Purpose Bone metastasis is a significant contributor to morbidity and mortality in advanced prostate cancer, and early diagnosis is challenging due to its insidious onset. The use of machine learning to obtain prognostic information from pathological images has been highlighted. However, there is a limited understanding of the potential of early prediction of bone metastasis through the feature combination method from various sources. This study presents a method of integrating multimodal data to enhance the feasibility of early diagnosis of bone metastasis in prostate cancer. Methods and materials Overall, 211 patients diagnosed with prostate cancer (PCa) at Gansu Provincial Hospital between January 2017 and February 2023 were included in this study. The patients were randomized (8:2) into a training group ( n = 169) and a validation group ( n = 42). The region of interest (ROI) were segmented from the three magnetic resonance imaging (MRI) sequences (T2WI, DWI, and ADC), and pathological features were extracted from tissue sections (hematoxylin and eosin [H&amp;E] staining, 10 × 20). A deep learning (DL) model using ResNet 50 was employed to extract deep transfer learning (DTL) features. The least absolute shrinkage and selection operator (LASSO) regression method was utilized for feature selection, feature construction, and reducing feature dimensions. Different machine learning classifiers were used to build predictive models. The performance of the models was evaluated using receiver operating characteristic curves. The net clinical benefit was assessed using decision curve analysis (DCA). The goodness of fit was evaluated using calibration curves. A joint model nomogram was eventually developed by combining clinically independent risk factors. Results The best prediction models based on DTL and pathomics features showed area under the curve (AUC) values of 0.89 (95% confidence interval [CI], 0.799–0.989) and 0.85 (95% CI, 0.714–0.989), respectively. The AUC for the best prediction model based on radiomics features and combining radiomics features, DTL features, and pathomics features were 0.86 (95% CI, 0.735–0.979) and 0.93 (95% CI, 0.854–1.000), respectively. Based on DCA and calibration curves, the model demonstrated good net clinical benefit and fit. Conclusion Multimodal radiomics and pathomics serve as valuable predictors of the risk of bone metastases in patients with primary PCa.","['deep learning (DL) model using ResNet 50', 'deep transfer learning (DTL) features', 'least absolute shrinkage and selection operator (LASSO) regression method']","The research idea addresses the significant challenge of early diagnosis of bone metastasis in advanced prostate cancer, which contributes greatly to patient morbidity and mortality due to its insidious onset. There is a limited understanding of how combining features from various sources can improve early prediction of bone metastasis. The study is motivated by the need to enhance the feasibility of early diagnosis by integrating multiple types of clinical and pathological information. The primary objective of the study is to develop and evaluate a method that integrates multimodal data to improve the early diagnosis of bone metastasis in patients with prostate cancer. This approach aims to identify valuable predictors of bone metastasis risk by combining different clinical and pathological features to support better prognostic assessment.","The research idea addresses the significant challenge of early diagnosis of bone metastasis in advanced prostate cancer, which contributes greatly to patient morbidity and mortality due to its insidious onset. There is a limited understanding of how combining features from various sources can improve early identification of bone metastasis. The study is motivated by the need to enhance the feasibility of early diagnosis by integrating multiple types of clinical and pathological information. The primary objective of the study is to develop and evaluate a method that integrates multimodal data to improve the early diagnosis of bone metastasis in patients with prostate cancer. This approach aims to identify valuable predictors of bone metastasis risk by combining different clinical and pathological features to support better prognostic assessment.",True
Medicine,NSGA-II-DL: Metaheuristic Optimal Feature Selection With Deep Learning Framework for HER2 Classification in Breast Cancer,"Immunohistochemistry (IHC) slides are graded for breast cancer based on visual markers and morphological characteristics of stained membrane regions. The usage of whole slide images (WSIs) from histology in digital pathology algorithms for computer-assisted evaluations has increased recently. Human epidermal growth factor receptor 2 (HER2)-stained microscopic images are challenging, time-consuming, and error-prone to evaluate manually. This is due to different staining, overlapped regions, and huge, non-homogeneous slides. Additionally, the classification of HER2 images by the selection of fundamental features must be used to capture the difficult elements of the images, such as the irregular cell structure and the coloring of the tissue of the cells. To solve the above problems, a transfer learning model-based, trainable metaheuristic method for choosing the best features is suggested in this paper. Moreover, the suggested model is efficient in reducing model complexity and computational costs as well as avoiding overfitting. The four main components of the proposed cascaded design are: (a) converting WSIs to tiled images and enhancing contrast with fast local Laplacian filtering (FlLpF); (b) extracting features with a ResNet50 CNN technique based on transfer learning; (c) selecting the most informative features with the help of a non-dominated sorting genetic algorithm (NSGA-II) optimizer; and (d) using a support vector machine (SVM) to classify HER2 scores. Results from the HER2SC and HER2GAN datasets show that the suggested model is superior to other methods already in use, with 94.4% accuracy, 93.71% precision, 98.07% specificity, 93.83% sensitivity, and a 93.71% F1-score for the HER2SC dataset being achieved.","['transfer learning', 'ResNet50 CNN', 'non-dominated sorting genetic algorithm (NSGA-II)', 'support vector machine (SVM)']","The research addresses the challenge of accurately grading breast cancer using immunohistochemistry slides stained for Human epidermal growth factor receptor 2 (HER2). Manual evaluation of these microscopic images is difficult, time-consuming, and prone to errors due to variations in staining, overlapping regions, and the large, heterogeneous nature of the slides. Additionally, the irregular cell structures and tissue coloring complicate the classification of HER2 images, necessitating effective selection of fundamental features to capture these complexities. The primary objective of the study is to improve the classification of HER2-stained microscopic images by developing a method that enhances feature selection to better capture the difficult elements of the images, thereby facilitating more accurate and reliable HER2 scoring for breast cancer diagnosis.","The research addresses the challenge of accurately grading breast cancer using immunohistochemistry slides stained for Human epidermal growth factor receptor 2 (HER2). Manual evaluation of these microscopic images is difficult, time-consuming, and prone to errors due to variations in staining, overlapping regions, and the large, heterogeneous nature of the slides. Additionally, the irregular cell structures and tissue coloring complicate the classification of HER2 images, necessitating effective identification of fundamental characteristics to capture these complexities. The primary objective of the study is to improve the classification of HER2-stained microscopic images by developing a method that enhances the identification of relevant visual attributes to better capture the difficult elements of the images, thereby facilitating more accurate and reliable HER2 scoring for breast cancer diagnosis.",True
Medicine,Generalizability of machine learning in predicting antimicrobial resistance in E. coli: a multi-country case study in Africa,"Abstract Background Antimicrobial resistance (AMR) remains a significant global health threat particularly impacting low- and middle-income countries (LMICs). These regions often grapple with limited healthcare resources and access to advanced diagnostic tools. Consequently, there is a pressing need for innovative approaches that can enhance AMR surveillance and management. Machine learning (ML) though underutilized in these settings, presents a promising avenue. This study leverages ML models trained on whole-genome sequencing data from England, where such data is more readily available, to predict AMR in E . coli , targeting key antibiotics such as ciprofloxacin, ampicillin, and cefotaxime. A crucial part of our work involved the validation of these models using an independent dataset from Africa, specifically from Uganda, Nigeria, and Tanzania, to ascertain their applicability and effectiveness in LMICs. Results Model performance varied across antibiotics. The Support Vector Machine excelled in predicting ciprofloxacin resistance (87% accuracy, F1 Score: 0.57), Light Gradient Boosting Machine for cefotaxime (92% accuracy, F1 Score: 0.42), and Gradient Boosting for ampicillin (58% accuracy, F1 Score: 0.66). In validation with data from Africa, Logistic Regression showed high accuracy for ampicillin (94%, F1 Score: 0.97), while Random Forest and Light Gradient Boosting Machine were effective for ciprofloxacin (50% accuracy, F1 Score: 0.56) and cefotaxime (45% accuracy, F1 Score:0.54), respectively. Key mutations associated with AMR were identified for these antibiotics. Conclusion As the threat of AMR continues to rise, the successful application of these models, particularly on genomic datasets from LMICs, signals a promising avenue for improving AMR prediction to support large AMR surveillance programs. This work thus not only expands our current understanding of the genetic underpinnings of AMR but also provides a robust methodological framework that can guide future research and applications in the fight against AMR.","['Support Vector Machine', 'Light Gradient Boosting Machine', 'Gradient Boosting', 'Logistic Regression', 'Random Forest']","The research idea centers on addressing the significant global health threat posed by antimicrobial resistance (AMR), which disproportionately affects low- and middle-income countries (LMICs) that often face limited healthcare resources and lack access to advanced diagnostic tools. There is a pressing need for innovative approaches to enhance AMR surveillance and management in these regions. The study aims to improve the prediction of AMR in Escherichia coli, focusing on key antibiotics such as ciprofloxacin, ampicillin, and cefotaxime, by leveraging genomic data. The primary objective of the study is to evaluate the applicability and effectiveness of predictive approaches trained on genomic data from England when validated with independent datasets from African countries including Uganda, Nigeria, and Tanzania, thereby supporting large AMR surveillance programs and expanding understanding of the genetic factors underlying AMR.","The research idea centers on addressing the significant global health threat posed by antimicrobial resistance (AMR), which disproportionately affects low- and middle-income countries (LMICs) that often face limited healthcare resources and lack access to advanced diagnostic tools. There is a pressing need for innovative approaches to enhance AMR surveillance and management in these regions. The study aims to improve the identification of AMR in Escherichia coli, focusing on key antibiotics such as ciprofloxacin, ampicillin, and cefotaxime, by utilizing genomic data. The primary objective of the study is to evaluate the applicability and effectiveness of analytical methods trained on genomic data from England when validated with independent datasets from African countries including Uganda, Nigeria, and Tanzania, thereby supporting large AMR surveillance programs and expanding understanding of the genetic factors underlying AMR.",True
Medicine,Cardiac Arrhythmia Classification Using Advanced Deep Learning Techniques on Digitized ECG Datasets,"ECG classification or heartbeat classification is an extremely valuable tool in cardiology. Deep learning-based techniques for the analysis of ECG signals assist human experts in the timely diagnosis of cardiac diseases and help save precious lives. This research aims at digitizing a dataset of images of ECG records into time series signals and then applying deep learning (DL) techniques on the digitized dataset. State-of-the-art DL techniques are proposed for the classification of the ECG signals into different cardiac classes. Multiple DL models, including a convolutional neural network (CNN), a long short-term memory (LSTM) network, and a self-supervised learning (SSL)-based model using autoencoders are explored and compared in this study. The models are trained on the dataset generated from ECG plots of patients from various healthcare institutes in Pakistan. First, the ECG images are digitized, segmenting the lead II heartbeats, and then the digitized signals are passed to the proposed deep learning models for classification. Among the different DL models used in this study, the proposed CNN model achieves the highest accuracy of ∼92%. The proposed model is highly accurate and provides fast inference for real-time and direct monitoring of ECG signals that are captured from the electrodes (sensors) placed on different parts of the body. Using the digitized form of ECG signals instead of images for the classification of cardiac arrhythmia allows cardiologists to utilize DL models directly on ECG signals from an ECG machine for the real-time and accurate monitoring of ECGs.","['convolutional neural network (CNN)', 'long short-term memory (LSTM) network', 'self-supervised learning (SSL)-based model using autoencoders']","The study addresses the critical need for accurate and timely classification of ECG signals to assist in the diagnosis of cardiac diseases, which is essential for saving lives in cardiology. It focuses on improving the interpretation of ECG records by converting image-based ECG data into a format that can be more effectively utilized for cardiac assessment. The primary aim of the research is to develop and evaluate methods for classifying ECG signals into different cardiac categories using a digitized dataset derived from ECG images of patients from various healthcare institutes. This approach seeks to enable real-time and precise monitoring of heartbeats, facilitating better clinical decision-making for cardiac arrhythmia detection.","The study addresses the critical need for accurate and timely classification of ECG signals to assist in the diagnosis of cardiac diseases, which is essential for saving lives in cardiology. It focuses on improving the interpretation of ECG records by converting image-based ECG data into a format that can be more effectively utilized for cardiac assessment. The primary aim of the research is to develop and evaluate methods for categorizing ECG signals into different cardiac categories using a digitized dataset derived from ECG images of patients from various healthcare institutes. This approach seeks to enable real-time and precise monitoring of heartbeats, facilitating better clinical decision-making for cardiac arrhythmia detection.",True
Medicine,Deep learning for lungs cancer detection: a review,"Abstract Although lung cancer has been recognized to be the deadliest type of cancer, a good prognosis and efficient treatment depend on early detection. Medical practitioners’ burden is reduced by deep learning techniques, especially Deep Convolutional Neural Networks (DCNN), which are essential in automating the diagnosis and classification of diseases. In this study, we use a variety of medical imaging modalities, including X-rays, WSI, CT scans, and MRI, to thoroughly investigate the use of deep learning techniques in the field of lung cancer diagnosis and classification. This study conducts a comprehensive Systematic Literature Review (SLR) using deep learning techniques for lung cancer research, providing a comprehensive overview of the methodology, cutting-edge developments, quality assessments, and customized deep learning approaches. It presents data from reputable journals and concentrates on the years 2015–2024. Deep learning techniques solve the difficulty of manually identifying and selecting abstract features from lung cancer images. This study includes a wide range of deep learning methods for classifying lung cancer but focuses especially on the most popular method, the Convolutional Neural Network (CNN). CNN can achieve maximum accuracy because of its multi-layer structure, automatic learning of weights, and capacity to communicate local weights. Various algorithms are shown with performance measures like precision, accuracy, specificity, sensitivity, and AUC; CNN consistently shows the greatest accuracy. The findings highlight the important contributions of DCNN in improving lung cancer detection and classification, making them an invaluable resource for researchers looking to gain a greater knowledge of deep learning’s function in medical applications.","['Deep Convolutional Neural Networks (DCNN)', 'Convolutional Neural Network (CNN)']","The research idea centers on the critical challenge of early detection in lung cancer, which is essential for improving prognosis and enabling efficient treatment of this deadliest type of cancer. The study addresses the burden on medical practitioners in diagnosing and classifying lung cancer by exploring advanced approaches to automate these processes using various medical imaging modalities such as X-rays, whole slide imaging, CT scans, and MRI. The research objective is to conduct a comprehensive review of existing methodologies and recent developments in lung cancer diagnosis and classification, focusing on the evaluation and comparison of different approaches to improve accuracy and reliability. This study aims to provide an in-depth overview of the effectiveness of these approaches in enhancing lung cancer detection and classification, thereby supporting further research and clinical application in this field.","The research idea centers on the critical challenge of early detection in lung cancer, which is essential for improving prognosis and enabling efficient treatment of this deadliest type of cancer. The study addresses the burden on medical practitioners in diagnosing and classifying lung cancer by exploring improved approaches to streamline these processes using various medical imaging modalities such as X-rays, whole slide imaging, CT scans, and MRI. The research objective is to conduct a comprehensive review of existing methodologies and recent developments in lung cancer diagnosis and classification, focusing on the evaluation and comparison of different approaches to improve accuracy and reliability. This study aims to provide an in-depth overview of the effectiveness of these approaches in enhancing lung cancer detection and classification, thereby supporting further research and clinical application in this field.",True
Medicine,"An Extensive Investigation into the Use of Machine Learning Tools and Deep Neural Networks for the Recognition of Skin Cancer: Challenges, Future Directions, and a Comprehensive Review","Skin cancer poses a serious risk to one’s health and can only be effectively treated with early detection. Early identification is critical since skin cancer has a higher fatality rate, and it expands gradually to different areas of the body. The rapid growth of automated diagnosis frameworks has led to the combination of diverse machine learning, deep learning, and computer vision algorithms for detecting clinical samples and atypical skin lesion specimens. Automated methods for recognizing skin cancer that use deep learning techniques are discussed in this article: convolutional neural networks, and, in general, artificial neural networks. The recognition of symmetries is a key point in dealing with the skin cancer image datasets; hence, in developing the appropriate architecture of neural networks, as it can improve the performance and release capacities of the network. The current study emphasizes the need for an automated method to identify skin lesions to reduce the amount of time and effort required for the diagnostic process, as well as the novel aspect of using algorithms based on deep learning for skin lesion detection. The analysis concludes with underlying research directions for the future, which will assist in better addressing the difficulties encountered in human skin cancer recognition. By highlighting the drawbacks and advantages of prior techniques, the authors hope to establish a standard for future analysis in the domain of human skin lesion diagnostics.","['convolutional neural networks', 'artificial neural networks']","The research idea centers on the serious health risk posed by skin cancer and the critical importance of early detection due to its higher fatality rate and gradual spread to different areas of the body. Early identification is essential for effective treatment, yet the diagnostic process can be time-consuming and labor-intensive. The study highlights the need to improve methods for recognizing skin lesions to facilitate quicker and more efficient diagnosis. The primary objective of the study is to emphasize the development of an automated approach for identifying skin lesions that can reduce the time and effort required in the diagnostic process. Additionally, the study aims to review existing techniques for skin lesion detection, discuss their advantages and drawbacks, and provide guidance for future research to better address challenges in human skin cancer recognition.","The research idea centers on the serious health risk posed by skin cancer and the critical importance of early detection due to its higher fatality rate and gradual spread to different areas of the body. Early identification is essential for effective treatment, yet the diagnostic process can be time-consuming and labor-intensive. The study highlights the need to improve methods for recognizing skin lesions to facilitate quicker and more efficient diagnosis. The primary objective of the study is to emphasize the development of a systematic approach for identifying skin lesions that can reduce the time and effort required in the diagnostic process. Additionally, the study aims to review existing techniques for skin lesion detection, discuss their advantages and drawbacks, and provide guidance for future research to better address challenges in human skin cancer recognition.",True
Medicine,DenRAM: neuromorphic dendritic architecture with RRAM for efficient temporal processing with delays,"An increasing number of studies are highlighting the importance of spatial dendritic branching in pyramidal neurons in the neocortex for supporting non-linear computation through localized synaptic integration. In particular, dendritic branches play a key role in temporal signal processing and feature detection. This is accomplished thanks to coincidence detection (CD) mechanisms enabled by the presence of synaptic delays that align temporally disparate inputs for effective integration. Computational studies on spiking neural networks further highlight the significance of delays for achieving spatio-temporal pattern recognition with pure feed-forward neural networks, without the need of resorting to recurrent architectures. In this work, we present ""DenRAM"", the first realization of a feed-forward spiking neural network with dendritic compartments, implemented using analog electronic circuits integrated into a 130 nm technology node and coupled with Resistive Random Access Memory (RRAM) technology. DenRAM's dendritic circuits use RRAM devices to implement both delays and synaptic weights in the network. By configuring the RRAM devices to reproduce bio-realistic timescales, and by exploiting their heterogeneity we experimentally demonstrate DenRAM's ability to replicate synaptic delay profiles, and to efficiently implement CD for spatio-temporal pattern recognition. To validate the architecture, we conduct comprehensive system-level simulations on two representative temporal benchmarks, demonstrating DenRAM's resilience to analog hardware noise, and its superior accuracy compared to recurrent architectures with an equivalent number of parameters. DenRAM not only brings rich temporal processing capabilities to neuromorphic architectures, but also reduces the memory footprint of edge devices, warrants high accuracy on temporal benchmarks, and represents a significant step-forward in low-power real-time signal processing technologies.",['feed-forward spiking neural network'],"The research idea centers on the critical role of spatial dendritic branching in pyramidal neurons of the neocortex for enabling non-linear computation through localized synaptic integration. Dendritic branches are essential for temporal signal processing and feature detection by facilitating coincidence detection mechanisms that align temporally disparate inputs for effective integration. The study addresses the importance of synaptic delays in supporting spatio-temporal pattern recognition in neural networks. The primary objective of the study is to experimentally demonstrate the ability to replicate synaptic delay profiles and efficiently implement coincidence detection for spatio-temporal pattern recognition using dendritic circuits. Additionally, the study aims to validate the architecture’s resilience to noise and its accuracy on temporal benchmarks, highlighting its potential for enhancing temporal processing capabilities and reducing memory requirements in real-time signal processing applications.","The research idea centers on the critical role of spatial dendritic branching in pyramidal neurons of the neocortex for enabling non-linear computation through localized synaptic integration. Dendritic branches are essential for temporal signal processing and feature detection by facilitating coincidence detection mechanisms that align temporally disparate inputs for effective integration. The study addresses the importance of synaptic delays in supporting spatio-temporal pattern recognition in neural circuits. The primary objective of the study is to experimentally demonstrate the ability to replicate synaptic delay profiles and efficiently implement coincidence detection for spatio-temporal pattern recognition using dendritic circuits. Additionally, the study aims to validate the architecture's resilience to noise and its accuracy on temporal benchmarks, highlighting its potential for enhancing temporal processing capabilities and reducing memory requirements in real-time signal processing applications.",True
Medicine,Colon and lung cancer classification from multi-modal images using resilient and efficient neural network architectures,"Automatic classification of colon and lung cancer images is crucial for early detection and accurate diagnostics. However, there is room for improvement to enhance accuracy, ensuring better diagnostic precision. This study introduces two novel dense architectures (D1 and D2) and emphasizes their effectiveness in classifying colon and lung cancer from diverse images. It also highlights their resilience, efficiency, and superior performance across multiple datasets. These architectures were tested on various types of datasets, including NCT-CRC-HE-100K (set of 100,000 non-overlapping image patches from hematoxylin and eosin (H&E) stained histological images of human colorectal cancer (CRC) and normal tissue), CRC-VAL-HE-7K (set of 7180 image patches from N=50 patients with colorectal adenocarcinoma, no overlap with patients in NCT-CRC-HE-100K), LC25000 (Lung and Colon Cancer Histopathological Image), and IQ-OTHNCCD (Iraq-Oncology Teaching Hospital/National Center for Cancer Diseases), showcasing their effectiveness in classifying colon and lung cancers from histopathological and Computed Tomography (CT) scan images. This underscores the multi-modal image classification capability of the proposed models. Moreover, the study addresses imbalanced datasets, particularly in CRC-VAL-HE-7K and IQ-OTHNCCD, with a specific focus on model resilience and robustness. To assess overall performance, the study conducted experiments in different scenarios. The D1 model achieved an impressive 99.80% accuracy on the NCT-CRC-HE-100K dataset, with a Jaccard Index (J) of 0.8371, a Matthew's Correlation Coefficient (MCC) of 0.9073, a Cohen's Kappa (Kp) of 0.9057, and a Critical Success Index (CSI) of 0.8213. When subjected to 10-fold cross-validation on LC25000, the D1 model averaged (avg) 99.96% accuracy (avg J, MCC, Kp, and CSI of 0.9993, 0.9987, 0.9853, and 0.9990), surpassing recent reported performances. Furthermore, the ensemble of D1 and D2 reached 93% accuracy (J, MCC, Kp, and CSI of 0.7556, 0.8839, 0.8796, and 0.7140) on the IQ-OTHNCCD dataset, exceeding recent benchmarks and aligning with other reported results. Efficiency evaluations were conducted in various scenarios. For instance, training on only 10% of LC25000 resulted in high accuracy rates of 99.19% (J, MCC, Kp, and CSI of 0.9840, 0.9898, 0.9898, and 0.9837) (D1) and 99.30% (J, MCC, Kp, and CSI of 0.9863, 0.9913, 0.9913, and 0.9861) (D2). In NCT-CRC-HE-100K, D2 achieved an impressive 99.53% accuracy (J, MCC, Kp, and CSI of 0.9906, 0.9946, 0.9946, and 0.9906) with training on only 30% of the dataset and testing on the remaining 70%. When tested on CRC-VAL-HE-7K, D1 and D2 achieved 95% accuracy (J, MCC, Kp, and CSI of 0.8845, 0.9455, 0.9452, and 0.8745) and 96% accuracy (J, MCC, Kp, and CSI of 0.8926, 0.9504, 0.9503, and 0.8798), respectively, outperforming previously reported results and aligning closely with others. Lastly, training D2 on just 10% of NCT-CRC-HE-100K and testing on CRC-VAL-HE-7K resulted in significant outperformance of InceptionV3, Xception, and DenseNet201 benchmarks, achieving an accuracy rate of 82.98% (J, MCC, Kp, and CSI of 0.7227, 0.8095, 0.8081, and 0.6671). Finally, using explainable AI algorithms such as Grad-CAM, Grad-CAM++, Score-CAM, and Faster Score-CAM, along with their emphasized versions, we visualized the features from the last layer of DenseNet201 for histopathological as well as CT-scan image samples. The proposed dense models, with their multi-modality, robustness, and efficiency in cancer image classification, hold the promise of significant advancements in medical diagnostics. They have the potential to revolutionize early cancer detection and improve healthcare accessibility worldwide.","['InceptionV3', 'Xception', 'DenseNet201', 'Grad-CAM', 'Grad-CAM++', 'Score-CAM', 'Faster Score-CAM']","The research idea centers on the critical need for improved classification of colon and lung cancer images to enable early detection and enhance diagnostic accuracy. Despite existing methods, there remains significant room for improvement in achieving higher precision in cancer diagnostics. This study addresses the challenge of accurately classifying diverse histopathological and CT scan images of colon and lung cancers, including handling imbalanced datasets, to support better clinical outcomes.

The primary objective of the study is to evaluate the effectiveness, resilience, and efficiency of two novel dense architectures in classifying colon and lung cancer images across multiple datasets. The study aims to demonstrate superior performance in cancer image classification, including multi-modal imaging, and to assess the models’ robustness in various scenarios, ultimately contributing to advancements in early cancer detection and improved healthcare accessibility.","The research idea centers on the critical need for improved classification of colon and lung cancer images to enable early detection and enhance diagnostic accuracy. Despite existing methods, there remains significant room for improvement in achieving higher precision in cancer diagnostics. This study addresses the challenge of accurately classifying diverse histopathological and CT scan images of colon and lung cancers, including handling imbalanced datasets, to support better clinical outcomes.

The primary objective of the study is to evaluate the effectiveness, resilience, and efficiency of two novel structural approaches in classifying colon and lung cancer images across multiple datasets. The study aims to demonstrate superior performance in cancer image classification, including multi-modal imaging, and to assess the methodologies' robustness in various scenarios, ultimately contributing to advancements in early cancer detection and improved healthcare accessibility.",True
Medicine,Comparing machine learning algorithms to predict vegetation fire detections in Pakistan,"Abstract Vegetation fires have major impacts on the ecosystem and present a significant threat to human life. Vegetation fires consists of forest fires, cropland fires, and other vegetation fires in this study. Currently, there is a limited amount of research on the long-term prediction of vegetation fires in Pakistan. The exact effect of every factor on the frequency of vegetation fires remains unclear when using standard analysis. This research utilized the high proficiency of machine learning algorithms to combine data from several sources, including the MODIS Global Fire Atlas dataset, topographic, climatic conditions, and different vegetation types acquired between 2001 and 2022. We tested many algorithms and ultimately chose four models for formal data processing. Their selection was based on their performance metrics, such as accuracy, computational efficiency, and preliminary test results. The model’s logistic regression, a random forest, a support vector machine, and an eXtreme Gradient Boosting were used to identify and select the nine key factors of forest and cropland fires and, in the case of other vegetation, seven key factors that cause a fire in Pakistan. The findings indicated that the vegetation fire prediction models achieved prediction accuracies ranging from 78.7 to 87.5% for forest fires, 70.4 to 84.0% for cropland fires, and 66.6 to 83.1% for other vegetation. Additionally, the area under the curve (AUC) values ranged from 83.6 to 93.4% in forest fires, 72.6 to 90.6% in cropland fires, and 74.2 to 90.7% in other vegetation. The random forest model had the highest accuracy rate of 87.5% in forest fires, 84.0% in cropland fires, and 83.1% in other vegetation and also the highest AUC value of 93.4% in forest fires, 90.6% in cropland fires, and 90.7% in other vegetation, proving to be the most optimal performance model. The models provided predictive insights into specific conditions and regional susceptibilities to fire occurrences, adding significant value beyond the initial MODIS detection data. The maps generated to analyze Pakistan’s vegetation fire risk showed the geographical distribution of areas with high, moderate, and low vegetation fire risks, highlighting predictive risk assessments rather than historical fire detections.","['logistic regression', 'random forest', 'support vector machine', 'eXtreme Gradient Boosting']","The study addresses the significant impact of vegetation fires, including forest fires, cropland fires, and other types, on ecosystems and human life, with a particular focus on Pakistan. There is a limited amount of research on the long-term prediction of vegetation fires in this region, and the exact influence of various factors on fire frequency remains unclear. The primary aim of the study is to identify key factors contributing to different types of vegetation fires in Pakistan and to provide predictive insights into specific conditions and regional susceptibilities to fire occurrences. This objective includes generating risk maps that highlight the geographical distribution of areas with varying levels of vegetation fire risk, thereby enhancing understanding beyond historical fire detections.","The study addresses the significant impact of vegetation fires, including forest fires, cropland fires, and other types, on ecosystems and human life, with a particular focus on Pakistan. There is a limited amount of research on the long-term prediction of vegetation fires in this region, and the exact influence of various factors on fire frequency remains unclear. The primary aim of the study is to identify key factors contributing to different types of vegetation fires in Pakistan and to provide insights into specific conditions and regional susceptibilities to fire occurrences. This objective includes generating risk maps that highlight the geographical distribution of areas with varying levels of vegetation fire risk, thereby enhancing understanding beyond historical fire detections.",True
Medicine,A comprehensive investigation of multimodal deep learning fusion strategies for breast cancer classification,"In breast cancer research, diverse data types and formats, such as radiological images, clinical records, histological data, and expression analysis, are employed. Given the intricate nature of natural phenomena, relying on the features of a single modality is seldom sufficient for comprehensive analysis. Therefore, it is possible to guarantee medical relevance and achieve improved clinical outcomes by combining several modalities. The presen study carefully maps and reviews 47 primary articles from six well-known digital libraries that were published between 2018 and 2023 for breast cancer classification based on multimodal deep learning fusion (MDLF) techniques. This systematic literature review encompasses various aspects, including the medical modalities combined, the datasets utilized in these studies, the techniques, models, and architectures used in MDLF and it also discusses the advantages and limitations of each approach. The analysis of selected papers has revealed a compelling trend: the emergence of new modalities and combinations that were previously unexplored in the context of breast cancer classification. This exploration has not only expanded the scope of predictive models but also introduced fresh perspectives for addressing diverse targets, ranging from screening to diagnosis and prognosis. The practical advantages of MDLF are evident in its ability to enhance the predictive capabilities of machine learning models, resulting in improved accuracy across diverse applications. The prevalence of deep learning models underscores their success in autonomously discerning complex patterns, offering a substantial departure from traditional machine learning approaches. Furthermore, the paper explores the challenges and future directions in this field, including the need for larger datasets, the use of ensemble learning methods, and the interpretation of multimodal models.",['ensemble learning methods'],"The research idea centers on the complexity of breast cancer diagnosis and prognosis, highlighting that relying on a single type of medical data is often insufficient for comprehensive understanding. Combining multiple medical modalities, such as radiological images, clinical records, histological data, and expression analysis, can enhance medical relevance and improve clinical outcomes. This approach addresses the need for more effective methods to capture the multifaceted nature of breast cancer. The research objective is to systematically review and map existing studies published between 2018 and 2023 that focus on breast cancer classification using combined medical modalities. The study aims to identify the various medical data types used, examine the advantages and limitations of different approaches, and explore emerging combinations of modalities that offer new perspectives for screening, diagnosis, and prognosis in breast cancer care.","The research idea centers on the complexity of breast cancer diagnosis and prognosis, highlighting that relying on a single type of medical data is often insufficient for comprehensive understanding. Combining multiple medical modalities, such as radiological images, clinical records, histological data, and expression analysis, can enhance medical relevance and improve clinical outcomes. This approach addresses the need for more effective methods to capture the multifaceted nature of breast cancer. The research objective is to systematically review and map existing studies published between 2018 and 2023 that focus on breast cancer characterization using combined medical modalities. The study aims to identify the various medical data types used, examine the advantages and limitations of different approaches, and explore emerging combinations of modalities that offer new perspectives for screening, diagnosis, and prognosis in breast cancer care.",True
Medicine,Risk predictions of surgical wound complications based on a machine learning algorithm: A systematic review,"Abstract Surgical wounds may arise due to harm inflicted upon soft tissue during surgical intervention, and many complications and injuries may accompany them. These complications can lead to prolonged hospitalization and poorer clinical outcomes. Also, Machine learning (ML) is a Section of artificial intelligence (AI) that has emerged in medical care and is increasingly used for diagnosis, complications, prognosis and recurrence prediction. This study aims to investigate surgical wound risk predictions and management using a ML algorithm by R programming language analysis. The systematic review, following PRISMA guidelines, spanned electronic databases using search terms like ‘machine learning’, ‘surgical’ and ‘wound’. Inclusion criteria covered experimental studies from 1990 to the present on ML's application in surgical wound evaluation. Exclusion criteria included studies lacking full text, focusing on ML in all surgeries, neglecting wound assessment and duplications. Two authors rigorously assessed titles, abstracts and full texts, excluding reviews and guidelines. Ultimately, relevant articles were then analysed. The present study identified nine articles employing ML for surgical wound management. The analysis encompassed various surgical procedures, including Cardiothoracic, Caesarean total abdominal colectomy, Burn plastic surgery, facial plastic surgery, laparotomy, minimal invasive surgery, hernia repair and unspecified surgeries. ML was skillful in evaluating surgical site infections (SSI) in seven studies, while two extended its use to burn‐grade diagnosis and wound classification. Support Vector Machine (SVM) and Convolutional Neural Network (CNN) were the most utilized algorithms. ANN achieved a 96% accuracy in facial plastic surgery wound management. CNN demonstrated commendable accuracies in various surgeries, and SVM exhibited high accuracy in multiple surgeries and burn plastic surgery. In sum, these findings underscore ML's potential for significant improvements in postoperative management and the development of enhanced care techniques, particularly in surgical wound management.","['Support Vector Machine (SVM)', 'Convolutional Neural Network (CNN)', 'Artificial Neural Network (ANN)']","The research idea centers on the challenges posed by surgical wounds, which result from soft tissue damage during surgical procedures and can lead to complications, prolonged hospitalization, and poorer clinical outcomes. Addressing these complications is crucial for improving patient recovery and postoperative care. The study’s primary objective is to investigate the prediction and management of surgical wound risks to enhance postoperative outcomes. Specifically, it aims to evaluate approaches for assessing surgical site infections and wound classification across various types of surgeries to improve surgical wound management and care techniques.","The research idea centers on the challenges posed by surgical wounds, which result from soft tissue damage during surgical procedures and can lead to complications, prolonged hospitalization, and poorer clinical outcomes. Addressing these complications is crucial for improving patient recovery and postoperative care. The study's primary objective is to investigate the identification and management of surgical wound risks to enhance postoperative outcomes. Specifically, it aims to evaluate approaches for assessing surgical site infections and wound classification across various types of surgeries to improve surgical wound management and care techniques.",True
Medicine,Prediction models for postoperative delirium in elderly patients with machine-learning algorithms and SHapley Additive exPlanations,"Abstract Postoperative delirium (POD) is a common and severe complication in elderly patients with hip fractures. Identifying high-risk patients with POD can help improve the outcome of patients with hip fractures. We conducted a retrospective study on elderly patients (≥65 years of age) who underwent orthopedic surgery with hip fracture between January 2014 and August 2019. Conventional logistic regression and five machine-learning algorithms were used to construct prediction models of POD. A nomogram for POD prediction was built with the logistic regression method. The area under the receiver operating characteristic curve (AUC-ROC), accuracy, sensitivity, and precision were calculated to evaluate different models. Feature importance of individuals was interpreted using Shapley Additive Explanations (SHAP). About 797 patients were enrolled in the study, with the incidence of POD at 9.28% (74/797). The age, renal insufficiency, chronic obstructive pulmonary disease (COPD), use of antipsychotics, lactate dehydrogenase (LDH), and C-reactive protein are used to build a nomogram for POD with an AUC of 0.71. The AUCs of five machine-learning models are 0.81 (Random Forest), 0.80 (GBM), 0.68 (AdaBoost), 0.77 (XGBoost), and 0.70 (SVM). The sensitivities of the six models range from 68.8% (logistic regression and SVM) to 91.9% (Random Forest). The precisions of the six machine-learning models range from 18.3% (logistic regression) to 67.8% (SVM). Six prediction models of POD in patients with hip fractures were constructed using logistic regression and five machine-learning algorithms. The application of machine-learning algorithms could provide convenient POD risk stratification to benefit elderly hip fracture patients.","['logistic regression', 'Random Forest', 'GBM', 'AdaBoost', 'XGBoost', 'SVM']","The research idea addresses the issue that postoperative delirium (POD) is a common and severe complication in elderly patients with hip fractures, and identifying patients at high risk for POD can help improve their outcomes. Given the significant impact of POD on this vulnerable population, there is a need for effective methods to predict which patients are more likely to develop this complication. The primary objective of the study is to construct and evaluate prediction models for POD in elderly patients undergoing orthopedic surgery for hip fractures. The study aims to develop tools that can stratify the risk of POD to ultimately benefit elderly hip fracture patients by enabling better clinical management and intervention.","The research idea addresses the issue that postoperative delirium (POD) is a common and severe complication in elderly patients with hip fractures, and identifying patients at high risk for POD can help improve their outcomes. Given the significant impact of POD on this vulnerable population, there is a need for effective methods to predict which patients are more likely to develop this complication. The primary objective of the study is to construct and evaluate prediction methods for POD in elderly patients undergoing orthopedic surgery for hip fractures. The study aims to develop tools that can stratify the risk of POD to ultimately benefit elderly hip fracture patients by enabling better clinical management and intervention.",True
Medicine,Random forest regression for prediction of Covid-19 daily cases and deaths in Turkey,"During pandemic periods, there is an intense flow of patients to hospitals. Depending on the disease, many patients may require hospitalization. In some cases, these patients must be taken to intensive care units and emergency interventions must be performed. However, finding a sufficient number of hospital beds or intensive care units during pandemic periods poses a big problem. In these periods, fast and effective planning is more important than ever. Another problem experienced during pandemic periods is the burial of the dead in case the number of deaths increases. This is also a situation that requires due planning. We can learn some lessons from Covid 19 pandemic and be prepared for the future ones. In this paper, statistical properties of the daily cases and daily deaths in Turkey, which is one of the most affected countries by the pandemic in the World, are studied. It is found that the characteristics are nonstationary. Then, random forest regression is applied to predict Covid-19 daily cases and deaths. In addition, seven other machine learning models, namely bagging, AdaBoost, gradient boosting, XGBoost, decision tree, LSTM and ARIMA regressors are built for comparison. The performance of the models are measured using accuracy, coefficient of variation, root-mean-square score and relative error metrics. When random forest regressors are employed, test data related to daily cases are predicted with an accuracy of 92.30% and with an r2 score of 0.9893. Besides, daily deaths are predicted with an accuracy of 91.39% and with an r2 score of 0.9834. The closest rival in predictions is the bagging regressor. Nevertheless, the results provided by this algoritm changed in different runs and this fact is shown in the study, as well. Comparisons are based on test data. Comparisons with the earlier works are also provided.","['random forest regression', 'bagging', 'AdaBoost', 'gradient boosting', 'XGBoost', 'decision tree', 'LSTM']","The research idea addresses the critical challenges faced during pandemic periods, such as the overwhelming influx of patients requiring hospitalization and intensive care, as well as the difficulties in managing hospital bed availability and the burial of increased numbers of deceased individuals. Effective and timely planning during such crises is essential to mitigate these problems. The study aims to learn from the Covid-19 pandemic experience in Turkey, one of the most affected countries, by examining the statistical properties of daily cases and deaths to better understand the dynamics of the disease spread and its impact. The primary objective of the study is to predict Covid-19 daily cases and deaths in Turkey to support more informed decision-making and planning during pandemic periods, thereby improving healthcare resource allocation and management.","The research idea addresses the critical challenges faced during pandemic periods, such as the overwhelming influx of patients requiring hospitalization and intensive care, as well as the difficulties in managing hospital bed availability and the burial of increased numbers of deceased individuals. Effective and timely planning during such crises is essential to mitigate these problems. The study aims to learn from the Covid-19 pandemic experience in Turkey, one of the most affected countries, by examining the statistical properties of daily cases and deaths to better understand the dynamics of the disease spread and its impact. The primary objective of the study is to forecast Covid-19 daily cases and deaths in Turkey to support more informed decision-making and planning during pandemic periods, thereby improving healthcare resource allocation and management.",True
Medicine,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,"Abstract Considering the profound transformation affecting pathology practice, we aimed to develop a scalable artificial intelligence (AI) system to diagnose colorectal cancer from whole-slide images (WSI). For this, we propose a deep learning (DL) system that learns from weak labels, a sampling strategy that reduces the number of training samples by a factor of six without compromising performance, an approach to leverage a small subset of fully annotated samples, and a prototype with explainable predictions, active learning features and parallelisation. Noting some problems in the literature, this study is conducted with one of the largest WSI colorectal samples dataset with approximately 10,500 WSIs. Of these samples, 900 are testing samples. Furthermore, the robustness of the proposed method is assessed with two additional external datasets (TCGA and PAIP) and a dataset of samples collected directly from the proposed prototype. Our proposed method predicts, for the patch-based tiles, a class based on the severity of the dysplasia and uses that information to classify the whole slide. It is trained with an interpretable mixed-supervision scheme to leverage the domain knowledge introduced by pathologists through spatial annotations. The mixed-supervision scheme allowed for an intelligent sampling strategy effectively evaluated in several different scenarios without compromising the performance. On the internal dataset, the method shows an accuracy of 93.44% and a sensitivity between positive (low-grade and high-grade dysplasia) and non-neoplastic samples of 0.996. On the external test samples varied with TCGA being the most challenging dataset with an overall accuracy of 84.91% and a sensitivity of 0.996.",['deep learning (DL) system that learns from weak labels'],"The research idea addresses the significant changes occurring in pathology practice, specifically focusing on the need for improved diagnostic approaches for colorectal cancer using whole-slide images. The study recognizes challenges in existing literature and emphasizes the importance of utilizing a large dataset to enhance diagnostic accuracy and robustness across different sample collections. The research objective is to develop a scalable and effective method for diagnosing colorectal cancer from whole-slide images, aiming to accurately classify tissue samples based on the severity of dysplasia. The study seeks to evaluate the performance and robustness of this diagnostic approach using a large internal dataset as well as multiple external datasets to ensure reliability and clinical relevance.","The research idea addresses the significant changes occurring in pathology practice, specifically focusing on the need for improved diagnostic approaches for colorectal cancer using whole-slide images. The study recognizes challenges in existing literature and emphasizes the importance of utilizing a large dataset to enhance diagnostic accuracy and robustness across different sample collections. The research objective is to develop a scalable and effective methodology for diagnosing colorectal cancer from whole-slide images, aiming to accurately classify tissue samples based on the severity of dysplasia. The study seeks to evaluate the performance and robustness of this diagnostic approach using a large internal dataset as well as multiple external datasets to ensure reliability and clinical relevance.",True
Medicine,A study on smart home use intention of elderly consumers based on technology acceptance models,"Purpose Smart home devices have great potential to improve the quality of life and independence of older people, positively impacting their health, safety, and comfort. However, Chinese research in this field is still in its early stages. Therefore, more comprehensive and in-depth studies are needed to comprehend the various aspects influencing the acceptance and use of smart homes by older users. Patients and methods This study adopted the Technology Acceptance Model (TAM) and included perceived usefulness, perceived ease of use, usage intention, intergenerational technology support, perceived value, and perceived risk as extension variables to delve deeper into the behavioral intentions of older users in smart home services. The study used a convenience sampling method to randomly distribute 236 questionnaires among older adults over the age of 60 in the school’s community and neighboring urban communities who have experience in smart home use and who can complete human-computer interactions either independently or with the help of others, mainly focusing on the four sections: user characteristics, family situation, experience of use, and usage intention. The study used structural equation modeling (SEM) and factor analysis to analyze the completion of questionnaires. Finally, we conducted a validation analysis of the rationality and scientificity of the model and derived the six dimensions of the model of the influencing factors on the use of smart home products by the elderly and the weight sizes of their corresponding 13 influencing factors. Results The results show that perceived usefulness and perceived ease of use have a positive effect on users’ intention to use smart homes. Perceived ease of use has a positive effect on the perceived usefulness of smart homes. In addition, intergenerational technology support, perceived value, and perceived risk impact users’ perceived usefulness and perceived ease of use of the smart home. Conclusion This research aims to describe the factors influencing older users’ willingness to use smart homes. The findings are not only significant for the elderly in China but also of broad value to other regions and countries facing similar demographic challenges. The development of smart homes not only involves the elderly but is also closely related to all segments of society. The government should increase policy support and guide more social forces to participate in the development of the smart home industry. Service providers and designers should fully understand the demand situation and user experience of target users to develop easy-to-use smart home solutions. At the same time, smart homes, as intelligent products for the elderly, need to focus not only on the basic needs of the elderly such as material life and home safety, but also on the spiritual needs of elderly users. Children or caregivers should always pay attention to the psychological state of the elderly and actively guide them to use smart homes to help them realize their self-worth. We look forward to more research focusing on this area in the future and further exploring the specific issues and solutions involved.",['factor analysis'],"The research idea centers on the potential of smart home devices to enhance the quality of life, independence, health, safety, and comfort of older adults, particularly in the context of China where research on this topic remains limited. There is a need for more comprehensive and in-depth studies to understand the various factors that influence the acceptance and use of smart homes by elderly users. The study aims to describe the factors affecting older adults' willingness to use smart home technologies, recognizing that this issue is relevant not only in China but also in other regions facing similar demographic challenges. The research highlights the importance of addressing both the material and spiritual needs of the elderly to improve their experience with smart home products. The primary objective of the study is to identify and analyze the key factors influencing older users’ behavioral intentions toward using smart home services. This includes examining how perceived usefulness, ease of use, intergenerational support, perceived value, and perceived risk affect their willingness to adopt smart home technologies. The study seeks to provide insights that can guide policymakers, service providers, and designers in developing user-friendly smart home solutions that meet the diverse needs of the elderly population.","The research idea centers on the potential of smart home devices to enhance the quality of life, independence, health, safety, and comfort of older adults, particularly in the context of China where research on this topic remains limited. There is a need for more comprehensive and in-depth studies to understand the various factors that influence the acceptance and use of smart homes by elderly users. The study aims to describe the factors affecting older adults' willingness to use smart home technologies, recognizing that this issue is relevant not only in China but also in other regions facing similar demographic challenges. The research highlights the importance of addressing both the material and spiritual needs of the elderly to improve their experience with smart home products. The primary objective of the study is to identify and analyze the key factors influencing older users' behavioral intentions toward using smart home services. This includes examining how perceived usefulness, ease of use, intergenerational support, perceived value, and perceived risk affect their willingness to adopt smart home technologies. The study seeks to provide insights that can guide policymakers, service providers, and designers in developing user-friendly smart home solutions that meet the diverse needs of the elderly population.",True
Medicine,Employing machine learning for enhanced abdominal fat prediction in cavitation post-treatment,"This study investigates the application of cavitation in non-invasive abdominal fat reduction and body contouring, a topic of considerable interest in the medical and aesthetic fields. We explore the potential of cavitation to alter abdominal fat composition and delve into the optimization of fat prediction models using advanced hyperparameter optimization techniques, Hyperopt and Optuna. Our objective is to enhance the predictive accuracy of abdominal fat dynamics post-cavitation treatment. Employing a robust dataset with abdominal fat measurements and cavitation treatment parameters, we evaluate the efficacy of our approach through regression analysis. The performance of Hyperopt and Optuna regression models is assessed using metrics such as mean squared error, mean absolute error, and R-squared score. Our results reveal that both models exhibit strong predictive capabilities, with R-squared scores reaching 94.12% and 94.11% for post-treatment visceral fat, and 71.15% and 70.48% for post-treatment subcutaneous fat predictions, respectively. Additionally, we investigate feature selection techniques to pinpoint critical predictors within the fat prediction models. Techniques including F-value selection, mutual information, recursive feature elimination with logistic regression and random forests, variance thresholding, and feature importance evaluation are utilized. The analysis identifies key features such as BMI, waist circumference, and pretreatment fat levels as significant predictors of post-treatment fat outcomes. Our findings underscore the effectiveness of hyperparameter optimization in refining fat prediction models and offer valuable insights for the advancement of non-invasive fat reduction methods. This research holds important implications for both the scientific community and clinical practitioners, paving the way for improved treatment strategies in the realm of body contouring.","['mutual information', 'recursive feature elimination with logistic regression', 'recursive feature elimination with random forests', 'variance thresholding']","The study addresses the growing interest in non-invasive abdominal fat reduction and body contouring, focusing on the use of cavitation to alter abdominal fat composition. This area is significant in both medical and aesthetic fields due to the demand for effective and less invasive fat reduction methods. The research aims to improve the understanding of how cavitation treatment impacts abdominal fat dynamics. The primary objective of the study is to enhance the accuracy of predicting changes in abdominal fat following cavitation treatment by identifying key factors such as BMI, waist circumference, and pretreatment fat levels that influence post-treatment fat outcomes, thereby contributing to the development of more effective non-invasive fat reduction strategies.","The study addresses the growing interest in non-invasive abdominal fat reduction and body contouring, focusing on the use of cavitation to alter abdominal fat composition. This area is significant in both medical and aesthetic fields due to the demand for effective and less invasive fat reduction methods. The research aims to improve the understanding of how cavitation treatment impacts abdominal fat dynamics. The primary objective of the study is to enhance the understanding of changes in abdominal fat following cavitation treatment by identifying key factors such as BMI, waist circumference, and pretreatment fat levels that influence post-treatment fat outcomes, thereby contributing to the development of more effective non-invasive fat reduction strategies.",True
Medicine,Advancements in Predictive Microbiology: Integrating New Technologies for Efficient Food Safety Models,"Predictive microbiology is a rapidly evolving field that has gained significant interest over the years due to its diverse application in food safety. Predictive models are widely used in food microbiology to estimate the growth of microorganisms in food products. These models represent the dynamic interactions between intrinsic and extrinsic food factors as mathematical equations and then apply these data to predict shelf life, spoilage, and microbial risk assessment. Due to their ability to predict the microbial risk, these tools are also integrated into hazard analysis critical control point (HACCP) protocols. However, like most new technologies, several limitations have been linked to their use. Predictive models have been found incapable of modeling the intricate microbial interactions in food colonized by different bacteria populations under dynamic environmental conditions. To address this issue, researchers are integrating several new technologies into predictive models to improve efficiency and accuracy. Increasingly, newer technologies such as whole genome sequencing (WGS), metagenomics, artificial intelligence, and machine learning are being rapidly adopted into newer-generation models. This has facilitated the development of devices based on robotics, the Internet of Things, and time-temperature indicators that are being incorporated into food processing both domestically and industrially globally. This study reviewed current research on predictive models, limitations, challenges, and newer technologies being integrated into developing more efficient models. Machine learning algorithms commonly employed in predictive modeling are discussed with emphasis on their application in research and industry and their advantages over traditional models.",['machine learning'],"The research idea centers on the importance of predictive microbiology in food safety, particularly its role in estimating the growth of microorganisms in food products to predict shelf life, spoilage, and microbial risk. Despite their widespread use and integration into hazard analysis critical control point (HACCP) protocols, current predictive models face limitations in accurately representing complex microbial interactions under dynamic environmental conditions. The study aims to review the current state of predictive models in food microbiology, focusing on their limitations and challenges. Its primary objective is to evaluate recent advancements and technologies integrated into predictive models to enhance their efficiency and accuracy in assessing microbial risks in food safety.","The research idea centers on the importance of predictive microbiology in food safety, particularly its role in estimating the growth of microorganisms in food products to predict shelf life, spoilage, and microbial risk. Despite their widespread use and integration into hazard analysis critical control point (HACCP) protocols, current predictive approaches face limitations in accurately representing complex microbial interactions under dynamic environmental conditions. The study aims to review the current state of prediction methods in food microbiology, focusing on their limitations and challenges. Its primary objective is to evaluate recent advancements and technologies integrated into these predictive frameworks to enhance their efficiency and accuracy in assessing microbial risks in food safety.",True
Medicine,Brain structure ages—A new biomarker for multi‐disease classification,"Age is an important variable to describe the expected brain's anatomy status across the normal aging trajectory. The deviation from that normative aging trajectory may provide some insights into neurological diseases. In neuroimaging, predicted brain age is widely used to analyze different diseases. However, using only the brain age gap information (i.e., the difference between the chronological age and the estimated age) can be not enough informative for disease classification problems. In this paper, we propose to extend the notion of global brain age by estimating brain structure ages using structural magnetic resonance imaging. To this end, an ensemble of deep learning models is first used to estimate a 3D aging map (i.e., voxel-wise age estimation). Then, a 3D segmentation mask is used to obtain the final brain structure ages. This biomarker can be used in several situations. First, it enables to accurately estimate the brain age for the purpose of anomaly detection at the population level. In this situation, our approach outperforms several state-of-the-art methods. Second, brain structure ages can be used to compute the deviation from the normal aging process of each brain structure. This feature can be used in a multi-disease classification task for an accurate differential diagnosis at the subject level. Finally, the brain structure age deviations of individuals can be visualized, providing some insights about brain abnormality and helping clinicians in real medical contexts.",['ensemble of deep learning models'],"The research idea centers on the importance of age as a variable to describe the expected status of brain anatomy throughout the normal aging process, with deviations from this normative trajectory potentially offering insights into neurological diseases. Current approaches that use only the difference between chronological age and estimated brain age may not provide sufficient information for disease classification. The study aims to improve understanding of brain aging by estimating the ages of specific brain structures, which could enhance the detection of anomalies and support differential diagnosis of multiple diseases. The primary objective of the study is to accurately estimate brain structure ages using structural magnetic resonance imaging to identify deviations from normal aging processes at both the population and individual levels. This approach seeks to enable more precise anomaly detection, improve multi-disease classification, and provide visualizations that assist clinicians in identifying brain abnormalities in real medical contexts.","The research idea centers on the importance of age as a variable to describe the expected status of brain anatomy throughout the normal aging process, with deviations from this normative trajectory potentially offering insights into neurological diseases. Current approaches that use only the difference between chronological age and estimated brain age may not provide sufficient information for disease identification. The study aims to improve understanding of brain aging by determining the ages of specific brain structures, which could enhance the detection of anomalies and support differential diagnosis of multiple diseases. The primary objective of the study is to accurately determine brain structure ages using structural magnetic resonance imaging to identify deviations from normal aging processes at both the population and individual levels. This approach seeks to enable more precise anomaly detection, improve multi-disease identification, and provide visualizations that assist clinicians in identifying brain abnormalities in real medical contexts.",True
Medicine,Toward Improving Breast Cancer Classification Using an Adaptive Voting Ensemble Learning Algorithm,"Over the past decade, breast cancer has been the most common type of cancer in women. Different methods were proposed for breast cancer detection. These methods mainly classify and categorize malignant and Benign tumors. Machine learning is a practical approach for breast cancer classification. Data mining and classification are effective methods to predict and categorize breast cancer. The optimum classification for detecting Breast Cancer (BC) is ensemble-based. The ensemble approach involves using multiple ways to find the best possible solution. This study used the Wisconsin Breast Cancer Diagnostic (WBCD) dataset. We created a voting ensemble classifier that combines four different machine learning models: Extra Trees Classifier (ETC), Light Gradient Boosting Machine (LightGBM), Ridge Classifier (RC), and Linear Discriminant Analysis (LDA). The proposed ELRL-E approach achieved an accuracy of 97.6%, a precision of 96.4%, a recall of 100%, and an F1 score of 98.1%. Various output evaluations are used to evaluate the performance and efficiency of the proposed model and other classifiers. Overall, the recommended strategy performed better. Results are directly compared with the individual classifier and different recognized state-of-the-art classifiers. The primary objective of this study is to identify the most influential ensemble machine learning classifier for breast cancer detection and diagnosis in terms of accuracy and AUC score.","['Machine learning', 'ensemble-based classification', 'voting ensemble classifier', 'Extra Trees Classifier (ETC)', 'Light Gradient Boosting Machine (LightGBM)', 'Ridge Classifier (RC)', 'Linear Discriminant Analysis (LDA)']","Breast cancer has been the most common type of cancer in women over the past decade, and accurate detection and classification of malignant and benign tumors remain critical challenges. Various methods have been proposed to improve breast cancer detection, aiming to enhance the ability to correctly categorize tumor types for better diagnosis and treatment planning. The primary objective of this study is to identify the most effective approach for breast cancer detection and diagnosis by evaluating different classification strategies in terms of accuracy and diagnostic performance. This study specifically aims to determine which classification method provides the highest accuracy and reliability for breast cancer identification.","Breast cancer has been the most common type of cancer in women over the past decade, and accurate detection and classification of malignant and benign tumors remain critical challenges. Various methods have been proposed to improve breast cancer detection, aiming to enhance the ability to correctly categorize tumor types for better diagnosis and treatment planning. The primary objective of this study is to identify the most effective approach for breast cancer detection and diagnosis by evaluating different analytical techniques in terms of accuracy and diagnostic performance. This study specifically aims to determine which methodology provides the highest accuracy and reliability for breast cancer identification.",True
Medicine,"The Prediction of Clinical Mastitis in Dairy Cows Based on Milk Yield, Rumination Time, and Milk Electrical Conductivity Using Machine Learning Algorithms","In commercial dairy farms, mastitis is associated with increased antimicrobial use and associated resistance, which may affect milk production. This study aimed to develop sensor-based prediction models for naturally occurring clinical bovine mastitis using nine machine learning algorithms with data from 447 mastitic and 2146 healthy cows obtained from five commercial farms in Northeast China. The variables were related to daily activity, rumination time, and daily milk yield of cows, as well as milk electrical conductivity. Both Z-standardized and non-standardized datasets pertaining to four specific stages of lactation were used to train and test prediction models. For all four subgroups, the Z-standardized dataset yielded better results than those of the non-standardized one, with the multilayer artificial neural net algorithm showing the best performance. Variables of importance had a similar rank in this algorithm, indicating the consistency of these variables as predictors for bovine mastitis in commercial farms with similar automatic systems. Moreover, the peak milk yield (PMY) of mastitic cows was significantly higher than that of healthy cows (p &lt; 0.005), indicating that high-yielding cattle are more prone to mastitis. Our results show that machine learning algorithms are effective tools for predicting mastitis in dairy cows for immediate intervention and management in commercial farms.",['multilayer artificial neural net algorithm'],"The research idea addresses the problem of mastitis in commercial dairy farms, which is linked to increased antimicrobial use and resistance, potentially impacting milk production. Mastitis poses a significant challenge to dairy farming due to its effects on animal health and productivity. The study’s primary objective is to develop prediction models for naturally occurring clinical bovine mastitis using data related to cows’ daily activity, rumination time, milk yield, and milk electrical conductivity. This aims to enable early detection and timely intervention for mastitis in commercial dairy farms to improve management and reduce negative outcomes.","The research idea addresses the problem of mastitis in commercial dairy farms, which is linked to increased antimicrobial use and resistance, potentially impacting milk production. Mastitis poses a significant challenge to dairy farming due to its effects on animal health and productivity. The study's primary objective is to develop analytical tools for identifying naturally occurring clinical bovine mastitis using data related to cows' daily activity, rumination time, milk yield, and milk electrical conductivity. This aims to enable early detection and timely intervention for mastitis in commercial dairy farms to improve management and reduce negative outcomes.",True
Medicine,"Machine learning in physical activity, sedentary, and sleep behavior research","Abstract The nature of human movement and non-movement behaviors is complex and multifaceted, making their study complicated and challenging. Thanks to the availability of wearable activity monitors, we can now monitor the full spectrum of physical activity, sedentary, and sleep behaviors better than ever before—whether the subjects are elite athletes, children, adults, or individuals with pre-existing medical conditions. The increasing volume of generated data, combined with the inherent complexities of human movement and non-movement behaviors, necessitates the development of new data analysis methods for the research of physical activity, sedentary, and sleep behaviors. The characteristics of machine learning (ML) methods, including their ability to deal with complicated data, make them suitable for such analysis and thus can be an alternative tool to deal with data of this nature. ML can potentially be an excellent tool for solving many traditional problems related to the research of physical activity, sedentary, and sleep behaviors such as activity recognition, posture detection, profile analysis, and correlates research. However, despite this potential, ML has not yet been widely utilized for analyzing and studying these behaviors. In this review, we aim to introduce experts in physical activity, sedentary behavior, and sleep research—individuals who may possess limited familiarity with ML—to the potential applications of these techniques for analyzing their data. We begin by explaining the underlying principles of the ML modeling pipeline, highlighting the challenges and issues that need to be considered when applying ML. We then present the types of ML: supervised and unsupervised learning, and introduce a few ML algorithms frequently used in supervised and unsupervised learning. Finally, we highlight three research areas where ML methodologies have already been used in physical activity, sedentary behavior, and sleep behavior research, emphasizing their successes and challenges. This paper serves as a resource for ML in physical activity, sedentary, and sleep behavior research, offering guidance and resources to facilitate its utilization.","['machine learning (ML)', 'supervised learning', 'unsupervised learning']","The study addresses the complexity and multifaceted nature of human movement and non-movement behaviors, which makes their investigation challenging. With the advent of wearable activity monitors, it is now possible to better monitor the full spectrum of physical activity, sedentary behavior, and sleep across diverse populations, including elite athletes, children, adults, and individuals with pre-existing medical conditions. However, the increasing volume and complexity of the data generated from these monitors require new approaches to effectively study these behaviors. The primary aim of the study is to introduce experts in physical activity, sedentary behavior, and sleep research to the potential applications of advanced analytical techniques for examining their data. The study seeks to provide guidance and resources to facilitate the utilization of these approaches in research, highlighting their successes and challenges in the context of physical activity, sedentary, and sleep behavior studies.","The study addresses the complexity and multifaceted nature of human movement and non-movement behaviors, which makes their investigation challenging. With the advent of wearable activity monitors, it is now possible to better monitor the full spectrum of physical activity, sedentary behavior, and sleep across diverse populations, including elite athletes, children, adults, and individuals with pre-existing medical conditions. However, the increasing volume and complexity of the data generated from these monitors require new methodological approaches to effectively study these behaviors. The primary aim of the study is to introduce experts in physical activity, sedentary behavior, and sleep research to the potential applications of innovative statistical methods for examining their data. The study seeks to provide guidance and resources to facilitate the utilization of these approaches in research, highlighting their successes and challenges in the context of physical activity, sedentary, and sleep behavior studies.",True
Medicine,Enhanced Jaya Optimization Algorithm with Deep Learning Assisted Oral Cancer Diagnosis on IoT Healthcare Systems,"Recently, healthcare systems integrate the power of deep learning (DL) models with the connectivity and data processing capabilities of the Internet of Things (IoT) to enhance the early recognition and diagnosis of disease. Oral cancer diagnosis comprises the detection of cancerous or pre-cancerous abrasions in the oral cavity. Timely identification is essential for successful treatment and enhanced prognosis. Here is an overview of the key aspects of oral cancer diagnosis. One potential benefit of utilizing DL for oral cancer detection is that it analyses huge counts of data fast and accurately, and it could not need clear programming of the rules for recognizing abnormalities. This can create the procedure of detecting oral cancer more effective and efficient. Thus, the study presents an Enhanced Jaya Optimization Algorithm with Deep Learning Based Oral Cancer Classification (EJOADL-OCC) method. The presented EJOADL-OCC method aims to classify and detect the existence of oral cancer accurately and effectively. To accomplish this, the presented EJOADL-OCC method initially exploits median filtering for the noise elimination. Next, the feature vector generation process is performed by the residual network (ResNetv2) model with EJOA as a hyperparameter optimizer. For accurate classification of oral cancer, a continuously restricted Boltzmann machine with a deep belief network (CRBM-DBN) model. The simulated validation of the EJOADL-OCC algorithm is tested by the series of simulations and the outcome demonstrates its supremacy over present DL approaches.","['Deep Learning (DL)', 'Residual Network (ResNetv2)']","The research idea focuses on improving the early recognition and diagnosis of oral cancer, which involves detecting cancerous or pre-cancerous lesions in the oral cavity. Timely identification of oral cancer is crucial for successful treatment and better prognosis, highlighting the need for more effective and efficient diagnostic methods. The primary objective of the study is to develop a method that accurately and effectively classifies and detects the presence of oral cancer. This aims to enhance the diagnostic process by improving the accuracy and efficiency of oral cancer detection.","The research idea focuses on improving the early recognition and diagnosis of oral cancer, which involves detecting cancerous or pre-cancerous lesions in the oral cavity. Timely identification of oral cancer is crucial for successful treatment and better prognosis, highlighting the need for more effective and efficient diagnostic methods. The primary objective of the study is to develop a method that accurately and effectively identifies the presence of oral cancer. This aims to enhance the diagnostic process by improving the accuracy and efficiency of oral cancer detection.",True
Medicine,Chatbots and Large Language Models in Radiology: A Practical Primer for Clinical and Research Applications,"Although chatbots have existed for decades, the emergence of transformer-based large language models (LLMs) has captivated the world through the most recent wave of artificial intelligence chatbots, including ChatGPT. Transformers are a type of neural network architecture that enables better contextual understanding of language and efficient training on massive amounts of unlabeled data, such as unstructured text from the internet. As LLMs have increased in size, their improved performance and emergent abilities have revolutionized natural language processing. Since language is integral to human thought, applications based on LLMs have transformative potential in many industries. In fact, LLM-based chatbots have demonstrated human-level performance on many professional benchmarks, including in radiology. LLMs offer numerous clinical and research applications in radiology, several of which have been explored in the literature with encouraging results. Multimodal LLMs can simultaneously interpret text and images to generate reports, closely mimicking current diagnostic pathways in radiology. Thus, from requisition to report, LLMs have the opportunity to positively impact nearly every step of the radiology journey. Yet, these impressive models are not without limitations. This article reviews the limitations of LLMs and mitigation strategies, as well as potential uses of LLMs, including multimodal models. Also reviewed are existing LLM-based applications that can enhance efficiency in supervised settings.","['transformer-based large language models (LLMs)', 'Transformers', 'Multimodal LLMs']","The research idea centers on the transformative potential of advanced language-based technologies in the field of radiology, highlighting their ability to understand and generate human-like language and to interpret both text and images in a manner that closely mimics current diagnostic processes. These technologies have demonstrated human-level performance on professional benchmarks and offer numerous clinical and research applications that could positively impact nearly every step of the radiology workflow, from requisition to report. The study aims to address the limitations of these technologies and explore strategies to mitigate them while examining their potential uses in enhancing efficiency within supervised clinical settings. The primary objective of the study is to review the limitations and mitigation strategies of these advanced language-based models, as well as to evaluate existing applications that utilize these models to improve efficiency and effectiveness in radiology practice.","The research idea centers on the transformative potential of advanced language-based technologies in the field of radiology, highlighting their ability to understand and generate human-like language and to interpret both text and images in a manner that closely mimics current diagnostic processes. These technologies have demonstrated exceptional performance on professional benchmarks and offer numerous clinical and research applications that could positively impact nearly every step of the radiology workflow, from requisition to report. The study aims to address the limitations of these technologies and explore strategies to mitigate them while examining their potential uses in enhancing efficiency within supervised clinical settings. The primary objective of the study is to review the limitations and mitigation strategies of these advanced language-based models, as well as to evaluate existing applications that utilize these technologies to improve efficiency and effectiveness in radiology practice.",True
Medicine,Improving large language models for clinical named entity recognition via prompt engineering,"Abstract Importance The study highlights the potential of large language models, specifically GPT-3.5 and GPT-4, in processing complex clinical data and extracting meaningful information with minimal training data. By developing and refining prompt-based strategies, we can significantly enhance the models’ performance, making them viable tools for clinical NER tasks and possibly reducing the reliance on extensive annotated datasets. Objectives This study quantifies the capabilities of GPT-3.5 and GPT-4 for clinical named entity recognition (NER) tasks and proposes task-specific prompts to improve their performance. Materials and Methods We evaluated these models on 2 clinical NER tasks: (1) to extract medical problems, treatments, and tests from clinical notes in the MTSamples corpus, following the 2010 i2b2 concept extraction shared task, and (2) to identify nervous system disorder-related adverse events from safety reports in the vaccine adverse event reporting system (VAERS). To improve the GPT models' performance, we developed a clinical task-specific prompt framework that includes (1) baseline prompts with task description and format specification, (2) annotation guideline-based prompts, (3) error analysis-based instructions, and (4) annotated samples for few-shot learning. We assessed each prompt's effectiveness and compared the models to BioClinicalBERT. Results Using baseline prompts, GPT-3.5 and GPT-4 achieved relaxed F1 scores of 0.634, 0.804 for MTSamples and 0.301, 0.593 for VAERS. Additional prompt components consistently improved model performance. When all 4 components were used, GPT-3.5 and GPT-4 achieved relaxed F1 socres of 0.794, 0.861 for MTSamples and 0.676, 0.736 for VAERS, demonstrating the effectiveness of our prompt framework. Although these results trail BioClinicalBERT (F1 of 0.901 for the MTSamples dataset and 0.802 for the VAERS), it is very promising considering few training samples are needed. Discussion The study’s findings suggest a promising direction in leveraging LLMs for clinical NER tasks. However, while the performance of GPT models improved with task-specific prompts, there's a need for further development and refinement. LLMs like GPT-4 show potential in achieving close performance to state-of-the-art models like BioClinicalBERT, but they still require careful prompt engineering and understanding of task-specific knowledge. The study also underscores the importance of evaluation schemas that accurately reflect the capabilities and performance of LLMs in clinical settings. Conclusion While direct application of GPT models to clinical NER tasks falls short of optimal performance, our task-specific prompt framework, incorporating medical knowledge and training samples, significantly enhances GPT models' feasibility for potential clinical applications.","['GPT-3.5', 'GPT-4', 'prompt-based strategies', 'few-shot learning', 'BioClinicalBERT']","The research idea centers on addressing the challenge of effectively extracting meaningful clinical information, such as medical problems, treatments, tests, and adverse events, from complex clinical data with minimal reliance on extensive annotated datasets. The study highlights the potential to improve clinical named entity recognition (NER) tasks by developing strategies that enhance performance despite limited training data. The primary objective of the study is to quantify the capabilities of specific language models for clinical NER tasks and to propose task-specific strategies aimed at improving their performance in extracting relevant clinical concepts from clinical notes and safety reports. This includes evaluating the effectiveness of these strategies in enhancing the identification of medical entities related to nervous system disorder adverse events and other clinical information.","The research idea centers on addressing the challenge of effectively extracting meaningful clinical information, such as medical problems, treatments, tests, and adverse events, from complex clinical data with minimal reliance on extensive annotated datasets. The study highlights the potential to improve clinical named entity recognition (NER) tasks by developing strategies that enhance performance despite limited training data. The primary objective of the study is to quantify the capabilities of specific text analysis approaches for clinical NER tasks and to propose task-specific strategies aimed at improving their performance in extracting relevant clinical concepts from clinical notes and safety reports. This includes evaluating the effectiveness of these strategies in enhancing the identification of medical entities related to nervous system disorder adverse events and other clinical information.",True
Medicine,The Image Biomarker Standardization Initiative: Standardized Convolutional Filters for Reproducible Radiomics and Enhanced Clinical Insights,"Filters are commonly used to enhance specific structures and patterns in images, such as vessels or peritumoral regions, to enable clinical insights beyond the visible image using radiomics. However, their lack of standardization restricts reproducibility and clinical translation of radiomics decision support tools. In this special report, teams of researchers who developed radiomics software participated in a three-phase study (September 2020 to December 2022) to establish a standardized set of filters. The first two phases focused on finding reference filtered images and reference feature values for commonly used convolutional filters: mean, Laplacian of Gaussian, Laws and Gabor kernels, separable and nonseparable wavelets (including decomposed forms), and Riesz transformations. In the first phase, 15 teams used digital phantoms to establish 33 reference filtered images of 36 filter configurations. In phase 2, 11 teams used a chest CT image to derive reference values for 323 of 396 features computed from filtered images using 22 filter and image processing configurations. Reference filtered images and feature values for Riesz transformations were not established. Reproducibility of standardized convolutional filters was validated on a public data set of multimodal imaging (CT, fluorodeoxyglucose PET, and T1-weighted MRI) in 51 patients with soft-tissue sarcoma. At validation, reproducibility of 486 features computed from filtered images using nine configurations × three imaging modalities was assessed using the lower bounds of 95% CIs of intraclass correlation coefficients. Out of 486 features, 458 were found to be reproducible across nine teams with lower bounds of 95% CIs of intraclass correlation coefficients greater than 0.75. In conclusion, eight filter types were standardized with reference filtered images and reference feature values for verifying and calibrating radiomics software packages. A web-based tool is available for compliance checking. © RSNA, 2024 Supplemental material is available for this article. See also the editorial by Huisman and D'Antonoli in this issue.",['nonseparable wavelets'],"The research idea addresses the challenge that filters used to enhance specific structures and patterns in medical images, such as vessels or peritumoral regions, lack standardization, which limits the reproducibility and clinical translation of radiomics decision support tools. This lack of consistency hinders the ability to reliably use radiomics features derived from filtered images for clinical insights. The primary objective of the study was to establish a standardized set of filters by creating reference filtered images and reference feature values for commonly used convolutional filters. The study aimed to validate the reproducibility of these standardized filters across multiple imaging modalities and teams, ultimately providing a foundation for verifying and calibrating radiomics software packages to improve clinical reliability.","The research idea addresses the challenge that filters used to enhance specific structures and patterns in medical images, such as vessels or peritumoral regions, lack standardization, which limits the reproducibility and clinical translation of radiomics decision support tools. This lack of consistency hinders the ability to reliably use radiomics features derived from filtered images for clinical insights. The primary objective of the study was to establish a standardized set of filters by creating reference filtered images and reference feature values for commonly used filters. The study aimed to validate the reproducibility of these standardized filters across multiple imaging modalities and teams, ultimately providing a foundation for verifying and calibrating radiomics software packages to improve clinical reliability.",True
Medicine,Can large language models replace humans in systematic reviews? Evaluating <scp>GPT</scp>‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages,"Systematic reviews are vital for guiding practice, research and policy, although they are often slow and labour-intensive. Large language models (LLMs) could speed up and automate systematic reviews, but their performance in such tasks has yet to be comprehensively evaluated against humans, and no study has tested Generative Pre-Trained Transformer (GPT)-4, the biggest LLM so far. This pre-registered study uses a ""human-out-of-the-loop"" approach to evaluate GPT-4's capability in title/abstract screening, full-text review and data extraction across various literature types and languages. Although GPT-4 had accuracy on par with human performance in some tasks, results were skewed by chance agreement and dataset imbalance. Adjusting for these caused performance scores to drop across all stages: for data extraction, performance was moderate, and for screening, it ranged from none in highly balanced literature datasets (~1:1) to moderate in those datasets where the ratio of inclusion to exclusion in studies was imbalanced (~1:3). When screening full-text literature using highly reliable prompts, GPT-4's performance was more robust, reaching ""human-like"" levels. Although our findings indicate that, currently, substantial caution should be exercised if LLMs are being used to conduct systematic reviews, they also offer preliminary evidence that, for certain review tasks delivered under specific conditions, LLMs can rival human performance.",['Generative Pre-Trained Transformer (GPT)-4'],"The research idea centers on the importance of systematic reviews for guiding clinical practice, research, and policy, while highlighting the challenges posed by their slow and labor-intensive nature. There is a need to explore ways to improve the efficiency of conducting systematic reviews without compromising accuracy. The study’s primary objective is to evaluate the capability of a large language model, specifically GPT-4, in performing key tasks involved in systematic reviews such as title and abstract screening, full-text review, and data extraction across diverse types of literature and languages. The aim is to compare its performance against human reviewers to determine whether it can reliably assist or potentially replace humans in these tasks under certain conditions.","The research idea centers on the importance of systematic reviews for guiding clinical practice, research, and policy, while highlighting the challenges posed by their slow and labor-intensive nature. There is a need to explore ways to improve the efficiency of conducting systematic reviews without compromising accuracy. The study's primary objective is to evaluate new technological approaches in performing key tasks involved in systematic reviews such as title and abstract screening, full-text review, and data extraction across diverse types of literature and languages. The aim is to compare the performance of these approaches against human reviewers to determine whether they can reliably assist or potentially replace humans in these tasks under certain conditions.",True
Medicine,Convolutional neural network classification of cancer cytopathology images: taking breast cancer as an example,"Breast cancer is a relatively common cancer among gynecological cancers. Its diagnosis often relies on the pathology of cells in the lesion. The pathological diagnosis of breast cancer not only requires professionals and time, but also sometimes involves subjective judgment. To address the challenges of dependence on pathologists expertise and the time-consuming nature of achieving accurate breast pathological image classification, this paper introduces an approach utilizing convolutional neural networks (CNNs) for the rapid categorization of pathological images, aiming to enhance the efficiency of breast pathological image detection. And the approach enables the rapid and automatic classification of pathological images into benign and malignant groups. The methodology involves utilizing a convolutional neural network (CNN) model leveraging the Inceptionv3 architecture and transfer learning algorithm for extracting features from pathological images. Utilizing a neural network with fully connected layers and employing the SoftMax function for image classification. Additionally, the concept of image partitioning is introduced to handle high-resolution images. To achieve the ultimate classification outcome, the classification probabilities of each image block are aggregated using three algorithms: summation, product, and maximum. Experimental validation was conducted on the BreaKHis public dataset, resulting in accuracy rates surpassing 0.92 across all four magnification coefficients (40X, 100X, 200X, and 400X). It demonstrates that the proposed method effectively enhances the accuracy in classifying pathological images of breast cancer.","['convolutional neural networks (CNNs)', 'Inceptionv3 architecture', 'transfer learning algorithm', 'neural network with fully connected layers']","The research idea addresses the challenges in diagnosing breast cancer through pathological examination, which requires specialized expertise, is time-consuming, and can involve subjective judgment. Accurate classification of breast pathological images is crucial for effective diagnosis but depends heavily on the availability and skill of pathologists. The study aims to improve the efficiency and accuracy of breast cancer pathological image classification to support timely and reliable diagnosis. The primary objective of the study is to develop a method for the rapid and automatic classification of breast pathological images into benign and malignant categories, thereby enhancing the accuracy and efficiency of breast cancer detection. The study seeks to validate this approach using a public dataset to demonstrate its effectiveness across different magnification levels of pathological images.","The research idea addresses the challenges in diagnosing breast cancer through pathological examination, which requires specialized expertise, is time-consuming, and can involve subjective judgment. Accurate classification of breast pathological images is crucial for effective diagnosis but depends heavily on the availability and skill of pathologists. The study aims to improve the efficiency and accuracy of breast cancer pathological image classification to support timely and reliable diagnosis. The primary objective of the study is to develop a method for the rapid and systematic classification of breast pathological images into benign and malignant categories, thereby enhancing the accuracy and efficiency of breast cancer detection. The study seeks to validate this approach using a public dataset to demonstrate its effectiveness across different magnification levels of pathological images.",True
Medicine,Investigating Spatial Effects through Machine Learning and Leveraging Explainable AI for Child Malnutrition in Pakistan,"While socioeconomic gradients in regional health inequalities are firmly established, the synergistic interactions between socioeconomic deprivation and climate vulnerability within convenient proximity and neighbourhood locations with health disparities remain poorly explored and thus require deep understanding within a regional context. Furthermore, disregarding the importance of spatial spillover effects and nonlinear effects of covariates on childhood stunting are inevitable in dealing with an enduring issue of regional health inequalities. The present study aims to investigate the spatial inequalities in childhood stunting at the district level in Pakistan and validate the importance of spatial lag in predicting childhood stunting. Furthermore, it examines the presence of any nonlinear relationships among the selected independent features with childhood stunting. The study utilized data related to socioeconomic features from MICS 2017–2018 and climatic data from Integrated Contextual Analysis. A multi-model approach was employed to address the research questions, which included Ordinary Least Squares Regression (OLS), various Spatial Models, Machine Learning Algorithms and Explainable Artificial Intelligence methods. Firstly, OLS was used to analyse and test the linear relationships among selected variables. Secondly, Spatial Durbin Error Model (SDEM) was used to detect and capture the impact of spatial spillover on childhood stunting. Third, XGBoost and Random Forest machine learning algorithms were employed to examine and validate the importance of the spatial lag component. Finally, EXAI methods such as SHapley were utilized to identify potential nonlinear relationships. The study found a clear pattern of spatial clustering and geographical disparities in childhood stunting, with multidimensional poverty, high climate vulnerability and early marriage worsening childhood stunting. In contrast, low climate vulnerability, high exposure to mass media and high women’s literacy were found to reduce childhood stunting. The use of machine learning algorithms, specifically XGBoost and Random Forest, highlighted the significant role played by the average value in the neighbourhood in predicting childhood stunting in nearby districts, confirming that the spatial spillover effect is not bounded by geographical boundaries. Furthermore, EXAI methods such as partial dependency plot reveal the existence of a nonlinear relationship between multidimensional poverty and childhood stunting. The study’s findings provide valuable insights into the spatial distribution of childhood stunting in Pakistan, emphasizing the importance of considering spatial effects in predicting childhood stunting. Individual and household-level factors such as exposure to mass media and women’s literacy have shown positive implications for childhood stunting. It further provides a justification for the usage of EXAI methods to draw better insights and propose customised intervention policies accordingly.","['XGBoost', 'Random Forest', 'partial dependency plot']","The research idea addresses the persistent issue of regional health inequalities, focusing on the poorly explored interactions between socioeconomic deprivation and climate vulnerability within local neighborhoods and their impact on childhood stunting. It highlights the need to understand spatial spillover effects and nonlinear relationships among factors contributing to childhood stunting in a regional context. The study aims to investigate spatial inequalities in childhood stunting at the district level in Pakistan and to validate the significance of spatial influences in predicting childhood stunting. Additionally, it seeks to examine whether nonlinear relationships exist between selected socioeconomic and climatic factors and childhood stunting.","The research idea addresses the persistent issue of regional health inequalities, focusing on the poorly explored interactions between socioeconomic deprivation and climate vulnerability within local neighborhoods and their impact on childhood stunting. It highlights the need to understand spatial spillover effects and nonlinear relationships among factors contributing to childhood stunting in a regional context. The study aims to investigate spatial inequalities in childhood stunting at the district level in Pakistan and to validate the significance of spatial influences in understanding childhood stunting. Additionally, it seeks to examine whether nonlinear relationships exist between selected socioeconomic and climatic factors and childhood stunting.",True
Medicine,Unmasking bias in artificial intelligence: a systematic review of bias detection and mitigation strategies in electronic health record-based models,"Abstract Objectives Leveraging artificial intelligence (AI) in conjunction with electronic health records (EHRs) holds transformative potential to improve healthcare. However, addressing bias in AI, which risks worsening healthcare disparities, cannot be overlooked. This study reviews methods to handle various biases in AI models developed using EHR data. Materials and Methods We conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and Meta-analyses guidelines, analyzing articles from PubMed, Web of Science, and IEEE published between January 01, 2010 and December 17, 2023. The review identified key biases, outlined strategies for detecting and mitigating bias throughout the AI model development, and analyzed metrics for bias assessment. Results Of the 450 articles retrieved, 20 met our criteria, revealing 6 major bias types: algorithmic, confounding, implicit, measurement, selection, and temporal. The AI models were primarily developed for predictive tasks, yet none have been deployed in real-world healthcare settings. Five studies concentrated on the detection of implicit and algorithmic biases employing fairness metrics like statistical parity, equal opportunity, and predictive equity. Fifteen studies proposed strategies for mitigating biases, especially targeting implicit and selection biases. These strategies, evaluated through both performance and fairness metrics, predominantly involved data collection and preprocessing techniques like resampling and reweighting. Discussion This review highlights evolving strategies to mitigate bias in EHR-based AI models, emphasizing the urgent need for both standardized and detailed reporting of the methodologies and systematic real-world testing and evaluation. Such measures are essential for gauging models’ practical impact and fostering ethical AI that ensures fairness and equity in healthcare.","['resampling', 'reweighting']","The research idea centers on the critical issue of bias in healthcare applications developed using electronic health records, which poses a risk of exacerbating healthcare disparities. Addressing these biases is essential to ensure fairness and equity in healthcare delivery. The study recognizes the transformative potential of improving healthcare outcomes but emphasizes that bias must be carefully managed to avoid negative consequences. The primary objective of the study is to review existing methods for identifying and mitigating various types of bias in healthcare models developed from electronic health record data. It aims to outline strategies for detecting and reducing bias and to analyze how bias assessment metrics are applied, with the goal of promoting ethical practices and ensuring that healthcare interventions are fair and effective in real-world settings.","The research idea centers on the critical issue of bias in healthcare applications developed using electronic health records, which poses a risk of exacerbating healthcare disparities. Addressing these biases is essential to ensure fairness and equity in healthcare delivery. The study recognizes the transformative potential of improving healthcare outcomes but emphasizes that bias must be carefully managed to avoid negative consequences. The primary objective of the study is to review existing methods for identifying and mitigating various types of bias in healthcare analyses developed from electronic health record data. It aims to outline strategies for detecting and reducing bias and to analyze how bias assessment metrics are applied, with the goal of promoting ethical practices and ensuring that healthcare interventions are fair and effective in real-world settings.",True
Medicine,Real-Time Plant Disease Dataset Development and Detection of Plant Disease Using Deep Learning,"Agriculture plays a significant role in meeting food needs and providing food security for the increasingly growing global population, which has increased by 0.88% since 2022. Plant diseases can reduce food production and affect food security. Worldwide crop loss due to plant disease is estimated to be around 14.1%. The lack of proper identification of plant disease at the early stages of infection can result in inappropriate disease control measures. Therefore, the automatic identification and diagnosis of plant diseases are highly recommended. Lack of availability of large amounts of data that are not processed to a large extent is one of the main challenges in plant disease diagnosis. In the current manuscript, we developed datasets for food grains specifically for rice, wheat, and maize to address the identified challenges. The developed datasets consider the common diseases (two bacterial diseases and two fungal diseases of rice, four fungal diseases of maize, and four fungal diseases of wheat) that affect crop yields and cause damage to the whole plant. The datasets developed were applied to eight fine-tuned deep learning models with the same training hyperparameters. The experimental results based on eight fine-tuned deep learning models show that, while recognizing maize leaf diseases, the models Xception and MobileNet performed best with a testing accuracy of 0.9580 and 0.9464 respectively. Similarly, while recognizing the wheat leaf diseases, the models MobileNetV2 and MobileNet performed best with a testing accuracy of 0.9632 and 0.9628 respectively. The Xception and Inception V3 models performed best, with a testing accuracy of 0.9728 and 0.9620, respectively, for recognizing rice leaf diseases. The research also proposes a new convolutional neural network (CNN) model trained from scratch on all three food grain datasets developed. The proposed model performs well and shows a testing accuracy of 0.9704, 0.9706, and 0.9808 respectively on the maize, rice, and wheat datasets.","['fine-tuned deep learning models', 'Xception', 'MobileNet', 'MobileNetV2', 'Inception V3', 'convolutional neural network (CNN) model trained from scratch']","The study addresses the significant impact of plant diseases on food production and food security, emphasizing that crop losses due to such diseases are substantial worldwide. Early and accurate identification of plant diseases is crucial to prevent inappropriate disease control measures and to protect crop yields. The research focuses on the challenge posed by the lack of properly processed data for diagnosing plant diseases affecting major food grains such as rice, wheat, and maize. The primary objective of the study is to develop comprehensive datasets for common bacterial and fungal diseases affecting these food grains and to evaluate the effectiveness of various approaches in accurately recognizing these diseases to support better disease diagnosis and management.","The study addresses the significant impact of plant diseases on food production and food security, emphasizing that crop losses due to such diseases are substantial worldwide. Early and accurate identification of plant diseases is crucial to prevent inappropriate disease control measures and to protect crop yields. The research focuses on the challenge posed by the lack of properly processed data for diagnosing plant diseases affecting major food grains such as rice, wheat, and maize. The primary objective of the study is to develop comprehensive datasets for common bacterial and fungal diseases affecting these food grains and to evaluate the effectiveness of various methodologies in accurately identifying these diseases to support better disease diagnosis and management.",True
Medicine,Vision–language foundation model for echocardiogram interpretation,"Abstract The development of robust artificial intelligence models for echocardiography has been limited by the availability of annotated clinical data. Here, to address this challenge and improve the performance of cardiac imaging models, we developed EchoCLIP, a vision–language foundation model for echocardiography, that learns the relationship between cardiac ultrasound images and the interpretations of expert cardiologists across a wide range of patients and indications for imaging. After training on 1,032,975 cardiac ultrasound videos and corresponding expert text, EchoCLIP performs well on a diverse range of benchmarks for cardiac image interpretation, despite not having been explicitly trained for individual interpretation tasks. EchoCLIP can assess cardiac function (mean absolute error of 7.1% when predicting left ventricular ejection fraction in an external validation dataset) and identify implanted intracardiac devices (area under the curve (AUC) of 0.84, 0.92 and 0.97 for pacemakers, percutaneous mitral valve repair and artificial aortic valves, respectively). We also developed a long-context variant (EchoCLIP-R) using a custom tokenizer based on common echocardiography concepts. EchoCLIP-R accurately identified unique patients across multiple videos (AUC of 0.86), identified clinical transitions such as heart transplants (AUC of 0.79) and cardiac surgery (AUC 0.77) and enabled robust image-to-text search (mean cross-modal retrieval rank in the top 1% of candidate text reports). These capabilities represent a substantial step toward understanding and applying foundation models in cardiovascular imaging for preliminary interpretation of echocardiographic findings.",['vision–language foundation model'],"The research addresses the challenge of limited availability of annotated clinical data for echocardiography, which has hindered the development of robust methods for cardiac imaging interpretation. Improving the understanding and performance of cardiac ultrasound image analysis is crucial for accurate assessment of cardiac function and identification of intracardiac devices across diverse patient populations and clinical indications. The primary aim of the study is to develop a comprehensive approach that learns the relationship between cardiac ultrasound images and expert cardiologists’ interpretations to enhance the preliminary interpretation of echocardiographic findings. This includes accurately assessing cardiac function, identifying implanted devices, and recognizing significant clinical transitions such as heart transplants and cardiac surgery.","The research addresses the challenge of limited availability of annotated clinical data for echocardiography, which has hindered the development of robust methods for cardiac imaging interpretation. Improving the understanding and performance of cardiac ultrasound image analysis is crucial for accurate assessment of cardiac function and identification of intracardiac devices across diverse patient populations and clinical indications. The primary aim of the study is to develop a comprehensive approach that captures the relationship between cardiac ultrasound images and expert cardiologists' interpretations to enhance the preliminary interpretation of echocardiographic findings. This includes accurately assessing cardiac function, identifying implanted devices, and recognizing significant clinical transitions such as heart transplants and cardiac surgery.",True
Medicine,Enhancing heart disease prediction using a self-attention-based transformer model,"Abstract Cardiovascular diseases (CVDs) continue to be the leading cause of more than 17 million mortalities worldwide. The early detection of heart failure with high accuracy is crucial for clinical trials and therapy. Patients will be categorized into various types of heart disease based on characteristics like blood pressure, cholesterol levels, heart rate, and other characteristics. With the use of an automatic system, we can provide early diagnoses for those who are prone to heart failure by analyzing their characteristics. In this work, we deploy a novel self-attention-based transformer model, that combines self-attention mechanisms and transformer networks to predict CVD risk. The self-attention layers capture contextual information and generate representations that effectively model complex patterns in the data. Self-attention mechanisms provide interpretability by giving each component of the input sequence a certain amount of attention weight. This includes adjusting the input and output layers, incorporating more layers, and modifying the attention processes to collect relevant information. This also makes it possible for physicians to comprehend which features of the data contributed to the model's predictions. The proposed model is tested on the Cleveland dataset, a benchmark dataset of the University of California Irvine (UCI) machine learning (ML) repository. Comparing the proposed model to several baseline approaches, we achieved the highest accuracy of 96.51%. Furthermore, the outcomes of our experiments demonstrate that the prediction rate of our model is higher than that of other cutting-edge approaches used for heart disease prediction.","['self-attention-based transformer model', 'self-attention mechanisms', 'transformer networks']","Cardiovascular diseases (CVDs) remain the leading cause of over 17 million deaths worldwide, highlighting the critical need for early and accurate detection of heart failure to improve clinical outcomes and therapy. Identifying patients at risk by categorizing them based on characteristics such as blood pressure, cholesterol levels, and heart rate is essential for timely intervention. The primary aim of this study is to provide early diagnoses for individuals prone to heart failure by analyzing their clinical characteristics, thereby facilitating better risk prediction and management of cardiovascular diseases. This work focuses on improving the accuracy of heart disease prediction to support clinical decision-making and enhance patient care.","Cardiovascular diseases (CVDs) remain the leading cause of over 17 million deaths worldwide, highlighting the critical need for early and accurate detection of heart failure to improve clinical outcomes and therapy. Identifying patients at risk by categorizing them based on characteristics such as blood pressure, cholesterol levels, and heart rate is essential for timely intervention. The primary aim of this study is to provide early diagnoses for individuals prone to heart failure by analyzing their clinical characteristics, thereby facilitating better risk assessment and management of cardiovascular diseases. This work focuses on improving the accuracy of heart disease identification to support clinical decision-making and enhance patient care.",True
Medicine,The diagnostic and triage accuracy of the GPT-3 artificial intelligence model: an observational study,"BackgroundArtificial intelligence (AI) applications in health care have been effective in many areas of medicine, but they are often trained for a single task using labelled data, making deployment and generalisability challenging. How well a general-purpose AI language model performs diagnosis and triage relative to physicians and laypeople is not well understood.MethodsWe compared the predictive accuracy of Generative Pre-trained Transformer 3 (GPT-3)'s diagnostic and triage ability for 48 validated synthetic case vignettes (<50 words; sixth-grade reading level or below) of both common (eg, viral illness) and severe (eg, heart attack) conditions to a nationally representative sample of 5000 lay people from the USA who could use the internet to find the correct options and 21 practising physicians at Harvard Medical School. There were 12 vignettes for each of four triage categories: emergent, within one day, within 1 week, and self-care. The correct diagnosis and triage category (ie, ground truth) for each vignette was determined by two general internists at Harvard Medical School. For each vignette, human respondents and GPT-3 were prompted to list diagnoses in order of likelihood, and the vignette was marked as correct if the ground-truth diagnosis was in the top three of the listed diagnoses. For triage accuracy, we examined whether the human respondents' and GPT-3's selected triage was exactly correct according to the four triage categories, or matched a dichotomised triage variable (emergent or within 1 day vs within 1 week or self-care). We estimated GPT-3's diagnostic and triage confidence on a given vignette using a modified bootstrap resampling procedure, and examined how well calibrated GPT-3's confidence was by computing calibration curves and Brier scores. We also performed subgroup analysis by case acuity, and an error analysis for triage advice to characterise how its advice might affect patients using this tool to decide if they should seek medical care immediately.FindingsAmong all cases, GPT-3 replied with the correct diagnosis in its top three for 88% (42/48, 95% CI 75–94) of cases, compared with 54% (2700/5000, 53–55) for lay individuals (p<0.0001) and 96% (637/666, 94–97) for physicians (p=0·012). GPT-3 triaged 70% correct (34/48, 57–82) versus 74% (3706/5000, 73–75; p=0.60) for lay individuals and 91% (608/666, 89–93%; p<0.0001) for physicians. As measured by the Brier score, GPT-3 confidence in its top prediction was reasonably well calibrated for diagnosis (Brier score=0·18) and triage (Brier score=0·22). We observed an inverse relationship between case acuity and GPT-3 accuracy (p<0·0001) with a fitted trend line of –8·33% decrease in accuracy for every level of increase in case acuity. For triage error analysis, GPT-3 deprioritised truly emergent cases in seven instances.InterpretationA general-purpose AI language model without any content-specific training could perform diagnosis at levels close to, but below, physicians and better than lay individuals. We found that GPT-3's performance was inferior to physicians for triage, sometimes by a large margin, and its performance was closer to that of lay individuals. Although the diagnostic performance of GPT-3 was comparable to physicians, it was significantly better than a typical person using a search engine.FundingThe National Heart, Lung, and Blood Institute.",['Generative Pre-trained Transformer 3 (GPT-3)'],"The study addresses the challenge of understanding how well a general-purpose diagnostic and triage tool performs compared to physicians and laypeople, particularly given the difficulty in deploying tools that are typically trained for single tasks. There is limited knowledge about the accuracy and reliability of such a tool in diagnosing and triaging a range of medical conditions, from common illnesses to severe emergencies. The primary aim of the study is to evaluate the diagnostic and triage accuracy of a general-purpose language-based tool using validated clinical case vignettes, comparing its performance directly to that of practicing physicians and lay individuals. The study seeks to determine how closely the tool’s diagnostic and triage decisions align with expert-established ground truth and to assess its potential impact on patient decision-making regarding seeking medical care.","The study addresses the challenge of understanding how well a general-purpose diagnostic and triage tool performs compared to physicians and laypeople, particularly given the difficulty in deploying tools that are typically designed for single tasks. There is limited knowledge about the accuracy and reliability of such a tool in diagnosing and triaging a range of medical conditions, from common illnesses to severe emergencies. The primary aim of the study is to evaluate the diagnostic and triage accuracy of a general-purpose language-based tool using validated clinical case vignettes, comparing its performance directly to that of practicing physicians and lay individuals. The study seeks to determine how closely the tool's diagnostic and triage decisions align with expert-established ground truth and to assess its potential impact on patient decision-making regarding seeking medical care.",True
Medicine,Machine Learning and Digital Biomarkers Can Detect Early Stages of Neurodegenerative Diseases,"Neurodegenerative diseases (NDs) such as Alzheimer’s Disease (AD) and Parkinson’s Disease (PD) are devastating conditions that can develop without noticeable symptoms, causing irreversible damage to neurons before any signs become clinically evident. NDs are a major cause of disability and mortality worldwide. Currently, there are no cures or treatments to halt their progression. Therefore, the development of early detection methods is urgently needed to delay neuronal loss as soon as possible. Despite advancements in Medtech, the early diagnosis of NDs remains a challenge at the intersection of medical, IT, and regulatory fields. Thus, this review explores “digital biomarkers” (tools designed for remote neurocognitive data collection and AI analysis) as a potential solution. The review summarizes that recent studies combining AI with digital biomarkers suggest the possibility of identifying pre-symptomatic indicators of NDs. For instance, research utilizing convolutional neural networks for eye tracking has achieved significant diagnostic accuracies. ROC-AUC scores reached up to 0.88, indicating high model performance in differentiating between PD patients and healthy controls. Similarly, advancements in facial expression analysis through tools have demonstrated significant potential in detecting emotional changes in ND patients, with some models reaching an accuracy of 0.89 and a precision of 0.85. This review follows a structured approach to article selection, starting with a comprehensive database search and culminating in a rigorous quality assessment and meaning for NDs of the different methods. The process is visualized in 10 tables with 54 parameters describing different approaches and their consequences for understanding various mechanisms in ND changes. However, these methods also face challenges related to data accuracy and privacy concerns. To address these issues, this review proposes strategies that emphasize the need for rigorous validation and rapid integration into clinical practice. Such integration could transform ND diagnostics, making early detection tools more cost-effective and globally accessible. In conclusion, this review underscores the urgent need to incorporate validated digital health tools into mainstream medical practice. This integration could indicate a new era in the early diagnosis of neurodegenerative diseases, potentially altering the trajectory of these conditions for millions worldwide. Thus, by highlighting specific and statistically significant findings, this review demonstrates the current progress in this field and the potential impact of these advancements on the global management of NDs.",['convolutional neural networks'],"The research idea addresses the critical challenge of neurodegenerative diseases such as Alzheimer’s Disease and Parkinson’s Disease, which often develop without noticeable symptoms and cause irreversible neuronal damage before clinical signs appear. These diseases are a leading cause of disability and mortality worldwide, and currently, there are no cures or treatments to stop their progression. Therefore, there is an urgent need for early detection methods to delay neuronal loss and improve patient outcomes. The study highlights the potential of emerging tools for remote neurocognitive data collection as a promising avenue to identify pre-symptomatic indicators of neurodegenerative diseases.

The primary objective of the study is to review and evaluate recent advancements in early detection approaches for neurodegenerative diseases, focusing on the potential of novel health tools to identify early signs before clinical symptoms emerge. The study aims to summarize current findings, assess the effectiveness and challenges of these approaches, and propose strategies for their rigorous validation and integration into clinical practice. Ultimately, the goal is to emphasize the importance of incorporating validated early detection tools into mainstream medicine to transform diagnostics and improve global management of neurodegenerative diseases.","The research idea addresses the critical challenge of neurodegenerative diseases such as Alzheimer's Disease and Parkinson's Disease, which often develop without noticeable symptoms and cause irreversible neuronal damage before clinical signs appear. These diseases are a leading cause of disability and mortality worldwide, and currently, there are no cures or treatments to stop their progression. Therefore, there is an urgent need for early detection methods to delay neuronal loss and improve patient outcomes. The study highlights the potential of emerging tools for remote neurocognitive data collection as a promising avenue to identify pre-symptomatic indicators of neurodegenerative diseases.

The primary objective of the study is to review and evaluate recent advancements in early detection approaches for neurodegenerative diseases, focusing on the potential of novel health tools to identify early signs before clinical symptoms emerge. The study aims to summarize current findings, assess the effectiveness and challenges of these approaches, and propose strategies for their rigorous validation and integration into clinical practice. Ultimately, the goal is to emphasize the importance of incorporating validated early detection tools into mainstream medicine to transform diagnostics and improve global management of neurodegenerative diseases.",True
Medicine,RanMerFormer: Randomized vision transformer with token merging for brain tumor classification,"Brains are the control center of the nervous system in human bodies, and brain tumor is one of the most deadly diseases. Currently, magnetic resonance imaging (MRI) is the most effective way to brain tumors early detection in clinical diagnoses due to its superior imaging quality for soft tissues. Manual analysis of brain MRI is error-prone which depends on empirical experience and the fatigue state of the radiologists to a large extent. Computer-aided diagnosis (CAD) systems are becoming more and more impactful because they can provide accurate prediction results based on medical images with advanced techniques from computer vision. Therefore, a novel CAD method for brain tumor classification named RanMerFormer is presented in this paper. A pre-trained vision transformer is used as the backbone model. Then, a merging mechanism is proposed to remove the redundant tokens in the vision transformer, which improves computing efficiency substantially. Finally, a randomized vector functional-link serves as the head in the proposed RanMerFormer, which can be trained swiftly. All the simulation results are obtained from two public benchmark datasets, which reveal that the proposed RanMerFormer can achieve state-of-the-art performance for brain tumor classification. The trained RanMerFormer can be applied in real-world scenarios to assist in brain tumor diagnosis.","['pre-trained vision transformer', 'randomized vector functional-link']","The study addresses the critical issue of brain tumors, which are among the most deadly diseases affecting the nervous system. Early detection of brain tumors is essential, and magnetic resonance imaging (MRI) is currently the most effective clinical method due to its superior imaging quality for soft tissues. However, manual analysis of brain MRI scans is prone to errors and heavily reliant on the radiologists' experience and fatigue levels, which can impact diagnostic accuracy. The primary aim of the study is to develop a novel approach to improve the classification of brain tumors, enhancing the accuracy and efficiency of diagnosis to better assist clinical decision-making in real-world scenarios.","The study addresses the critical issue of brain tumors, which are among the most deadly diseases affecting the nervous system. Early detection of brain tumors is essential, and magnetic resonance imaging (MRI) is currently the most effective clinical method due to its superior imaging quality for soft tissues. However, manual analysis of brain MRI scans is prone to errors and heavily reliant on the radiologists' experience and fatigue levels, which can impact diagnostic accuracy. The primary aim of the study is to develop a new methodology to improve the identification and categorization of brain tumors, enhancing the accuracy and efficiency of diagnosis to better assist clinical decision-making in real-world scenarios.",True
Medicine,A hybrid deep CNN model for brain tumor image multi-classification,"Abstract The current approach to diagnosing and classifying brain tumors relies on the histological evaluation of biopsy samples, which is invasive, time-consuming, and susceptible to manual errors. These limitations underscore the pressing need for a fully automated, deep-learning-based multi-classification system for brain malignancies. This article aims to leverage a deep convolutional neural network (CNN) to enhance early detection and presents three distinct CNN models designed for different types of classification tasks. The first CNN model achieves an impressive detection accuracy of 99.53% for brain tumors. The second CNN model, with an accuracy of 93.81%, proficiently categorizes brain tumors into five distinct types: normal, glioma, meningioma, pituitary, and metastatic. Furthermore, the third CNN model demonstrates an accuracy of 98.56% in accurately classifying brain tumors into their different grades. To ensure optimal performance, a grid search optimization approach is employed to automatically fine-tune all the relevant hyperparameters of the CNN models. The utilization of large, publicly accessible clinical datasets results in robust and reliable classification outcomes. This article conducts a comprehensive comparison of the proposed models against classical models, such as AlexNet, DenseNet121, ResNet-101, VGG-19, and GoogleNet, reaffirming the superiority of the deep CNN-based approach in advancing the field of brain tumor classification and early detection.","['deep convolutional neural network (CNN)', 'AlexNet', 'DenseNet121', 'ResNet-101', 'VGG-19', 'GoogleNet']","The current approach to diagnosing and classifying brain tumors relies on the histological evaluation of biopsy samples, which is invasive, time-consuming, and susceptible to manual errors. These limitations highlight the need for improved methods that can facilitate early detection and accurate classification of brain malignancies. The primary aim of this study is to enhance early detection of brain tumors and to accurately classify them into different types and grades. This is achieved by developing and evaluating models designed for multi-classification tasks to improve the diagnosis and categorization of brain tumors.","The current approach to diagnosing and classifying brain tumors relies on the histological evaluation of biopsy samples, which is invasive, time-consuming, and susceptible to manual errors. These limitations highlight the need for improved methods that can facilitate early detection and accurate classification of brain malignancies. The primary aim of this study is to enhance early detection of brain tumors and to accurately classify them into different types and grades. This is achieved by developing and evaluating analytical approaches designed for multi-classification tasks to improve the diagnosis and categorization of brain tumors.",True
Medicine,Automated Tool Support for Glaucoma Identification With Explainability Using Fundus Images,"Glaucoma is a progressive eye condition that causes irreversible vision loss due to damage to the optic nerve. Recent developments in deep learning and the accessibility of computing resources have provided tool support for automated glaucoma diagnosis. Despite deep learning's advances in disease diagnosis using medical images, generic convolutional neural networks are still not widely used in medical practices due to the limited trustworthiness of these models. Although deep learning-based glaucoma classification has gained popularity in recent years, only a few of them have addressed the explainability and interpretability of the models, which increases confidence in using such applications. This study presents state-of-the-art deep learning techniques to segment and classify fundus images to predict glaucoma conditions and applies visualization techniques to explain the results to ease understandability. Our predictions are based on U-Net with attention mechanisms with ResNet50 for the segmentation process and a modified Inception V3 architecture for the classification. Attention U-Net with modified ResNet50 backbone obtained 99.58% and 98.05% accuracies for optic disc segmentation and optic cup segmentation, respectively for the RIM-ONE dataset. Additionally, we generate heatmaps that highlight the regions that impacted the glaucoma diagnosis using both Gradient-weighted Class Activation Mapping (Grad-CAM) and Grad-CAM++. Our model that classifies the segmented images achieves accuracy, sensitivity, and specificity values of 98.97%, 99.42%, and 95.59%, respectively, with the RIM-ONE dataset. This model can be used as a support tool for automated glaucoma identification using fundus images.","['U-Net with attention mechanisms', 'ResNet50', 'modified Inception V3 architecture', 'Attention U-Net with modified ResNet50 backbone', 'Gradient-weighted Class Activation Mapping (Grad-CAM)', 'Grad-CAM++']","The research idea centers on addressing the challenge of diagnosing glaucoma, a progressive eye disease that leads to irreversible vision loss due to optic nerve damage. Despite advances in automated diagnosis using medical images, there remains limited trust and acceptance of these methods in clinical practice, partly because of insufficient explainability and interpretability of the diagnostic results. The study aims to improve confidence in automated glaucoma diagnosis by enhancing the clarity and understanding of the diagnostic process. The primary objective of the study is to develop an approach for segmenting and classifying fundus images to predict glaucoma conditions accurately, while also providing visual explanations of the diagnostic outcomes to facilitate better understanding and trust in the results.","The research idea centers on addressing the challenge of diagnosing glaucoma, a progressive eye disease that leads to irreversible vision loss due to optic nerve damage. Despite advances in diagnostic methods using medical images, there remains limited trust and acceptance of these approaches in clinical practice, partly because of insufficient explainability and interpretability of the diagnostic results. The study aims to improve confidence in glaucoma diagnosis by enhancing the clarity and understanding of the diagnostic process. The primary objective of the study is to develop an approach for segmenting and classifying fundus images to predict glaucoma conditions accurately, while also providing visual explanations of the diagnostic outcomes to facilitate better understanding and trust in the results.",True
Medicine,Foresight—a generative pretrained transformer for modelling of patient timelines using electronic health records: a retrospective modelling study,"BackgroundAn electronic health record (EHR) holds detailed longitudinal information about a patient's health status and general clinical history, a large portion of which is stored as unstructured, free text. Existing approaches to model a patient's trajectory focus mostly on structured data and a subset of single-domain outcomes. This study aims to evaluate the effectiveness of Foresight, a generative transformer in temporal modelling of patient data, integrating both free text and structured formats, to predict a diverse array of future medical outcomes, such as disorders, substances (eg, to do with medicines, allergies, or poisonings), procedures, and findings (eg, relating to observations, judgements, or assessments).MethodsForesight is a novel transformer-based pipeline that uses named entity recognition and linking tools to convert EHR document text into structured, coded concepts, followed by providing probabilistic forecasts for future medical events, such as disorders, substances, procedures, and findings. The Foresight pipeline has four main components: (1) CogStack (data retrieval and preprocessing); (2) the Medical Concept Annotation Toolkit (structuring of the free-text information from EHRs); (3) Foresight Core (deep-learning model for biomedical concept modelling); and (4) the Foresight web application. We processed the entire free-text portion from three different hospital datasets (King's College Hospital [KCH], South London and Maudsley [SLaM], and the US Medical Information Mart for Intensive Care III [MIMIC-III]), resulting in information from 811 336 patients and covering both physical and mental health institutions. We measured the performance of models using custom metrics derived from precision and recall.FindingsForesight achieved a precision@10 (ie, of 10 forecasted candidates, at least one is correct) of 0·68 (SD 0·0027) for the KCH dataset, 0·76 (0·0032) for the SLaM dataset, and 0·88 (0·0018) for the MIMIC-III dataset, for forecasting the next new disorder in a patient timeline. Foresight also achieved a precision@10 value of 0·80 (0·0013) for the KCH dataset, 0·81 (0·0026) for the SLaM dataset, and 0·91 (0·0011) for the MIMIC-III dataset, for forecasting the next new biomedical concept. In addition, Foresight was validated on 34 synthetic patient timelines by five clinicians and achieved a relevancy of 33 (97% [95% CI 91–100]) of 34 for the top forecasted candidate disorder. As a generative model, Foresight can forecast follow-on biomedical concepts for as many steps as required.InterpretationForesight is a general-purpose model for biomedical concept modelling that can be used for real-world risk forecasting, virtual trials, and clinical research to study the progression of disorders, to simulate interventions and counterfactuals, and for educational purposes.FundingNational Health Service Artificial Intelligence Laboratory, National Institute for Health and Care Research Biomedical Research Centre, and Health Data Research UK.","['generative transformer', 'named entity recognition']","The research idea addresses the challenge of utilizing the detailed longitudinal information contained in electronic health records (EHRs), much of which is stored as unstructured free text, to better understand and predict a patient’s future medical outcomes. Existing approaches primarily focus on structured data and limited clinical outcomes, leaving a gap in effectively integrating comprehensive patient information to forecast a wide range of medical events. This study is motivated by the need to improve temporal modeling of patient data by incorporating both free-text and structured formats to capture diverse clinical trajectories.

The primary objective of the study is to evaluate the effectiveness of a novel approach that integrates free-text and structured EHR data to predict a broad array of future medical outcomes, including disorders, substances related to medicines or allergies, procedures, and clinical findings. The study aims to assess the accuracy and relevance of these predictions across multiple hospital datasets, thereby demonstrating the potential for improved risk forecasting and clinical research applications focused on the progression and management of various health conditions.","The research idea addresses the challenge of utilizing the detailed longitudinal information contained in electronic health records (EHRs), much of which is stored as unstructured free text, to better understand and predict a patient's future medical outcomes. Existing approaches primarily focus on structured data and limited clinical outcomes, leaving a gap in effectively integrating comprehensive patient information to forecast a wide range of medical events. This study is motivated by the need to improve temporal modeling of patient data by incorporating both free-text and structured formats to capture diverse clinical trajectories.

The primary objective of the study is to evaluate the effectiveness of a novel approach that integrates free-text and structured EHR data to predict a broad array of future medical outcomes, including disorders, substances related to medicines or allergies, procedures, and clinical findings. The study aims to assess the accuracy and relevance of these predictions across multiple hospital datasets, thereby demonstrating the potential for improved risk forecasting and clinical research applications focused on the progression and management of various health conditions.",True
Medicine,Evaluating the accuracy of a state-of-the-art large language model for prediction of admissions from the emergency room,"Abstract Background Artificial intelligence (AI) and large language models (LLMs) can play a critical role in emergency room operations by augmenting decision-making about patient admission. However, there are no studies for LLMs using real-world data and scenarios, in comparison to and being informed by traditional supervised machine learning (ML) models. We evaluated the performance of GPT-4 for predicting patient admissions from emergency department (ED) visits. We compared performance to traditional ML models both naively and when informed by few-shot examples and/or numerical probabilities. Methods We conducted a retrospective study using electronic health records across 7 NYC hospitals. We trained Bio-Clinical-BERT and XGBoost (XGB) models on unstructured and structured data, respectively, and created an ensemble model reflecting ML performance. We then assessed GPT-4 capabilities in many scenarios: through Zero-shot, Few-shot with and without retrieval-augmented generation (RAG), and with and without ML numerical probabilities. Results The Ensemble ML model achieved an area under the receiver operating characteristic curve (AUC) of 0.88, an area under the precision-recall curve (AUPRC) of 0.72 and an accuracy of 82.9%. The naïve GPT-4's performance (0.79 AUC, 0.48 AUPRC, and 77.5% accuracy) showed substantial improvement when given limited, relevant data to learn from (ie, RAG) and underlying ML probabilities (0.87 AUC, 0.71 AUPRC, and 83.1% accuracy). Interestingly, RAG alone boosted performance to near peak levels (0.82 AUC, 0.56 AUPRC, and 81.3% accuracy). Conclusions The naïve LLM had limited performance but showed significant improvement in predicting ED admissions when supplemented with real-world examples to learn from, particularly through RAG, and/or numerical probabilities from traditional ML models. Its peak performance, although slightly lower than the pure ML model, is noteworthy given its potential for providing reasoning behind predictions. Further refinement of LLMs with real-world data is necessary for successful integration as decision-support tools in care settings.","['GPT-4', 'XGBoost (XGB)', 'Ensemble model', 'Zero-shot', 'Few-shot', 'Retrieval-augmented generation (RAG)']","The research idea addresses the challenge of improving decision-making about patient admissions in emergency room operations by exploring new approaches to predict admissions from emergency department visits. There is a need to evaluate the effectiveness of emerging methods using real-world clinical data and scenarios to enhance the accuracy and reliability of admission predictions. The study aims to assess the potential of advanced language-based tools in comparison to traditional approaches for this critical healthcare task. The primary objective of the study is to evaluate the performance of a large language-based tool for predicting patient admissions from emergency department visits and to compare its effectiveness to that of established traditional methods. The study seeks to determine whether supplementing this tool with real-world examples and numerical probabilities from traditional methods can improve its predictive accuracy. Ultimately, the goal is to understand the feasibility of integrating such tools as decision-support aids in clinical care settings.","The research idea addresses the challenge of improving decision-making about patient admissions in emergency room operations by exploring new approaches to predict admissions from emergency department visits. There is a need to evaluate the effectiveness of emerging methods using real-world clinical data and scenarios to enhance the accuracy and reliability of admission predictions. The study aims to assess the potential of advanced language-based tools in comparison to traditional approaches for this critical healthcare task. The primary objective of the study is to evaluate the performance of a language-based tool for predicting patient admissions from emergency department visits and to compare its effectiveness to that of established traditional methods. The study seeks to determine whether supplementing this tool with real-world examples and numerical probabilities from traditional methods can improve its predictive accuracy. Ultimately, the goal is to understand the feasibility of integrating such tools as decision-support aids in clinical care settings.",True
Medicine,Present and Future Innovations in AI and Cardiac MRI,"Cardiac MRI is used to diagnose and treat patients with a multitude of cardiovascular diseases. Despite the growth of clinical cardiac MRI, complicated image prescriptions and long acquisition protocols limit the specialty and restrain its impact on the practice of medicine. Artificial intelligence (AI)-the ability to mimic human intelligence in learning and performing tasks-will impact nearly all aspects of MRI. Deep learning (DL) primarily uses an artificial neural network to learn a specific task from example data sets. Self-driving scanners are increasingly available, where AI automatically controls cardiac image prescriptions. These scanners offer faster image collection with higher spatial and temporal resolution, eliminating the need for cardiac triggering or breath holding. In the future, fully automated inline image analysis will most likely provide all contour drawings and initial measurements to the reader. Advanced analysis using radiomic or DL features may provide new insights and information not typically extracted in the current analysis workflow. AI may further help integrate these features with clinical, genetic, wearable-device, and ""omics"" data to improve patient outcomes. This article presents an overview of AI and its application in cardiac MRI, including in image acquisition, reconstruction, and processing, and opportunities for more personalized cardiovascular care through extraction of novel imaging markers.","['Deep learning (DL)', 'artificial neural network']","The research idea addresses the challenges in cardiac MRI, including complicated image prescriptions and lengthy acquisition protocols, which limit its widespread use and impact in clinical practice for diagnosing and treating cardiovascular diseases. Despite the growth of cardiac MRI, these limitations restrain its full potential in improving patient care. The study’s primary objective is to explore advancements that can enhance cardiac MRI by enabling faster image collection with improved resolution and reducing patient burden during imaging. It aims to provide an overview of innovations that could lead to more personalized cardiovascular care through improved image acquisition and the extraction of novel imaging markers.","The research idea addresses the challenges in cardiac MRI, including complicated image prescriptions and lengthy acquisition protocols, which limit its widespread use and impact in clinical practice for diagnosing and treating cardiovascular diseases. Despite the growth of cardiac MRI, these limitations restrain its full potential in improving patient care. The study's primary objective is to explore advancements that can enhance cardiac MRI by enabling faster image collection with improved resolution and reducing patient burden during imaging. It aims to provide an overview of innovations that could lead to more personalized cardiovascular care through improved image acquisition and the extraction of novel imaging markers.",True
Medicine,Artificial intelligence-driven virtual rehabilitation for people living in the community: A scoping review,"Abstract Virtual Rehabilitation (VRehab) is a promising approach to improving the physical and mental functioning of patients living in the community. The use of VRehab technology results in the generation of multi-modal datasets collected through various devices. This presents opportunities for the development of Artificial Intelligence (AI) techniques in VRehab, namely the measurement, detection, and prediction of various patients’ health outcomes. The objective of this scoping review was to explore the applications and effectiveness of incorporating AI into home-based VRehab programs. PubMed/MEDLINE, Embase, IEEE Xplore, Web of Science databases, and Google Scholar were searched from inception until June 2023 for studies that applied AI for the delivery of VRehab programs to the homes of adult patients. After screening 2172 unique titles and abstracts and 51 full-text studies, 13 studies were included in the review. A variety of AI algorithms were applied to analyze data collected from various sensors and make inferences about patients’ health outcomes, most involving evaluating patients’ exercise quality and providing feedback to patients. The AI algorithms used in the studies were mostly fuzzy rule-based methods, template matching, and deep neural networks. Despite the growing body of literature on the use of AI in VRehab, very few studies have examined its use in patients’ homes. Current research suggests that integrating AI with home-based VRehab can lead to improved rehabilitation outcomes for patients. However, further research is required to fully assess the effectiveness of various forms of AI-driven home-based VRehab, taking into account its unique challenges and using standardized metrics.","['fuzzy rule-based methods', 'deep neural networks']","The research idea centers on the potential of virtual rehabilitation (VRehab) to enhance the physical and mental functioning of patients living in the community, particularly through home-based programs. Despite the promise of VRehab, there is limited understanding of its application and effectiveness when delivered in patients’ homes. The study’s primary objective was to explore the applications and effectiveness of incorporating advanced techniques into home-based VRehab programs for adult patients. This review aimed to assess how these approaches have been used to evaluate patients’ exercise quality and provide feedback, with the goal of improving rehabilitation outcomes in a home setting.","The research idea centers on the potential of virtual rehabilitation (VRehab) to enhance the physical and mental functioning of patients living in the community, particularly through home-based programs. Despite the promise of VRehab, there is limited understanding of its application and effectiveness when delivered in patients' homes. The study's primary objective was to explore the applications and effectiveness of incorporating innovative methods into home-based VRehab programs for adult patients. This review aimed to assess how these approaches have been used to evaluate patients' exercise quality and provide feedback, with the goal of improving rehabilitation outcomes in a home setting.",True
Medicine,A Lesion-Based Diabetic Retinopathy Detection Through Hybrid Deep Learning Model,"Diabetic retinopathy (DR) can be defined as visual impairment caused by prolonged diabetes affecting the blood vessels in the retina. Globally, it stands as the primary contributor to blindness, impacting approximately 191 million individuals. While prior research has addressed DR classification using retinal fundus images, existing methods often focus on isolated lesion detection, lacking a comprehensive framework for the simultaneous identification of all lesions. Previous studies concentrated on early-stage features like exudates, aneurysms, hemorrhages, and blood vessels, sidelining severe-stage lesions such as cotton wool spots, venous beading, very severe intraretinal microvascular abnormalities (IRMA), diffuse intraretinal hemorrhages, capillary degeneration, highly activated microglia, and retinal pigment epithelium (RPE) damage. In this study, a deep learning approach is proposed to classify DR fundus images by severity levels, utilizing GoogleNet and ResNet models based on adaptive particle swarm optimizer (APSO), for enhanced feature extraction. The extracted features from the hybrid model are further used with different machine learning models like random forest, support vector machine, decision tree, and linear regression models. Experimental results showcased the proposed hybrid framework outperforming advanced approaches with a remarkable 94% accuracy on the benchmark dataset. This method demonstrates potential enhancements in precision, recall, accuracy, and F1 score for different DR severity levels.","['deep learning', 'GoogleNet', 'ResNet', 'adaptive particle swarm optimizer (APSO)', 'random forest', 'support vector machine', 'decision tree', 'linear regression']","Diabetic retinopathy (DR) is a major cause of visual impairment and blindness worldwide, affecting approximately 191 million individuals due to damage to the blood vessels in the retina caused by prolonged diabetes. Existing research has primarily focused on detecting early-stage lesions in DR, such as exudates, aneurysms, hemorrhages, and blood vessel abnormalities, but has often overlooked the identification of severe-stage lesions like cotton wool spots, venous beading, and retinal pigment epithelium damage. The primary aim of this study is to develop a comprehensive approach for classifying diabetic retinopathy fundus images according to severity levels, enabling the simultaneous identification of both early and severe-stage lesions. This classification seeks to improve the accuracy and reliability of DR severity assessment to better support clinical diagnosis and treatment planning.","Diabetic retinopathy (DR) is a major cause of visual impairment and blindness worldwide, affecting approximately 191 million individuals due to damage to the blood vessels in the retina caused by prolonged diabetes. Existing research has primarily focused on detecting early-stage lesions in DR, such as exudates, aneurysms, hemorrhages, and blood vessel abnormalities, but has often overlooked the identification of severe-stage lesions like cotton wool spots, venous beading, and retinal pigment epithelium damage. The primary aim of this study is to develop a comprehensive approach for categorizing diabetic retinopathy fundus images according to severity levels, enabling the simultaneous identification of both early and severe-stage lesions. This categorization seeks to improve the accuracy and reliability of DR severity assessment to better support clinical diagnosis and treatment planning.",True
Medicine,Advanced Ensemble Machine Learning Techniques for Optimizing Diabetes Mellitus Prognostication: A Detailed Examination of Hospital Data,"Diabetes is a chronic disease that affects millions of people worldwide. Early diagnosis and effective management are crucial for reducing its complications. Diabetes is the fourth-highest cause of mortality due to its association with various comorbidities, including heart disease, nerve damage, blood vessel damage, and blindness. The potential of machine learning algorithms in predicting Diabetes and related conditions is significant, and mining diabetes data is an efficient method for extracting new insights.The primary objective of this study is to develop an enhanced ensemble model to predict Diabetes with improved accuracy by leveraging various machine learning algorithms.This study tested several popular machine learning algorithms commonly used in diabetes prediction, including Naive Bayes (NB), Generalized Linear Model (GLM), Logistic Regression (LR), Fast Large Margin (FLM), Deep Learning (DL), Decision Tree (DT), Random Forest (RF), Gradient Boosted Trees (GBT), and Support Vector Machine (SVM). The performance of these algorithms was compared, and two different ensemble techniques—stacking and voting—were used to build a more accurate predictive model.The top three algorithms based on accuracy were Deep Learning, Naive Bayes, and Gradient Boosted Trees. The machine learning algorithms revealed that individuals with Diabetes are significantly affected by the number of chronic conditions they have, as well as their gender and age. The ensemble models, particularly the stacking method, provided higher accuracy than individual algorithms. The stacking ensemble model achieved a slightly better accuracy of 99.94% compared to 99.34% for the voting method.Building an ensemble model significantly increased the accuracy of predicting Diabetes and related conditions. The stacking ensemble model, in particular, demonstrated superior performance, highlighting the importance of combining multiple machine learning approaches to enhance predictive accuracy","['Naive Bayes (NB)', 'Logistic Regression (LR)', 'Deep Learning (DL)', 'Decision Tree (DT)', 'Random Forest (RF)', 'Gradient Boosted Trees (GBT)', 'Support Vector Machine (SVM)', 'stacking ensemble', 'voting ensemble']","Diabetes is a chronic disease that affects millions of people worldwide and is a leading cause of mortality due to its association with various comorbidities such as heart disease, nerve damage, blood vessel damage, and blindness. Early diagnosis and effective management are crucial for reducing the complications associated with diabetes. The primary objective of this study is to improve the accuracy of diabetes prediction by developing an enhanced approach that combines multiple methods. This study aims to identify the most effective strategies for predicting diabetes and related conditions to support better clinical outcomes.","Diabetes is a chronic disease that affects millions of people worldwide and is a leading cause of mortality due to its association with various comorbidities such as heart disease, nerve damage, blood vessel damage, and blindness. Early diagnosis and effective management are crucial for reducing the complications associated with diabetes. The primary objective of this study is to improve the accuracy of diabetes prediction by developing an enhanced analytical approach that combines multiple methodologies. This study aims to identify the most effective strategies for predicting diabetes and related conditions to support better clinical outcomes.",True
Medicine,Performance of Generative Pretrained Transformer on the National Medical Licensing Examination in Japan,"The remarkable performance of ChatGPT, launched in November 2022, has significantly impacted the field of natural language processing, inspiring the application of large language models as supportive tools in clinical practice and research worldwide. Although GPT-3.5 recently scored high on the United States Medical Licensing Examination, its performance on medical licensing examinations of other nations, especially non-English speaking nations, has not been sufficiently evaluated. This study assessed GPT’s performance on the National Medical Licensing Examination (NMLE) in Japan and compared it with the actual minimal passing rate for this exam. In particular, the performances of both the GPT-3.5 and GPT-4 models were considered for the comparative analysis. We initially used the GPT models and several prompts for 290 questions without image data from the 116 th NMLE (held in February 2022 in Japan) to maximize the performance for delivering correct answers and explanations of the questions. Thereafter, we tested the performance of the best GPT model (GPT-4) with optimized prompts on a dataset of 262 questions without images from the latest 117 th NMLE (held in February 2023). The best model with the optimized prompts scored 82.7% for the essential questions and 77.2% for the basic and clinical questions, both of which sufficed the minimum passing scoring rates of 80.0% and 74.6%, respectively. After an exploratory analysis of 56 incorrect answers from the model, we identified the three major factors contributing to the generation of the incorrect answers—insufficient medical knowledge, information on Japan-specific medical system and guidelines, and mathematical errors. In conclusion, GPT-4 with our optimized prompts achieved a minimum passing scoring rate in the latest 117 th NMLE in Japan. Beyond its original design of answering examination questions for humans, these artificial intelligence (AI) models can serve as one of the best “sidekicks” for solving problems and addressing the unmet needs in the medical and healthcare fields.","['GPT-3.5', 'GPT-4']","The study addresses the need to evaluate the performance of advanced language models on medical licensing examinations outside of English-speaking countries, focusing on Japan where such assessments have been insufficient. It highlights the importance of understanding how well these models can meet the standards required for medical knowledge and clinical reasoning in different national contexts. The primary aim of the study was to assess the performance of language models on the National Medical Licensing Examination (NMLE) in Japan and to compare their scores with the actual minimal passing rates for this exam. Specifically, the study sought to determine whether these models could achieve passing scores on essential, basic, and clinical questions from recent NMLE tests and to identify factors contributing to incorrect answers.","The study addresses the need to evaluate the performance of professional reference systems on medical licensing examinations outside of English-speaking countries, focusing on Japan where such assessments have been insufficient. It highlights the importance of understanding how well these systems can meet the standards required for medical knowledge and clinical reasoning in different national contexts. The primary aim of the study was to assess the performance of reference tools on the National Medical Licensing Examination (NMLE) in Japan and to compare their scores with the actual minimal passing rates for this exam. Specifically, the study sought to determine whether these tools could achieve passing scores on essential, basic, and clinical questions from recent NMLE tests and to identify factors contributing to incorrect answers.",True
Medicine,A novel SpaSA based hyper-parameter optimized FCEDN with adaptive CNN classification for skin cancer detection,"Abstract Skin cancer is the most prevalent kind of cancer in people. It is estimated that more than 1 million people get skin cancer every year in the world. The effectiveness of the disease’s therapy is significantly impacted by early identification of this illness. Preprocessing is the initial detecting stage in enhancing the quality of skin images by removing undesired background noise and objects. This study aims is to compile preprocessing techniques for skin cancer imaging that are currently accessible. Researchers looking into automated skin cancer diagnosis might use this article as an excellent place to start. The fully convolutional encoder–decoder network and Sparrow search algorithm (FCEDN-SpaSA) are proposed in this study for the segmentation of dermoscopic images. The individual wolf method and the ensemble ghosting technique are integrated to generate a neighbour-based search strategy in SpaSA for stressing the correct balance between navigation and exploitation. The classification procedure is accomplished by using an adaptive CNN technique to discriminate between normal skin and malignant skin lesions suggestive of disease. Our method provides classification accuracies comparable to commonly used incremental learning techniques while using less energy, storage space, memory access, and training time (only network updates with new training samples, no network sharing). In a simulation, the segmentation performance of the proposed technique on the ISBI 2017, ISIC 2018, and PH2 datasets reached accuracies of 95.28%, 95.89%, 92.70%, and 98.78%, respectively, on the same dataset and assessed the classification performance. It is accurate 91.67% of the time. The efficiency of the suggested strategy is demonstrated through comparisons with cutting-edge methodologies.","['fully convolutional encoder–decoder network', 'Sparrow search algorithm (SpaSA)', 'adaptive CNN technique', 'incremental learning techniques']","The research idea addresses the high prevalence of skin cancer worldwide, with over 1 million new cases annually, emphasizing that early identification significantly impacts the effectiveness of therapy. Improving the quality of skin images by removing undesired background noise and objects during the initial detection stage is crucial for enhancing diagnosis. The study focuses on compiling existing preprocessing techniques for skin cancer imaging to support researchers in the field of automated diagnosis. The research objective is to develop and evaluate a method for segmenting and classifying dermoscopic images to accurately distinguish between normal skin and malignant skin lesions. The study aims to achieve high classification accuracy while optimizing resource use, thereby improving the diagnosis process for skin cancer.","The research idea addresses the high prevalence of skin cancer worldwide, with over 1 million new cases annually, emphasizing that early identification significantly impacts the effectiveness of therapy. Improving the quality of skin images by removing undesired background noise and objects during the initial detection stage is crucial for enhancing diagnosis. The study focuses on compiling existing preprocessing techniques for skin cancer imaging to support researchers in the field of diagnosis. The research objective is to develop and evaluate a method for segmenting dermoscopic images to accurately distinguish between normal skin and malignant skin lesions. The study aims to achieve high accuracy while optimizing resource use, thereby improving the diagnosis process for skin cancer.",True
Medicine,Reliability of ChatGPT for performing triage task in the emergency department using the Korean Triage and Acuity Scale,"Background Artificial intelligence (AI) technology can enable more efficient decision-making in healthcare settings. There is a growing interest in improving the speed and accuracy of AI systems in providing responses for given tasks in healthcare settings. Objective This study aimed to assess the reliability of ChatGPT in determining emergency department (ED) triage accuracy using the Korean Triage and Acuity Scale (KTAS). Methods Two hundred and two virtual patient cases were built. The gold standard triage classification for each case was established by an experienced ED physician. Three other human raters (ED paramedics) were involved and rated the virtual cases individually. The virtual cases were also rated by two different versions of the chat generative pre-trained transformer (ChatGPT, 3.5 and 4.0). Inter-rater reliability was examined using Fleiss’ kappa and intra-class correlation coefficient (ICC). Results The kappa values for the agreement between the four human raters and ChatGPTs were .523 (version 4.0) and .320 (version 3.5). Of the five levels, the performance was poor when rating patients at levels 1 and 5, as well as case scenarios with additional text descriptions. There were differences in the accuracy of the different versions of GPTs. The ICC between version 3.5 and the gold standard was .520, and that between version 4.0 and the gold standard was .802. Conclusions A substantial level of inter-rater reliability was revealed when GPTs were used as KTAS raters. The current study showed the potential of using GPT in emergency healthcare settings. Considering the shortage of experienced manpower, this AI method may help improve triaging accuracy.","['chat generative pre-trained transformer (ChatGPT, 3.5 and 4.0)']","The research idea addresses the challenge of ensuring accurate and efficient triage decisions in emergency department settings, which is critical for patient care and resource allocation. There is a recognized need to improve the speed and reliability of triage assessments to support healthcare providers, especially given the shortage of experienced personnel. The study focuses on evaluating the consistency and accuracy of triage classifications using the Korean Triage and Acuity Scale (KTAS) in emergency care scenarios. The primary objective of the study was to assess the reliability of ChatGPT in determining emergency department triage accuracy according to the KTAS. This involved comparing triage ratings from ChatGPT with those from experienced emergency department physicians and paramedics to evaluate agreement and consistency in triage decisions.","The research idea addresses the challenge of ensuring accurate and efficient triage decisions in emergency department settings, which is critical for patient care and resource allocation. There is a recognized need to improve the speed and reliability of triage assessments to support healthcare providers, especially given the shortage of experienced personnel. The study focuses on evaluating the consistency and accuracy of triage classifications using the Korean Triage and Acuity Scale (KTAS) in emergency care scenarios. The primary objective of the study was to assess the reliability of automated systems in determining emergency department triage accuracy according to the KTAS. This involved comparing triage ratings from these systems with those from experienced emergency department physicians and paramedics to evaluate agreement and consistency in triage decisions.",True
Medicine,Assessing generalisability of deep learning-based polyp detection and segmentation methods through a computer vision challenge,"Abstract Polyps are well-known cancer precursors identified by colonoscopy. However, variability in their size, appearance, and location makes the detection of polyps challenging. Moreover, colonoscopy surveillance and removal of polyps are highly operator-dependent procedures and occur in a highly complex organ topology. There exists a high missed detection rate and incomplete removal of colonic polyps. To assist in clinical procedures and reduce missed rates, automated methods for detecting and segmenting polyps using machine learning have been achieved in past years. However, the major drawback in most of these methods is their ability to generalise to out-of-sample unseen datasets from different centres, populations, modalities, and acquisition systems. To test this hypothesis rigorously, we, together with expert gastroenterologists, curated a multi-centre and multi-population dataset acquired from six different colonoscopy systems and challenged the computational expert teams to develop robust automated detection and segmentation methods in a crowd-sourcing Endoscopic computer vision challenge. This work put forward rigorous generalisability tests and assesses the usability of devised deep learning methods in dynamic and actual clinical colonoscopy procedures. We analyse the results of four top performing teams for the detection task and five top performing teams for the segmentation task. Our analyses demonstrate that the top-ranking teams concentrated mainly on accuracy over the real-time performance required for clinical applicability. We further dissect the devised methods and provide an experiment-based hypothesis that reveals the need for improved generalisability to tackle diversity present in multi-centre datasets and routine clinical procedures.","['machine learning', 'deep learning']","The research idea addresses the challenge of detecting colonic polyps during colonoscopy, which is complicated by variability in polyp size, appearance, and location, as well as the highly operator-dependent nature of surveillance and removal procedures within a complex organ topology. There is a significant issue with high missed detection rates and incomplete removal of polyps, which can impact cancer prevention efforts. The study is motivated by the need to improve the reliability and generalisability of polyp detection methods across diverse clinical settings and populations.

The primary objective of the study is to rigorously evaluate the generalisability and clinical applicability of automated polyp detection and segmentation methods using a multi-centre, multi-population dataset acquired from different colonoscopy systems. The study aims to assess the performance of these methods in real-world clinical procedures, identify limitations related to accuracy and real-time usability, and highlight the necessity for improved approaches that can effectively handle the diversity encountered in routine colonoscopy practice.","The research idea addresses the challenge of detecting colonic polyps during colonoscopy, which is complicated by variability in polyp size, appearance, and location, as well as the highly operator-dependent nature of surveillance and removal procedures within a complex organ topology. There is a significant issue with high missed detection rates and incomplete removal of polyps, which can impact cancer prevention efforts. The study is motivated by the need to improve the reliability and generalisability of polyp detection methods across diverse clinical settings and populations.

The primary objective of the study is to rigorously evaluate the generalisability and clinical applicability of computational tools for polyp detection and segmentation using a multi-centre, multi-population dataset acquired from different colonoscopy systems. The study aims to assess the performance of these tools in real-world clinical procedures, identify limitations related to accuracy and real-time usability, and highlight the necessity for improved approaches that can effectively handle the diversity encountered in routine colonoscopy practice.",True
Medicine,"Clinical gait analysis using video-based pose estimation: Multiple perspectives, clinical populations, and measuring change","Gait dysfunction is common in many clinical populations and often has a profound and deleterious impact on independence and quality of life. Gait analysis is a foundational component of rehabilitation because it is critical to identify and understand the specific deficits that should be targeted prior to the initiation of treatment. Unfortunately, current state-of-the-art approaches to gait analysis (e.g., marker-based motion capture systems, instrumented gait mats) are largely inaccessible due to prohibitive costs of time, money, and effort required to perform the assessments. Here, we demonstrate the ability to perform quantitative gait analyses in multiple clinical populations using only simple videos recorded using low-cost devices (tablets). We report four primary advances: 1) a novel, versatile workflow that leverages an open-source human pose estimation algorithm (OpenPose) to perform gait analyses using videos recorded from multiple different perspectives (e.g., frontal, sagittal), 2) validation of this workflow in three different populations of participants (adults without gait impairment, persons post-stroke, and persons with Parkinson’s disease) via comparison to ground-truth three-dimensional motion capture, 3) demonstration of the ability to capture clinically relevant, condition-specific gait parameters, and 4) tracking of within-participant changes in gait, as is required to measure progress in rehabilitation and recovery. Importantly, our workflow has been made freely available and does not require prior gait analysis expertise. The ability to perform quantitative gait analyses in nearly any setting using only low-cost devices and computer vision offers significant potential for dramatic improvement in the accessibility of clinical gait analysis across different patient populations.",['human pose estimation algorithm (OpenPose)'],"Gait dysfunction is common in many clinical populations and often has a profound and deleterious impact on independence and quality of life. Gait analysis is a foundational component of rehabilitation because it is critical to identify and understand the specific deficits that should be targeted prior to the initiation of treatment. The study’s primary aim is to demonstrate the ability to perform quantitative gait analyses in multiple clinical populations using only simple videos recorded with low-cost devices. It seeks to validate this approach across different participant groups, capture clinically relevant gait parameters specific to conditions, and track changes in gait within individuals to measure rehabilitation progress.","Gait dysfunction is common in many clinical populations and often has a profound and deleterious impact on independence and quality of life. Gait analysis is a foundational component of rehabilitation because it is critical to identify and understand the specific deficits that should be targeted prior to the initiation of treatment. The study's primary aim is to demonstrate the ability to perform quantitative gait analyses in multiple clinical populations using only simple videos recorded with low-cost devices. It seeks to validate this approach across different participant groups, capture clinically relevant gait parameters specific to conditions, and track changes in gait within individuals to measure rehabilitation progress.",True
Medicine,GPT-4 Turbo with Vision fails to outperform text-only GPT-4 Turbo in the Japan Diagnostic Radiology Board Examination,"Abstract Purpose To assess the performance of GPT-4 Turbo with Vision (GPT-4TV), OpenAI’s latest multimodal large language model, by comparing its ability to process both text and image inputs with that of the text-only GPT-4 Turbo (GPT-4 T) in the context of the Japan Diagnostic Radiology Board Examination (JDRBE). Materials and methods The dataset comprised questions from JDRBE 2021 and 2023. A total of six board-certified diagnostic radiologists discussed the questions and provided ground-truth answers by consulting relevant literature as necessary. The following questions were excluded: those lacking associated images, those with no unanimous agreement on answers, and those including images rejected by the OpenAI application programming interface. The inputs for GPT-4TV included both text and images, whereas those for GPT-4 T were entirely text. Both models were deployed on the dataset, and their performance was compared using McNemar’s exact test. The radiological credibility of the responses was assessed by two diagnostic radiologists through the assignment of legitimacy scores on a five-point Likert scale. These scores were subsequently used to compare model performance using Wilcoxon's signed-rank test. Results The dataset comprised 139 questions. GPT-4TV correctly answered 62 questions (45%), whereas GPT-4 T correctly answered 57 questions (41%). A statistical analysis found no significant performance difference between the two models (P = 0.44). The GPT-4TV responses received significantly lower legitimacy scores from both radiologists than the GPT-4 T responses. Conclusion No significant enhancement in accuracy was observed when using GPT-4TV with image input compared with that of using text-only GPT-4 T for JDRBE questions.",['GPT-4 Turbo (GPT-4 T)'],"The research idea centers on evaluating the effectiveness of incorporating both text and image inputs in assessing diagnostic radiology knowledge, specifically within the context of the Japan Diagnostic Radiology Board Examination (JDRBE). The study addresses the need to understand whether the addition of visual information improves the accuracy and credibility of responses to radiology examination questions. The primary objective of the study is to compare the performance of a multimodal approach that processes both text and images with a text-only approach in answering JDRBE questions. This comparison aims to determine if including image inputs leads to a significant enhancement in accuracy or radiological credibility of the responses.","The research idea centers on evaluating the effectiveness of incorporating both text and image inputs in assessing diagnostic radiology knowledge, specifically within the context of the Japan Diagnostic Radiology Board Examination (JDRBE). The study addresses the need to understand whether the addition of visual information improves the accuracy and credibility of responses to radiology examination questions. The primary objective of the study is to compare the performance of a multimodal methodology that processes both text and images with a text-only methodology in answering JDRBE questions. This comparison aims to determine if including image inputs leads to a significant enhancement in accuracy or radiological credibility of the responses.",True
Medicine,LBO-MPAM: Ladybug Beetle Optimization-based multilayer perceptron attention module for segmenting the skin lesion and automatic localization,"In recent years, skin cancer has been the most dangerous disease noticed among people worldwide. Skin cancer should be identified earlier to reduce the rate of mortality. Employing dermoscopic images can identify and categorise skin cancer effectively. But, the visual evaluation is a complex procedure to be done in the dermoscopic image. However, Deep learning (DL) is an efficient method for skin cancer detection; however, segmenting the skin lesion and automatic localisation in an earlier stage is complicated. In this paper, a novel Ladybug Beetle Optimization-Double Attention Based Multilevel 1-D CNN (LBO-DAM 1-D CNN) technique is proposed to detect and classify skin cancer. To improve skin lesion type discriminability, the two types of attention modules are introduced. The Ultra-Lightweight Subspace Attention Module (ULSAM) is utilised for classifying the feature maps into different stages to validate the frequency from different image samples. However, the multilayer perceptron attention module (MLPAM) is determined to provide information regarding skin cancer classification and diminish the noise and unwanted data. To minimise data loss, it is then combined with hierarchical complementarity during classification. Second, a modified MLPAM is used to extract significant feature spaces for network learning, select the most important information, and reduce feature space redundancy. The Ladybug Beetle Optimization (LBO) algorithm provides the optimal classification solution by minimising the loss rate of DAM 1-D CNN architecture. The experimentation is conducted on three different datasets such as ISIC2020, HAM10000, and the melanoma detection dataset. The experimental results revealed that the proposed method is compared with different existing methods such as IMFO-KELM, Mask RCNN, M-SVM, DCNN-9, and TL-CNN with different datasets. These methods attained 94.56, 92.65, 90.56, 88.65, and 95.5 for the ISIC2020 dataset but the proposed method enhanced the classification performance by attaining 97.02. Also, the validation is based on metrics, namely, accuracy, precision, sensitivity, and F1-score of 97.03%, 97.05%, 97.58%, and 97.27% for a total of 500 epochs.","['Deep learning (DL)', 'Mask RCNN', 'M-SVM']","The research idea addresses the critical need for early identification of skin cancer to reduce mortality rates, highlighting the challenges associated with visual evaluation of dermoscopic images for effective detection and categorization. The complexity of segmenting skin lesions and localizing them at an early stage presents a significant obstacle in improving diagnostic accuracy. The primary objective of the study is to develop a novel approach to detect and classify skin cancer more effectively by enhancing the discriminability of skin lesion types and minimizing data loss during classification. The study aims to improve classification performance on multiple skin cancer datasets, thereby contributing to more accurate and early diagnosis of the disease.","The research idea addresses the critical need for early identification of skin cancer to reduce mortality rates, highlighting the challenges associated with visual evaluation of dermoscopic images for effective detection and categorization. The complexity of segmenting skin lesions and localizing them at an early stage presents a significant obstacle in improving diagnostic accuracy. The primary objective of the study is to develop a novel approach to detect and classify skin cancer more effectively by enhancing the distinction between skin lesion types and minimizing data loss during analysis. The study aims to improve identification performance on multiple skin cancer datasets, thereby contributing to more accurate and early diagnosis of the disease.",True
Medicine,Leveraging artificial intelligence in vaccine development: A narrative review,"Vaccine development stands as a cornerstone of public health efforts, pivotal in curbing infectious diseases and reducing global morbidity and mortality. However, traditional vaccine development methods are often time-consuming, costly, and inefficient. The advent of artificial intelligence (AI) has ushered in a new era in vaccine design, offering unprecedented opportunities to expedite the process. This narrative review explores the role of AI in vaccine development, focusing on antigen selection, epitope prediction, adjuvant identification, and optimization strategies. AI algorithms, including machine learning and deep learning, leverage genomic data, protein structures, and immune system interactions to predict antigenic epitopes, assess immunogenicity, and prioritize antigens for experimentation. Furthermore, AI-driven approaches facilitate the rational design of immunogens and the identification of novel adjuvant candidates with optimal safety and efficacy profiles. Challenges such as data heterogeneity, model interpretability, and regulatory considerations must be addressed to realize the full potential of AI in vaccine development. Integrating emerging technologies, such as single-cell omics and synthetic biology, promises to enhance vaccine design precision and scalability. This review underscores the transformative impact of AI on vaccine development and highlights the need for interdisciplinary collaborations and regulatory harmonization to accelerate the delivery of safe and effective vaccines against infectious diseases.","['machine learning', 'deep learning']","The research idea centers on the critical role of vaccine development in public health as a means to control infectious diseases and reduce global morbidity and mortality. Traditional vaccine development methods are often slow, costly, and inefficient, creating a need for more effective approaches to expedite the process. The study highlights the challenges and opportunities involved in improving vaccine design to enhance safety, efficacy, and scalability. The primary objective of the study is to explore advancements that can accelerate vaccine development by improving antigen selection, epitope prediction, adjuvant identification, and optimization strategies. It aims to emphasize the importance of interdisciplinary collaboration and regulatory harmonization to facilitate the delivery of safe and effective vaccines against infectious diseases.","The research idea centers on the critical role of vaccine development in public health as a means to control infectious diseases and reduce global morbidity and mortality. Traditional vaccine development methods are often slow, costly, and inefficient, creating a need for more effective approaches to expedite the process. The study highlights the challenges and opportunities involved in improving vaccine design to enhance safety, efficacy, and scalability. The primary objective of the study is to explore advancements that can accelerate vaccine development by improving antigen selection, epitope identification, adjuvant identification, and optimization strategies. It aims to emphasize the importance of interdisciplinary collaboration and regulatory harmonization to facilitate the delivery of safe and effective vaccines against infectious diseases.",True
Medicine,A domain adaptation approach to damage classification with an application to bridge monitoring,"Data-driven machine-learning algorithms generally suffer from a lack of labelled health-state data, mainly those referring to damage conditions. To address such an issue, population-based structural health monitoring seeks to enrich the original dataset by transferring knowledge from a population of monitored structures. Within this context, this paper presents a transfer learning approach, based on domain adaptation, to leverage information from completely-labelled bridge structure data to accurately predict new instances of an unknown target domain. Since intrinsic structural differences may cause distribution shifts, domain adaptation attempts to minimise the distance between the domains and to learn a mapping within a shared feature space. Specifically, the methodology involves the long-term acquisition of natural frequencies from several structural scenarios. Such damage-sensitive features are then aligned via domain adaptation so that a machine-learning algorithm can effectively utilise the labelled source domain data and generalise well to the unlabelled target-domain data. The described procedure is applied to two case studies, including the Z24 and the S101 benchmark bridges and their finite element models, respectively. The results demonstrate the successful exchange of health-state labels to identify the damage class within a population of bridges equipped with SHM systems, showing potential to reduce computational efforts and to deal with scarce or poor data sets in application to bridge network monitoring.","['transfer learning', 'domain adaptation']","The research idea addresses the challenge of limited labeled health-state data related to damage conditions in structural health monitoring of bridges. This scarcity hinders accurate identification and assessment of damage within individual structures. The study is motivated by the need to improve damage detection by leveraging information from a population of monitored bridges to enhance the understanding of structural health states. The primary objective of the study is to develop an approach that enables the transfer of health-state information from fully labeled bridge data to new, unlabeled bridge instances, thereby improving the accuracy of damage classification across different bridge structures. The study aims to demonstrate the feasibility of exchanging health-state labels within a population of bridges to better identify damage classes and address the issue of scarce or poor data in bridge network monitoring.","The research idea addresses the challenge of limited labeled health-state data related to damage conditions in structural health monitoring of bridges. This scarcity hinders accurate identification and assessment of damage within individual structures. The study is motivated by the need to improve damage detection by leveraging information from a population of monitored bridges to enhance the understanding of structural health states. The primary objective of the study is to develop an approach that enables the transfer of health-state information from fully labeled bridge data to new, unlabeled bridge instances, thereby improving the accuracy of damage identification across different bridge structures. The study aims to demonstrate the feasibility of exchanging health-state labels within a population of bridges to better identify damage classes and address the issue of scarce or poor data in bridge network monitoring.",True
Medicine,Revolutionizing core muscle analysis in female sexual dysfunction based on machine learning,"Abstract The purpose of this study is to investigate the role of core muscles in female sexual dysfunction (FSD) and develop comprehensive rehabilitation programs to address this issue. We aim to answer the following research questions: what are the roles of core muscles in FSD, and how can machine and deep learning models accurately predict changes in core muscles during FSD? FSD is a common condition that affects women of all ages, characterized by symptoms such as decreased libido, difficulty achieving orgasm, and pain during intercourse. We conducted a comprehensive analysis of changes in core muscles during FSD using machine and deep learning. We evaluated the performance of multiple models, including multi-layer perceptron (MLP), long short-term memory (LSTM), convolutional neural network (CNN), recurrent neural network (RNN), ElasticNetCV, random forest regressor, SVR, and Bagging regressor. The models were evaluated based on mean squared error (MSE), mean absolute error (MAE), and R-squared (R 2 ) score. Our results show that CNN and random forest regressor are the most accurate models for predicting changes in core muscles during FSD. CNN achieved the lowest MSE (0.002) and the highest R 2 score (0.988), while random forest regressor also performed well with an MSE of 0.0021 and an R 2 score of 0.9905. Our study demonstrates that machine and deep learning models can accurately predict changes in core muscles during FSD. The neglected core muscles play a significant role in FSD, highlighting the need for comprehensive rehabilitation programs that address these muscles. By developing these programs, we can improve the quality of life for women with FSD and help them achieve optimal sexual health.","['multi-layer perceptron (MLP)', 'long short-term memory (LSTM)', 'convolutional neural network (CNN)', 'recurrent neural network (RNN)', 'ElasticNetCV', 'random forest regressor', 'SVR', 'Bagging regressor']","The research idea centers on the significant yet often overlooked role of core muscles in female sexual dysfunction (FSD), a condition affecting women of all ages and characterized by symptoms such as decreased libido, difficulty achieving orgasm, and pain during intercourse. Understanding how changes in core muscles contribute to FSD is crucial for addressing this common health issue. The study’s primary objective is to investigate the roles of core muscles in FSD and to develop comprehensive rehabilitation programs that specifically target these muscles. By focusing on these rehabilitation strategies, the study aims to improve the quality of life and sexual health outcomes for women experiencing FSD.","The research idea centers on the significant yet often overlooked role of core muscles in female sexual dysfunction (FSD), a condition affecting women of all ages and characterized by symptoms such as decreased libido, difficulty achieving orgasm, and pain during intercourse. Understanding how changes in core muscles contribute to FSD is crucial for addressing this common health issue. The study's primary objective is to investigate the roles of core muscles in FSD and to develop comprehensive rehabilitation programs that specifically target these muscles. By focusing on these rehabilitation strategies, the study aims to improve the quality of life and sexual health outcomes for women experiencing FSD.",True
Medicine,Empowering personalized pharmacogenomics with generative AI solutions,"Abstract Objective This study evaluates an AI assistant developed using OpenAI’s GPT-4 for interpreting pharmacogenomic (PGx) testing results, aiming to improve decision-making and knowledge sharing in clinical genetics and to enhance patient care with equitable access. Materials and Methods The AI assistant employs retrieval-augmented generation (RAG), which combines retrieval and generative techniques, by harnessing a knowledge base (KB) that comprises data from the Clinical Pharmacogenetics Implementation Consortium (CPIC). It uses context-aware GPT-4 to generate tailored responses to user queries from this KB, further refined through prompt engineering and guardrails. Results Evaluated against a specialized PGx question catalog, the AI assistant showed high efficacy in addressing user queries. Compared with OpenAI’s ChatGPT 3.5, it demonstrated better performance, especially in provider-specific queries requiring specialized data and citations. Key areas for improvement include enhancing accuracy, relevancy, and representative language in responses. Discussion The integration of context-aware GPT-4 with RAG significantly enhanced the AI assistant’s utility. RAG’s ability to incorporate domain-specific CPIC data, including recent literature, proved beneficial. Challenges persist, such as the need for specialized genetic/PGx models to improve accuracy and relevancy and addressing ethical, regulatory, and safety concerns. Conclusion This study underscores generative AI’s potential for transforming healthcare provider support and patient accessibility to complex pharmacogenomic information. While careful implementation of large language models like GPT-4 is necessary, it is clear that they can substantially improve understanding of pharmacogenomic data. With further development, these tools could augment healthcare expertise, provider productivity, and the delivery of equitable, patient-centered healthcare services.",['retrieval-augmented generation (RAG)'],"The research idea centers on addressing the challenges in interpreting pharmacogenomic testing results to improve clinical decision-making and knowledge sharing in clinical genetics, ultimately enhancing patient care and ensuring equitable access to complex pharmacogenomic information. The study recognizes the need for better tools to support healthcare providers in understanding and utilizing pharmacogenomic data effectively. The primary objective of the study is to evaluate a newly developed assistant designed to interpret pharmacogenomic testing results with the aim of improving decision-making and knowledge dissemination among healthcare providers. The study seeks to assess the assistant’s effectiveness in supporting clinical genetics and enhancing patient care through improved accessibility and understanding of pharmacogenomic information.","The research idea centers on addressing the challenges in interpreting pharmacogenomic testing results to improve clinical decision-making and knowledge sharing in clinical genetics, ultimately enhancing patient care and ensuring equitable access to complex pharmacogenomic information. The study recognizes the need for better tools to support healthcare providers in understanding and utilizing pharmacogenomic data effectively. The primary objective of the study is to evaluate a newly developed interpretive resource designed to explain pharmacogenomic testing results with the aim of improving decision-making and knowledge dissemination among healthcare providers. The study seeks to assess this resource's effectiveness in supporting clinical genetics and enhancing patient care through improved accessibility and understanding of pharmacogenomic information.",True
Medicine,Performance of an Open-Source Large Language Model in Extracting Information from Free-Text Radiology Reports,"Purpose To assess the performance of a local open-source large language model (LLM) in various information extraction tasks from real-life emergency brain MRI reports. Materials and Methods All consecutive emergency brain MRI reports written in 2022 from a French quaternary center were retrospectively reviewed. Two radiologists identified MRI scans that were performed in the emergency department for headaches. Four radiologists scored the reports' conclusions as either normal or abnormal. Abnormalities were labeled as either headache-causing or incidental. Vicuna (LMSYS Org), an open-source LLM, performed the same tasks. Vicuna's performance metrics were evaluated using the radiologists' consensus as the reference standard. Results Among the 2398 reports during the study period, radiologists identified 595 that included headaches in the indication (median age of patients, 35 years [IQR, 26-51 years]; 68% [403 of 595] women). A positive finding was reported in 227 of 595 (38%) cases, 136 of which could explain the headache. The LLM had a sensitivity of 98.0% (95% CI: 96.5, 99.0) and specificity of 99.3% (95% CI: 98.8, 99.7) for detecting the presence of headache in the clinical context, a sensitivity of 99.4% (95% CI: 98.3, 99.9) and specificity of 98.6% (95% CI: 92.2, 100.0) for the use of contrast medium injection, a sensitivity of 96.0% (95% CI: 92.5, 98.2) and specificity of 98.9% (95% CI: 97.2, 99.7) for study categorization as either normal or abnormal, and a sensitivity of 88.2% (95% CI: 81.6, 93.1) and specificity of 73% (95% CI: 62, 81) for causal inference between MRI findings and headache. Conclusion An open-source LLM was able to extract information from free-text radiology reports with excellent accuracy without requiring further training.","['Vicuna (open-source large language model, LLM)']","The research idea addresses the challenge of accurately extracting clinically relevant information from emergency brain MRI reports, particularly in cases involving patients presenting with headaches. This is important for improving the interpretation and utilization of radiology reports in urgent medical settings. The study focuses on evaluating the ability to identify abnormalities that could explain headaches and distinguish them from incidental findings. The research objective is to assess the performance of a local open-source tool in extracting specific information from real-life emergency brain MRI reports, including detecting the presence of headache in the clinical context, use of contrast medium, categorization of studies as normal or abnormal, and determining the causal relationship between MRI findings and headache, using radiologists’ consensus as the reference standard.","The research idea addresses the challenge of accurately extracting clinically relevant information from emergency brain MRI reports, particularly in cases involving patients presenting with headaches. This is important for improving the interpretation and utilization of radiology reports in urgent medical settings. The study focuses on evaluating the ability to identify abnormalities that could explain headaches and distinguish them from incidental findings. The research objective is to assess the performance of a specialized clinical information extraction tool in gathering specific information from real-life emergency brain MRI reports, including detecting the presence of headache in the clinical context, use of contrast medium, categorization of studies as normal or abnormal, and determining the causal relationship between MRI findings and headache, using radiologists' consensus as the reference standard.",True
Medicine,"Triage Performance Across Large Language Models, ChatGPT, and Untrained Doctors in Emergency Medicine: Comparative Study","Background Large language models (LLMs) have demonstrated impressive performances in various medical domains, prompting an exploration of their potential utility within the high-demand setting of emergency department (ED) triage. This study evaluated the triage proficiency of different LLMs and ChatGPT, an LLM-based chatbot, compared to professionally trained ED staff and untrained personnel. We further explored whether LLM responses could guide untrained staff in effective triage. Objective This study aimed to assess the efficacy of LLMs and the associated product ChatGPT in ED triage compared to personnel of varying training status and to investigate if the models’ responses can enhance the triage proficiency of untrained personnel. Methods A total of 124 anonymized case vignettes were triaged by untrained doctors; different versions of currently available LLMs; ChatGPT; and professionally trained raters, who subsequently agreed on a consensus set according to the Manchester Triage System (MTS). The prototypical vignettes were adapted from cases at a tertiary ED in Germany. The main outcome was the level of agreement between raters’ MTS level assignments, measured via quadratic-weighted Cohen κ. The extent of over- and undertriage was also determined. Notably, instances of ChatGPT were prompted using zero-shot approaches without extensive background information on the MTS. The tested LLMs included raw GPT-4, Llama 3 70B, Gemini 1.5, and Mixtral 8x7b. Results GPT-4–based ChatGPT and untrained doctors showed substantial agreement with the consensus triage of professional raters (κ=mean 0.67, SD 0.037 and κ=mean 0.68, SD 0.056, respectively), significantly exceeding the performance of GPT-3.5–based ChatGPT (κ=mean 0.54, SD 0.024; P&lt;.001). When untrained doctors used this LLM for second-opinion triage, there was a slight but statistically insignificant performance increase (κ=mean 0.70, SD 0.047; P=.97). Other tested LLMs performed similar to or worse than GPT-4–based ChatGPT or showed odd triaging behavior with the used parameters. LLMs and ChatGPT models tended toward overtriage, whereas untrained doctors undertriaged. Conclusions While LLMs and the LLM-based product ChatGPT do not yet match professionally trained raters, their best models’ triage proficiency equals that of untrained ED doctors. In its current form, LLMs or ChatGPT thus did not demonstrate gold-standard performance in ED triage and, in the setting of this study, failed to significantly improve untrained doctors’ triage when used as decision support. Notable performance enhancements in newer LLM versions over older ones hint at future improvements with further technological development and specific training.","['zero-shot approaches', 'GPT-4', 'Llama 3 70B', 'GPT-3.5']","The study addresses the challenge of accurately performing emergency department (ED) triage, a critical and high-demand medical task typically conducted by professionally trained staff. There is a need to explore alternative approaches that could potentially support or enhance triage decisions, especially by personnel with varying levels of training. The research aims to evaluate the effectiveness of different approaches in triaging ED cases compared to professional raters and to determine whether these approaches can improve the triage proficiency of untrained staff. Specifically, the study’s primary objective is to assess the triage performance of various methods relative to professionally trained ED personnel and to investigate if their use can enhance the accuracy of triage decisions made by untrained doctors.","The study addresses the challenge of accurately performing emergency department (ED) triage, a critical and high-demand medical task typically conducted by professionally trained staff. There is a need to explore alternative approaches that could potentially support or enhance triage decisions, especially by personnel with varying levels of training. The research aims to evaluate the effectiveness of different approaches in triaging ED cases compared to professional raters and to determine whether these approaches can improve the triage proficiency of untrained staff. Specifically, the study's primary objective is to assess the triage performance of various methods relative to professionally trained ED personnel and to investigate if their use can enhance the accuracy of triage decisions made by untrained doctors.",True
Medicine,"Prediction of atmospheric PM2.5 level by machine learning techniques in Isfahan, Iran","Abstract With increasing levels of air pollution, air quality prediction has attracted more attention. Mathematical models are being developed by researchers to achieve precise predictions. Monitoring and prediction of atmospheric PM 2.5 levels, as a predominant pollutant, is essential in emission mitigation programs. In this study, meteorological datasets from 9 years in Isfahan city, a large metropolis of Iran, were applied to predict the PM 2.5 levels, using four machine learning algorithms including Artificial Neural |Networks (ANNs), K-Nearest-Neighbors (KNN), Support Vector |Machines (SVMs) and ensembles of classification trees Random Forest (RF). The data from 7 air quality monitoring stations located in Isfahan City were taken into consideration. The Confusion Matrix and Cross-Entropy Loss were used to analyze the performance of classification models. Several parameters, including sensitivity, specificity, accuracy, F1 score, precision, and the area under the curve (AUC), are computed to assess model performance. Finally, by introducing the predicted data for 2020 into ArcGIS software and using the IDW (Inverse Distance Weighting) method, interpolation was conducted for the area of Isfahan city and the pollution map was illustrated for each month of the year. The results showed that, based on the accuracy percentage, the ANN model has a better performance (90.1%) in predicting PM 2.5 grades compared to the other models for the applied meteorological dataset, followed by RF (86.1%), SVM (84.6%) and KNN (82.2%) models, respectively. Therefore, ANN modelling provides a feasible procedure for the managerial planning of air pollution control.","['Artificial Neural Networks (ANNs)', 'K-Nearest-Neighbors (KNN)', 'Support Vector Machines (SVMs)', 'Random Forest (RF)']","The study addresses the growing concern of air pollution and the critical need for accurate prediction of atmospheric PM 2.5 levels, which are a predominant pollutant affecting public health and environmental quality. Monitoring and predicting these pollutant levels are essential components of emission mitigation programs, especially in large urban areas like Isfahan city. The primary aim of the study is to predict PM 2.5 levels in Isfahan city using meteorological data collected over nine years from multiple air quality monitoring stations. This prediction is intended to support effective managerial planning and control of air pollution in the region.","The study addresses the growing concern of air pollution and the critical need for accurate prediction of atmospheric PM 2.5 levels, which are a predominant pollutant affecting public health and environmental quality. Monitoring and predicting these pollutant levels are essential components of emission mitigation programs, especially in large urban areas like Isfahan city. The primary aim of the study is to forecast PM 2.5 levels in Isfahan city using meteorological data collected over nine years from multiple air quality monitoring stations. This forecasting is intended to support effective managerial planning and control of air pollution in the region.",True
Medicine,Machine Learning–Based Prediction of Suicidality in Adolescents With Allergic Rhinitis: Derivation and Validation in 2 Independent Nationwide Cohorts,"Background Given the additional risk of suicide-related behaviors in adolescents with allergic rhinitis (AR), it is important to use the growing field of machine learning (ML) to evaluate this risk. Objective This study aims to evaluate the validity and usefulness of an ML model for predicting suicide risk in patients with AR. Methods We used data from 2 independent survey studies, Korea Youth Risk Behavior Web-based Survey (KYRBS; n=299,468) for the original data set and Korea National Health and Nutrition Examination Survey (KNHANES; n=833) for the external validation data set, to predict suicide risks of AR in adolescents aged 13 to 18 years, with 3.45% (10,341/299,468) and 1.4% (12/833) of the patients attempting suicide in the KYRBS and KNHANES studies, respectively. The outcome of interest was the suicide attempt risks. We selected various ML-based models with hyperparameter tuning in the discovery and performed an area under the receiver operating characteristic curve (AUROC) analysis in the train, test, and external validation data. Results The study data set included 299,468 (KYRBS; original data set) and 833 (KNHANES; external validation data set) patients with AR recruited between 2005 and 2022. The best-performing ML model was the random forest model with a mean AUROC of 84.12% (95% CI 83.98%-84.27%) in the original data set. Applying this result to the external validation data set revealed the best performance among the models, with an AUROC of 89.87% (sensitivity 83.33%, specificity 82.58%, accuracy 82.59%, and balanced accuracy 82.96%). While looking at feature importance, the 5 most important features in predicting suicide attempts in adolescent patients with AR are depression, stress status, academic achievement, age, and alcohol consumption. Conclusions This study emphasizes the potential of ML models in predicting suicide risks in patients with AR, encouraging further application of these models in other conditions to enhance adolescent health and decrease suicide rates.",['random forest'],"The research idea addresses the increased risk of suicide-related behaviors in adolescents with allergic rhinitis (AR), highlighting the importance of evaluating this risk to improve adolescent health outcomes. Given the significant impact of suicide attempts among this population, there is a critical need to identify factors that contribute to suicide risk in patients with AR. The study’s primary objective is to assess the validity and usefulness of a predictive approach for identifying suicide risk in adolescents aged 13 to 18 years with AR. Specifically, the study aims to evaluate how well this approach can predict suicide attempts in this group, using data from large-scale surveys to inform prevention strategies and ultimately reduce suicide rates among adolescents with AR.","The research idea addresses the increased risk of suicide-related behaviors in adolescents with allergic rhinitis (AR), highlighting the importance of evaluating this risk to improve adolescent health outcomes. Given the significant impact of suicide attempts among this population, there is a critical need to identify factors that contribute to suicide risk in patients with AR. The study's primary objective is to assess the validity and usefulness of a risk assessment framework for identifying suicide risk in adolescents aged 13 to 18 years with AR. Specifically, the study aims to evaluate how effectively this framework can identify potential suicide attempts in this group, using data from large-scale surveys to inform prevention strategies and ultimately reduce suicide rates among adolescents with AR.",True
Medicine,"Developing Deep LSTMs With Later Temporal Attention for Predicting COVID-19 Severity, Clinical Outcome, and Antibody Level by Screening Serological Indicators Over Time","Objective: The clinical course of COVID-19, as well as the immunological reaction, is notable for its extreme variability. Identifying the main associated factors might help understand the disease progression and physiological status of COVID-19 patients. The dynamic changes of the antibody against Spike protein are crucial for understanding the immune response. This work explores a temporal attention (TA) mechanism of deep learning to predict COVID-19 disease severity, clinical outcomes, and Spike antibody levels by screening serological indicators over time. Methods: We use feature selection techniques to filter feature subsets that are highly correlated with the target. The specific deep Long Short-Term Memory (LSTM) models are employed to capture the dynamic changes of disease severity, clinical outcome, and Spike antibody level. We also propose deep LSTMs with a TA mechanism to emphasize the later blood test records because later records often attract more attention from doctors. Results: Risk factors highly correlated with COVID-19 are revealed. LSTM achieves the highest classification accuracy for disease severity prediction. Temporal Attention Long Short-Term Memory (TA-LSTM) achieves the best performance for clinical outcome prediction. For Spike antibody level prediction, LSTM achieves the best permanence. Conclusion: The experimental results demonstrate the effectiveness of the proposed models. The proposed models can provide a computer-aided medical diagnostics system by simply using time series of serological indicators.","['deep Long Short-Term Memory (LSTM) models', 'deep LSTMs with a temporal attention (TA) mechanism']","The clinical course of COVID-19 and the immunological response exhibit significant variability among patients, making it challenging to understand disease progression and physiological status. Identifying the main factors associated with COVID-19 severity and outcomes is essential for improving patient management. The dynamic changes in antibodies against the Spike protein are particularly important for understanding the immune response to the virus. This study aims to predict COVID-19 disease severity, clinical outcomes, and Spike antibody levels by examining the temporal changes in serological indicators over time. The primary objective is to identify risk factors correlated with COVID-19 and to assess the progression of disease severity, clinical outcomes, and immune response through monitoring serological markers longitudinally.","The clinical course of COVID-19 and the immunological response exhibit significant variability among patients, making it challenging to understand disease progression and physiological status. Identifying the main factors associated with COVID-19 severity and outcomes is essential for improving patient management. The dynamic changes in antibodies against the Spike protein are particularly important for understanding the immune response to the virus. This study aims to determine COVID-19 disease severity, clinical outcomes, and Spike antibody levels by examining the temporal changes in serological indicators over time. The primary objective is to identify risk factors correlated with COVID-19 and to assess the progression of disease severity, clinical outcomes, and immune response through monitoring serological markers longitudinally.",True
Medicine,A novel fusion framework of deep bottleneck residual convolutional neural network for breast cancer classification from mammogram images,"With over 2.1 million new cases of breast cancer diagnosed annually, the incidence and mortality rate of this disease pose severe global health issues for women. Identifying the disease’s influence is the only practical way to lessen it immediately. Numerous research works have developed automated methods using different medical imaging to identify BC. Still, the precision of each strategy differs based on the available resources, the issue’s nature, and the dataset being used. We proposed a novel deep bottleneck convolutional neural network with a quantum optimization algorithm for breast cancer classification and diagnosis from mammogram images. Two novel deep architectures named three-residual blocks bottleneck and four-residual blocks bottle have been proposed with parallel and single paths. Bayesian Optimization (BO) has been employed to initialize hyperparameter values and train the architectures on the selected dataset. Deep features are extracted from the global average pool layer of both models. After that, a kernel-based canonical correlation analysis and entropy technique is proposed for the extracted deep features fusion. The fused feature set is further refined using an optimization technique named quantum generalized normal distribution optimization. The selected features are finally classified using several neural network classifiers, such as bi-layered and wide-neural networks. The experimental process was conducted on a publicly available mammogram imaging dataset named INbreast, and a maximum accuracy of 96.5% was obtained. Moreover, for the proposed method, the sensitivity rate is 96.45, the precision rate is 96.5, the F1 score value is 96.64, the MCC value is 92.97%, and the Kappa value is 92.97%, respectively. The proposed architectures are further utilized for the diagnosis process of infected regions. In addition, a detailed comparison has been conducted with a few recent techniques showing the proposed framework’s higher accuracy and precision rate.","['deep bottleneck convolutional neural network', 'Bayesian Optimization (BO)', 'kernel-based canonical correlation analysis', 'bi-layered neural network classifier', 'wide-neural network classifier']","The research idea addresses the significant global health challenge posed by breast cancer, with over 2.1 million new cases diagnosed annually and high incidence and mortality rates among women. The study emphasizes the urgent need for effective identification and diagnosis of breast cancer to reduce its impact promptly. Despite various existing methods using medical imaging for breast cancer detection, there remains variability in precision depending on resources and datasets. The research aims to improve the accuracy and reliability of breast cancer classification and diagnosis from mammogram images.

The primary objective of the study is to develop and evaluate novel deep learning architectures for the classification and diagnosis of breast cancer using mammogram images. The study seeks to enhance diagnostic performance by proposing new network structures and optimizing feature extraction and selection processes. The ultimate goal is to achieve high accuracy, sensitivity, and precision in identifying breast cancer to support better clinical decision-making.","The research idea addresses the significant global health challenge posed by breast cancer, with over 2.1 million new cases diagnosed annually and high incidence and mortality rates among women. The study emphasizes the urgent need for effective identification and diagnosis of breast cancer to reduce its impact promptly. Despite various existing methods using medical imaging for breast cancer detection, there remains variability in precision depending on resources and datasets. The research aims to improve the accuracy and reliability of breast cancer classification and diagnosis from mammogram images.

The primary objective of the study is to develop and evaluate novel analytical approaches for the classification and diagnosis of breast cancer using mammogram images. The study seeks to enhance diagnostic performance by proposing new methodological structures and optimizing image analysis processes. The ultimate goal is to achieve high accuracy, sensitivity, and precision in identifying breast cancer to support better clinical decision-making.",True
Medicine,Potential of Large Language Models in Health Care: Delphi Study,"Background A large language model (LLM) is a machine learning model inferred from text data that captures subtle patterns of language use in context. Modern LLMs are based on neural network architectures that incorporate transformer methods. They allow the model to relate words together through attention to multiple words in a text sequence. LLMs have been shown to be highly effective for a range of tasks in natural language processing (NLP), including classification and information extraction tasks and generative applications. Objective The aim of this adapted Delphi study was to collect researchers’ opinions on how LLMs might influence health care and on the strengths, weaknesses, opportunities, and threats of LLM use in health care. Methods We invited researchers in the fields of health informatics, nursing informatics, and medical NLP to share their opinions on LLM use in health care. We started the first round with open questions based on our strengths, weaknesses, opportunities, and threats framework. In the second and third round, the participants scored these items. Results The first, second, and third rounds had 28, 23, and 21 participants, respectively. Almost all participants (26/28, 93% in round 1 and 20/21, 95% in round 3) were affiliated with academic institutions. Agreement was reached on 103 items related to use cases, benefits, risks, reliability, adoption aspects, and the future of LLMs in health care. Participants offered several use cases, including supporting clinical tasks, documentation tasks, and medical research and education, and agreed that LLM-based systems will act as health assistants for patient education. The agreed-upon benefits included increased efficiency in data handling and extraction, improved automation of processes, improved quality of health care services and overall health outcomes, provision of personalized care, accelerated diagnosis and treatment processes, and improved interaction between patients and health care professionals. In total, 5 risks to health care in general were identified: cybersecurity breaches, the potential for patient misinformation, ethical concerns, the likelihood of biased decision-making, and the risk associated with inaccurate communication. Overconfidence in LLM-based systems was recognized as a risk to the medical profession. The 6 agreed-upon privacy risks included the use of unregulated cloud services that compromise data security, exposure of sensitive patient data, breaches of confidentiality, fraudulent use of information, vulnerabilities in data storage and communication, and inappropriate access or use of patient data. Conclusions Future research related to LLMs should not only focus on testing their possibilities for NLP-related tasks but also consider the workflows the models could contribute to and the requirements regarding quality, integration, and regulations needed for successful implementation in practice.","['large language model (LLM)', 'transformer methods']","The research idea centers on understanding the potential impact of large language models on health care by exploring their strengths, weaknesses, opportunities, and threats as perceived by experts in related medical fields. This study addresses the need to evaluate how these models might influence various aspects of health care delivery, including clinical tasks, documentation, medical research, and patient education, while also considering associated risks such as misinformation, ethical concerns, and data privacy issues. The primary objective of the study was to gather and reach consensus on researchers’ opinions regarding the influence of large language models in health care, focusing on their use cases, benefits, risks, reliability, adoption factors, and future implications. The study aimed to identify agreed-upon benefits such as improved efficiency, quality of care, personalized treatment, and enhanced patient-provider interaction, as well as to highlight key risks and privacy concerns that must be addressed for successful integration into health care practice.","The research idea centers on understanding the potential impact of advanced textual communication systems on health care by exploring their strengths, weaknesses, opportunities, and threats as perceived by experts in related medical fields. This study addresses the need to evaluate how these systems might influence various aspects of health care delivery, including clinical tasks, documentation, medical research, and patient education, while also considering associated risks such as misinformation, ethical concerns, and data privacy issues. The primary objective of the study was to gather and reach consensus on researchers' opinions regarding the influence of these text-based technologies in health care, focusing on their use cases, benefits, risks, reliability, adoption factors, and future implications. The study aimed to identify agreed-upon benefits such as improved efficiency, quality of care, personalized treatment, and enhanced patient-provider interaction, as well as to highlight key risks and privacy concerns that must be addressed for successful integration into health care practice.",True
Medicine,Tomato Leaf Disease Detection using Convolutional Neural Networks,"One of the most important crops that is grown in enormous amounts and has an excellent market value comprises the tomato. They are grown and eaten in large quantities not only in India but also globally. Disease is the primary factor affecting the quantity and quality of this crop’s production. In earlier research, the plant’s leaves solely were taken into account for disease identification; however, in many cases, the illness only affects the fruit, leaving the other plant parts healthy. Using the unaided eye to diagnose a disease can occasionally lead to a prognosis that is off, meaning the wrong pesticide is applied and the plant may get spoiled. The farmers find it challenging to diagnose the disease because specialists are scarce in many of the affected areas. It’s an expensive and time-consuming process, even though professionals are accessible in certain sectors. Early disease detection would lessen the impact on plants and increase agricultural yield. As a result, it is essential to recognise these illnesses accurately and use the appropriate pesticide. These is- sues can be resolved by an automated system. We have developed a system to tackle this problem, which employs a convolutional neural network (CNN) to detect the ailment and recommends a pesticide to aid in its eradication. Since CNN offers its highest level of accuracy, our system incorporates it.",['convolutional neural network (CNN)'],"The study addresses the significant problem of disease affecting tomato crops, which impacts both the quantity and quality of production. Traditional disease identification methods have focused mainly on the plant’s leaves, but many diseases affect only the fruit, making diagnosis by the unaided eye difficult and often inaccurate. This leads to incorrect pesticide application, crop spoilage, and challenges for farmers, especially in areas lacking specialists. Early and accurate disease detection is crucial to reduce damage to plants and improve agricultural yield. The primary aim of the study is to develop a method for accurate recognition of tomato diseases affecting different parts of the plant and to recommend appropriate pesticides for effective disease management. This approach seeks to assist farmers in timely and precise disease diagnosis to enhance crop protection and productivity.","The study addresses the significant problem of disease affecting tomato crops, which impacts both the quantity and quality of production. Traditional disease identification methods have focused mainly on the plant's leaves, but many diseases affect only the fruit, making diagnosis by the unaided eye difficult and often inaccurate. This leads to incorrect pesticide application, crop spoilage, and challenges for farmers, especially in areas lacking specialists. Early and accurate disease detection is crucial to reduce damage to plants and improve agricultural yield. The primary aim of the study is to develop a method for accurate recognition of tomato diseases affecting different parts of the plant and to recommend appropriate pesticides for effective disease management. This approach seeks to assist farmers in timely and precise disease diagnosis to enhance crop protection and productivity.",True
Engineering,Advanced Modelling of Soil Organic Carbon Content in Coal Mining Areas Using Integrated Spectral Analysis: A Dengcao Coal Mine Case Study,"Effective modelling and integrated spectral analysis approaches can advance modelling precision. To develop an integrated spectral forecast modelling of soil organic carbon (SOC), this research investigated a mining coal in Dengcao Coal Mine Area, Zhengzhou. The study utilizes the Lasso and Ranger algorithms were utilized in spectral band analysis. Four primary models employed during this process include Artificial Neural Network (ANN), Support Vector Machine, Random Forest (RF), and Partial Least Squares Regression (PLSR). The ideal model was chosen. The results showed that, in contrast to when band collection was based on Lasso algorithm modelling, model precision was higher when it was based on the Ranger algorithm. ANN model had an ideal goodness acceptance, and the modelling developed by RF showed the steadiest modelling consequences. Based on the results, a distinct method is proposed in this study for band assortment at the earlier stage of integrated spectral modelling of SOC. The Ranger method can be used to check the spectral particles, and RF or ANN can be chosen to develop the prediction modelling based on different statistics sets, which is appropriate to create the prediction modelling of SOC content in Dengcao Coal Mine Area. This research avails a position for the integrated spectral of Analysis for Advanced Modelling of Soil Organic Carbon Content in Coal Sources alongside a theoretical foundation for innovating portable device for the integrated spectral assessment of SOC content in coal mining habitats. This study might be significant for the changing modelling and monitoring of SOC in mining and environmental areas.","['Lasso', 'Artificial Neural Network (ANN)', 'Support Vector Machine', 'Random Forest (RF)']","The research addresses the challenge of improving the precision of soil organic carbon (SOC) content estimation in coal mining areas through integrated spectral analysis. Accurate modelling of SOC is essential for monitoring and managing soil quality in mining environments, which has implications for environmental sustainability and resource management. The study focuses on the Dengcao Coal Mine Area in Zhengzhou, aiming to enhance the methods used for spectral forecasting of SOC content. The primary objective of the research is to develop an integrated spectral forecast approach for SOC by investigating and comparing different spectral band selection methods and modelling techniques to identify the most effective strategy for predicting SOC content in coal mining habitats. This work seeks to provide a theoretical foundation for advanced SOC assessment and to support the innovation of portable devices for on-site spectral evaluation in mining and environmental contexts.","The research addresses the challenge of improving the precision of soil organic carbon (SOC) content estimation in coal mining areas through integrated spectral analysis. Accurate assessment of SOC is essential for monitoring and managing soil quality in mining environments, which has implications for environmental sustainability and resource management. The study focuses on the Dengcao Coal Mine Area in Zhengzhou, aiming to enhance the methods used for spectral forecasting of SOC content. The primary objective of the research is to develop an integrated spectral forecast approach for SOC by investigating and comparing different spectral band selection methods and analytical techniques to identify the most effective strategy for determining SOC content in coal mining habitats. This work seeks to provide a theoretical foundation for advanced SOC assessment and to support the innovation of portable devices for on-site spectral evaluation in mining and environmental contexts.",True
Engineering,Fusion of finite element and machine learning methods to predict rock shear strength parameters,"Abstract The trial-and-error method for calibrating rock mechanics parameters has the disadvantages of complexity, being time-consuming, and difficulty in ensuring accuracy. Harnessing the repeatability and scalability intrinsic to numerical simulation calculations and amalgamating them with the data-driven attributes of machine learning methods, this study uses the finite element analysis software RS2 to establish 252 sets of sandstone sample data. The recursive feature elimination and cross-validation method is employed for feature selection. The shear strength parameters of sandstone are predicted using machine learning models optimized by the particle swarm optimization (PSO) algorithm, including the backpropagation neural network, Bayesian ridge regression, support vector regression (SVR), and light gradient boosting machine. The predicted value of cohesion is proposed as the input feature to predict the friction angle. The results indicate that the optimal input characteristics for predicting cohesion are elastic modulus, Poisson's ratio, peak stress, and peak strain, while the optimal input characteristics for predicting friction angle are peak stress and cohesion. The PSO-SVR model demonstrates the best performance. The maximum error between the predicted values of cohesion and friction angle and the calculated results of RSData program are 3.5% and 4.31%, respectively. The finite element calculation is in good agreement with the stress–strain curve obtained in the laboratory. The sensitivity analysis indicates that SVR's prediction performance for cohesion and friction angle tends to be stable when the sample size is &amp;gt;25. These results offer a valuable reference for accurately predicting rock mechanics parameters.","['recursive feature elimination', 'backpropagation neural network', 'Bayesian ridge regression', 'support vector regression (SVR)', 'light gradient boosting machine']","The research idea addresses the limitations of the traditional trial-and-error method for calibrating rock mechanics parameters, which is complex, time-consuming, and challenging to ensure accuracy. There is a need for a more efficient and reliable approach to determine the shear strength parameters of sandstone, which are critical for understanding rock behavior under stress. The study aims to improve the prediction of these parameters by leveraging numerical simulation data and identifying key influencing factors. The primary objective of the study is to accurately predict the shear strength parameters of sandstone, specifically cohesion and friction angle, by establishing relationships with mechanical properties such as elastic modulus, Poisson's ratio, peak stress, and peak strain. The research seeks to provide a reliable reference for predicting these rock mechanics parameters with improved precision and consistency, validated by comparison with laboratory stress–strain curves.","The research idea addresses the limitations of the traditional trial-and-error method for calibrating rock mechanics parameters, which is complex, time-consuming, and challenging to ensure accuracy. There is a need for a more efficient and reliable approach to determine the shear strength parameters of sandstone, which are critical for understanding rock behavior under stress. The study aims to improve the estimation of these parameters by utilizing numerical simulation data and identifying key influencing factors. The primary objective of the study is to accurately determine the shear strength parameters of sandstone, specifically cohesion and friction angle, by establishing relationships with mechanical properties such as elastic modulus, Poisson's ratio, peak stress, and peak strain. The research seeks to provide a reliable reference for estimating these rock mechanics parameters with improved precision and consistency, validated by comparison with laboratory stress–strain curves.",True
Engineering,The deep learning applications in IoT-based bio- and medical informatics: a systematic literature review,"Abstract Nowadays, machine learning (ML) has attained a high level of achievement in many contexts. Considering the significance of ML in medical and bioinformatics owing to its accuracy, many investigators discussed multiple solutions for developing the function of medical and bioinformatics challenges using deep learning (DL) techniques. The importance of DL in Internet of Things (IoT)-based bio- and medical informatics lies in its ability to analyze and interpret large amounts of complex and diverse data in real time, providing insights that can improve healthcare outcomes and increase efficiency in the healthcare industry. Several applications of DL in IoT-based bio- and medical informatics include diagnosis, treatment recommendation, clinical decision support, image analysis, wearable monitoring, and drug discovery. The review aims to comprehensively evaluate and synthesize the existing body of the literature on applying deep learning in the intersection of the IoT with bio- and medical informatics. In this paper, we categorized the most cutting-edge DL solutions for medical and bioinformatics issues into five categories based on the DL technique utilized: convolutional neural network , recurrent neural network , generative adversarial network , multilayer perception , and hybrid methods. A systematic literature review was applied to study each one in terms of effective properties, like the main idea, benefits, drawbacks, methods, simulation environment, and datasets. After that, cutting-edge research on DL approaches and applications for bioinformatics concerns was emphasized. In addition, several challenges that contributed to DL implementation for medical and bioinformatics have been addressed, which are predicted to motivate more studies to develop medical and bioinformatics research progressively. According to the findings, most articles are evaluated using features like accuracy, sensitivity, specificity, F -score, latency, adaptability, and scalability.","['convolutional neural network', 'recurrent neural network', 'generative adversarial network', 'hybrid methods']","The research idea centers on addressing the challenges in medical and bioinformatics by improving the analysis and interpretation of complex and diverse data to enhance healthcare outcomes and efficiency. The study highlights the significance of advanced approaches in the context of Internet of Things (IoT)-based bio- and medical informatics, where timely and accurate insights are crucial for applications such as diagnosis, treatment recommendation, clinical decision support, image analysis, wearable monitoring, and drug discovery. The research objective is to comprehensively evaluate and synthesize the existing literature on innovative solutions applied to medical and bioinformatics problems within the IoT framework. This includes categorizing the most advanced approaches based on their characteristics, assessing their benefits and drawbacks, and addressing the challenges that hinder their implementation, with the aim of motivating further progress in medical and bioinformatics research.","The research idea centers on addressing the challenges in medical and bioinformatics by improving the analysis and interpretation of complex and diverse data to enhance healthcare outcomes and efficiency. The study highlights the significance of methodical approaches in the context of Internet of Things (IoT)-based bio- and medical informatics, where timely and accurate insights are crucial for applications such as diagnosis, treatment recommendation, clinical decision support, image analysis, wearable monitoring, and drug discovery. The research objective is to comprehensively evaluate and synthesize the existing literature on innovative solutions applied to medical and bioinformatics problems within the IoT framework. This includes categorizing the most effective methodologies based on their characteristics, assessing their benefits and drawbacks, and addressing the challenges that hinder their implementation, with the aim of motivating further progress in medical and bioinformatics research.",True
Engineering,Battery safety: Machine learning-based prognostics,"Lithium-ion batteries play a pivotal role in a wide range of applications, from electronic devices to large-scale electrified transportation systems and grid-scale energy storage. Nevertheless, they are vulnerable to both progressive aging and unexpected failures, which can result in catastrophic events such as explosions or fires. Given their expanding global presence, the safety of these batteries and potential hazards from serious malfunctions are now major public concerns. Over the past decade, scholars and industry experts are intensively exploring methods to monitor battery safety, spanning from materials to cell, pack and system levels and across various spectral, spatial, and temporal scopes. In this Review, we start by summarizing the mechanisms and nature of battery failures. Following this, we explore the intricacies in predicting battery system evolution and delve into the specialized knowledge essential for data-driven, machine learning models. We offer an exhaustive review spotlighting the latest strides in battery fault diagnosis and failure prognosis via an array of machine learning approaches. Our discussion encompasses: (1) supervised and reinforcement learning integrated with battery models, apt for predicting faults/failures and probing into failure causes and safety protocols at the cell level; (2) unsupervised, semi-supervised, and self-supervised learning, advantageous for harnessing vast data sets from battery modules/packs; (3) few-shot learning tailored for gleaning insights from scarce examples, alongside physics-informed machine learning to bolster model generalization and optimize training in data-scarce settings. We conclude by casting light on the prospective horizons of comprehensive, real-world battery prognostics and management.","['supervised learning', 'reinforcement learning', 'unsupervised learning', 'semi-supervised learning', 'self-supervised learning', 'few-shot learning', 'physics-informed machine learning']","The research idea centers on the critical importance of lithium-ion battery safety due to their widespread use in electronic devices, transportation systems, and energy storage, coupled with their susceptibility to aging and unexpected failures that can lead to hazardous events such as explosions or fires. As these batteries become more prevalent globally, addressing the risks associated with their malfunctions has become a significant public concern. The study’s primary objective is to review and summarize the mechanisms and nature of battery failures, as well as to examine current advancements in fault diagnosis and failure prognosis at various levels of battery architecture. This includes exploring approaches to monitor battery safety and predict system evolution to enhance understanding and management of battery faults and failures.","The research idea centers on the critical importance of lithium-ion battery safety due to their widespread use in electronic devices, transportation systems, and energy storage, coupled with their susceptibility to aging and unexpected failures that can lead to hazardous events such as explosions or fires. As these batteries become more prevalent globally, addressing the risks associated with their malfunctions has become a significant public concern. The study's primary objective is to review and summarize the mechanisms and nature of battery failures, as well as to examine current advancements in fault diagnosis and failure prognosis at various levels of battery architecture. This includes exploring approaches to monitor battery safety and predict system behavior to enhance understanding and management of battery faults and failures.",True
Engineering,Machine learning-based predictive model for thermal comfort and energy optimization in smart buildings,"In the current context of energy transition and increasing climate change, optimizing building performance has become a critical objective. Efficient energy use and occupant comfort are paramount considerations in building design and operation. To address these challenges, this study introduces a predictive model leveraging Machine Learning (ML) algorithms. The model aims to predict thermal comfort levels and optimize energy consumption in Heating, Ventilation, and Air Conditioning (HVAC) systems. Four distinct ML algorithms Support Vector Machine (SVM), Artificial Neural Network (ANN), Random Forest (RF), and EXtreme Gradient Boosting (XGBOOST) are employed for this purpose. Data for the model is collected using a network of Raspberry Pi boards equipped with multiple sensors. Performance evaluation of the ML algorithms is conducted using statistical error metrics, including, Root Mean Square Error (RMSE), Mean Square Error (MSE), Mean Absolute Error (MAE), and coefficient of determination (R2). Results reveal that the RF and XGBOOST algorithms exhibit superior performance, achieving accuracies of 96.7% and 9.64% respectively. In contrast, the SVM algorithm demonstrates inferior performance with a R2 of 81.1%. These findings underscore the predictive capability of the RF and XGBOOST model in forecasting Predicted Mean Vote (PMV) values. The proposed model holds promise for enhancing occupant thermal comfort in buildings while simultaneously optimizing energy consumption in HVAC systems. Further research could explore the practical applications of these findings in building design and operation.","['Support Vector Machine (SVM)', 'Artificial Neural Network (ANN)', 'Random Forest (RF)', 'EXtreme Gradient Boosting (XGBOOST)']","The study addresses the critical need to optimize building performance amid the ongoing energy transition and increasing climate change concerns. It focuses on improving efficient energy use and enhancing occupant comfort, which are essential factors in building design and operation. The primary aim of the study is to develop a method to predict thermal comfort levels and optimize energy consumption in Heating, Ventilation, and Air Conditioning (HVAC) systems. This objective seeks to enhance occupant thermal comfort in buildings while simultaneously improving the efficiency of energy use within HVAC operations.","The study addresses the critical need to optimize building performance amid the ongoing energy transition and increasing climate change concerns. It focuses on improving efficient energy use and enhancing occupant comfort, which are essential factors in building design and operation. The primary aim of the study is to develop a method to assess thermal comfort levels and improve energy consumption in Heating, Ventilation, and Air Conditioning (HVAC) systems. This objective seeks to enhance occupant thermal comfort in buildings while simultaneously improving the efficiency of energy use within HVAC operations.",True
Engineering,"A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions","Automating the monitoring of industrial processes has the potential to enhance efficiency and optimize quality by promptly detecting abnormal events and thus facilitating timely interventions. Deep learning, with its capacity to discern non-trivial patterns within large datasets, plays a pivotal role in this process. Standard deep learning methods are suitable to solve a specific task given a specific type of data. During training, deep learning demands large volumes of labeled data. However, due to the dynamic nature of the industrial processes and environment, it is impractical to acquire large-scale labeled data for standard deep learning training for every slightly different case anew. Deep transfer learning offers a solution to this problem. By leveraging knowledge from related tasks and accounting for variations in data distributions, the transfer learning framework solves new tasks with little or even no additional labeled data. The approach bypasses the need to retrain a model from scratch for every new setup and dramatically reduces the labeled data requirement. This survey first provides an in-depth review of deep transfer learning, examining the problem settings of transfer learning and classifying the prevailing deep transfer learning methods. Moreover, we delve into applications of deep transfer learning in the context of a broad spectrum of time series anomaly detection tasks prevalent in primary industrial domains, e.g., manufacturing process monitoring, predictive maintenance, energy management, and infrastructure facility monitoring. We discuss the challenges and limitations of deep transfer learning in industrial contexts and conclude the survey with practical directions and actionable suggestions to address the need to leverage diverse time series data for anomaly detection in an increasingly dynamic production environment.","['deep learning', 'deep transfer learning', 'transfer learning framework']","The research idea centers on improving the monitoring of industrial processes to enhance efficiency and optimize quality by enabling the prompt detection of abnormal events, which facilitates timely interventions. The dynamic nature of industrial environments makes it impractical to obtain large-scale labeled data for every variation in process conditions, creating a challenge for effective monitoring across different scenarios. The primary objective of the study is to explore approaches that address the need for effective anomaly detection in industrial time series data despite limited labeled data availability for each new case. The study aims to review and classify existing methods that leverage knowledge from related tasks to reduce the requirement for extensive labeled data, thereby supporting anomaly detection across diverse industrial domains such as manufacturing process monitoring, predictive maintenance, energy management, and infrastructure facility monitoring.","The research idea centers on improving the monitoring of industrial processes to enhance efficiency and optimize quality by enabling the prompt detection of abnormal events, which facilitates timely interventions. The dynamic nature of industrial environments makes it impractical to obtain large-scale labeled data for every variation in process conditions, creating a challenge for effective monitoring across different scenarios. The primary objective of the study is to explore approaches that address the need for effective anomaly detection in industrial time series data despite limited labeled data availability for each new case. The study aims to review and classify existing methods that leverage knowledge from related tasks to reduce the requirement for extensive labeled data, thereby supporting anomaly detection across diverse industrial domains such as manufacturing process monitoring, preventive maintenance, energy management, and infrastructure facility monitoring.",True
Engineering,A novel framework for developing environmentally sustainable and cost-effective ultra-high-performance concrete (UHPC) using advanced machine learning and multi-objective optimization techniques,"This study aims to propose a novel framework for strength prediction and multi-objective optimization (MOO) of economical and environmentally sustainable ultra-high-performance concrete (UHPC) which aids in intelligent, sustainable, and resilient construction. Different tree- and boosting ensemble-based machine learning (ML) models are integrated to form an accurate and reliable prediction model for the uniaxial compressive strength of UHPC. The optimized models are integrated into a super learner model, resulting in a robust predictive model that is used as one of the objective functions in the MOO problem. A total of 19 objective functions are considered, including cost, uniaxial compressive strength, and 17 environmental impact categories that comprehensively evaluate the environmental sustainability of the UHPC mix. The resulting impacts from the mid-point indicators were calculated using the Eco-invent v3.7 Life Cycle Inventory database. The results showed that the super learner model accurately predicted the uniaxial compressive strength of UHPC. The MOO resulted in Pareto fronts, demonstrating the trade-off among the uniaxial compressive strength, cost, and environmental sustainability of the mix and a broad range of solutions that can be obtained for the 19 objectives. The study provides a useful tool for designers and decision-makers to select the optimal UHPC mixture that meets specific project requirements. Finally, for the practical application of the ML predictive model and MOO algorithm for UHPC, a graphical user interface-based software tool, FAI-OSUSCONCRET, was developed. This software tool offers fast, accurate, and intelligent predictions and multi-objective optimizations tailored to specific project requirements, thus resulting in a UHPC mixture that perfectly meets project needs.","['tree-based ensemble machine learning models', 'boosting ensemble-based machine learning models', 'super learner model']","The research addresses the challenge of developing ultra-high-performance concrete (UHPC) that is both economical and environmentally sustainable, aiming to support intelligent, sustainable, and resilient construction practices. It focuses on balancing multiple factors such as cost, compressive strength, and various environmental impact categories to comprehensively evaluate and improve the sustainability of UHPC mixtures. The primary objective of the study is to establish a framework for predicting the uniaxial compressive strength of UHPC and to perform multi-objective optimization considering cost, strength, and environmental impacts. This framework aims to provide designers and decision-makers with a practical tool to select optimal UHPC mixtures that meet specific project requirements while addressing economic and environmental sustainability.","The research addresses the challenge of developing ultra-high-performance concrete (UHPC) that is both economical and environmentally sustainable, aiming to support sustainable and resilient construction practices. It focuses on balancing multiple factors such as cost, compressive strength, and various environmental impact categories to comprehensively evaluate and improve the sustainability of UHPC mixtures. The primary objective of the study is to establish a framework for evaluating the uniaxial compressive strength of UHPC and to perform multi-objective optimization considering cost, strength, and environmental impacts. This framework aims to provide designers and decision-makers with a practical tool to select optimal UHPC mixtures that meet specific project requirements while addressing economic and environmental sustainability.",True
Engineering,Utilizing Hybrid Machine Learning and Soft Computing Techniques for Landslide Susceptibility Mapping in a Drainage Basin,"The hydrological system of thebasin of Lake Urmia is complex, deriving its supply from a network comprising 13 perennial rivers, along withnumerous small springs and direct precipitation onto the lake’s surface. Among these contributors, approximately half of the inflow is attributed to the Zarrineh River and the Simineh River. Remarkably, Lake Urmia lacks a natural outlet, with its water loss occurring solely through evaporation processes. This study employed a comprehensive methodology integrating ground surveys, remote sensing analyses, and meticulous documentation of historical landslides within the basin as primary information sources. Through this investigative approach, we preciselyidentified and geolocated a total of 512 historical landslide occurrences across the Urmia Lake drainage basin, leveraging GPS technology for precision. Thisarticle introduces a suite of hybrid machine learning predictive models, such as support-vector machine (SVM), random forest (RF), decision trees (DT), logistic regression (LR), fuzzy logic (FL), and the technique for order of preference by similarity to the ideal solution (TOPSIS). These models were strategically deployed to assess landslide susceptibility within the region. The outcomes of the landslide susceptibility assessment reveal that the main high susceptible zones for landslide occurrence are concentrated in the northwestern, northern, northeastern, and some southern and southeastern areas of the region. Moreover, when considering the implementation of predictions using different algorithms, it became evident that SVM exhibited superior performance regardingboth accuracy (0.89) and precision (0.89), followed by RF, with and accuracy of 0.83 and a precision of 0.83. However, it is noteworthy that TOPSIS yielded the lowest accuracy value among the algorithms assessed.","['support-vector machine (SVM)', 'random forest (RF)', 'decision trees (DT)', 'logistic regression (LR)']","The hydrological system of the Lake Urmia basin is complex, receiving water from 13 perennial rivers, numerous small springs, and direct precipitation, with about half of the inflow coming from the Zarrineh and Simineh Rivers. Lake Urmia has no natural outlet, and water loss occurs solely through evaporation, making the understanding of factors influencing the basin’s stability critical. This study focuses on the identification and spatial distribution of historical landslides within the basin, which are significant for assessing regional geological hazards. The primary objective of the study is to assess landslide susceptibility across the Urmia Lake drainage basin by precisely identifying and geolocating historical landslide occurrences, thereby determining the main zones prone to landslides in the region.","The hydrological system of the Lake Urmia basin is complex, receiving water from 13 perennial rivers, numerous small springs, and direct precipitation, with about half of the inflow coming from the Zarrineh and Simineh Rivers. Lake Urmia has no natural outlet, and water loss occurs solely through evaporation, making the understanding of factors influencing the basin's stability critical. This study focuses on the identification and spatial distribution of historical landslides within the basin, which are significant for assessing regional geological hazards. The primary objective of the study is to assess landslide susceptibility across the Urmia Lake drainage basin by precisely identifying and geolocating historical landslide occurrences, thereby determining the main zones prone to landslides in the region.",True
Engineering,Comparison of Random Forest and XGBoost Classifiers Using Integrated Optical and SAR Features for Mapping Urban Impervious Surface,"The integration of optical and SAR datasets through ensemble machine learning models shows promising results in urban remote sensing applications. The integration of multi-sensor datasets enhances the accuracy of information extraction. This research presents a comparison of two ensemble machine learning classifiers (random forest and extreme gradient boost (XGBoost)) classifiers using an integration of optical and SAR features and simple layer stacking (SLS) techniques. Therefore, Sentinel-1 (SAR) and Landsat 8 (optical) datasets were used with SAR textures and enhanced modified indices to extract features for the year 2023. The classification process utilized two machine learning algorithms, random forest and XGBoost, for urban impervious surface extraction. The study focused on three significant East Asian cities with diverse urban dynamics: Jakarta, Manila, and Seoul. This research proposed a novel index called the Normalized Blue Water Index (NBWI), which distinguishes water from other features and was utilized as an optical feature. Results showed an overall accuracy of 81% for UIS classification using XGBoost and 77% with RF while classifying land use land cover into four major classes (water, vegetation, bare soil, and urban impervious). However, the proposed framework with the XGBoost classifier outperformed the RF algorithm and Dynamic World (DW) data product and comparatively showed higher classification accuracy. Still, all three results show poor separability with bare soil class compared to ground truth data. XGBoost outperformed random forest and Dynamic World in classification accuracy, highlighting its potential use in urban remote sensing applications.","['ensemble machine learning models', 'random forest', 'extreme gradient boost (XGBoost)']","The research addresses the challenge of accurately extracting urban impervious surfaces by integrating optical and synthetic aperture radar (SAR) datasets, which enhances the precision of information retrieval in urban remote sensing. The study is motivated by the need to improve land use and land cover classification in rapidly changing urban environments, particularly in diverse East Asian cities such as Jakarta, Manila, and Seoul. The primary objective of the study is to compare the effectiveness of different classification approaches using combined optical and SAR features for urban impervious surface extraction. Additionally, the research aims to introduce and utilize a novel index, the Normalized Blue Water Index (NBWI), to better distinguish water bodies from other land cover types and to evaluate classification accuracy across major land cover classes including water, vegetation, bare soil, and urban impervious surfaces.","The research addresses the challenge of accurately extracting urban impervious surfaces by integrating optical and synthetic aperture radar (SAR) datasets, which enhances the precision of information retrieval in urban remote sensing. The study is motivated by the need to improve land use and land cover classification in rapidly changing urban environments, particularly in diverse East Asian cities such as Jakarta, Manila, and Seoul. The primary objective of the study is to compare the effectiveness of different analytical approaches using combined optical and SAR features for urban impervious surface extraction. Additionally, the research aims to introduce and utilize a novel index, the Normalized Blue Water Index (NBWI), to better distinguish water bodies from other land cover types and to evaluate classification accuracy across major land cover classes including water, vegetation, bare soil, and urban impervious surfaces.",True
Engineering,A vehicular network based intelligent transport system for smart cities using machine learning algorithms,"Abstract Smart cities and the Internet of Things have enabled the integration of communicating devices for efficient decision-making. Notably, traffic congestion is one major problem faced by daily commuters in urban cities. In developed countries, specialized sensors are deployed to gather traffic information to predict traffic patterns. Any traffic updates are shared with the commuters via the Internet. Such solutions become impracticable when physical infrastructure and Internet connectivity are either non-existent or very limited. In case of developing countries, no roadside units are available and Internet connectivity is still an issue in remote areas. Internet traffic analysis is a thriving field of study due to the myriad ways in which it may be put to practical use. In the intelligent Internet-of-Vehicles (IOVs), traffic congestion can be predicted and identified using cutting-edge technologies. Using tree-based decision-tree, random-forest, extra-tree, and XGBoost machine learning (ML) strategies, this research proposes an intelligent-transport-system for the IOVs-based vehicular network traffic in a smart city set-up. The suggested system uses ensemble learning and averages the selection of crucial features to give high detection accuracy at minimal computational costs, as demonstrated by the simulation results. For IOV-based vehicular network traffic, the tree-based ML approaches with feature-selection (FS) outperformed those without FS. When contrasted to the lowest KNN accuracy of 96.6% and the highest SVM accuracy of 98.01%, the Stacking approach demonstrates superior accuracy as 99.05%.","['decision-tree', 'random-forest', 'extra-tree', 'XGBoost', 'ensemble learning', 'feature-selection (FS)', 'KNN', 'SVM', 'Stacking']","The study addresses the significant challenge of traffic congestion faced by daily commuters in urban cities, particularly highlighting the limitations in developing countries where physical infrastructure and Internet connectivity are inadequate or absent. It emphasizes the impracticality of existing traffic information gathering methods that rely on specialized sensors and Internet connectivity, which are often unavailable in remote or underdeveloped areas. The primary aim of the research is to develop an intelligent transport solution tailored for vehicular network traffic within a smart city context, focusing on improving the detection and prediction of traffic congestion. This objective seeks to enhance traffic management by utilizing approaches that effectively identify crucial traffic features to achieve high accuracy in congestion detection while minimizing resource requirements.","The study addresses the significant challenge of traffic congestion faced by daily commuters in urban cities, particularly highlighting the limitations in developing countries where physical infrastructure and Internet connectivity are inadequate or absent. It emphasizes the impracticality of existing traffic information gathering methods that rely on specialized sensors and Internet connectivity, which are often unavailable in remote or underdeveloped areas. The primary aim of the research is to develop a transport solution tailored for vehicular network traffic within a city context, focusing on improving the detection and analysis of traffic congestion. This objective seeks to enhance traffic management by utilizing approaches that effectively identify crucial traffic features to achieve high accuracy in congestion detection while minimizing resource requirements.",True
Engineering,Conventional to Deep Ensemble Methods for Hyperspectral Image Classification: A Comprehensive Survey,"Hyperspectral image classification has become a hot research topic. HSI has been widely used in a wide range of real-world application areas due to the in-depth spectral information stored within each pixel. Noticeably, the detailed features - i.e., a nonlinear correlation between the obtained spectral data and the correlating HSI data object, generate efficient classification results that are complex for traditional techniques. Deep Learning (DL) has recently been validated as an influential feature extractor that efficiently identifies the nonlinear issues that have arisen in various computer vision challenges. This motivates using DL for Hyperspectral Image Classification (HSIC), which shows promising results. This survey provides a brief description of DL for HSIC and compares cutting-edge methodologies in the field. We will first summarize the key challenges for HSIC, and then we will discuss the superiority of DL and DL-ensemble in addressing these issues. In this article, we divide the state-of-the-art DL methodologies and DL with ensemble into spectral features, spatial features, and combined spatial-spectral features in order to comprehensively and critically evaluate the progress (future research directions as well) of such methodologies for HSIC. Furthermore, we will take into account that DL involves a substantial percentage of labeled training images, whereas obtaining such a number for HSI is time and cost-consuming. As a result, this survey describes some methodologies for improving the classification performance of DL techniques, which can serve as future recommendations.",['Deep Learning (DL)'],"The research idea focuses on the challenge of classifying hyperspectral images, which contain detailed spectral information within each pixel that is crucial for various real-world applications. Traditional techniques struggle to effectively handle the complex nonlinear relationships present in hyperspectral data, making accurate classification difficult. The study addresses the need to improve classification performance by overcoming these challenges inherent in hyperspectral image analysis. The primary objective of the study is to review and critically evaluate recent advancements in methodologies for hyperspectral image classification, highlighting key challenges and progress in extracting spectral, spatial, and combined spatial-spectral features. Additionally, the study aims to discuss approaches that enhance classification accuracy while considering the practical limitations related to the availability of labeled hyperspectral images.","The research idea focuses on the challenge of classifying hyperspectral images, which contain detailed spectral information within each pixel that is crucial for various real-world applications. Conventional approaches struggle to effectively handle the complex nonlinear relationships present in hyperspectral data, making accurate classification difficult. The study addresses the need to improve classification performance by overcoming these challenges inherent in hyperspectral image analysis. The primary objective of the study is to review and critically evaluate recent advancements in methodologies for hyperspectral image classification, highlighting key challenges and progress in extracting spectral, spatial, and combined spatial-spectral features. Additionally, the study aims to discuss approaches that enhance classification accuracy while considering the practical limitations related to the availability of labeled hyperspectral images.",True
Engineering,Data-driven evolution of water quality models: An in-depth investigation of innovative outlier detection approaches-A case study of Irish Water Quality Index (IEWQI) model,"Recently, there has been a significant advancement in the water quality index (WQI) models utilizing data-driven approaches, especially those integrating machine learning and artificial intelligence (ML/AI) technology. Although, several recent studies have revealed that the data-driven model has produced inconsistent results due to the data outliers, which significantly impact model reliability and accuracy. The present study was carried out to assess the impact of data outliers on a recently developed Irish Water Quality Index (IEWQI) model, which relies on data-driven techniques. To the author's best knowledge, there has been no systematic framework for evaluating the influence of data outliers on such models. For the purposes of assessing the outlier impact of the data outliers on the water quality (WQ) model, this was the first initiative in research to introduce a comprehensive approach that combines machine learning with advanced statistical techniques. The proposed framework was implemented in Cork Harbour, Ireland, to evaluate the IEWQI model's sensitivity to outliers in input indicators to assess the water quality. In order to detect the data outlier, the study utilized two widely used ML techniques, including Isolation Forest (IF) and Kernel Density Estimation (KDE) within the dataset, for predicting WQ with and without these outliers. For validating the ML results, the study used five commonly used statistical measures. The performance metric (R2) indicates that the model performance improved slightly (R2 increased from 0.92 to 0.95) in predicting WQ after removing the data outlier from the input. But the IEWQI scores revealed that there were no statistically significant differences among the actual values, predictions with outliers, and predictions without outliers, with a 95% confidence interval at p < 0.05. The results of model uncertainty also revealed that the model contributed <1% uncertainty to the final assessment results for using both datasets (with and without outliers). In addition, all statistical measures indicated that the ML techniques provided reliable results that can be utilized for detecting outliers and their impacts on the IEWQI model. The findings of the research reveal that although the data outliers had no significant impact on the IEWQI model architecture, they had moderate impacts on the rating schemes' of the model. This finding indicated that detecting the data outliers could improve the accuracy of the IEWQI model in rating WQ as well as be helpful in mitigating the model eclipsing problem. In addition, the results of the research provide evidence of how the data outliers influenced the data-driven model in predicting WQ and reliability, particularly since the study confirmed that the IEWQI model's could be effective for accurately rating WQ despite the presence of the data outliers in the input. It could occur due to the spatio-temporal variability inherent in WQ indicators. However, the research assesses the influence of data input outliers on the IEWQI model and underscores important areas for future investigation. These areas include expanding temporal analysis using multi-year data, examining spatial outlier patterns, and evaluating detection methods. Moreover, it is essential to explore the real-world impacts of revised rating categories, involve stakeholders in outlier management, and fine-tune model parameters. Analysing model performance across varying temporal and spatial resolutions and incorporating additional environmental data can significantly enhance the accuracy of WQ assessment. Consequently, this study offers valuable insights to strengthen the IEWQI model's robustness and provides avenues for enhancing its utility in broader WQ assessment applications. Moreover, the study successfully adopted the framework for evaluating how data input outliers affect the data-driven model, such as the IEWQI model. The current study has been carried out in Cork Harbour for only a single year of WQ data. The framework should be tested across various domains for evaluating the response of the IEWQI model's in terms of the spatio-temporal resolution of the domain. Nevertheless, the study recommended that future research should be conducted to adjust or revise the IEWQI model's rating schemes and investigate the practical effects of data outliers on updated rating categories. However, the study provides potential recommendations for enhancing the IEWQI model's adaptability and reveals its effectiveness in expanding its applicability in more general WQ assessment scenarios.",['Isolation Forest (IF)'],"The research idea centers on addressing the challenges posed by data outliers in water quality assessment models, specifically focusing on the recently developed Irish Water Quality Index (IEWQI). Although advancements have been made in water quality index models, inconsistent results caused by data outliers have raised concerns about the reliability and accuracy of these assessments. This study highlights the need for a systematic evaluation of how such outliers influence the performance and rating schemes of water quality models. The motivation is to improve the robustness and accuracy of water quality evaluations despite the inherent variability in water quality indicators.

The primary objective of the study is to assess the impact of data outliers on the IEWQI model’s ability to rate water quality accurately. The research aims to evaluate the sensitivity of the IEWQI model to outliers in input indicators and determine whether removing these outliers significantly affects the model’s predictions and rating outcomes. Additionally, the study seeks to provide insights into improving the model’s rating schemes and to offer recommendations for enhancing the model’s adaptability and applicability in broader water quality assessment contexts.","The research idea centers on addressing the challenges posed by data outliers in water quality assessment models, specifically focusing on the recently developed Irish Water Quality Index (IEWQI). Although advancements have been made in water quality index models, inconsistent results caused by data outliers have raised concerns about the reliability and accuracy of these assessments. This study highlights the need for a systematic evaluation of how such outliers influence the performance and rating schemes of water quality models. The motivation is to improve the robustness and accuracy of water quality evaluations despite the inherent variability in water quality indicators.

The primary objective of the study is to assess the impact of data outliers on the IEWQI model's ability to rate water quality accurately. The research aims to evaluate the sensitivity of the IEWQI model to outliers in input indicators and determine whether removing these outliers significantly affects the model's outcomes and rating results. Additionally, the study seeks to provide insights into improving the model's rating schemes and to offer recommendations for enhancing the model's adaptability and applicability in broader water quality assessment contexts.",True
Engineering,Machine learning for multi-dimensional performance optimization and predictive modelling of nanopowder-mixed electric discharge machining (EDM),"Abstract Aluminium 6061 (Al6061) is a widely used material for various industrial applications due to low density and high strength. Nevertheless, the conventional machining operations are not the best choice for the machining purposes. Therefore, amongst all the non-conventional machining operations, electric discharge machining (EDM) is opted to carry out the research due to its wide ability to cut the materials. But the high electrode wear rate (EWR) and high dimensional inaccuracy or overcut (OC) of EDM limit its usage. Consequently, nanopowder is added to the dielectric medium to address the abovementioned issues. Nanopowder mixed EDM (NPMEDM) process is a complex process in terms of performance predictability for different materials. Similarly, the interactions between the process parameters such as peak current ( I p ), spark voltage ( S v ), pulse on time ( P on ) and powder concentration ( C p ) in dielectric enhance the parametric sensitivity. In addition, the cryogenic treatment (CT) of electrodes makes the process complex limiting conventional simulation approaches for modelling inter-relationships. An alternative approach requires experimental exploration and systematic investigation to model EWR and overcutting problems of EDM. Thus, artificial neural networks (ANNs) are used for predictive modelling of the process which are integrated with multi-objective genetic algorithm (MOGA) for parametric optimization. The approach uses experimental data based on response surface methodology (RSM) design of experiments. Moreover, the process physics is thoroughly discussed with parametric effect analysis supported with evidence of microscopic images, scanning electron microscopy (SEM) and 3D surface topographic images. Based on multi-dimensional optimization results, the NT brass electrode showed an improvement of 65.02% in EWR and 59.73% in OC using deionized water. However, CT brass electrode showed 78.41% reduction in EWR and 67.79% improved dimensional accuracy in deionized water. In addition to that, CT brass electrode gave 27.69% less EWR and 81.40% improved OC in deionized water compared to kerosene oil.","['artificial neural networks (ANNs)', 'multi-objective genetic algorithm (MOGA)']","The research addresses the challenges associated with machining Aluminium 6061, a material valued for its low density and high strength, where conventional machining methods are inadequate. Specifically, electric discharge machining (EDM), despite its capability to cut various materials, suffers from high electrode wear rate (EWR) and dimensional inaccuracies such as overcut (OC), which limit its practical application. To overcome these issues, the study explores modifications in the EDM process, including the addition of nanopowder to the dielectric medium and the use of cryogenic treatment on electrodes, aiming to improve machining performance and accuracy.

The primary objective of the study is to systematically investigate and model the effects of process parameters on electrode wear rate and overcut in EDM of Aluminium 6061. The research aims to optimize these parameters to reduce electrode wear and enhance dimensional accuracy, thereby improving the overall efficiency and precision of the EDM process for this material. Additionally, the study seeks to provide a thorough understanding of the process physics through experimental exploration and parametric effect analysis supported by microscopic and surface imaging techniques.","The research addresses the challenges associated with machining Aluminium 6061, a material valued for its low density and high strength, where conventional machining methods are inadequate. Specifically, electric discharge machining (EDM), despite its capability to cut various materials, suffers from high electrode wear rate (EWR) and dimensional inaccuracies such as overcut (OC), which limit its practical application. To overcome these issues, the study explores modifications in the EDM process, including the addition of nanopowder to the dielectric medium and the use of cryogenic treatment on electrodes, aiming to improve machining performance and accuracy.

The primary objective of the study is to systematically investigate the effects of process parameters on electrode wear rate and overcut in EDM of Aluminium 6061. The research aims to optimize these parameters to reduce electrode wear and enhance dimensional accuracy, thereby improving the overall efficiency and precision of the EDM process for this material. Additionally, the study seeks to provide a thorough understanding of the process physics through experimental exploration and parametric effect analysis supported by microscopic and surface imaging techniques.",True
Engineering,Predicting the mechanical properties of plastic concrete: An optimization method by using genetic programming and ensemble learners,"This study presents a comparative analysis of individual and ensemble learning algorithms (ELAs) to predict the compressive strength (CS) and flexural strength (FS) of plastic concrete. Multilayer perceptron neuron network (MLPNN), Support vector machine (SVM), random forest (RF), and decision tree (DT) were used as base learners, which were then combined with bagging and Adaboost methods to improve the predictive performance. In addition, gene expression programming (GEP) was used to develop computational equations that can be used to predict the CS and FS of plastic concrete. An extensive database containing 357 and 125 data points was obtained from the literature, and the eight most impactful ingredients were used in the model's development. The accuracy of all models was assessed using several statistical measures, including an error matrix, Akaike information criterion (AIC), K-fold cross-validation, and other external validation equations. Furthermore, sensitivity and SHAP analysis were performed to evaluate input variables' relative significance and impact on the anticipated CS and FS. Based on statistical measures and other validation criteria, GEP outpaces all other individual models, whereas, in ELAs, the SVR ensemble with Adaboost and RF modified with the Bagging technique demonstrated superior performance. SHapley Additive exPlanations (SHAP) and sensitivity analysis reveal that plastic, cement, water, and the age of the specimens have the highest influence, while superplasticizer has the lowest impact, which is consistent with experimental studies. Moreover, GUI and GEP-based simple mathematical correlation can enhance the practical scope of this study and be an effective tool for the pre-mix design of plastic concrete.","['Multilayer perceptron neuron network (MLPNN)', 'Support vector machine (SVM)', 'random forest (RF)', 'decision tree (DT)', 'bagging', 'Adaboost', 'gene expression programming (GEP)', 'SVR ensemble with Adaboost']","The research idea addresses the challenge of accurately predicting the compressive strength and flexural strength of plastic concrete, which are critical properties for ensuring the material's performance and durability in construction applications. Understanding the influence of various ingredients on these strength parameters is essential for optimizing the mix design and improving the quality of plastic concrete. The study aims to enhance the ability to estimate these mechanical properties reliably using available experimental data.

The primary objective of the study is to develop and compare different approaches for predicting the compressive strength and flexural strength of plastic concrete based on key ingredient inputs. The research seeks to identify the most effective predictive approach and to establish simple mathematical correlations that can be practically applied for pre-mix design. Additionally, the study aims to evaluate the relative significance of input variables affecting the strength characteristics to support better material formulation decisions.","The research idea addresses the challenge of accurately predicting the compressive strength and flexural strength of plastic concrete, which are critical properties for ensuring the material's performance and durability in construction applications. Understanding the influence of various ingredients on these strength parameters is essential for optimizing the mix design and improving the quality of plastic concrete. The study aims to enhance the ability to estimate these mechanical properties reliably using available experimental data.

The primary objective of the study is to develop and compare different approaches for predicting the compressive strength and flexural strength of plastic concrete based on key ingredient inputs. The research seeks to identify the most effective analytical method and to establish simple mathematical correlations that can be practically applied for pre-mix design. Additionally, the study aims to evaluate the relative significance of input variables affecting the strength characteristics to support better material formulation decisions.",True
Engineering,Machine learning-assisted in-situ adaptive strategies for the control of defects and anomalies in metal additive manufacturing,"In metal additive manufacturing (AM), the material microstructure and part geometry are formed incrementally. Consequently, the resulting part could be defect- and anomaly-free if sufficient care is taken to deposit each layer under optimal process conditions. Conventional closed-loop control (CLC) engineering solutions which sought to achieve this were deterministic and rule-based, thus resulting in limited success in the stochastic environment experienced in the highly dynamic AM process. On the other hand, emerging machine learning (ML) based strategies are better suited to providing the robustness, scope, flexibility, and scalability required for process control in an uncertain environment. Offline ML models that help optimise AM process parameters before a build begins and online ML models that efficiently processed in-situ sensory data to detect and diagnose flaws in real-time (or near-real-time) have been developed. However, ML models that enable a process to take evasive or corrective actions in relation to flaws via on the fly decision-making are only emerging. These models must possess prognostic capabilities to provide context-sensitive recommendations for in-situ process control based on real-time diagnostics. In this article, we pinpoint the shortcomings in traditional CLC strategies, and provide a framework for defect and anomaly control through ML-assisted CLC in AM. We discuss flaws in terms of their causes, in-situ detectability, and controllability, and examine their management under three scenarios: avoidance, mitigation, and repair. Then, we summarise the research into ML models developed for offline optimisation and in-situ diagnosis before initiating a detailed conversation on the implementation of ML-assisted in-situ process control. We found that researchers favoured reinforcement learning approaches or inverse ML models for making rapid, situation-aware control decisions. We also observed that, to-date, the defects addressed were those that may be quantified relatively easily autonomously, and that mitigation (rather than avoidance or repair) was the aim of ML-assisted in-situ control strategies. Additionally, we highlight the various technologies that must seamlessly combine to advance the field of autonomous in-situ control so that it becomes a reality in industrial settings. Finally, we raise awareness of seldom discussed, yet highly pertinent, topics relevant to adaptive control. Our work closes a significant gap in the current AM literature by broaching wide-ranging discussions on matters relevant to in-situ adaptive control in AM.","['online ML models', 'reinforcement learning approaches']","The research idea centers on the challenge of achieving defect- and anomaly-free parts in metal additive manufacturing by carefully controlling the deposition of each layer under optimal process conditions. Traditional closed-loop control methods have shown limited success due to the highly dynamic and stochastic nature of the additive manufacturing process. There is a need for more robust and flexible approaches to manage defects and anomalies during the build process to improve part quality and reliability. The study addresses the shortcomings of existing control strategies and explores ways to enhance in-situ process control for better defect management.

The primary objective of the study is to provide a comprehensive framework for defect and anomaly control in metal additive manufacturing through advanced closed-loop control strategies that enable avoidance, mitigation, and repair of flaws during the build. The research aims to examine the causes, detectability, and controllability of defects and to discuss the implementation of adaptive in-situ process control that can respond effectively to real-time diagnostics. Additionally, the study seeks to highlight the integration of various technologies necessary to realize autonomous in-situ control in industrial settings and to raise awareness of important considerations relevant to adaptive control in additive manufacturing.","The research idea centers on the challenge of achieving defect- and anomaly-free parts in metal additive manufacturing by carefully controlling the deposition of each layer under optimal process conditions. Traditional closed-loop control methods have shown limited success due to the highly dynamic and stochastic nature of the additive manufacturing process. There is a need for more robust and flexible approaches to manage defects and anomalies during the build process to improve part quality and reliability. The study addresses the shortcomings of existing control strategies and explores ways to enhance in-situ process control for better defect management.

The primary objective of the study is to provide a comprehensive framework for defect and anomaly control in metal additive manufacturing through improved closed-loop control strategies that enable avoidance, mitigation, and repair of flaws during the build. The research aims to examine the causes, detectability, and controllability of defects and to discuss the implementation of responsive in-situ process control that can adjust effectively to real-time diagnostics. Additionally, the study seeks to highlight the integration of various technologies necessary to realize effective in-situ control in industrial settings and to raise awareness of important considerations relevant to adaptive control in additive manufacturing.",True
Engineering,Performance assessment of machine learning algorithms for mapping of land use/land cover using remote sensing data,"The rapid increase in population accelerates the rate of change of Land use/Land cover (LULC) in various parts of the world. This phenomenon caused a huge strain for natural resources. Hence, continues monitoring of LULC changes gained a significant importance for management of natural resources and assessing the climate change impacts. Recently, application of machine learning algorithms on RS (remote sensing) data for rapid and accurate mapping of LULC gained significant importance due to growing need of LULC estimation for ecosystem services, natural resource management and environmental management. Hence, it is crucial to access and compare the performance of different machine learning classifiers for accurate mapping of LULC. The primary objective of this study was to compare the performance of CART (Classification and Regression Tree), RF (Random Forest) and SVM (Support Vector Machine) for LULC estimation by processing RS data on Google Earth Engine (GEE). In total four classes of LULC (Water Bodies, Vegetation Cover, Urban Land and Barren Land) for city of Lahore were extracted using satellite images from Landsat-7, Landsat-8 and Landsat-9 for years 2008, 2015 and 2022, respectively. According to results, RF is the best performing classifier with maximum overall accuracy of 95.2% and highest Kappa coefficient value of 0.87, SVM achieved maximum accuracy of 89.8% with highest Kappa of 0.84 and CART showed maximum overall accuracy of 89.7% with Kappa value of 0.79. Results from this study can give assistance for decision makers, planners and RS experts to choose a suitable machine learning algorithm for LULC classification in an unplanned urbanized city like Lahore.","['Classification and Regression Tree (CART)', 'Random Forest (RF)', 'Support Vector Machine (SVM)']","The rapid increase in population has accelerated the rate of Land Use/Land Cover (LULC) changes in various parts of the world, placing significant strain on natural resources. Continuous monitoring of these LULC changes is essential for effective management of natural resources and for assessing the impacts of climate change. The primary objective of this study was to compare the performance of different classification approaches for accurate mapping of LULC in the city of Lahore. Specifically, the study aimed to evaluate and contrast the effectiveness of three classification methods in extracting four LULC classes—Water Bodies, Vegetation Cover, Urban Land, and Barren Land—using satellite imagery from multiple years to support better decision-making and planning in an unplanned urbanized environment.","The rapid increase in population has accelerated the rate of Land Use/Land Cover (LULC) changes in various parts of the world, placing significant strain on natural resources. Continuous monitoring of these LULC changes is essential for effective management of natural resources and for assessing the impacts of climate change. The primary objective of this study was to compare the performance of different categorization approaches for accurate mapping of LULC in the city of Lahore. Specifically, the study aimed to evaluate and contrast the effectiveness of three analytical methods in extracting four LULC classes—Water Bodies, Vegetation Cover, Urban Land, and Barren Land—using satellite imagery from multiple years to support better decision-making and planning in an unplanned urbanized environment.",True
Engineering,Pedestrian 3D Shape Understanding for Person Re-Identification via Multi-View Learning,"Recent development in computing power has resulted in performance improvements on holistic(none-occluded) person Re-Identification (ReID) tasks. Nevertheless, the precision of the recent research will diminish when a pedestrian is obstructed by obstacles. Within the realm of 2D space, the loss of information from obstructed objects continues to pose significant challenges in the context of person ReID. Person is a 3D non-grid object, and thus semantic representation learning in only 2D space limits the understanding of occluded person. In the present work, we propose a network based on 3D multi-view learning, allowing it to acquire geometric and shape details of an occluded pedestrian from 3D space. Simultaneously, it capitalizes on advancements in 2D-based networks to extract semantic representations from 3D multi-views. Specifically, the surface random selection strategy is proposed to convert images of 2D RGB into 3D multi-views. Using this strategy, we build four extensive 3D multi-view data collections for person ReID. After that, Pedestrian 3D Shape Understanding for Person Re-Identification via Multi-View Learning(MV-3DSReID), is proposed for identifying the person by learning person geometry and structure representation from the groups of multi-view images. In comparison to alternative data formats (e.g., 2D RGB, 3D point cloud), multi-view images complement each other's detailed features of the 3D object by adjusting rendering viewpoints, thus facilitating a more comprehensive understanding of the person for both holistic and occluded ReID situations. Experiments on occluded and holistic ReID tasks demonstrate performance levels comparable to state-of-the-art methods, validating the effectiveness of our proposed approach in tackling challenges related to occlusion. The code is available at https://github.com/hangjiaqi1/MV-TransReID.",['3D multi-view learning'],"The research addresses the challenge of accurately identifying pedestrians when they are partially obstructed by obstacles, which leads to a loss of information in traditional two-dimensional representations. Since a person is a three-dimensional object, relying solely on 2D space limits the ability to understand and recognize occluded individuals effectively. The study focuses on overcoming the difficulties posed by occlusion in person identification by enhancing the geometric and shape understanding of pedestrians in 3D space. The primary objective of the study is to improve person identification by learning the geometry and structural representation of pedestrians through multiple views in three-dimensional space. This approach aims to provide a more comprehensive understanding of both fully visible and occluded pedestrians by integrating detailed features from various viewpoints, thereby addressing the limitations of existing 2D-based methods in occluded scenarios.","The research addresses the challenge of accurately identifying pedestrians when they are partially obstructed by obstacles, which leads to a loss of information in traditional two-dimensional representations. Since a person is a three-dimensional object, relying solely on 2D space limits the ability to understand and recognize occluded individuals effectively. The study focuses on overcoming the difficulties posed by occlusion in person identification by enhancing the geometric and shape understanding of pedestrians in 3D space. The primary objective of the study is to improve person identification by developing a comprehensive geometric and structural representation of pedestrians through multiple views in three-dimensional space. This approach aims to provide a more thorough understanding of both fully visible and occluded pedestrians by integrating detailed features from various viewpoints, thereby addressing the limitations of existing 2D-based methods in occluded scenarios.",True
Engineering,A machine learning-based framework for clustering residential electricity load profiles to enhance demand response programs,"Load shapes derived from smart meter data are frequently employed to analyze daily energy consumption patterns, particularly in the context of applications like Demand Response (DR). Nevertheless, one of the most important challenges to this endeavor lies in identifying the most suitable consumer clusters with similar consumption behaviors. In this paper, we present a novel machine learning based framework in order to achieve optimal load profiling through a real case study, utilizing data from almost 5000 households in London. Four widely used clustering algorithms are applied specifically K-means, K-medoids, Hierarchical Agglomerative Clustering and Density-based Spatial Clustering. An empirical analysis as well as multiple evaluation metrics are leveraged to assess those algorithms. Following that, we redefine the problem as a probabilistic classification one, with the classifier emulating the behavior of a clustering algorithm, leveraging Explainable AI (xAI) to enhance the interpretability of our solution. According to the clustering algorithm analysis the optimal number of clusters for this case is seven. Despite that, our methodology shows that two of the clusters, almost 10% of the dataset, exhibit significant internal dissimilarity. As a result, these clusters have been excluded from consideration for DR programs. The scalability and versatility of our solution makes it an ideal choice for power utility companies aiming to segment their users for creating more targeted DR programs.","['K-means', 'K-medoids', 'Hierarchical Agglomerative Clustering', 'Density-based Spatial Clustering', 'probabilistic classification', 'Explainable AI (xAI)']","The study addresses the challenge of accurately identifying consumer groups with similar daily energy consumption patterns to improve the effectiveness of Demand Response (DR) programs. This problem is critical because appropriate clustering of consumers enables better understanding and management of energy usage behaviors. The primary aim of the research is to achieve optimal load profiling by determining the most suitable number and composition of consumer clusters based on real household energy consumption data. The study seeks to refine the identification of consumer groups to exclude those with significant internal dissimilarity, thereby enhancing the targeting of DR initiatives for power utility companies.","The study addresses the challenge of accurately identifying consumer groups with similar daily energy consumption patterns to improve the effectiveness of Demand Response (DR) programs. This problem is critical because appropriate grouping of consumers enables better understanding and management of energy usage behaviors. The primary aim of the research is to achieve optimal load profiling by determining the most suitable number and composition of consumer segments based on real household energy consumption data. The study seeks to refine the identification of consumer groups to exclude those with significant internal dissimilarity, thereby enhancing the targeting of DR initiatives for power utility companies.",True
Engineering,The diagnostic and triage accuracy of the GPT-3 artificial intelligence model: an observational study,"BackgroundArtificial intelligence (AI) applications in health care have been effective in many areas of medicine, but they are often trained for a single task using labelled data, making deployment and generalisability challenging. How well a general-purpose AI language model performs diagnosis and triage relative to physicians and laypeople is not well understood.MethodsWe compared the predictive accuracy of Generative Pre-trained Transformer 3 (GPT-3)'s diagnostic and triage ability for 48 validated synthetic case vignettes (<50 words; sixth-grade reading level or below) of both common (eg, viral illness) and severe (eg, heart attack) conditions to a nationally representative sample of 5000 lay people from the USA who could use the internet to find the correct options and 21 practising physicians at Harvard Medical School. There were 12 vignettes for each of four triage categories: emergent, within one day, within 1 week, and self-care. The correct diagnosis and triage category (ie, ground truth) for each vignette was determined by two general internists at Harvard Medical School. For each vignette, human respondents and GPT-3 were prompted to list diagnoses in order of likelihood, and the vignette was marked as correct if the ground-truth diagnosis was in the top three of the listed diagnoses. For triage accuracy, we examined whether the human respondents' and GPT-3's selected triage was exactly correct according to the four triage categories, or matched a dichotomised triage variable (emergent or within 1 day vs within 1 week or self-care). We estimated GPT-3's diagnostic and triage confidence on a given vignette using a modified bootstrap resampling procedure, and examined how well calibrated GPT-3's confidence was by computing calibration curves and Brier scores. We also performed subgroup analysis by case acuity, and an error analysis for triage advice to characterise how its advice might affect patients using this tool to decide if they should seek medical care immediately.FindingsAmong all cases, GPT-3 replied with the correct diagnosis in its top three for 88% (42/48, 95% CI 75–94) of cases, compared with 54% (2700/5000, 53–55) for lay individuals (p<0.0001) and 96% (637/666, 94–97) for physicians (p=0·012). GPT-3 triaged 70% correct (34/48, 57–82) versus 74% (3706/5000, 73–75; p=0.60) for lay individuals and 91% (608/666, 89–93%; p<0.0001) for physicians. As measured by the Brier score, GPT-3 confidence in its top prediction was reasonably well calibrated for diagnosis (Brier score=0·18) and triage (Brier score=0·22). We observed an inverse relationship between case acuity and GPT-3 accuracy (p<0·0001) with a fitted trend line of –8·33% decrease in accuracy for every level of increase in case acuity. For triage error analysis, GPT-3 deprioritised truly emergent cases in seven instances.InterpretationA general-purpose AI language model without any content-specific training could perform diagnosis at levels close to, but below, physicians and better than lay individuals. We found that GPT-3's performance was inferior to physicians for triage, sometimes by a large margin, and its performance was closer to that of lay individuals. Although the diagnostic performance of GPT-3 was comparable to physicians, it was significantly better than a typical person using a search engine.FundingThe National Heart, Lung, and Blood Institute.",['Generative Pre-trained Transformer 3 (GPT-3)'],"The research addresses the challenge of accurately diagnosing and triaging medical conditions, highlighting the difficulty in achieving reliable performance comparable to physicians and the general public. It focuses on understanding how well a general-purpose approach can perform these critical healthcare tasks across a range of common and severe conditions. The study aims to evaluate the diagnostic and triage accuracy of a general-purpose language-based tool relative to practicing physicians and laypeople, using validated clinical case scenarios. Specifically, it seeks to determine the tool’s ability to correctly identify diagnoses and appropriate triage categories, assess its confidence calibration, and analyze its performance variations with case severity.","The research addresses the challenge of accurately diagnosing and triaging medical conditions, highlighting the difficulty in achieving reliable performance comparable to physicians and the general public. It focuses on understanding how well a general-purpose approach can perform these critical healthcare tasks across a range of common and severe conditions. The study aims to evaluate the diagnostic and triage accuracy of a general-purpose language-based assessment method relative to practicing physicians and laypeople, using validated clinical case scenarios. Specifically, it seeks to determine the method's ability to correctly identify diagnoses and appropriate triage categories, assess its reliability across different confidence levels, and analyze its performance variations with case severity.",True
Engineering,Internet of things sensors and support vector machine integrated intelligent irrigation system for agriculture industry,"Abstract Because there is more demand for freshwater around the world and the world’s population is growing at the same time, there is a severe lack of freshwater resources in the central part of the planet. The world’s current population of 7.2 billion people is expected to grow to over 9 billion by the year 2050. The vast majority of freshwater is used for things like cooking, cleaning, and farming. Most industrialised countries are in desperate need of smart irrigation systems, which are now a must-have because of how quickly technology is improving. In article presents IoT based Sensor integrated intelligent irrigation system for agriculture industry. IoT based humidity and soil sensors are used to collect soil related data. This data is stored in a centralized cloud. Features are selected by CFS algorithm. This will help in discarding irrelevant data. Clustering of data is performed by K means algorithm. This will help in keeping similar data together. Then classification model is build using the SVM, Random Forest and Naïve Bayes algorithm. Model is trained, validated and tested using the acquired data. Historical soil and humidity related data is also used in training the model. K-means SVM hybrid classifier is achieving better results for classification, prediction of water demand and saving fresh water by intelligent irrigation. K-means SVM hybrid classifier has achieved accuracy rate of 98.5 percent. Specificity, recall and precision of K-means SVM hybrid classifier is also higher than random forest and naïve bayes classifier.","['CFS algorithm', 'K means algorithm', 'SVM', 'Random Forest', 'Naïve Bayes algorithm']","The research addresses the critical issue of freshwater scarcity driven by the increasing global population and the growing demand for water in essential activities such as cooking, cleaning, and farming. There is an urgent need for efficient irrigation solutions in industrialized countries to conserve freshwater resources amid these challenges. The primary objective of the study is to develop an intelligent irrigation approach that optimizes water usage in agriculture by utilizing soil and humidity information to better predict water demand. This aims to contribute to freshwater conservation by enabling more precise and effective irrigation practices.","The research addresses the critical issue of freshwater scarcity driven by the increasing global population and the growing demand for water in essential activities such as cooking, cleaning, and farming. There is an urgent need for efficient irrigation solutions in industrialized countries to conserve freshwater resources amid these challenges. The primary objective of the study is to develop an improved irrigation approach that optimizes water usage in agriculture by utilizing soil and humidity information to better determine water demand. This aims to contribute to freshwater conservation by enabling more precise and effective irrigation practices.",True
Engineering,RanMerFormer: Randomized vision transformer with token merging for brain tumor classification,"Brains are the control center of the nervous system in human bodies, and brain tumor is one of the most deadly diseases. Currently, magnetic resonance imaging (MRI) is the most effective way to brain tumors early detection in clinical diagnoses due to its superior imaging quality for soft tissues. Manual analysis of brain MRI is error-prone which depends on empirical experience and the fatigue state of the radiologists to a large extent. Computer-aided diagnosis (CAD) systems are becoming more and more impactful because they can provide accurate prediction results based on medical images with advanced techniques from computer vision. Therefore, a novel CAD method for brain tumor classification named RanMerFormer is presented in this paper. A pre-trained vision transformer is used as the backbone model. Then, a merging mechanism is proposed to remove the redundant tokens in the vision transformer, which improves computing efficiency substantially. Finally, a randomized vector functional-link serves as the head in the proposed RanMerFormer, which can be trained swiftly. All the simulation results are obtained from two public benchmark datasets, which reveal that the proposed RanMerFormer can achieve state-of-the-art performance for brain tumor classification. The trained RanMerFormer can be applied in real-world scenarios to assist in brain tumor diagnosis.","['pre-trained vision transformer', 'randomized vector functional-link']","The research addresses the critical challenge of early and accurate detection of brain tumors, which are among the most deadly diseases affecting the human nervous system. Magnetic resonance imaging (MRI) is currently the most effective clinical tool for identifying brain tumors due to its high-quality imaging of soft tissues, but manual interpretation of these images is prone to errors and heavily reliant on the experience and fatigue levels of radiologists. The primary objective of the study is to develop a novel approach for brain tumor classification that enhances diagnostic accuracy and efficiency. This approach aims to improve the process of analyzing brain MRI scans to support timely and reliable brain tumor diagnosis in clinical practice.","The research addresses the critical challenge of early and accurate detection of brain tumors, which are among the most deadly diseases affecting the human nervous system. Magnetic resonance imaging (MRI) is currently the most effective clinical tool for identifying brain tumors due to its high-quality imaging of soft tissues, but manual interpretation of these images is prone to errors and heavily reliant on the experience and fatigue levels of radiologists. The primary objective of the study is to develop a novel methodological framework for brain tumor classification that enhances diagnostic accuracy and efficiency. This approach aims to improve the process of analyzing brain MRI scans to support timely and reliable brain tumor diagnosis in clinical practice.",True
Engineering,Short-term power load forecasting based on AC-BiLSTM model,"The practice of ultra-short-term power load forecasting serves as a critical strategy for enabling rapid response and real-time dispatch in power systems. By improving the accuracy of load forecasting, both the safety of power systems and the efficiency of electricity usage can be significantly enhanced. Addressing the challenges posed by the non-linear and temporal characteristics of grid load data, this study introduces a novel ultra-short-term power load forecasting model, integrating Convolutional Neural Networks (CNN), Bidirectional Long Short-Term Memory networks (BiLSTM), and an Attention mechanism, referred to as the AC-BiLSTM model. This innovative approach harnesses the power of CNN and BiLSTM to extract spatio-temporal features of load data, while the Attention mechanism allocates optimal weights to the hidden states of the BiLSTM model, thereby amplifying crucial historical load sequence data and minimizing information loss. The final output of the model is then determined through a fully connected layer. To validate the efficacy of this approach, an empirical study was conducted using real load data from a specific region. The results, obtained from two contrasting experimental scenarios, demonstrate a significant enhancement in forecasting accuracy. This finding underscores the potential of the AC-BiLSTM model as a reliable tool for both strategic planning and maintaining operational stability in power systems.","['Convolutional Neural Networks (CNN)', 'Bidirectional Long Short-Term Memory networks (BiLSTM)', 'Attention mechanism']","The research addresses the critical need for ultra-short-term power load forecasting to enable rapid response and real-time dispatch in power systems. Improving the accuracy of load forecasting is essential for enhancing both the safety of power systems and the efficiency of electricity usage, particularly given the complex non-linear and temporal characteristics of grid load data. The primary objective of the study is to develop and validate a novel approach for ultra-short-term power load forecasting that effectively captures the spatio-temporal features of load data and emphasizes important historical load sequences. This aims to significantly improve forecasting accuracy, thereby supporting strategic planning and maintaining operational stability in power systems.","The research addresses the critical need for ultra-short-term power load forecasting to enable rapid response and real-time dispatch in power systems. Improving the accuracy of load forecasting is essential for enhancing both the safety of power systems and the efficiency of electricity usage, particularly given the complex non-linear and temporal characteristics of grid load data. The primary objective of the study is to develop and validate a new analytical method for ultra-short-term power load forecasting that effectively captures the spatio-temporal features of load data and emphasizes important historical load sequences. This aims to significantly improve forecasting accuracy, thereby supporting strategic planning and maintaining operational stability in power systems.",True
Engineering,Assessment of surrogate models for flood inundation: The physics-guided LSG model vs. state-of-the-art machine learning models,"Hydrodynamic models can accurately simulate flood inundation but are limited by their high computational demand that scales non-linearly with model complexity, resolution, and domain size. Therefore, it is often not feasible to use high-resolution hydrodynamic models for real-time flood predictions or when a large number of predictions are needed for probabilistic flood design. Computationally efficient surrogate models have been developed to address this issue. The recently developed Low-fidelity, Spatial analysis, and Gaussian Process Learning (LSG) model has shown strong performance in both computational efficiency and simulation accuracy. The LSG model is a physics-guided surrogate model that simulates flood inundation by first using an extremely coarse and simplified (i.e. low-fidelity) hydrodynamic model to provide an initial estimate of flood inundation. Then, the low-fidelity estimate is upskilled via Empirical Orthogonal Functions (EOF) analysis and Sparse Gaussian Process models to provide accurate high-resolution predictions. Despite the promising results achieved thus far, the LSG model has not been benchmarked against other surrogate models. Such a comparison is needed to fully understand the value of the LSG model and to provide guidance for future research efforts in flood inundation simulation. This study compares the LSG model to four state-of-the-art surrogate flood inundation models. The surrogate models are assessed for their ability to simulate the temporal and spatial evolution of flood inundation for events both within and beyond the range used for model training. The models are evaluated for three distinct case studies in Australia and the United Kingdom. The LSG model is found to be superior in accuracy for both flood extent and water depth, including when applied to flood events outside the range of training data used, while achieving high computational efficiency. In addition, the low-fidelity model is found to play a crucial role in achieving the overall superior performance of the LSG model.","['Gaussian Process Learning', 'Sparse Gaussian Process models']","The research addresses the challenge of using high-resolution hydrodynamic models for flood inundation simulation, which are limited by their high computational demand that increases non-linearly with model complexity, resolution, and domain size. This limitation makes it difficult to apply such models for real-time flood predictions or for generating numerous predictions needed in probabilistic flood design. The study focuses on evaluating alternative approaches that can provide accurate flood inundation predictions with greater computational efficiency. The primary objective of the study is to compare the recently developed Low-fidelity, Spatial analysis, and Gaussian Process Learning (LSG) model against four other surrogate flood inundation models to assess their ability to simulate the temporal and spatial evolution of flood inundation for various events. The evaluation is conducted through three distinct case studies in Australia and the United Kingdom, with the aim of determining the accuracy and computational efficiency of the LSG model relative to existing methods, including its performance on flood events beyond the range of training data.","The research addresses the challenge of using high-resolution hydrodynamic models for flood inundation simulation, which are limited by their high computational demand that increases non-linearly with model complexity, resolution, and domain size. This limitation makes it difficult to apply such models for real-time flood predictions or for generating numerous predictions needed in probabilistic flood design. The study focuses on evaluating alternative approaches that can provide accurate flood inundation predictions with greater computational efficiency. The primary objective of the study is to compare the recently developed Low-fidelity, Spatial analysis, and Gaussian Process (LSG) model against four other surrogate flood inundation models to assess their ability to simulate the temporal and spatial evolution of flood inundation for various events. The evaluation is conducted through three distinct case studies in Australia and the United Kingdom, with the aim of determining the accuracy and computational efficiency of the LSG model relative to existing methods, including its performance on flood events beyond the range of previously analyzed data.",True
Engineering,Prompt Engineering or Fine-Tuning? A Case Study on Phishing Detection with Large Language Models,"Large Language Models (LLMs) are reshaping the landscape of Machine Learning (ML) application development. The emergence of versatile LLMs capable of undertaking a wide array of tasks has reduced the necessity for intensive human involvement in training and maintaining ML models. Despite these advancements, a pivotal question emerges: can these generalized models negate the need for task-specific models? This study addresses this question by comparing the effectiveness of LLMs in detecting phishing URLs when utilized with prompt-engineering techniques versus when fine-tuned. Notably, we explore multiple prompt-engineering strategies for phishing URL detection and apply them to two chat models, GPT-3.5-turbo and Claude 2. In this context, the maximum result achieved was an F1-score of 92.74% by using a test set of 1000 samples. Following this, we fine-tune a range of base LLMs, including GPT-2, Bloom, Baby LLaMA, and DistilGPT-2—all primarily developed for text generation—exclusively for phishing URL detection. The fine-tuning approach culminated in a peak performance, achieving an F1-score of 97.29% and an AUC of 99.56% on the same test set, thereby outperforming existing state-of-the-art methods. These results highlight that while LLMs harnessed through prompt engineering can expedite application development processes, achieving a decent performance, they are not as effective as dedicated, task-specific LLMs.",['fine-tuning'],"The research idea centers on evaluating whether generalized models can replace task-specific models in the context of detecting phishing URLs, addressing the challenge of reducing the need for intensive human involvement in developing specialized detection methods. This study investigates the effectiveness of different approaches to phishing URL detection to determine if versatile models can achieve comparable performance to those tailored specifically for this task. The primary objective of the study is to compare the performance of generalized models against task-specific models in phishing URL detection, aiming to identify which approach yields higher accuracy and reliability. The study seeks to demonstrate whether dedicated models fine-tuned for phishing URL detection can outperform more generalized approaches, thereby informing the development of more effective detection techniques.","The research idea centers on evaluating whether generalized models can replace task-specific models in the context of detecting phishing URLs, addressing the challenge of reducing the need for intensive human involvement in developing specialized detection methods. This study investigates the effectiveness of different approaches to phishing URL detection to determine if versatile analytical frameworks can achieve comparable performance to those tailored specifically for this task. The primary objective of the study is to compare the performance of generalized analytical methods against task-specific techniques in phishing URL detection, aiming to identify which approach yields higher accuracy and reliability. The study seeks to demonstrate whether dedicated analytical methods optimized for phishing URL detection can outperform more generalized approaches, thereby informing the development of more effective detection techniques.",True
Engineering,Artificial intelligence-based evaluation of the factors affecting the sales of an iron and steel company,"It is important to predict the sales of an iron and steel company and to identify the variables that influence these sales for future planning. The aim in this study was to identify and model the key factors that influence the sales volume of an iron and steel company using artificial neural networks (ANNs). We attempted to obtain an integrated result from the performance/sales levels of 5 models, to use the ANN approach with hybrid algorithms, and also to present an exemplary application in the base metals industry, where there is a limited number of studies. This study contributes to the literature as the first application of artificial intelligence methods in the iron and steel industry. The ANN models incorporated 6 macroeconomic variables and price-to-sales data and their results were evaluated. An ordinary least squares regression model was also used to facilitate the comparison of results, while gray relational analysis (GRA) was used to draw a comprehensive conclusion based on the ANN results. The results showed that the variables USD/TL exchange rate, product prices, and interest rates, in descending order, had the highest degree of influence in determining the sales of the iron and steel company. Furthermore, these variables are crucial for forecasting future sales and strategic planning. The study showed that the ANN outperformed classical regression models in terms of prediction accuracy. In the model applications conducted for 5 different product groups, it was observed that 3 models (models 2, 3, and 4), including model 4, which sold a higher volume of products than the total of the other products, had an overall performance above 80%. In addition, GRA was found to be a valuable tool for synthesizing insights from different ANN models based on their respective performance levels.","['artificial neural networks (ANNs)', 'ordinary least squares regression model']","The research addresses the importance of predicting sales and identifying the key variables that influence sales volume in an iron and steel company to support future planning and strategic decision-making. Understanding these influential factors is critical for the base metals industry, where limited studies exist on sales forecasting. The primary objective of the study was to identify and model the key factors affecting the sales volume of an iron and steel company by examining the impact of macroeconomic variables and price-to-sales data. The study aimed to determine which variables have the highest influence on sales and to provide insights that can be used for forecasting future sales and enhancing strategic planning within the industry.","The research addresses the importance of predicting sales and identifying the key variables that influence sales volume in an iron and steel company to support future planning and strategic decision-making. Understanding these influential factors is critical for the base metals industry, where limited studies exist on sales forecasting. The primary objective of the study was to identify and analyze the key factors affecting the sales volume of an iron and steel company by examining the impact of macroeconomic variables and price-to-sales data. The study aimed to determine which variables have the highest influence on sales and to provide insights that can be used for anticipating future sales and enhancing strategic planning within the industry.",True
Engineering,Cost-sensitive learning for imbalanced medical data: a review,"Abstract Integrating Machine Learning (ML) in medicine has unlocked many opportunities to harness complex medical data, enhancing patient outcomes and advancing the field. However, the inherent imbalanced distribution of medical data poses a significant challenge, resulting in biased ML models that perform poorly on minority classes. Mitigating the impact of class imbalance has prompted researchers to explore various strategies, wherein Cost-Sensitive Learning (CSL) arises as a promising approach to improve the accuracy and reliability of ML models. This paper presents the first review of CSL for imbalanced medical data. A comprehensive exploration of the existing literature encompassed papers published from January 2010 to December 2022 and sourced from five major digital libraries. A total of 173 papers were selected, analysed, and classified based on key criteria, including publication years, channels and sources, research types, empirical types, medical sub-fields, medical tasks, CSL approaches, strengths and weaknesses of CSL, frequently used datasets and data types, evaluation metrics, and development tools. The results indicate a noteworthy publication rise, particularly since 2020, and a strong preference for CSL direct approaches. Data type analysis unveiled diverse modalities, with medical images prevailing. The underutilisation of cost-related metrics and the prevalence of Python as the primary programming tool are highlighted. The strengths and weaknesses analysis covered three aspects: CSL strategy, CSL approaches, and relevant works. This study serves as a valuable resource for researchers seeking to explore the current state of research, identify strengths and gaps in the existing literature and advance CSL’s application for imbalanced medical data.","['Machine Learning (ML)', 'Cost-Sensitive Learning (CSL)']","The research idea addresses the challenge posed by the imbalanced distribution of medical data, which leads to biased outcomes and poor performance in handling minority classes within medical applications. This imbalance significantly affects the accuracy and reliability of approaches used to interpret complex medical information, thereby impacting patient outcomes and the advancement of the medical field. The study recognizes the need to explore strategies that can effectively mitigate these issues to improve the handling of imbalanced medical data. The primary objective of the study is to present a comprehensive review of cost-sensitive learning approaches applied to imbalanced medical data, analyzing a wide range of literature published between 2010 and 2022. It aims to classify and evaluate existing research based on various criteria such as medical sub-fields, tasks, strengths and weaknesses of the approaches, and data types, thereby providing a valuable resource to identify current trends, gaps, and opportunities for advancing the application of cost-sensitive strategies in this context.","The research idea addresses the challenge posed by the imbalanced distribution of medical data, which leads to biased outcomes and poor performance in handling minority classes within medical applications. This imbalance significantly affects the accuracy and reliability of methods used to interpret complex medical information, thereby impacting patient outcomes and the advancement of the medical field. The study recognizes the need to explore strategies that can effectively mitigate these issues to improve the handling of imbalanced medical data. The primary objective of the study is to present a comprehensive review of cost-sensitive approaches applied to imbalanced medical data, analyzing a wide range of literature published between 2010 and 2022. It aims to classify and evaluate existing research based on various criteria such as medical sub-fields, tasks, strengths and weaknesses of the approaches, and data types, thereby providing a valuable resource to identify current trends, gaps, and opportunities for advancing the application of cost-sensitive strategies in this context.",True
Engineering,Remote sensing based forest cover classification using machine learning,"Abstract Pakistan falls significantly below the recommended forest coverage level of 20 to 30 percent of total area, with less than 6 percent of its land under forest cover. This deficiency is primarily attributed to illicit deforestation for wood and charcoal, coupled with a failure to embrace advanced techniques for forest estimation, monitoring, and supervision. Remote sensing techniques leveraging Sentinel-2 satellite images were employed. Both single-layer stacked images and temporal layer stacked images from various dates were utilized for forest classification. The application of an artificial neural network (ANN) supervised classification algorithm yielded notable results. Using a single-layer stacked image from Sentinel-2, an impressive 91.37% training overall accuracy and 0.865 kappa coefficient were achieved, along with 93.77% testing overall accuracy and a 0.902 kappa coefficient. Furthermore, the temporal layer stacked image approach demonstrated even better results. This method yielded 98.07% overall training accuracy, 97.75% overall testing accuracy, and kappa coefficients of 0.970 and 0.965, respectively. The random forest (RF) algorithm, when applied, achieved 99.12% overall training accuracy, 92.90% testing accuracy, and kappa coefficients of 0.986 and 0.882. Notably, with the temporal layer stacked image of the Sentinel-2 satellite, the RF algorithm reached exceptional performance with 99.79% training accuracy, 96.98% validation accuracy, and kappa coefficients of 0.996 and 0.954. In terms of forest cover estimation, the ANN algorithm identified 31.07% total forest coverage in the District Abbottabad region. In comparison, the RF algorithm recorded a slightly higher 31.17% of the total forested area. This research highlights the potential of advanced remote sensing techniques and machine learning algorithms in improving forest cover assessment and monitoring strategies.","['artificial neural network (ANN) supervised classification algorithm', 'random forest (RF) algorithm']","The study addresses the critical issue of Pakistan's forest coverage being significantly below the recommended level of 20 to 30 percent, with less than 6 percent of land currently under forest cover. This shortfall is mainly due to illegal deforestation for wood and charcoal, as well as inadequate adoption of advanced methods for forest estimation, monitoring, and supervision. The primary aim of the research is to improve the assessment and monitoring of forest cover in Pakistan, specifically in the District Abbottabad region, by employing advanced remote sensing techniques to achieve more accurate forest classification and coverage estimation. The study seeks to provide enhanced strategies for forest cover evaluation to support better management and conservation efforts.","The study addresses the critical issue of Pakistan's forest coverage being significantly below the recommended level of 20 to 30 percent, with less than 6 percent of land currently under forest cover. This shortfall is mainly due to illegal deforestation for wood and charcoal, as well as inadequate adoption of improved methods for forest estimation, monitoring, and supervision. The primary aim of the research is to improve the assessment and monitoring of forest cover in Pakistan, specifically in the District Abbottabad region, by employing remote sensing techniques to achieve more accurate forest classification and coverage estimation. The study seeks to provide enhanced strategies for forest cover evaluation to support better management and conservation efforts.",True
Engineering,Explainability and Interpretability in Electric Load Forecasting Using Machine Learning Techniques – A Review,"Electric Load Forecasting (ELF) is the central instrument for planning and controlling demand response programs, electricity trading, and consumption optimization. Due to the increasing automation of these processes, meaningful and transparent forecasts become more and more important. Still, at the same time, the complexity of the used machine learning models and architectures increases. Because there is an increasing interest in interpretable and explainable load forecasting methods, this work conducts a literature review to present already applied approaches regarding explainability and interpretability for load forecasts using Machine Learning. Based on extensive literature research covering eight publication portals, recurring modeling approaches, trends, and modeling techniques are identified and clustered by properties to achieve more interpretable and explainable load forecasts. The results on interpretability show an increase in the use of probabilistic models, methods for time series decomposition and the use of fuzzy logic in addition to classically interpretable models. Dominant explainable approaches are Feature Importance and Attention mechanisms. The discussion shows that a lot of knowledge from the related field of time series forecasting still needs to be adapted to the problems in ELF. Compared to other applications of explainable and interpretable methods such as clustering, there are currently relatively few research results, but with an increasing trend.","['probabilistic models', 'Attention mechanisms']","The research idea addresses the critical role of electric load forecasting in planning and controlling demand response programs, electricity trading, and consumption optimization, emphasizing the growing need for meaningful and transparent forecasts amid increasing automation. The study highlights the challenge posed by the rising complexity of forecasting approaches and the corresponding demand for more interpretable and explainable methods in this engineering context. The primary objective of the study is to review existing approaches related to interpretability and explainability in electric load forecasting by examining and clustering recurring methods and trends found in the literature. This aims to provide insights into how current techniques can be adapted and improved to enhance the transparency and understanding of load forecasts in engineering applications.","The research idea addresses the critical role of electric load forecasting in planning and controlling demand response programs, electricity trading, and consumption optimization, emphasizing the growing need for meaningful and transparent forecasts amid increasing automation. The study highlights the challenge posed by the rising complexity of forecasting approaches and the corresponding demand for more interpretable and explainable methods in this engineering context. The primary objective of the study is to review existing approaches related to interpretability and explainability in electric load forecasting by examining and categorizing recurring methods and trends found in the literature. This aims to provide insights into how current techniques can be adapted and improved to enhance the transparency and understanding of load forecasts in engineering applications.",True
Engineering,Charging management of electric vehicles with the presence of renewable resources,"Considering the increasing use of electric vehicles, the establishment of charging stations to exchange power between the grid and electric devices, and the integration of charging stations with solar power generation sources, the optimal use of electric vehicle charging stations in the power system. The purpose of cost reduction in the presence of the intelligent environment is a challenge that must be investigated so that this platform is suitable for predicting the behaviour of vehicles and, as a result, optimizing their presence in the power network. This research presents a relatively complete radial distribution network development planning model in two scenarios. In the first scenario, the effects of electric vehicles are not considered, and only the effects of distributed production (renewable and dispatchable) are considered. Studies have been done on a sample 54-bus network, a common system in most Distribution expansion planning (DEP) articles for distribution networks. In addition, the real data of American highways have been used to create raw input data. Also, due to the distance limit, the information on vehicles under 100 miles has been received as electric vehicle information. The clustering method and Capiola multivariate probability distribution functions have created suitable vehicle scenarios during different planning years. Capiola's method increases the accuracy of vehicle load forecasting according to a predetermined growth rate. The DEP problem in this research is modeled as an optimization problem based on scenario, dynamic, and in 5 one-year time frames (5-year time horizon and one-year accuracy). The results indicate that, in the presence of electric vehicles and distributed production sources, the technical characteristics of the network are improved. Similarly, the use of DGs, in addition to reducing the cost of equipment, has reduced undistributed energy in the system. But 10,000 vehicles, which have been applied to the network as an uncontrolled load, have caused an increase in undistributed energy. The cost of equipment required for the network development is almost as much as 5%.",['clustering method'],"The research idea addresses the growing integration of electric vehicles and solar power generation into the power distribution network, focusing on the challenges of optimally utilizing electric vehicle charging stations within the power system to reduce costs and improve network performance. With the increasing presence of electric vehicles, there is a need to understand their impact on the distribution network and how to effectively plan for their integration alongside distributed energy resources. The primary objective of the study is to develop a comprehensive planning model for radial distribution network expansion over a five-year horizon, considering scenarios both with and without the effects of electric vehicles and distributed generation sources. This model aims to evaluate the technical and economic impacts of electric vehicles and distributed production on the network, ultimately improving network characteristics and reducing equipment costs and energy losses.","The research idea addresses the growing integration of electric vehicles and solar power generation into the power distribution network, focusing on the challenges of optimally utilizing electric vehicle charging stations within the power system to reduce costs and improve network performance. With the increasing presence of electric vehicles, there is a need to understand their impact on the distribution network and how to effectively plan for their integration alongside distributed energy resources. The primary objective of the study is to develop a comprehensive planning framework for radial distribution network expansion over a five-year horizon, considering scenarios both with and without the effects of electric vehicles and distributed generation sources. This framework aims to evaluate the technical and economic impacts of electric vehicles and distributed production on the network, ultimately improving network characteristics and reducing equipment costs and energy losses.",True
Engineering,Machine learning for the management of biochar yield and properties of biomass sources for sustainable energy,"Abstract Biochar is emerging as a potential solution for biomass conversion to meet the ever increasing demand for sustainable energy. Efficient management systems are needed in order to exploit fully the potential of biochar. Modern machine learning (ML) techniques, and in particular ensemble approaches and explainable AI methods, are valuable for forecasting the properties and efficiency of biochar properly. Machine‐learning‐based forecasts, optimization, and feature selection are critical for improving biomass management techniques. In this research, we explore the influences of these techniques on the accurate forecasting of biochar yield and properties for a range of biomass sources. We emphasize the importance of the interpretability of a model, as this improves human comprehension and trust in ML predictions. Sensitivity analysis is shown to be an effective technique for finding crucial biomass characteristics that influence the synthesis of biochar. Precision prognostics have far‐reaching ramifications, influencing industries such as biomass logistics, conversion technologies, and the successful use of biomass as renewable energy. These advances can make a substantial contribution to a greener future and can encourage the development of a circular biobased economy. This work emphasizes the importance of using sophisticated data‐driven methodologies such as ML in biochar synthesis, to usher in ecologically friendly energy solutions. These breakthroughs hold the key to a more sustainable and environmentally friendly future.","['ensemble approaches', 'feature selection']","The research addresses the growing need for sustainable energy solutions through the conversion of biomass into biochar, highlighting the importance of efficient management systems to fully exploit biochar's potential. It recognizes the critical role of accurately forecasting biochar yield and properties from various biomass sources to improve biomass management techniques and support industries such as biomass logistics and conversion technologies. The primary aim of the study is to explore factors influencing the accurate prediction of biochar yield and properties across different biomass types, with a focus on identifying key biomass characteristics that affect biochar synthesis. This objective seeks to enhance the effective use of biomass as a renewable energy source, contributing to the development of a greener and more circular biobased economy.","The research addresses the growing need for sustainable energy solutions through the conversion of biomass into biochar, highlighting the importance of efficient management systems to fully exploit biochar's potential. It recognizes the critical role of accurately forecasting biochar yield and properties from various biomass sources to improve biomass management techniques and support industries such as biomass logistics and conversion technologies. The primary aim of the study is to explore factors influencing the accurate estimation of biochar yield and properties across different biomass types, with a focus on identifying key biomass characteristics that affect biochar synthesis. This objective seeks to enhance the effective use of biomass as a renewable energy source, contributing to the development of a greener and more circular biobased economy.",True
Engineering,Effective lung nodule detection using deep CNN with dual attention mechanisms,"Abstract Novel methods are required to enhance lung cancer detection, which has overtaken other cancer-related causes of death as the major cause of cancer-related mortality. Radiologists have long-standing methods for locating lung nodules in patients with lung cancer, such as computed tomography (CT) scans. Radiologists must manually review a significant amount of CT scan pictures, which makes the process time-consuming and prone to human error. Computer-aided diagnosis (CAD) systems have been created to help radiologists with their evaluations in order to overcome these difficulties. These systems make use of cutting-edge deep learning architectures. These CAD systems are designed to improve lung nodule diagnosis efficiency and accuracy. In this study, a bespoke convolutional neural network (CNN) with a dual attention mechanism was created, which was especially crafted to concentrate on the most important elements in images of lung nodules. The CNN model extracts informative features from the images, while the attention module incorporates both channel attention and spatial attention mechanisms to selectively highlight significant features. After the attention module, global average pooling is applied to summarize the spatial information. To evaluate the performance of the proposed model, extensive experiments were conducted using benchmark dataset of lung nodules. The results of these experiments demonstrated that our model surpasses recent models and achieves state-of-the-art accuracy in lung nodule detection and classification tasks.","['convolutional neural network (CNN)', 'dual attention mechanism', 'channel attention', 'spatial attention', 'global average pooling']","The research addresses the critical need for improved methods to enhance lung cancer detection, given that lung cancer has become the leading cause of cancer-related mortality. Traditional approaches require radiologists to manually review numerous computed tomography (CT) scan images, a process that is both time-consuming and susceptible to human error. The study focuses on overcoming these challenges to increase the efficiency and accuracy of identifying lung nodules in patients. The primary objective of the study is to develop a specialized approach that concentrates on the most important features within lung nodule images to improve diagnosis. This approach aims to extract informative characteristics from CT scans and emphasize significant elements to facilitate more accurate lung nodule detection and classification, ultimately enhancing diagnostic performance.","The research addresses the critical need for improved methods to enhance lung cancer detection, given that lung cancer has become the leading cause of cancer-related mortality. Traditional approaches require radiologists to manually review numerous computed tomography (CT) scan images, a process that is both time-consuming and susceptible to human error. The study focuses on overcoming these challenges to increase the efficiency and accuracy of identifying lung nodules in patients. The primary objective of the study is to develop a specialized approach that concentrates on the most important features within lung nodule images to improve diagnosis. This approach aims to identify informative characteristics from CT scans and emphasize significant elements to facilitate more accurate lung nodule detection and classification, ultimately enhancing diagnostic performance.",True
Engineering,Transformer-based Generative Adversarial Networks in Computer Vision: A Comprehensive Survey,"Generative Adversarial Networks (GANs) have been very successful for synthesizing the images in a given dataset. The artificially generated images by GANs are very realistic. The GANs have shown potential usability in several computer vision applications, including image generation, image-to-image translation, video synthesis, etc. Conventionally, the generator network is the backbone of GANs, which generates the samples and the discriminator network is used to facilitate the training of the generator network. The generator and discriminator networks are usually a Convolutional Neural Network (CNN). The convolution-based networks exploit the local relationship in a layer, which requires the deep networks to extract the abstract features. However, recently developed Transformer networks are able to exploit the global relationship with tremendous performance improvement for several problems in computer vision. Motivated from the success of Transformer networks and GANs, recent works have tried to exploit the Transformers in GAN framework for the image/video synthesis. This paper presents a comprehensive survey on the developments and advancements in GANs utilizing the Transformer networks for computer vision applications. The performance comparison for several applications on benchmark datasets is also performed and analyzed. The conducted survey will be very useful to understand the research trends & gaps related with Transformer-based GANs and to develop the advanced GAN architectures by exploiting the global and local relationships for different applications.","['Generative Adversarial Networks (GANs)', 'Convolutional Neural Network (CNN)', 'Transformer networks']","The research idea addresses the challenge of improving the synthesis of realistic images and videos by exploring new approaches that can better capture both local and global relationships within visual data. Traditional methods rely heavily on local feature extraction, which often requires deep structures to achieve abstract representations, while recent advancements suggest that incorporating global relationships can significantly enhance performance in image and video generation tasks. This study is motivated by the need to understand and advance the techniques that combine these aspects to improve the quality and applicability of synthesized visual content. The primary objective of the study is to provide a comprehensive survey of recent developments and advancements in image and video synthesis techniques that integrate global and local relationship exploitation. It aims to analyze and compare the performance of these approaches across various benchmark applications, identify current research trends and gaps, and support the development of improved architectures for generating realistic visual content in engineering applications.","The research idea addresses the challenge of improving the synthesis of realistic images and videos by exploring new approaches that can better capture both local and global relationships within visual data. Traditional methods rely heavily on local feature extraction, which often requires complex structures to achieve abstract representations, while recent advancements suggest that incorporating global relationships can significantly enhance performance in image and video generation tasks. This study is motivated by the need to understand and advance the techniques that combine these aspects to improve the quality and applicability of synthesized visual content. The primary objective of the study is to provide a comprehensive survey of recent developments and advancements in image and video synthesis techniques that integrate global and local relationship exploitation. It aims to analyze and compare the performance of these approaches across various benchmark applications, identify current research trends and gaps, and support the development of improved methodologies for generating realistic visual content in engineering applications.",True
Engineering,A new integrated intelligent computing paradigm for predicting joints shear strength,"Joints shear strength is a critical parameter during the design and construction of geotechnical engineering structures. The prevailing models mostly adopt the form of empirical functions, employing mathematical regression techniques to represent experimental data. As an alternative approach, this paper proposes a new integrated intelligent computing paradigm that aims to predict joints shear strength. Five metaheuristic optimization algorithms, including the chameleon swarm algorithm (CSA), slime mold algorithm, transient search optimization algorithm, equilibrium optimizer and social network search algorithm, were employed to enhance the performance of the multilayered perception (MLP) model. Efficiency comparisons were conducted between the proposed CSA-MLP model and twelve classical models, employing statistical indicators such as root mean square error (RMSE), correlation coefficient (R2), mean absolute error (MAE), and variance accounted for (VAF) to evaluate the performance of each model. The sensitivity analysis of parameters that impact joints shear strength was conducted. Finally, the feasibility and limitations of this study were discussed. The results revealed that, in comparison to other models, the CSA-MLP model exhibited the most appropriate performance in terms of R2 (0.88), RMSE (0.19), MAE (0.15), and VAF (90.32%) values. The result of sensitivity analysis showed that the normal stress and the joint roughness coefficient were the most critical factors influencing joints shear strength. This paper presented an efficacious attempt toward swift prediction of joints shear strength, thus avoiding the need for costly in-site and laboratory tests.","['slime mold algorithm', 'equilibrium optimizer']","The research addresses the critical importance of accurately determining joints shear strength in the design and construction of geotechnical engineering structures, highlighting the limitations of prevailing empirical models that rely on mathematical regression to represent experimental data. The study is motivated by the need for a more effective approach to predict joints shear strength, which is essential for ensuring structural stability and safety while potentially reducing the reliance on expensive and time-consuming in-site and laboratory testing. The primary objective of the study is to develop and evaluate a new approach for predicting joints shear strength that improves prediction accuracy compared to existing models. Additionally, the study aims to identify the key parameters influencing joints shear strength, with a focus on enhancing the understanding of factors such as normal stress and joint roughness coefficient that critically affect shear strength outcomes.","The research addresses the critical importance of accurately determining joints shear strength in the design and construction of geotechnical engineering structures, highlighting the limitations of prevailing empirical models that rely on mathematical representations to interpret experimental data. The study is motivated by the need for a more effective approach to predict joints shear strength, which is essential for ensuring structural stability and safety while potentially reducing the reliance on expensive and time-consuming in-site and laboratory testing. The primary objective of the study is to develop and evaluate a new approach for predicting joints shear strength that improves prediction accuracy compared to existing models. Additionally, the study aims to identify the key parameters influencing joints shear strength, with a focus on enhancing the understanding of factors such as normal stress and joint roughness coefficient that critically affect shear strength outcomes.",True
Engineering,"A Comprehensive Review on the Role of Artificial Intelligence in Power System Stability, Control, and Protection: Insights and Future Directions","This review comprehensively examines the burgeoning field of intelligent techniques to enhance power systems’ stability, control, and protection. As global energy demands increase and renewable energy sources become more integrated, maintaining the stability and reliability of both conventional power systems and smart grids is crucial. Traditional methods are increasingly insufficient for handling today’s power grids’ complex, dynamic nature. This paper discusses the adoption of advanced intelligence methods, including artificial intelligence (AI), deep learning (DL), machine learning (ML), metaheuristic optimization algorithms, and other AI techniques such as fuzzy logic, reinforcement learning, and model predictive control to address these challenges. It underscores the critical importance of power system stability and the new challenges of integrating diverse energy sources. The paper reviews various intelligent methods used in power system analysis, emphasizing their roles in predictive maintenance, fault detection, real-time control, and monitoring. It details extensive research on the capabilities of AI and ML algorithms to enhance the precision and efficiency of protection systems, showing their effectiveness in accurately identifying and resolving faults. Additionally, it explores the potential of fuzzy logic in decision-making under uncertainty, reinforcement learning for dynamic stability control, and the integration of IoT and big data analytics for real-time system monitoring and optimization. Case studies from the literature are presented, offering valuable insights into practical applications. The review concludes by identifying current limitations and suggesting areas for future research, highlighting the need for more robust, flexible, and scalable intelligent systems in the power sector. This paper is a valuable resource for researchers, engineers, and policymakers, providing a detailed understanding of the current and future potential of intelligent techniques in power system stability, control, and protection.","['deep learning (DL)', 'machine learning (ML)', 'reinforcement learning']","The research idea centers on addressing the increasing challenges in maintaining the stability, control, and protection of power systems amid rising global energy demands and the growing integration of renewable energy sources. Traditional methods are becoming inadequate for managing the complex and dynamic nature of modern power grids, including both conventional systems and smart grids. Ensuring the reliability and stability of these evolving power systems is critical due to the diverse energy sources and operational complexities involved. The research objective is to comprehensively examine and evaluate advanced approaches aimed at enhancing power system stability, control, and protection. The study aims to provide a detailed understanding of various techniques that improve fault detection, real-time control, monitoring, and decision-making under uncertainty, thereby contributing to more robust and efficient power system operations.","The research idea centers on addressing the increasing challenges in maintaining the stability, control, and protection of power systems amid rising global energy demands and the growing integration of renewable energy sources. Traditional methods are becoming inadequate for managing the complex and dynamic nature of modern power grids, including both conventional systems and smart grids. Ensuring the reliability and stability of these evolving power systems is critical due to the diverse energy sources and operational complexities involved. The research objective is to comprehensively examine and evaluate advanced approaches aimed at enhancing power system stability, control, and protection. The study aims to provide a detailed understanding of various techniques that improve fault detection, real-time control, monitoring, and operational response under uncertainty, thereby contributing to more robust and efficient power system operations.",True
Engineering,Geographically weighted machine learning for modeling spatial heterogeneity in traffic crash frequency and determinants in US,"Spatial analyses of traffic crashes have drawn much interest due to the nature of the spatial dependence and spatial heterogeneity in the crash data. This study makes the best of Geographically Weighted Random Forest (GW-RF) model to explore the local associations between crash frequency and various influencing factors in the US, including road network attributes, socio-economic characteristics, and land use factors collected from multiple data sources. Special emphasis is put on modeling the spatial heterogeneity in the effects of a factor on crash frequency in different geographical areas in a data-driven way. The GW-RF model outperforms global models (e.g. Random Forest) and conventional geographically weighted regression, demonstrating superior predictive accuracy and elucidating spatial variations. The GW-RF model reveals spatial distinctions in the effects of certain factors on crash frequency. For example, the importance of intersection density varies significantly across regions, with high significance in the southern and northeastern areas. Low-grade road density emerges as influential in specific cities. The findings highlight the significance of different factors in influencing crash frequency across zones. Road network factors, particularly intersection density, exhibit high importance universally, while socioeconomic variables demonstrate moderate effects. Interestingly, land use variables show relatively lower importance. The outcomes could help to allocate resources and implement tailored interventions to reduce the likelihood of crashes.","['Geographically Weighted Random Forest (GW-RF)', 'Random Forest']","The research idea addresses the spatial dependence and heterogeneity present in traffic crash data, emphasizing the need to understand how various factors such as road network attributes, socio-economic characteristics, and land use influence crash frequency differently across geographical areas in the US. This study is motivated by the importance of capturing local variations in the effects of these factors to better comprehend the spatial distinctions in crash occurrences. The primary objective of the study is to explore the local associations between crash frequency and multiple influencing factors, with a focus on modeling the spatial heterogeneity in their effects across different regions. The study aims to identify the varying significance of factors like intersection density and road density in different areas to support targeted resource allocation and interventions for reducing crash likelihood.","The research idea addresses the spatial dependence and heterogeneity present in traffic crash data, emphasizing the need to understand how various factors such as road network attributes, socio-economic characteristics, and land use influence crash frequency differently across geographical areas in the US. This study is motivated by the importance of capturing local variations in the effects of these factors to better comprehend the spatial distinctions in crash occurrences. The primary objective of the study is to explore the local associations between crash frequency and multiple influencing factors, with a focus on examining the spatial heterogeneity in their effects across different regions. The study aims to identify the varying significance of factors like intersection density and road density in different areas to support targeted resource allocation and interventions for reducing crash likelihood.",True
Engineering,"Hybrid physics-machine learning models for predicting rate of penetration in the Halahatang oil field, Tarim Basin","Abstract Rate of penetration (ROP) is a key factor in drilling optimization, cost reduction and drilling cycle shortening. Due to the systematicity, complexity and uncertainty of drilling operations, however, it has always been a problem to establish a highly accurate and interpretable ROP prediction model to guide and optimize drilling operations. To solve this problem in the Tarim Basin, this study proposes four categories of hybrid physics-machine learning (ML) methods for modeling. One of which is residual modeling, in which an ML model learns to predict errors or residuals, via a physical model; the second is integrated coupling, in which the output of the physical model is used as an input to the ML model; the third is simple average, in which predictions from both the physical model and the ML model are combined; and the last is bootstrap aggregating (bagging), which follows the idea of ensemble learning to combine different physical models’ advantages. A total of 5655 real data points from the Halahatang oil field were used to test the performance of the various models. The results showed that the residual modeling model, with an R 2 of 0.9936, had the best performance, followed by the simple average model and bagging with R 2 values of 0.9394 and 0.5998, respectively. From the view of prediction accuracy, and model interpretability, the hybrid physics-ML model with residual modeling is the optimal method for ROP prediction.","['residual modeling', 'bootstrap aggregating (bagging)']","The research idea centers on the challenge of accurately predicting the rate of penetration (ROP) during drilling operations, which is crucial for optimizing drilling performance, reducing costs, and shortening the drilling cycle. Due to the complexity, systematic nature, and uncertainty inherent in drilling processes, establishing a highly accurate and interpretable ROP prediction approach has been a persistent problem, particularly in the Tarim Basin. The primary objective of the study is to develop and evaluate different hybrid modeling approaches that combine physical understanding with other methods to improve the accuracy and interpretability of ROP predictions. The study aims to identify the most effective approach for guiding and optimizing drilling operations by testing these methods with real data from the Halahatang oil field.","The research idea centers on the challenge of accurately predicting the rate of penetration (ROP) during drilling operations, which is crucial for optimizing drilling performance, reducing costs, and shortening the drilling cycle. Due to the complexity, systematic nature, and uncertainty inherent in drilling processes, establishing a highly accurate and interpretable ROP prediction approach has been a persistent problem, particularly in the Tarim Basin. The primary objective of the study is to develop and evaluate different hybrid analytical approaches that combine physical understanding with other methods to improve the accuracy and interpretability of ROP predictions. The study aims to identify the most effective approach for guiding and optimizing drilling operations by testing these methods with real data from the Halahatang oil field.",True
Engineering,Enhancing Skin Cancer Diagnosis Using Swin Transformer with Hybrid Shifted Window-Based Multi-head Self-attention and SwiGLU-Based MLP,"Abstract Skin cancer is one of the most frequently occurring cancers worldwide, and early detection is crucial for effective treatment. Dermatologists often face challenges such as heavy data demands, potential human errors, and strict time limits, which can negatively affect diagnostic outcomes. Deep learning–based diagnostic systems offer quick, accurate testing and enhanced research capabilities, providing significant support to dermatologists. In this study, we enhanced the Swin Transformer architecture by implementing the hybrid shifted window-based multi-head self-attention (HSW-MSA) in place of the conventional shifted window-based multi-head self-attention (SW-MSA). This adjustment enables the model to more efficiently process areas of skin cancer overlap, capture finer details, and manage long-range dependencies, while maintaining memory usage and computational efficiency during training. Additionally, the study replaces the standard multi-layer perceptron (MLP) in the Swin Transformer with a SwiGLU-based MLP, an upgraded version of the gated linear unit (GLU) module, to achieve higher accuracy, faster training speeds, and better parameter efficiency. The modified Swin model-base was evaluated using the publicly accessible ISIC 2019 skin dataset with eight classes and was compared against popular convolutional neural networks (CNNs) and cutting-edge vision transformer (ViT) models. In an exhaustive assessment on the unseen test dataset, the proposed Swin-Base model demonstrated exceptional performance, achieving an accuracy of 89.36%, a recall of 85.13%, a precision of 88.22%, and an F1-score of 86.65%, surpassing all previously reported research and deep learning models documented in the literature.","['Swin Transformer architecture', 'shifted window-based multi-head self-attention (SW-MSA)', 'multi-layer perceptron (MLP)', 'SwiGLU-based MLP', 'gated linear unit (GLU) module', 'convolutional neural networks (CNNs)', 'vision transformer (ViT) models']","The research addresses the critical challenge of early and accurate detection of skin cancer, which is essential for effective treatment but is hindered by limitations such as heavy workload, potential human errors, and time constraints faced by dermatologists. Improving diagnostic accuracy and efficiency in identifying various types of skin cancer remains a significant concern in the medical engineering field. The primary objective of the study is to enhance the capability of diagnostic approaches for skin cancer by refining the processing of overlapping skin cancer regions and capturing finer details to improve detection accuracy. This is achieved by modifying existing techniques to better manage complex skin cancer characteristics while maintaining efficiency, ultimately aiming to provide more reliable and precise diagnostic outcomes.","The research addresses the critical challenge of early and accurate detection of skin cancer, which is essential for effective treatment but is hindered by limitations such as heavy workload, potential human errors, and time constraints faced by dermatologists. Improving diagnostic accuracy and efficiency in identifying various types of skin cancer remains a significant concern in the medical engineering field. The primary objective of the study is to enhance the capability of diagnostic approaches for skin cancer by refining the processing of overlapping skin cancer regions and capturing finer details to improve detection accuracy. This is achieved by modifying existing methodologies to better manage complex skin cancer characteristics while maintaining efficiency, ultimately aiming to provide more reliable and precise diagnostic outcomes.",True
Engineering,A Novel Fuzzy Neural Network Architecture Search Framework for Defect Recognition With Uncertainties,"Defect recognition is an important task in intelligent manufacturing. Due to the subjectivity of human annotation, the collected defect data usually contains a lot of noise and unpredictable uncertainties, which have a great negative influence on defect recognition. It is a significant challenge to discover an effective defect recognition model with satisfactory uncertainty processing ability. A natural way is to automatically search for an efficient deep model, which can be realized by neural architecture search (NAS). To achieve this, we propose an efficient fuzzy NAS framework for defect recognition, where the searched architecture can effectively handle uncertain information from the given datasets. Specifically, we first design a fuzzy search space and the related encoding strategy for fuzzy NAS. Then, we propose a comparator-based evolutionary search approach, where an online end-to-end comparator is learned to directly determine the selection of candidate architectures from the evolutionary population. The comparator works in an end-to-end way and it transforms the complex ranking problem of evaluating architectures into a simple classification task, which overcomes the rank disorder issue suffered from traditional performance predictors. A series of experimental results demonstrate that the architecture with fewer #Params (1.22 M) search by fuzzy neural architecture search framework for defect recognition method achieves higher accuracy (92.26%) compared to the state-of-the-art results (i.e., DARTS-PV) on the ELPV dataset, as well as competitive results (accuracy = 76.4%, #Params = 1.04 M) on the CODEBRIM dataset. Experimental results show the effectiveness and efficiency of our proposed method in handling uncertain problems.",['neural architecture search (NAS)'],"The research idea addresses the challenge of defect recognition in intelligent manufacturing, where the presence of noise and unpredictable uncertainties in defect data due to subjective human annotation negatively impacts the accuracy of defect identification. Effectively managing these uncertainties is crucial for improving defect recognition performance. The primary objective of the study is to develop an approach that can effectively handle uncertain information in defect datasets to enhance the accuracy and reliability of defect recognition. The study aims to achieve this by designing a framework that improves the processing of uncertain data, thereby advancing the capability to accurately identify defects in manufacturing contexts.","The research idea addresses the challenge of defect recognition in manufacturing, where the presence of noise and unpredictable uncertainties in defect data due to subjective human annotation negatively impacts the accuracy of defect identification. Effectively managing these uncertainties is crucial for improving defect recognition performance. The primary objective of the study is to develop an approach that can effectively handle uncertain information in defect datasets to enhance the accuracy and reliability of defect recognition. The study aims to achieve this by designing a framework that improves the processing of uncertain data, thereby advancing the capability to accurately identify defects in manufacturing contexts.",True
Engineering,Eco-friendly mix design of slag-ash-based geopolymer concrete using explainable deep learning,"Geopolymer concrete is a sustainable and eco-friendly substitute for traditional OPC (Ordinary Portland Cement) based concrete, as it reduces greenhouse gas emissions. With various supplementary cementitious materials, the compressive strength of geopolymer concrete should be accurately predicted. Recent studies have applied deep learning techniques to predict the compressive strength of geopolymer concrete yet its hidden decision-making criteria diminish the end-users' trust in predictions. To bridge this gap, the authors first developed three deep learning models: an artificial neural network (ANN), a deep neural network (DNN), and a 1D convolution neural network (CNN) to predict the compressive strength of slag ash-based geopolymer concrete. The performance indices for accuracy revealed that the DNN model outperforms the other two models. Subsequently, Shapley additive explanations (SHAP) were used to explain the best-performed deep learning model, DNN, and its compressive strength predictions. SHAP exhibited how the importance of each feature and its relationship contributes to the compressive strength prediction of the DNN model. Finally, the authors developed a novel DNN-based open-source software interface to predict the mix design proportions for a given target compressive strength (using inverse modeling technique) for slag ash-based geopolymer concrete. Additionally, the software calculates the Global Warming Potential (kg CO2 equivalent) for each mix design to select the mix designs with low greenhouse emissions.","['artificial neural network (ANN)', 'deep neural network (DNN)', '1D convolution neural network (CNN)', 'Shapley additive explanations (SHAP)']","The research addresses the need for sustainable and eco-friendly alternatives to traditional Ordinary Portland Cement (OPC) based concrete by focusing on geopolymer concrete, which reduces greenhouse gas emissions. Accurately predicting the compressive strength of geopolymer concrete made with various supplementary cementitious materials is essential for its effective application. The primary aim of the study is to develop a reliable approach to predict the compressive strength of slag ash-based geopolymer concrete and to provide a means to determine appropriate mix design proportions for achieving a target compressive strength. Additionally, the study seeks to evaluate the environmental impact of different mix designs by calculating their associated greenhouse gas emissions, thereby facilitating the selection of more sustainable concrete mixtures.","The research addresses the need for sustainable and eco-friendly alternatives to traditional Ordinary Portland Cement (OPC) based concrete by focusing on geopolymer concrete, which reduces greenhouse gas emissions. Accurately determining the compressive strength of geopolymer concrete made with various supplementary cementitious materials is essential for its effective application. The primary aim of the study is to develop a reliable method to estimate the compressive strength of slag ash-based geopolymer concrete and to provide a means to determine appropriate mix design proportions for achieving a target compressive strength. Additionally, the study seeks to evaluate the environmental impact of different mix designs by calculating their associated greenhouse gas emissions, thereby facilitating the selection of more sustainable concrete mixtures.",True
Engineering,Metal–Organic Framework Stability in Water and Harsh Environments from Data-Driven Models Trained on the Diverse WS24 Data Set,"Metal-organic frameworks (MOFs) are porous materials with applications in gas separations and catalysis, but a lack of water stability often limits their practical use given the ubiquity of water. Consequently, it is useful to predict whether a MOF is water-stable before investing time and resources into synthesis. Existing heuristics for designing water-stable MOFs lack generality and limit the diversity of explored chemistry due to narrowly defined criteria. Machine learning (ML) models offer the promise to improve the generality of predictions but require data. In an improvement on previous efforts, we enlarge the available training data for MOF water stability prediction by over 400%, adding 911 MOFs with water stability labels assigned through semiautomated manuscript analysis to curate the new data set WS24. The additional data are shown to improve ML model performance (test ROC-AUC > 0.8) over diverse chemistry for the prediction of both water stability and stability in harsher acidic conditions. We illustrate how the expanded data set and models can be used with a previously developed activation stability model in combination with genetic algorithms to quickly screen ∼10,000 MOFs from a space of hundreds of thousands for candidates with multivariate stability (upon activation, in water, and in acid). We uncover metal- and geometry-specific design rules for robust MOFs. The data set and ML models developed in this work, which we disseminate through an easy-to-use web interface, are expected to contribute toward the accelerated discovery of novel, water-stable MOFs for applications such as direct air gas capture and water treatment.",['genetic algorithms'],"The research addresses the challenge of water instability in metal-organic frameworks (MOFs), which limits their practical applications in gas separations and catalysis due to the widespread presence of water. Existing design guidelines for water-stable MOFs are not sufficiently general, restricting the exploration of diverse chemical compositions. The primary aim of the study is to enhance the prediction of MOF water stability by significantly expanding the available data set with additional MOFs labeled for water stability, thereby improving the ability to identify candidates that maintain stability under various conditions, including activation, water exposure, and acidic environments. This work seeks to uncover specific design principles related to metal types and geometries that contribute to robust MOFs, ultimately facilitating the accelerated discovery of novel water-stable materials for engineering applications such as direct air gas capture and water treatment.","The research addresses the challenge of water instability in metal-organic frameworks (MOFs), which limits their practical applications in gas separations and catalysis due to the widespread presence of water. Existing design guidelines for water-stable MOFs are not sufficiently general, restricting the exploration of diverse chemical compositions. The primary aim of the study is to enhance the understanding of MOF water stability by significantly expanding the available data set with additional MOFs labeled for water stability, thereby improving the ability to identify candidates that maintain stability under various conditions, including activation, water exposure, and acidic environments. This work seeks to uncover specific design principles related to metal types and geometries that contribute to robust MOFs, ultimately facilitating the accelerated discovery of novel water-stable materials for engineering applications such as direct air gas capture and water treatment.",True
Engineering,Deep learning approaches for visual faults diagnosis of photovoltaic systems: State-of-the-Art review,"PV systems are prone to external environmental conditions that affect PV system operations. Visual inspection of the impacts of faults on PV system is considered a better practice rather than onsite fault detection mechanisms. Faults such as hotspot, dark area, cracks, glass break, wavy lines, snail tracks, corrosion, discoloration, junction box failure and delamination faults have different visual symptoms. EL technology, infrared thermography, and photoluminescence approaches are used to extract and visualize the impact of faults on PV modules. DL based algorithms such as, CNN, ANN, RNN, AE, DBN, TL and hybrid algorithms have shown promising results in domain of visual PV fault detection. This article critically overviews working mechanism of DL algorithms in terms of their limitations, complexity, interpretability, training dataset requirements and capability to work with another DL algorithms. This research article also reviews, critically analyzes, and systematically presents different clustering algorithms based on their clustering mechanism, distance metrics, convergence criteria. Additionally, their performance is also evaluated in terms of DI, CHI, DBI, S-score, and homogeneity. Moreover, this research work explicitly identifies and explains the limitations and contributions of recent and older techniques employed for features extraction, data preprocessing, and decision making by performing SWOT analysis. This research work also recommends future research directions for industry and academia.","['CNN', 'ANN', 'RNN', 'AE', 'clustering algorithms']","The research addresses the challenge of identifying and understanding the impacts of various faults on photovoltaic (PV) systems caused by external environmental conditions, which affect their operation and efficiency. Visual inspection is emphasized as a preferred approach for detecting faults such as hotspots, cracks, corrosion, and delamination, each exhibiting distinct visual symptoms that influence PV module performance. The primary objective of the study is to critically review and analyze existing methods used to visualize and detect faults in PV modules, focusing on their mechanisms, limitations, and effectiveness. Additionally, the research aims to evaluate different clustering approaches for fault characterization and to identify the strengths and weaknesses of current techniques, ultimately providing recommendations for future advancements in the field.","The research addresses the challenge of identifying and understanding the impacts of various faults on photovoltaic (PV) systems caused by external environmental conditions, which affect their operation and efficiency. Visual inspection is emphasized as a preferred approach for detecting faults such as hotspots, cracks, corrosion, and delamination, each exhibiting distinct visual symptoms that influence PV module performance. The primary objective of the study is to critically review and analyze existing methods used to visualize and detect faults in PV modules, focusing on their mechanisms, limitations, and effectiveness. Additionally, the research aims to evaluate different categorization methods for fault characterization and to identify the strengths and weaknesses of current techniques, ultimately providing recommendations for future advancements in the field.",True
Engineering,Metaheuristic optimization algorithms-based prediction modeling for titanium dioxide-Assisted photocatalytic degradation of air contaminants,"Airborne contaminants pose significant environmental and health challenges. Titanium dioxide (TiO2) has emerged as a leading photocatalyst in the degradation of air contaminants compared to other photocatalysts due to its inherent inertness, cost-effectiveness, and photostability. To assess its effectiveness, laboratory examinations are frequently employed to measure the photocatalytic degradation rate of TiO2. However, this approach involves time-consuming requirements, labor-intensive tasks, and high costs. In literature, ensemble or standalone models are commonly used for assessing the performance of TiO2 photocatalytic degradation of water and air contaminants. Nonetheless, the application of metaheuristic hybrid models has the potential to be more effective in predictive accuracy and efficiency. Accordingly, this research utilized hybrid machine learning (ML) algorithms to estimate the photo-degradation rate constants of organic air pollutants using TiO2 nanoparticles and exposure to ultraviolet light. Six metaheuristics optimization algorithms, namely, nuclear reaction optimization (NRO), differential evolution algorithm (DEA), human felicity algorithm (HFA), lightning search algorithm (LSA), Harris hawks algorithm (HHA), and tunicate swarm algorithm (TSA) were combined with random forest (RF) technique to establish the hybrid models. A database of 200 data points was acquired from experimental studies for model training and testing. Furthermore, multiple statistical indicators and 10-fold cross-validation were employed to examine the established hybrid model's accuracy and robustness. The TSA-RF model demonstrated superior prediction accuracy among the six suggested models, achieving an impressive correlation (R) of 0.90 and a lower root mean square error (RMSE) of 0.25. In contrast, the HFA-RF, HHA-RF, and NRO-RF models exhibited a slightly lower R-value of 0.88, with RMSE scores of 0.32. The DEA-RF and LSA-RF models, while effective, showed a marginally lower R-value of 0.85, with RMSE values of 0.45 and 0.44, respectively. Moreover, the SHapley Additive exPlanation (SHAP) results indicated that the degradation rates of air contaminants through photocatalysis were most notably influenced by factors such as the reactor sizes, photocatalyst dosage, humidity, and intensity.","['random forest (RF)', 'differential evolution algorithm (DEA)', 'Harris hawks algorithm (HHA)', 'SHapley Additive exPlanation (SHAP)']","The research addresses the significant environmental and health challenges posed by airborne contaminants and focuses on the use of titanium dioxide (TiO2) as a photocatalyst for degrading these contaminants due to its inertness, cost-effectiveness, and photostability. Traditional laboratory methods for assessing the photocatalytic degradation rate of TiO2 are time-consuming, labor-intensive, and costly, highlighting the need for more efficient approaches. The primary aim of the study is to estimate the photo-degradation rate constants of organic air pollutants using TiO2 nanoparticles under ultraviolet light exposure. This objective seeks to improve the understanding of factors influencing the degradation rates, such as reactor size, photocatalyst dosage, humidity, and light intensity, to enhance the effectiveness of TiO2 in air contaminant removal.","The research addresses the significant environmental and health challenges posed by airborne contaminants and focuses on the use of titanium dioxide (TiO2) as a photocatalyst for degrading these contaminants due to its inertness, cost-effectiveness, and photostability. Traditional laboratory methods for assessing the photocatalytic degradation rate of TiO2 are time-consuming, labor-intensive, and costly, highlighting the need for more efficient approaches. The primary aim of the study is to determine the photo-degradation rate constants of organic air pollutants using TiO2 nanoparticles under ultraviolet light exposure. This objective seeks to improve the understanding of factors influencing the degradation rates, such as reactor size, photocatalyst dosage, humidity, and light intensity, to enhance the effectiveness of TiO2 in air contaminant removal.",True
Engineering,Artificial intelligence alphafold model for molecular biology and drug discovery: a machine-learning-driven informatics investigation,"AlphaFold model has reshaped biological research. However, vast unstructured data in the entire AlphaFold field requires further analysis to fully understand the current research landscape and guide future exploration. Thus, this scientometric analysis aimed to identify critical research clusters, track emerging trends, and highlight underexplored areas in this field by utilizing machine-learning-driven informatics methods. Quantitative statistical analysis reveals that the AlphaFold field is enjoying an astonishing development trend (Annual Growth Rate = 180.13%) and global collaboration (International Co-authorship = 33.33%). Unsupervised clustering algorithm, time series tracking, and global impact assessment point out that Cluster 3 (Artificial Intelligence-Powered Advancements in AlphaFold for Structural Biology) has the greatest influence (Average Citation = 48.36 ± 184.98). Additionally, regression curve and hotspot burst analysis highlight ""structure prediction"" (s = 12.40, R2 = 0.9480, p = 0.0051), ""artificial intelligence"" (s = 5.00, R2 = 0.8096, p = 0.0375), ""drug discovery"" (s = 1.90, R2 = 0.7987, p = 0.0409), and ""molecular dynamics"" (s = 2.40, R2 = 0.8000, p = 0.0405) as core hotspots driving the research frontier. More importantly, the Walktrap algorithm further reveals that ""structure prediction, artificial intelligence, molecular dynamics"" (Relevance Percentage[RP] = 100%, Development Percentage[DP] = 25.0%), ""sars-cov-2, covid-19, vaccine design"" (RP = 97.8%, DP = 37.5%), and ""homology modeling, virtual screening, membrane protein"" (RP = 89.9%, DP = 26.1%) are closely intertwined with the AlphaFold model but remain underexplored, which implies a broad exploration space. In conclusion, through the machine-learning-driven informatics methods, this scientometric analysis offers an objective and comprehensive overview of global AlphaFold research, identifying critical research clusters and hotspots while prospectively pointing out underexplored critical areas.",['unsupervised clustering algorithm'],"The research addresses the challenge of managing and understanding the vast and unstructured body of research related to the AlphaFold model within the field of structural biology. There is a need to comprehensively map the current research landscape to identify key areas of focus, emerging trends, and gaps that require further exploration. The primary objective of the study is to provide an objective and comprehensive overview of global research on AlphaFold by identifying critical research clusters, tracking emerging trends, and highlighting underexplored areas that could guide future investigations in this domain. This aims to facilitate a clearer understanding of the development and impact of AlphaFold-related research in structural biology and related applications.","The research addresses the challenge of managing and understanding the vast and unstructured body of research related to the AlphaFold system within the field of structural biology. There is a need to comprehensively map the current research landscape to identify key areas of focus, emerging trends, and gaps that require further exploration. The primary objective of the study is to provide an objective and comprehensive overview of global research on AlphaFold by identifying critical research clusters, tracking emerging trends, and highlighting underexplored areas that could guide future investigations in this domain. This aims to facilitate a clearer understanding of the development and impact of AlphaFold-related research in structural biology and related applications.",True
Engineering,Hybrid KNN-SVM machine learning approach for solar power forecasting,"Predictions about solar power will have a significant impact on large-scale renewable energy plants. Photovoltaic (PV) power generation forecasting is particularly sensitive to measuring the uncertainty in weather conditions. Although several conventional techniques like long short-term memory (LSTM), support vector machine (SVM), etc. are available, but due to some restrictions, their application is limited. To enhance the precision of forecasting solar power from solar farms, a hybrid machine learning model that includes blends of the K-Nearest Neighbor (KNN) machine learning technique with the SVM to increase reliability for power system operators is proposed in this investigation. The conventional LSTM technique is also implemented to compare the performance of the proposed hybrid technique. The suggested hybrid model is improved by the use of structural diversity and data diversity in KNN and SVM, respectively. For the solar power predictions, the suggested method was tested on the Jodhpur real-time series dataset obtained from the data centers of weather stations using Meteonorm. The data set includes metrics such as Hourly Average Temperature (HAT), Hourly Total Sunlight Duration (HTSD), Hourly Total Global Solar Radiation (HTGSR), and Hourly Total Photovoltaic Energy Generation (HTPEG). The collated data has been segmented into training data, validation data, and testing data. Furthermore, the proposed technique performed better when evaluated on the three performance indices, viz., accuracy, sensitivity, and specificity. Compared with the conventional LSTM technique, the hybrid technique improved the prediction with 98% accuracy.","['long short-term memory (LSTM)', 'support vector machine (SVM)', 'K-Nearest Neighbor (KNN)', 'hybrid machine learning model (KNN + SVM)']","The research idea addresses the critical need for accurate solar power forecasting in large-scale renewable energy plants, emphasizing the challenge posed by uncertainty in weather conditions that affect photovoltaic power generation. Improving the precision of solar power predictions is essential for enhancing the reliability and operational efficiency of power system operators managing solar farms. The primary objective of this study is to develop a method that increases the accuracy and reliability of solar power forecasts by leveraging diverse approaches to better capture the variability in weather-related factors. The study aims to test and validate this approach using real-time solar and weather data to demonstrate improved forecasting performance compared to existing conventional techniques.","The research idea addresses the critical need for accurate solar power forecasting in large-scale renewable energy plants, emphasizing the challenge posed by uncertainty in weather conditions that affect photovoltaic power generation. Improving the precision of solar power predictions is essential for enhancing the reliability and operational efficiency of power system operators managing solar farms. The primary objective of this study is to develop a method that increases the accuracy and reliability of solar power forecasts by considering diverse approaches to better capture the variability in weather-related factors. The study aims to test and validate this approach using real-time solar and weather data to demonstrate improved forecasting performance compared to existing conventional techniques.",True
Engineering,A Reliable and Robust Deep Learning Model for Effective Recyclable Waste Classification,"In response to the growing waste problem caused by industrialization and modernization, the need for an automated waste sorting and recycling system for sustainable waste management has become ever more pressing. Deep learning has made significant advancements in image classification, making it ideally suited for waste sorting applications. This application depends on the development of a suitable deep learning model capable of accurately categorizing various categories of waste. In this study, we present RWC-Net (recyclable waste classification network), a novel deep learning model designed for the classification of six distinct waste categories using the TrashNet dataset of 2,527 images of waste. The performance of our model is subjected to intensive quantitative and qualitative evaluations and is compared to various state-of-art waste classification techniques. The proposed model outperformed several state-of-the-art models by obtaining a remarkable overall accuracy rate of 95.01 percent. In addition, it receives high F1-scores for each of the six waste categories: 97.24% for cardboard, 96.18% for glass, 94% for metal, 95.73% for paper, 93.67% for plastic, and 88.55% for litter. The reliability of the model is demonstrated qualitatively through the saliency maps generated by Score-CAM (class activation mapping) model, which provide visual insights into its performance across various waste categories. These results highlight the model's accuracy and demonstrate its potential as an effective automated waste classification and management solution.","['Deep learning', 'Score-CAM (class activation mapping)']","The increasing waste problem resulting from industrialization and modernization has created an urgent need for an automated waste sorting and recycling solution to support sustainable waste management. Efficient classification of various waste categories is essential to improve recycling processes and reduce environmental impact. The primary aim of this study is to develop a method capable of accurately categorizing six distinct types of recyclable waste, thereby enhancing the effectiveness of waste sorting. This approach seeks to achieve high accuracy in identifying different waste materials to facilitate better recycling and waste management practices.","The increasing waste problem resulting from industrialization and modernization has created an urgent need for improved waste sorting and recycling solutions to support sustainable waste management. Efficient classification of various waste categories is essential to improve recycling processes and reduce environmental impact. The primary aim of this study is to develop a method capable of accurately categorizing six distinct types of recyclable waste, thereby enhancing the effectiveness of waste sorting. This approach seeks to achieve high accuracy in identifying different waste materials to facilitate better recycling and waste management practices.",True
Engineering,Learning optimal inter-class margin adaptively for few-shot class-incremental learning via neural collapse-based meta-learning,"Few-Shot Class-Incremental Learning (FSCIL) aims to learn new classes incrementally with a limited number of samples per class. It faces issues of forgetting previously learned classes and overfitting on few-shot classes. An efficient strategy is to learn features that are discriminative in both base and incremental sessions. Current methods improve discriminability by manually designing inter-class margins based on empirical observations, which can be suboptimal. The emerging Neural Collapse (NC) theory provides a theoretically optimal inter-class margin for classification, serving as a basis for adaptively computing the margin. Yet, it is designed for closed, balanced data, not for sequential or few-shot imbalanced data. To address this gap, we propose a Meta-learning- and NC-based FSCIL method, MetaNC-FSCIL, to compute the optimal margin adaptively and maintain it at each incremental session. Specifically, we first compute the theoretically optimal margin based on the NC theory. Then we introduce a novel loss function to ensure that the loss value is minimized precisely when the inter-class margin reaches its theoretically best. Motivated by the intuition that ""learn how to preserve the margin"" matches the meta-learning's goal of ""learn how to learn"", we embed the loss function in base-session meta-training to preserve the margin for future meta-testing sessions. Experimental results demonstrate the effectiveness of MetaNC-FSCIL, achieving superior performance on multiple datasets. The code is available at https://github.com/qihangran/metaNC-FSCIL.","['Few-Shot Class-Incremental Learning (FSCIL)', 'Neural Collapse (NC) theory', 'Meta-learning']","The research addresses the challenge of incrementally learning new categories with very limited samples while preventing the loss of knowledge about previously learned categories and avoiding overfitting on the new, scarce data. It highlights the difficulty in maintaining discriminative features across both initial and subsequent learning phases, especially when existing approaches rely on manually set class separation margins that may not be optimal. The primary objective of the study is to develop a strategy that adaptively computes and preserves an optimal separation margin between classes during each incremental learning phase, ensuring consistent discriminability as new categories are introduced. This approach aims to maintain the best possible class separation throughout the learning process to improve overall classification performance in scenarios with limited and sequentially introduced data.","The research addresses the challenge of incrementally learning new categories with very limited samples while preventing the loss of knowledge about previously learned categories and avoiding inaccurate generalizations on the new, scarce data. It highlights the difficulty in maintaining discriminative features across both initial and subsequent learning phases, especially when existing approaches rely on manually set class separation margins that may not be optimal. The primary objective of the study is to develop a strategy that adaptively computes and preserves an optimal separation margin between classes during each incremental learning phase, ensuring consistent discriminability as new categories are introduced. This approach aims to maintain the best possible class separation throughout the learning process to improve overall categorization performance in scenarios with limited and sequentially introduced data.",True
Engineering,Characterizing land use/land cover change dynamics by an enhanced random forest machine learning model: a Google Earth Engine implementation,"Abstract Land use and land cover (LULC) analysis is crucial for understanding societal development and assessing changes during the Anthropocene era. Conventional LULC mapping faces challenges in capturing changes under cloud cover and limited ground truth data. To enhance the accuracy and comprehensiveness of the descriptions of LULC changes, this investigation employed a combination of advanced techniques. Specifically, multitemporal 30 m resolution Landsat-8 satellite imagery was utilized, in addition to the cloud computing capabilities of the Google Earth Engine (GEE) platform. Additionally, the study incorporated the random forest (RF) algorithm. This study aimed to generate continuous LULC maps for 2014 and 2020 for the Shrirampur area of Maharashtra, India. A novel multiple composite RF approach based on LULC classification was utilized to generate the final LULC classification maps utilizing the RF-50 and RF-100 tree models. Both RF models utilized seven input bands (B1 to B7) as the dataset for LULC classification. By incorporating these bands, the models were able to influence the spectral information captured by each band to classify the LULC categories accurately. The inclusion of multiple bands enhanced the discrimination capabilities of the classifiers, increasing the comprehensiveness of the assessment of the LULC classes. The analysis indicated that RF-100 exhibited higher training and validation/testing accuracy for 2014 and 2020 (0.99 and 0.79/0.80, respectively). The study further revealed that agricultural land, built-up land, and water bodies have changed adequately and have undergone substantial variation among the LULC classes in the study area. Overall, this research provides novel insights into the application of machine learning (ML) models for LULC mapping and emphasizes the importance of selecting the optimal tree combination for enhancing the accuracy and reliability of LULC maps based on the GEE and different RF tree models. The present investigation further enabled the interpretation of pixel-level LULC interactions while improving image classification accuracy and suggested the best models for the classification of LULC maps through the identification of changes in LULC classes.",['random forest (RF) algorithm'],"The study addresses the challenge of accurately mapping land use and land cover (LULC) changes, which is essential for understanding societal development and environmental transformations during the Anthropocene era. Conventional LULC mapping methods face difficulties in capturing changes due to factors such as cloud cover and limited availability of ground truth data, which affect the comprehensiveness and precision of LULC assessments. The primary objective of the research is to generate continuous and accurate LULC maps for the years 2014 and 2020 in the Shrirampur area of Maharashtra, India. This involves improving the classification and discrimination of various LULC categories, such as agricultural land, built-up areas, and water bodies, to better identify and interpret the substantial variations and changes occurring within the study region.","The study addresses the challenge of accurately mapping land use and land cover (LULC) changes, which is essential for understanding societal development and environmental transformations during the Anthropocene era. Conventional LULC mapping methods face difficulties in capturing changes due to factors such as cloud cover and limited availability of ground truth data, which affect the comprehensiveness and precision of LULC assessments. The primary objective of the research is to generate continuous and accurate LULC maps for the years 2014 and 2020 in the Shrirampur area of Maharashtra, India. This involves improving the identification and discrimination of various LULC categories, such as agricultural land, built-up areas, and water bodies, to better identify and interpret the substantial variations and changes occurring within the study region.",True
Engineering,Seeking in Ride-on-Demand Service: A Reinforcement Learning Model With Dynamic Price Prediction,"Recent years witness the increasing popularity of ride-on-demand (RoD) services such as Uber and Didi. Compared with traditional taxi, RoD service is more ""data-driven"" and adopts dynamic pricing to manipulate the supply and demand in real time. Dynamic price could be viewed as an accurate and quantitative indicator of the supply and demand, and could provide clues to drivers, passengers, and the service providers, possibly reshaping the ways in which some problems are solved. In this paper, we focus on the seeking route recommendation problem that aims at increasing driver revenue by recommending highly profitable seeking routes to drivers of vacant cars with the help of dynamic prices. We first justify our motivation by showing the importance of route recommendation and answering why it is necessary to consider dynamic prices, based on the analysis of real service data. We then design a dynamic price prediction model to generate the dynamic prices at any given time and location based on multi-source urban data. After that, a reinforcement learning model is adopted to perform seeking route recommendation based on predicted dynamic prices. We conduct extensive experiments in different spatio-temporal combinations and make comparisons with multiple baselines. Results first show that our dynamic price prediction model achieves an accuracy ranging from 83.82% to 90.67% under different settings. It also proves that considering the real-time predicted dynamic prices significantly increases driver revenue by, for example, 12% and 47.5% during weekday evening rush hours, than merely using the average prices or completely ignoring dynamic prices.",['reinforcement learning model'],"The research idea addresses the growing use of ride-on-demand services and the role of dynamic pricing as a precise and quantitative indicator of supply and demand, which influences drivers, passengers, and service providers. This dynamic pricing mechanism has the potential to reshape how certain operational challenges, such as route selection for vacant vehicles, are approached in order to enhance efficiency and profitability. The study’s primary objective is to increase driver revenue by recommending highly profitable seeking routes to drivers of vacant cars through the consideration of dynamic prices. It aims to demonstrate the importance of route recommendation in the context of dynamic pricing and to validate that incorporating real-time price variations can significantly improve driver earnings compared to using average or static pricing information.","The research idea addresses the growing use of ride-on-demand services and the role of dynamic pricing as a precise and quantitative indicator of supply and demand, which influences drivers, passengers, and service providers. This dynamic pricing mechanism has the potential to reshape how certain operational challenges, such as route selection for vacant vehicles, are approached in order to enhance efficiency and profitability. The study's primary objective is to increase driver revenue by recommending highly profitable seeking routes to drivers of vacant cars through the consideration of dynamic prices. It aims to demonstrate the importance of route recommendation in the context of dynamic pricing and to validate that incorporating real-time price variations can significantly improve driver earnings compared to using average or static pricing information.",True
Engineering,Data oversampling and imbalanced datasets: an investigation of performance for machine learning and feature engineering,"Abstract The classification of imbalanced datasets is a prominent task in text mining and machine learning. The number of samples in each class is not uniformly distributed; one class contains a large number of samples while the other has a small number. Overfitting of the model occurs as a result of imbalanced datasets, resulting in poor performance. In this study, we compare different oversampling techniques like synthetic minority oversampling technique (SMOTE), support vector machine SMOTE (SVM-SMOTE), Border-line SMOTE, K-means SMOTE, and adaptive synthetic (ADASYN) oversampling to address the issue of imbalanced datasets and enhance the performance of machine learning models. Preprocessing significantly enhances the quality of input data by reducing noise, redundant data, and unnecessary data. This enables the machines to identify crucial patterns that facilitate the extraction of significant and pertinent information from the preprocessed data. This study preprocesses the data using various top-level preprocessing steps. Furthermore, two imbalanced Twitter datasets are used to compare the performance of oversampling techniques with six machine learning models including random forest (RF), SVM, K-nearest neighbor (KNN), AdaBoost (ADA), logistic regression (LR), and decision tree (DT). In addition, the bag of words (BoW) and term frequency and inverse document frequency (TF-IDF) features extraction approaches are used to extract features from the tweets. The experiments indicate that SMOTE and ADASYN perform much better than other techniques thus providing higher accuracy. Additionally, overall results show that SVM with ’linear’ kernel tends to attain the highest accuracy and recall score of 99.67% and 1.00% on ADASYN oversampled datasets and 99.57% accuracy on SMOTE oversampled dataset with TF-IDF features. The SVM model using 10-fold cross-validation experiments achieved 97.40 mean accuracy with a 0.008 standard deviation. Our approach achieved 2.62% greater accuracy as compared to other current methods.","['synthetic minority oversampling technique (SMOTE)', 'Border-line SMOTE', 'K-means SMOTE', 'adaptive synthetic (ADASYN) oversampling', 'random forest (RF)', 'SVM', 'K-nearest neighbor (KNN)', 'AdaBoost (ADA)', 'logistic regression (LR)', 'decision tree (DT)']","The research idea addresses the challenge of imbalanced datasets, where the number of samples in each class is unevenly distributed, leading to overfitting and poor performance in classification tasks. This imbalance affects the ability to accurately identify important patterns and extract relevant information from the data. The study aims to improve the quality of input data by applying preprocessing steps that reduce noise and redundancy, thereby enhancing the overall effectiveness of classification. The primary objective of the study is to compare various oversampling techniques to mitigate the issue of imbalanced datasets and improve classification performance. The research specifically evaluates these techniques on imbalanced Twitter datasets to determine which approach yields higher accuracy and better results in handling class imbalance.","The research idea addresses the challenge of imbalanced datasets, where the number of samples in each class is unevenly distributed, leading to distorted results and poor performance in categorization tasks. This imbalance affects the ability to accurately identify important patterns and extract relevant information from the data. The study aims to improve the quality of input data by applying preprocessing steps that reduce noise and redundancy, thereby enhancing the overall effectiveness of categorization. The primary objective of the study is to compare various oversampling techniques to mitigate the issue of imbalanced datasets and improve categorization performance. The research specifically evaluates these techniques on imbalanced Twitter datasets to determine which approach yields higher accuracy and better results in handling class imbalance.",True
Engineering,Making Sense of Machine Learning: A Review of Interpretation Techniques and Their Applications,"Transparency in AI models is essential for promoting human–AI collaboration and ensuring regulatory compliance. However, interpreting these models is a complex process influenced by various methods and datasets. This study presents a comprehensive overview of foundational interpretation techniques, meticulously referencing the original authors and emphasizing their pivotal contributions. Recognizing the seminal work of these pioneers is imperative for contextualizing the evolutionary trajectory of interpretation in the field of AI. Furthermore, this research offers a retrospective analysis of interpretation techniques, critically evaluating their inherent strengths and limitations. We categorize these techniques into model-based, representation-based, post hoc, and hybrid methods, delving into their diverse applications. Furthermore, we analyze publication trends over time to see how the adoption of advanced computational methods within various categories of interpretation techniques has shaped the development of AI interpretability over time. This analysis highlights a notable preference shift towards data-driven approaches in the field. Moreover, we consider crucial factors such as the suitability of these techniques for generating local or global insights and their compatibility with different data types, including images, text, and tabular data. This structured categorization serves as a guide for practitioners navigating the landscape of interpretation techniques in AI. In summary, this review not only synthesizes various interpretation techniques but also acknowledges the contributions of their original authors. By emphasizing the origins of these techniques, we aim to enhance AI model explainability and underscore the importance of recognizing biases, uncertainties, and limitations inherent in the methods and datasets. This approach promotes the ethical and practical use of interpretation insights, empowering AI practitioners, researchers, and professionals to make informed decisions when selecting techniques for responsible AI implementation in real-world scenarios.","['model-based methods', 'representation-based methods', 'hybrid methods']","The research addresses the challenge of understanding and interpreting complex models to promote effective collaboration and ensure compliance with regulations. It highlights the importance of recognizing foundational contributions to the development of interpretation techniques and the need to critically evaluate their strengths, limitations, and applicability across different types of data. The study aims to provide a comprehensive overview and structured categorization of various interpretation techniques, emphasizing their evolution and practical relevance. By doing so, it seeks to guide practitioners in selecting appropriate methods for generating meaningful insights while acknowledging inherent biases and uncertainties, ultimately supporting responsible and ethical application in engineering contexts.","The research addresses the challenge of understanding and interpreting complex systems to promote effective collaboration and ensure compliance with regulations. It highlights the importance of recognizing foundational contributions to the development of interpretation techniques and the need to critically evaluate their strengths, limitations, and applicability across different types of data. The study aims to provide a comprehensive overview and structured categorization of various interpretation techniques, emphasizing their evolution and practical relevance. By doing so, it seeks to guide practitioners in selecting appropriate methods for generating meaningful insights while acknowledging inherent biases and uncertainties, ultimately supporting responsible and ethical application in engineering contexts.",True
Engineering,Advanced Modelling of Soil Organic Carbon Content in Coal Mining Areas Using Integrated Spectral Analysis: A Dengcao Coal Mine Case Study,"Effective modelling and integrated spectral analysis approaches can advance modelling precision. To develop an integrated spectral forecast modelling of soil organic carbon (SOC), this research investigated a mining coal in Dengcao Coal Mine Area, Zhengzhou. The study utilizes the Lasso and Ranger algorithms were utilized in spectral band analysis. Four primary models employed during this process include Artificial Neural Network (ANN), Support Vector Machine, Random Forest (RF), and Partial Least Squares Regression (PLSR). The ideal model was chosen. The results showed that, in contrast to when band collection was based on Lasso algorithm modelling, model precision was higher when it was based on the Ranger algorithm. ANN model had an ideal goodness acceptance, and the modelling developed by RF showed the steadiest modelling consequences. Based on the results, a distinct method is proposed in this study for band assortment at the earlier stage of integrated spectral modelling of SOC. The Ranger method can be used to check the spectral particles, and RF or ANN can be chosen to develop the prediction modelling based on different statistics sets, which is appropriate to create the prediction modelling of SOC content in Dengcao Coal Mine Area. This research avails a position for the integrated spectral of Analysis for Advanced Modelling of Soil Organic Carbon Content in Coal Sources alongside a theoretical foundation for innovating portable device for the integrated spectral assessment of SOC content in coal mining habitats. This study might be significant for the changing modelling and monitoring of SOC in mining and environmental areas.","['Lasso', 'Artificial Neural Network (ANN)', 'Support Vector Machine', 'Random Forest (RF)']","The research addresses the challenge of improving the precision of soil organic carbon (SOC) content estimation in coal mining areas through integrated spectral analysis. Accurate modelling of SOC is crucial for monitoring and managing soil quality in mining environments, which has implications for environmental sustainability and resource management. The study focuses on the Dengcao Coal Mine Area in Zhengzhou, where effective spectral forecasting of SOC can enhance understanding and control of soil conditions affected by mining activities. The primary aim of the study is to develop an integrated spectral forecast approach for SOC content by investigating and comparing different spectral band selection methods and modelling techniques to identify the most precise and reliable approach for SOC prediction in the coal mining context. This objective includes proposing a method for early-stage spectral band selection and establishing a theoretical foundation for future development of portable devices for SOC assessment in mining habitats.","The research addresses the challenge of improving the precision of soil organic carbon (SOC) content estimation in coal mining areas through integrated spectral analysis. Accurate estimation of SOC is crucial for monitoring and managing soil quality in mining environments, which has implications for environmental sustainability and resource management. The study focuses on the Dengcao Coal Mine Area in Zhengzhou, where effective spectral forecasting of SOC can enhance understanding and control of soil conditions affected by mining activities. The primary aim of the study is to develop an integrated spectral forecast approach for SOC content by investigating and comparing different spectral band selection methods and analytical techniques to identify the most precise and reliable approach for SOC prediction in the coal mining context. This objective includes proposing a method for early-stage spectral band selection and establishing a theoretical foundation for future development of portable devices for SOC assessment in mining habitats.",True
Engineering,Autopilot control unmanned aerial vehicle system for sewage defect detection using deep learning,"Abstract This work proposes the use of an unmanned aerial vehicle (UAV) with an autopilot to identify the defects present in municipal sewerage pipes. The framework also includes an effective autopilot control mechanism that can direct the flight path of a UAV within a sewer line. Both of these breakthroughs have been addressed throughout this work. The UAV's camera proved useful throughout a sewage inspection, providing important contextual data that helped analyze the sewerage line's internal condition. A plethora of information useful for understanding the sewerage line's inner functioning and extracting interior visual details can be obtained from camera‐recorded sewerage imagery if a defect is present. In the case of sewerage inspections, nevertheless, the impact of a false negative is significantly higher than that of a false positive. One of the trickiest parts of the procedure is identifying defective sewerage pipelines and false negatives. In order to get rid of the false negative outcome or false positive outcome, a guided image filter (GIF) is implemented in this proposed method during the pre‐processing stage. Afterwards, the algorithms Gabor transform (GT) and stroke width transform (SWT) were used to obtain the features of the UAV‐captured surveillance image. The UAV camera's sewerage image is then classified as “defective” or “not defective” using the obtained features by a Weighted Naive Bayes Classifier (WNBC). Next, images of the sewerage lines captured by the UAV are analyzed using speed‐up robust features (SURF) and deep learning to identify different types of defects. As a result, the proposed methodology achieved more favorable outcomes than prior existing approaches in terms of the following metrics: mean PSNR (71.854), mean MSE (0.0618), mean RMSE (0.2485), mean SSIM (98.71%), mean accuracy (98.372), mean specificity (97.837%), mean precision (93.296%), mean recall (94.255%), mean F1‐score (93.773%), and mean processing time (35.43 min).","['Weighted Naive Bayes Classifier (WNBC)', 'deep learning']","The research addresses the challenge of identifying defects in municipal sewerage pipes, emphasizing the importance of accurately detecting damaged sections to maintain the integrity and functionality of sewer systems. It highlights the difficulty in minimizing false negatives during sewer inspections, as overlooking defects can lead to significant issues in sewer maintenance and operation. The primary objective of the study is to develop a method for inspecting sewerage lines using an unmanned aerial vehicle equipped with a camera to capture internal images of the pipes. This approach aims to accurately classify sewerage pipe conditions as defective or not defective, thereby improving the detection of various types of defects within the sewer infrastructure.","The research addresses the challenge of identifying defects in municipal sewerage pipes, emphasizing the importance of accurately detecting damaged sections to maintain the integrity and functionality of sewer systems. It highlights the difficulty in minimizing false negatives during sewer inspections, as overlooking defects can lead to significant issues in sewer maintenance and operation. The primary objective of the study is to develop a method for inspecting sewerage lines using an unmanned aerial vehicle equipped with a camera to capture internal images of the pipes. This approach aims to accurately categorize sewerage pipe conditions as defective or not defective, thereby improving the detection of various types of defects within the sewer infrastructure.",True
Engineering,DEA-Net: Single Image Dehazing Based on Detail-Enhanced Convolution and Content-Guided Attention,"Single image dehazing is a challenging ill-posed problem which estimates latent haze-free images from observed hazy images. Some existing deep learning based methods are devoted to improving the model performance via increasing the depth or width of convolution. The learning ability of Convolutional Neural Network (CNN) structure is still under-explored. In this paper, a Detail-Enhanced Attention Block (DEAB) consisting of Detail-Enhanced Convolution (DEConv) and Content-Guided Attention (CGA) is proposed to boost the feature learning for improving the dehazing performance. Specifically, the DEConv contains difference convolutions which can integrate prior information to complement the vanilla one and enhance the representation capacity. Then by using the re-parameterization technique, DEConv is equivalently converted into a vanilla convolution to reduce parameters and computational cost. By assigning the unique Spatial Importance Map (SIM) to every channel, CGA can attend more useful information encoded in features. In addition, a CGA-based mixup fusion scheme is presented to effectively fuse the features and aid the gradient flow. By combining above mentioned components, we propose our Detail-Enhanced Attention Network (DEA-Net) for recovering high-quality haze-free images. Extensive experimental results demonstrate the effectiveness of our DEA-Net, outperforming the state-of-the-art (SOTA) methods by boosting the PSNR index over 41 dB with only 3.653 M parameters. (The source code of our DEA-Net is available at https://github.com/cecret3350/DEA-Net.).","['Convolutional Neural Network (CNN)', 're-parameterization technique']","The research idea addresses the challenging problem of recovering clear, haze-free images from single hazy images, which is an ill-posed issue in image restoration. Existing approaches have focused on enhancing performance by increasing the complexity of convolution operations, but the potential of improving feature learning within the convolutional structure itself remains insufficiently explored. The study aims to improve the quality of dehazed images by enhancing the representation and extraction of important image details. The primary objective of the study is to develop a novel approach that enhances feature learning to improve image dehazing performance, ultimately recovering high-quality haze-free images with greater accuracy and efficiency. This involves integrating prior information to strengthen feature representation and effectively combining features to aid the restoration process, resulting in superior image clarity compared to existing methods.","The research idea addresses the challenging problem of recovering clear, haze-free images from single hazy images, which is an ill-posed issue in image restoration. Existing approaches have focused on enhancing performance by increasing the complexity of operations, but the potential of improving feature analysis within the structural framework itself remains insufficiently explored. The study aims to improve the quality of dehazed images by enhancing the representation and extraction of important image details. The primary objective of the study is to develop a novel approach that enhances information processing to improve image dehazing performance, ultimately recovering high-quality haze-free images with greater accuracy and efficiency. This involves integrating prior information to strengthen visual representation and effectively combining image elements to aid the restoration process, resulting in superior image clarity compared to existing methods.",True
Engineering,TransUNet: Rethinking the U-Net architecture design for medical image segmentation through the lens of transformers,"Medical image segmentation is crucial for healthcare, yet convolution-based methods like U-Net face limitations in modeling long-range dependencies. To address this, Transformers designed for sequence-to-sequence predictions have been integrated into medical image segmentation. However, a comprehensive understanding of Transformers' self-attention in U-Net components is lacking. TransUNet, first introduced in 2021, is widely recognized as one of the first models to integrate Transformer into medical image analysis. In this study, we present the versatile framework of TransUNet that encapsulates Transformers' self-attention into two key modules: (1) a Transformer encoder tokenizing image patches from a convolution neural network (CNN) feature map, facilitating global context extraction, and (2) a Transformer decoder refining candidate regions through cross-attention between proposals and U-Net features. These modules can be flexibly inserted into the U-Net backbone, resulting in three configurations: Encoder-only, Decoder-only, and Encoder+Decoder. TransUNet provides a library encompassing both 2D and 3D implementations, enabling users to easily tailor the chosen architecture. Our findings highlight the encoder's efficacy in modeling interactions among multiple abdominal organs and the decoder's strength in handling small targets like tumors. It excels in diverse medical applications, such as multi-organ segmentation, pancreatic tumor segmentation, and hepatic vessel segmentation. Notably, our TransUNet achieves a significant average Dice improvement of 1.06% and 4.30% for multi-organ segmentation and pancreatic tumor segmentation, respectively, when compared to the highly competitive nn-UNet, and surpasses the top-1 solution in the BrasTS2021 challenge. 2D/3D Code and models are available at https://github.com/Beckschen/TransUNet and https://github.com/Beckschen/TransUNet-3D, respectively.","['U-Net', 'Transformers', 'TransUNet', 'Transformer encoder', 'Transformer decoder', 'convolution neural network (CNN)', 'nn-UNet']","The research addresses the challenge of improving medical image segmentation, which is essential for healthcare, by overcoming the limitations of existing convolution-based methods in capturing long-range dependencies within images. There is a need for a better understanding of how self-attention mechanisms can be effectively incorporated into key components of segmentation frameworks to enhance performance in identifying complex anatomical structures and small targets such as tumors. The primary objective of the study is to present a versatile framework that integrates self-attention mechanisms into different modules of a segmentation architecture, allowing flexible configurations to improve the extraction of global context and refinement of candidate regions. This framework aims to enhance segmentation accuracy across various medical applications, including multi-organ segmentation, pancreatic tumor segmentation, and hepatic vessel segmentation, demonstrating measurable improvements over existing competitive approaches.","The research addresses the challenge of improving medical image segmentation, which is essential for healthcare, by overcoming the limitations of existing methods in capturing long-range relationships within images. There is a need for a better understanding of how attention-focusing techniques can be effectively incorporated into key components of segmentation frameworks to enhance performance in identifying complex anatomical structures and small targets such as tumors. The primary objective of the study is to present a versatile framework that integrates contextual awareness mechanisms into different modules of a segmentation architecture, allowing flexible configurations to improve the extraction of global context and refinement of candidate regions. This framework aims to enhance segmentation accuracy across various medical applications, including multi-organ segmentation, pancreatic tumor segmentation, and hepatic vessel segmentation, demonstrating measurable improvements over existing competitive approaches.",True
Engineering,Fusion of finite element and machine learning methods to predict rock shear strength parameters,"Abstract The trial-and-error method for calibrating rock mechanics parameters has the disadvantages of complexity, being time-consuming, and difficulty in ensuring accuracy. Harnessing the repeatability and scalability intrinsic to numerical simulation calculations and amalgamating them with the data-driven attributes of machine learning methods, this study uses the finite element analysis software RS2 to establish 252 sets of sandstone sample data. The recursive feature elimination and cross-validation method is employed for feature selection. The shear strength parameters of sandstone are predicted using machine learning models optimized by the particle swarm optimization (PSO) algorithm, including the backpropagation neural network, Bayesian ridge regression, support vector regression (SVR), and light gradient boosting machine. The predicted value of cohesion is proposed as the input feature to predict the friction angle. The results indicate that the optimal input characteristics for predicting cohesion are elastic modulus, Poisson's ratio, peak stress, and peak strain, while the optimal input characteristics for predicting friction angle are peak stress and cohesion. The PSO-SVR model demonstrates the best performance. The maximum error between the predicted values of cohesion and friction angle and the calculated results of RSData program are 3.5% and 4.31%, respectively. The finite element calculation is in good agreement with the stress–strain curve obtained in the laboratory. The sensitivity analysis indicates that SVR's prediction performance for cohesion and friction angle tends to be stable when the sample size is &amp;gt;25. These results offer a valuable reference for accurately predicting rock mechanics parameters.","['recursive feature elimination', 'backpropagation neural network', 'Bayesian ridge regression', 'support vector regression (SVR)', 'light gradient boosting machine']","The study addresses the challenges associated with the traditional trial-and-error method for calibrating rock mechanics parameters, which is complex, time-consuming, and difficult to ensure accuracy. There is a need for a more efficient and reliable approach to determine the shear strength parameters of sandstone, which are critical for understanding its mechanical behavior. The primary aim of the study is to accurately predict the shear strength parameters of sandstone, specifically cohesion and friction angle, by identifying optimal input characteristics and validating the predictions against finite element calculations and laboratory stress–strain curves. This objective seeks to provide a valuable reference for improving the precision and efficiency of rock mechanics parameter estimation.","The study addresses the challenges associated with the traditional trial-and-error method for calibrating rock mechanics parameters, which is complex, time-consuming, and difficult to ensure accuracy. There is a need for a more efficient and reliable approach to determine the shear strength parameters of sandstone, which are critical for understanding its mechanical behavior. The primary aim of the study is to accurately determine the shear strength parameters of sandstone, specifically cohesion and friction angle, by identifying optimal input characteristics and validating the results against finite element calculations and laboratory stress–strain curves. This objective seeks to provide a valuable reference for improving the precision and efficiency of rock mechanics parameter estimation.",True
Engineering,"Generative AI for Transformative Healthcare: A Comprehensive Study of Emerging Models, Applications, Case Studies, and Limitations","Generative artificial intelligence (GAI) can be broadly described as an artificial intelligence system capable of generating images, text, and other media types with human prompts. GAI models like ChatGPT, DALL-E, and Bard have recently caught the attention of industry and academia equally. GAI applications span various industries like art, gaming, fashion, and healthcare. In healthcare, GAI shows promise in medical research, diagnosis, treatment, and patient care and is already making strides in real-world deployments. There has yet to be any detailed study concerning the applications and scope of GAI in healthcare. Addressing this research gap, we explore several applications, real-world scenarios, and limitations of GAI in healthcare. We examine how GAI models like ChatGPT and DALL-E can be leveraged to aid in the applications of medical imaging, drug discovery, personalized patient treatment, medical simulation and training, clinical trial optimization, mental health support, healthcare operations and research, medical chatbots, human movement simulation, and a few more applications. Along with applications, we cover four real-world healthcare scenarios that employ GAI: visual snow syndrome diagnosis, molecular drug optimization, medical education, and dentistry. We also provide an elaborate discussion on seven healthcare-customized LLMs like Med-PaLM, BioGPT, DeepHealth, etc.,Since GAI is still evolving, it poses challenges like the lack of professional expertise in decision making, risk of patient data privacy, issues in integrating with existing healthcare systems, and the problem of data bias which are elaborated on in this work along with several other challenges. We also put forward multiple directions for future research in GAI for healthcare.","['healthcare-customized LLMs like Med-PaLM', 'healthcare-customized LLMs like BioGPT']","The research idea centers on the emerging role and potential of generative artificial intelligence in healthcare, highlighting its applications across medical research, diagnosis, treatment, and patient care. Despite its growing use, there is a lack of detailed studies exploring the full scope and practical implications of these technologies within healthcare settings. The study aims to address this gap by investigating various applications and real-world scenarios where generative AI contributes to fields such as medical imaging, drug discovery, personalized treatment, medical simulation and training, clinical trial optimization, mental health support, healthcare operations, and human movement simulation. The primary objective of the study is to explore and evaluate the applications, real-world use cases, and limitations of generative AI in healthcare, while also discussing challenges related to professional expertise, patient data privacy, system integration, and data bias, and proposing directions for future research in this area.","The research idea centers on the emerging role and potential of innovative computational technologies in healthcare, highlighting their applications across medical research, diagnosis, treatment, and patient care. Despite their growing use, there is a lack of detailed studies exploring the full scope and practical implications of these technologies within healthcare settings. The study aims to address this gap by investigating various applications and real-world scenarios where advanced generative technologies contribute to fields such as medical imaging, drug discovery, personalized treatment, medical simulation and training, clinical trial optimization, mental health support, healthcare operations, and human movement simulation. The primary objective of the study is to explore and evaluate the applications, real-world use cases, and limitations of these emerging technologies in healthcare, while also discussing challenges related to professional expertise, patient data privacy, system integration, and data bias, and proposing directions for future research in this area.",True
Engineering,The deep learning applications in IoT-based bio- and medical informatics: a systematic literature review,"Abstract Nowadays, machine learning (ML) has attained a high level of achievement in many contexts. Considering the significance of ML in medical and bioinformatics owing to its accuracy, many investigators discussed multiple solutions for developing the function of medical and bioinformatics challenges using deep learning (DL) techniques. The importance of DL in Internet of Things (IoT)-based bio- and medical informatics lies in its ability to analyze and interpret large amounts of complex and diverse data in real time, providing insights that can improve healthcare outcomes and increase efficiency in the healthcare industry. Several applications of DL in IoT-based bio- and medical informatics include diagnosis, treatment recommendation, clinical decision support, image analysis, wearable monitoring, and drug discovery. The review aims to comprehensively evaluate and synthesize the existing body of the literature on applying deep learning in the intersection of the IoT with bio- and medical informatics. In this paper, we categorized the most cutting-edge DL solutions for medical and bioinformatics issues into five categories based on the DL technique utilized: convolutional neural network , recurrent neural network , generative adversarial network , multilayer perception , and hybrid methods. A systematic literature review was applied to study each one in terms of effective properties, like the main idea, benefits, drawbacks, methods, simulation environment, and datasets. After that, cutting-edge research on DL approaches and applications for bioinformatics concerns was emphasized. In addition, several challenges that contributed to DL implementation for medical and bioinformatics have been addressed, which are predicted to motivate more studies to develop medical and bioinformatics research progressively. According to the findings, most articles are evaluated using features like accuracy, sensitivity, specificity, F -score, latency, adaptability, and scalability.","['convolutional neural network', 'recurrent neural network', 'generative adversarial network', 'hybrid methods']","The research idea centers on addressing the challenges in medical and bioinformatics by improving the ability to analyze and interpret large amounts of complex and diverse data in real time, which is crucial for enhancing healthcare outcomes and increasing efficiency in the healthcare industry. The study recognizes the significance of advanced approaches in supporting various applications such as diagnosis, treatment recommendation, clinical decision support, image analysis, wearable monitoring, and drug discovery. The primary objective of the study is to comprehensively evaluate and synthesize the existing literature on solutions applied at the intersection of the Internet of Things with bio- and medical informatics. This includes categorizing the most advanced approaches for medical and bioinformatics issues, highlighting their effective properties, benefits, drawbacks, and addressing the challenges that influence their implementation to motivate further progressive research in the field.","The research idea centers on addressing the challenges in medical and bioinformatics by improving the ability to analyze and interpret large amounts of complex and diverse data in real time, which is crucial for enhancing healthcare outcomes and increasing efficiency in the healthcare industry. The study recognizes the significance of innovative methodologies in supporting various applications such as diagnosis, treatment recommendation, clinical decision support, image analysis, wearable monitoring, and drug discovery. The primary objective of the study is to comprehensively evaluate and synthesize the existing literature on solutions applied at the intersection of the Internet of Things with bio- and medical informatics. This includes categorizing the current methodologies for medical and bioinformatics issues, highlighting their effective properties, benefits, drawbacks, and addressing the challenges that influence their implementation to motivate further progressive research in the field.",True
Engineering,Battery safety: Machine learning-based prognostics,"Lithium-ion batteries play a pivotal role in a wide range of applications, from electronic devices to large-scale electrified transportation systems and grid-scale energy storage. Nevertheless, they are vulnerable to both progressive aging and unexpected failures, which can result in catastrophic events such as explosions or fires. Given their expanding global presence, the safety of these batteries and potential hazards from serious malfunctions are now major public concerns. Over the past decade, scholars and industry experts are intensively exploring methods to monitor battery safety, spanning from materials to cell, pack and system levels and across various spectral, spatial, and temporal scopes. In this Review, we start by summarizing the mechanisms and nature of battery failures. Following this, we explore the intricacies in predicting battery system evolution and delve into the specialized knowledge essential for data-driven, machine learning models. We offer an exhaustive review spotlighting the latest strides in battery fault diagnosis and failure prognosis via an array of machine learning approaches. Our discussion encompasses: (1) supervised and reinforcement learning integrated with battery models, apt for predicting faults/failures and probing into failure causes and safety protocols at the cell level; (2) unsupervised, semi-supervised, and self-supervised learning, advantageous for harnessing vast data sets from battery modules/packs; (3) few-shot learning tailored for gleaning insights from scarce examples, alongside physics-informed machine learning to bolster model generalization and optimize training in data-scarce settings. We conclude by casting light on the prospective horizons of comprehensive, real-world battery prognostics and management.","['supervised learning', 'reinforcement learning', 'unsupervised learning', 'semi-supervised learning', 'self-supervised learning', 'few-shot learning', 'physics-informed machine learning']","The research idea centers on the critical importance of lithium-ion battery safety due to their widespread use in electronic devices, transportation systems, and energy storage, coupled with their susceptibility to aging and unexpected failures that can lead to catastrophic events such as explosions or fires. As these batteries become more prevalent globally, addressing the potential hazards arising from serious malfunctions has become a major public concern. The research objective is to comprehensively review the mechanisms and nature of battery failures and to explore the challenges involved in predicting the evolution of battery systems. The study aims to provide an extensive overview of recent advancements in diagnosing faults and forecasting failures in lithium-ion batteries, with a focus on improving safety protocols at various levels from individual cells to larger battery packs.","The research idea centers on the critical importance of lithium-ion battery safety due to their widespread use in electronic devices, transportation systems, and energy storage, coupled with their susceptibility to aging and unexpected failures that can lead to catastrophic events such as explosions or fires. As these batteries become more prevalent globally, addressing the potential hazards arising from serious malfunctions has become a major public concern. The research objective is to comprehensively review the mechanisms and nature of battery failures and to explore the challenges involved in understanding the evolution of battery systems. The study aims to provide an extensive overview of recent advancements in diagnosing faults and anticipating failures in lithium-ion batteries, with a focus on improving safety protocols at various levels from individual cells to larger battery packs.",True
Engineering,DA-TransUNet: integrating spatial and channel dual attention with transformer U-net for medical image segmentation,"Accurate medical image segmentation is critical for disease quantification and treatment evaluation. While traditional U-Net architectures and their transformer-integrated variants excel in automated segmentation tasks. Existing models also struggle with parameter efficiency and computational complexity, often due to the extensive use of Transformers. However, they lack the ability to harness the image’s intrinsic position and channel features. Research employing Dual Attention mechanisms of position and channel have not been specifically optimized for the high-detail demands of medical images. To address these issues, this study proposes a novel deep medical image segmentation framework, called DA-TransUNet, aiming to integrate the Transformer and dual attention block (DA-Block) into the traditional U-shaped architecture. Also, DA-TransUNet tailored for the high-detail requirements of medical images, optimizes the intermittent channels of Dual Attention (DA) and employs DA in each skip-connection to effectively filter out irrelevant information. This integration significantly enhances the model’s capability to extract features, thereby improving the performance of medical image segmentation. DA-TransUNet is validated in medical image segmentation tasks, consistently outperforming state-of-the-art techniques across 5 datasets. In summary, DA-TransUNet has made significant strides in medical image segmentation, offering new insights into existing techniques. It strengthens model performance from the perspective of image features, thereby advancing the development of high-precision automated medical image diagnosis. The codes and parameters of our model will be publicly available at https://github.com/SUN-1024/DA-TransUnet .","['U-Net architectures', 'Dual Attention mechanisms', 'DA-TransUNet']","The research addresses the challenge of achieving accurate medical image segmentation, which is essential for disease quantification and treatment evaluation. Existing approaches face difficulties related to parameter efficiency and computational complexity, and they do not fully utilize the intrinsic positional and channel features of medical images. The study aims to develop a segmentation framework tailored to the high-detail requirements of medical images by optimizing the use of dual attention mechanisms in the architecture. The primary objective is to enhance the capability to extract relevant image features and improve the performance of medical image segmentation, thereby advancing high-precision automated medical image diagnosis.","The research addresses the challenge of achieving accurate medical image segmentation, which is essential for disease quantification and treatment evaluation. Existing approaches face difficulties related to parameter efficiency and computational complexity, and they do not fully utilize the intrinsic positional and channel features of medical images. The study aims to develop a segmentation framework tailored to the high-detail requirements of medical images by optimizing the use of dual attention mechanisms in the architecture. The primary objective is to enhance the capability to extract relevant image features and improve the performance of medical image segmentation, thereby advancing high-precision medical image diagnosis.",True
Engineering,Mental-LLM,"Advances in large language models (LLMs) have empowered a variety of applications. However, there is still a significant gap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health. In this work, we present a comprehensive evaluation of multiple LLMs on various mental health prediction tasks via online text data, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4. We conduct a broad range of experiments, covering zero-shot prompting, few-shot prompting, and instruction fine-tuning. The results indicate a promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9% on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8%. They further perform on par with the state-of-the-art task-specific language model. We also conduct an exploratory case study on LLMs' capability on mental health reasoning tasks, illustrating the promising capability of certain models such as GPT-4. We summarize our findings into a set of action guidelines for potential methods to enhance LLMs' capability for mental health tasks. Meanwhile, we also emphasize the important limitations before achieving deployability in real-world mental health settings, such as known racial and gender bias. We highlight the important ethical risks accompanying this line of research.","['zero-shot prompting', 'few-shot prompting', 'instruction fine-tuning']","The research addresses the significant gap in understanding and improving the capabilities of advanced language technologies in the context of mental health applications. It focuses on evaluating the effectiveness of these technologies in predicting mental health conditions through online text data, highlighting the challenges and limitations in their current performance. The primary objective of the study is to comprehensively assess multiple language technologies on various mental health prediction tasks and to identify approaches that can enhance their accuracy and reliability. Additionally, the study aims to provide guidelines for improving these capabilities while acknowledging important ethical considerations and limitations related to bias and real-world deployment.","The research addresses the significant gap in understanding and improving the capabilities of advanced language analysis approaches in the context of mental health applications. It focuses on evaluating the effectiveness of these approaches in identifying mental health conditions through online text data, highlighting the challenges and limitations in their current performance. The primary objective of the study is to comprehensively assess multiple text analysis methods on various mental health identification tasks and to identify approaches that can enhance their accuracy and reliability. Additionally, the study aims to provide guidelines for improving these capabilities while acknowledging important ethical considerations and limitations related to bias and real-world deployment.",True
Engineering,FI-NPI: Exploring Optimal Control in Parallel Platform Systems,"Typically, the current and speed loop closure of servo motor of the parallel platform is accomplished with incremental PI regulation. The control method has strong robustness, but the parameter tuning process is cumbersome, and it is difficult to achieve the optimal control state. In order to further optimize the performance, this paper proposes a double-loop control structure based on fuzzy integral and neuron proportional integral (FI-NPI). The structure makes full use of the control advantages of the fuzzy controller and integrator to improve the performance of speed closed-loop control. And through the feedforward branch, the speed error is used as the teacher signal for neuron supervised learning, which improves the effect of current closed-loop control. Through comparative simulation experiments, this paper verifies that the FI-NPI controller has a faster dynamic response speed than the traditional PI controller. Finally, in this paper, the FI-NPI controller is implemented in C language in the servo-driven lower computer, and the speed closed-loop test of the BLDC motor is carried out. The experimental results show that the FI-NPI double-loop controller is better than the traditional double-PI controller in performance indicators such as convergence rate and RMSE, which confirms that the FI-NPI double-loop controller is more suitable for BLDC servo control.",['fuzzy integral'],"The research addresses the challenge of optimizing the current and speed loop closure of servo motors in parallel platforms, where the conventional incremental PI regulation method, despite its robustness, involves a cumbersome parameter tuning process and struggles to achieve optimal control performance. The study aims to enhance the speed closed-loop control performance by proposing a double-loop control structure that leverages the advantages of fuzzy integral and neuron proportional integral components. The primary objective is to improve the dynamic response and overall control effectiveness of the servo motor, specifically for brushless DC (BLDC) motors, by implementing and testing the proposed double-loop controller and demonstrating its superiority over traditional double-PI controllers in terms of convergence rate and error metrics.","The research addresses the challenge of optimizing the current and speed loop closure of servo motors in parallel platforms, where the conventional incremental PI regulation method, despite its robustness, involves a cumbersome parameter adjustment process and struggles to achieve optimal control performance. The study aims to enhance the speed closed-loop control performance by proposing a double-loop control structure that leverages the advantages of fuzzy integral and advanced proportional integral components. The primary objective is to improve the dynamic response and overall control effectiveness of the servo motor, specifically for brushless DC (BLDC) motors, by implementing and testing the proposed double-loop controller and demonstrating its superiority over traditional double-PI controllers in terms of response rate and error metrics.",True
Engineering,"A review on microgrid optimization with meta-heuristic techniques: Scopes, trends and recommendation","Microgrids (MGs) use renewable sources to meet the growing demand for energy with increasing consumer needs and technological advancement. They operate independently as small-scale energy networks using distributed energy resources. However, the intermittent nature of renewable energy sources and poor power quality are essential operational problems that must be mitigated to improve the MG's performance. To address these challenges, researchers have introduced heuristic optimization mechanisms for MGs. However, local minima and the inability to find a global minimum in heuristic methods create errors in non-linear and nonconvex optimization, posing challenges in dealing with several operational aspects of MG such as energy management optimization, cost-effective dispatch, dependability, storage sizing, cyber-attack minimization, and grid integration. These challenges affect MG's performance by adding complexity to the management of storage capacity, cost minimization, reliability assurance, and balance of renewable sources, which accelerates the need for meta-heuristic optimization algorithms (MHOAs). This paper presents a state-of-the-art review of MHOAs and their role in improving the operational performance of MGs. Firstly, the fundamentals of MG optimization are discussed to explore the scopes, requisites, and opportunities of MHOAs in MG networks. Secondly, several MHOAs in the MG domain are described, and their recent trends in MG's techno-economic analysis, load forecasting, resiliency improvement, control operation, fault diagnosis, and energy management are summarized. The summary reveals that nearly 25% of the research in these areas utilizes the particle swarm optimization method, while the genetic and grey wolf algorithms are utilized by nearly 10% and 5% of the works studied in this paper, respectively, for optimizing the MG's performance. This result summarizes that MHOA presents a system-agnostic optimization approach, offering a new avenue for enhancing the effectiveness of future MGs. Finally, we highlight some challenges that emerge during the integration of MHOAs into MGs, potentially motivating researchers to conduct further studies in this area.","['genetic algorithm', 'grey wolf algorithm']","The research idea centers on addressing the operational challenges faced by microgrids (MGs) that utilize renewable energy sources, particularly the intermittent nature of these sources and poor power quality, which hinder the overall performance of MGs. These challenges complicate the management of energy storage, cost minimization, reliability, and the balance of renewable energy integration within MGs. The study aims to explore optimization approaches that can overcome issues such as local minima and nonconvex optimization problems affecting various operational aspects like energy management, cost-effective dispatch, dependability, storage sizing, and grid integration. The primary objective of the study is to review and summarize the role of advanced optimization strategies in enhancing the operational performance of microgrids by examining their applications in techno-economic analysis, load forecasting, resiliency improvement, control operation, fault diagnosis, and energy management. Additionally, the study seeks to identify current trends and challenges in applying these optimization approaches to microgrids, thereby providing insights that could guide future research efforts in improving MG effectiveness.","The research idea centers on addressing the operational challenges faced by microgrids (MGs) that utilize renewable energy sources, particularly the intermittent nature of these sources and poor power quality, which hinder the overall performance of MGs. These challenges complicate the management of energy storage, cost minimization, reliability, and the balance of renewable energy integration within MGs. The study aims to explore optimization approaches that can overcome issues such as local minima and nonconvex optimization problems affecting various operational aspects like energy management, cost-effective dispatch, dependability, storage sizing, and grid integration. The primary objective of the study is to review and summarize the role of advanced optimization strategies in enhancing the operational performance of microgrids by examining their applications in techno-economic analysis, energy demand estimation, resiliency improvement, control operation, fault diagnosis, and energy management. Additionally, the study seeks to identify current trends and challenges in applying these optimization approaches to microgrids, thereby providing insights that could guide future research efforts in improving MG effectiveness.",True
Engineering,A novel Swin transformer approach utilizing residual multi-layer perceptron for diagnosing brain tumors in MRI images,"Abstract Serious consequences due to brain tumors necessitate a timely and accurate diagnosis. However, obstacles such as suboptimal imaging quality, issues with data integrity, varying tumor types and stages, and potential errors in interpretation hinder the achievement of precise and prompt diagnoses. The rapid identification of brain tumors plays a pivotal role in ensuring patient safety. Deep learning-based systems hold promise in aiding radiologists to make diagnoses swiftly and accurately. In this study, we present an advanced deep learning approach based on the Swin Transformer. The proposed method introduces a novel Hybrid Shifted Windows Multi-Head Self-Attention module (HSW-MSA) along with a rescaled model. This enhancement aims to improve classification accuracy, reduce memory usage, and simplify training complexity. The Residual-based MLP (ResMLP) replaces the traditional MLP in the Swin Transformer, thereby improving accuracy, training speed, and parameter efficiency. We evaluate the Proposed-Swin model on a publicly available brain MRI dataset with four classes, using only test data. Model performance is enhanced through the application of transfer learning and data augmentation techniques for efficient and robust training. The Proposed-Swin model achieves a remarkable accuracy of 99.92%, surpassing previous research and deep learning models. This underscores the effectiveness of the Swin Transformer with HSW-MSA and ResMLP improvements in brain tumor diagnosis. This method introduces an innovative diagnostic approach using HSW-MSA and ResMLP in the Swin Transformer, offering potential support to radiologists in timely and accurate brain tumor diagnosis, ultimately improving patient outcomes and reducing risks.","['Swin Transformer', 'Residual-based MLP (ResMLP)', 'transfer learning']","The research addresses the critical challenge of achieving timely and accurate diagnosis of brain tumors, which is essential due to the serious consequences associated with delayed or incorrect identification. Difficulties such as suboptimal imaging quality, variability in tumor types and stages, and potential errors in interpretation complicate the diagnostic process and hinder prompt and precise detection. The primary objective of the study is to develop an enhanced approach that improves the accuracy and efficiency of brain tumor classification, thereby supporting rapid identification. This aims to facilitate better diagnostic outcomes, ultimately improving patient safety and reducing associated risks.","The research addresses the critical challenge of achieving timely and accurate diagnosis of brain tumors, which is essential due to the serious consequences associated with delayed or incorrect identification. Difficulties such as suboptimal imaging quality, variability in tumor types and stages, and potential errors in interpretation complicate the diagnostic process and hinder prompt and precise detection. The primary objective of the study is to develop an enhanced approach that improves the accuracy and efficiency of brain tumor identification, thereby supporting rapid diagnosis. This aims to facilitate better diagnostic outcomes, ultimately improving patient safety and reducing associated risks.",True
Engineering,Machine learning-based predictive model for thermal comfort and energy optimization in smart buildings,"In the current context of energy transition and increasing climate change, optimizing building performance has become a critical objective. Efficient energy use and occupant comfort are paramount considerations in building design and operation. To address these challenges, this study introduces a predictive model leveraging Machine Learning (ML) algorithms. The model aims to predict thermal comfort levels and optimize energy consumption in Heating, Ventilation, and Air Conditioning (HVAC) systems. Four distinct ML algorithms Support Vector Machine (SVM), Artificial Neural Network (ANN), Random Forest (RF), and EXtreme Gradient Boosting (XGBOOST) are employed for this purpose. Data for the model is collected using a network of Raspberry Pi boards equipped with multiple sensors. Performance evaluation of the ML algorithms is conducted using statistical error metrics, including, Root Mean Square Error (RMSE), Mean Square Error (MSE), Mean Absolute Error (MAE), and coefficient of determination (R2). Results reveal that the RF and XGBOOST algorithms exhibit superior performance, achieving accuracies of 96.7% and 9.64% respectively. In contrast, the SVM algorithm demonstrates inferior performance with a R2 of 81.1%. These findings underscore the predictive capability of the RF and XGBOOST model in forecasting Predicted Mean Vote (PMV) values. The proposed model holds promise for enhancing occupant thermal comfort in buildings while simultaneously optimizing energy consumption in HVAC systems. Further research could explore the practical applications of these findings in building design and operation.","['Support Vector Machine (SVM)', 'Artificial Neural Network (ANN)', 'Random Forest (RF)', 'EXtreme Gradient Boosting (XGBOOST)']","The research addresses the critical need to optimize building performance amid the ongoing energy transition and the challenges posed by climate change. It emphasizes the importance of efficient energy use and maintaining occupant comfort as key factors in building design and operation. The study aims to develop a method to predict thermal comfort levels and improve energy consumption in Heating, Ventilation, and Air Conditioning (HVAC) systems. The primary objective is to enhance occupant thermal comfort in buildings while simultaneously optimizing energy consumption in HVAC systems, thereby contributing to more sustainable and efficient building management.","The research addresses the critical need to optimize building performance amid the ongoing energy transition and the challenges posed by climate change. It emphasizes the importance of efficient energy use and maintaining occupant comfort as key factors in building design and operation. The study aims to develop a method to assess thermal comfort levels and improve energy consumption in Heating, Ventilation, and Air Conditioning (HVAC) systems. The primary objective is to enhance occupant thermal comfort in buildings while simultaneously optimizing energy consumption in HVAC systems, thereby contributing to more sustainable and efficient building management.",True
Engineering,"A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions","Automating the monitoring of industrial processes has the potential to enhance efficiency and optimize quality by promptly detecting abnormal events and thus facilitating timely interventions. Deep learning, with its capacity to discern non-trivial patterns within large datasets, plays a pivotal role in this process. Standard deep learning methods are suitable to solve a specific task given a specific type of data. During training, deep learning demands large volumes of labeled data. However, due to the dynamic nature of the industrial processes and environment, it is impractical to acquire large-scale labeled data for standard deep learning training for every slightly different case anew. Deep transfer learning offers a solution to this problem. By leveraging knowledge from related tasks and accounting for variations in data distributions, the transfer learning framework solves new tasks with little or even no additional labeled data. The approach bypasses the need to retrain a model from scratch for every new setup and dramatically reduces the labeled data requirement. This survey first provides an in-depth review of deep transfer learning, examining the problem settings of transfer learning and classifying the prevailing deep transfer learning methods. Moreover, we delve into applications of deep transfer learning in the context of a broad spectrum of time series anomaly detection tasks prevalent in primary industrial domains, e.g., manufacturing process monitoring, predictive maintenance, energy management, and infrastructure facility monitoring. We discuss the challenges and limitations of deep transfer learning in industrial contexts and conclude the survey with practical directions and actionable suggestions to address the need to leverage diverse time series data for anomaly detection in an increasingly dynamic production environment.","['deep learning', 'deep transfer learning', 'transfer learning framework']","The research addresses the challenge of monitoring industrial processes efficiently to enhance operational quality by promptly detecting abnormal events, which enables timely interventions. A significant problem is the impracticality of acquiring large volumes of labeled data for every variation in dynamic industrial environments, making traditional approaches less effective for new or slightly different cases. The primary aim of the study is to explore methods that reduce the need for extensive labeled data by leveraging knowledge from related tasks and adapting to variations in data distributions, thereby facilitating anomaly detection across diverse industrial applications such as manufacturing process monitoring, predictive maintenance, energy management, and infrastructure facility monitoring. The study also seeks to identify challenges and limitations within these approaches and provide practical recommendations to improve anomaly detection in dynamic production settings.","The research addresses the challenge of monitoring industrial processes efficiently to enhance operational quality by promptly detecting abnormal events, which enables timely interventions. A significant problem is the impracticality of acquiring large volumes of labeled data for every variation in dynamic industrial environments, making conventional approaches less effective for new or slightly different cases. The primary aim of the study is to explore methods that reduce the need for extensive labeled data by utilizing knowledge from related tasks and adapting to variations in data distributions, thereby facilitating anomaly detection across diverse industrial applications such as manufacturing process monitoring, predictive maintenance, energy management, and infrastructure facility monitoring. The study also seeks to identify challenges and limitations within these approaches and provide practical recommendations to improve anomaly detection in dynamic production settings.",True
Engineering,A novel and dynamic land use/cover change research framework based on an improved PLUS model and a fuzzy multiobjective programming model,"Spatial reconstruction and scenario simulation of historical processes and future trends of land use/cover change (LUCC) can help to reveal the historical background of land conversion and the spatial distribution of future land. Moreover, there is a close relationship between the spatiotemporal dynamics of land use/cover and changes in different ecosystem services (ESs). Using this relationship to simulate future land use scenarios is important. In this study, an LUCC dynamic analysis framework (LSTM-PLUS-FMOP) was constructed based on a deep learning time series forecasting model (LSTM), a parallelized urban land use simulation (PLUS) model and a fuzzy multiobjective programming (FMOP) model. The PLUS model was used to analyze the driving mechanism of land expansion and explore the land conversion pattern. In addition, three land conversion scenarios were established: natural land expansion (NLE), economic development priority (EDP) and regional sustainable development (RSD). The FMOP model and the relationship between LUCC and ES were used to perform a spatial simulation of land conversion. The uncertainty parameters in the model were treated by intuitionistic fuzzy numbers (IFSs). This study applied the constructed framework to the Yellow River Basin of Shaanxi Province (YRB-SX). The results showed that (1) from 2000 to 2020, the cropland area of the YRB-SX continuously decreased by 12.67 × 104 ha, while the built-up area continuously increased by 28.25 × 104 ha. The net reduction in woodland and grassland area was 13.90 × 104 ha. (2) The relative error range of land prediction using the LSTM model was 0.0003– 0.0042. This model had better accuracy than the Markov chain prediction model. (3) The cropland area decreased by 0.26% (NLE), 0.85% (EDP) and 1.68% (RSD) under the three scenarios. The built-up area increased by 25.01%, 32.76% and 14.72%, respectively. The RSD scenario followed the principles of ecological protection and spatial constraints, which mitigated the degradation of the ecosystem to some extent. This coupled simulation framework will help to obtain land allocation schemes that meet the requirements of ecological protection and provide solutions for rational land management.",['LSTM'],"The research addresses the challenge of understanding and simulating the spatial and temporal changes in land use and land cover, which are closely linked to variations in ecosystem services. It emphasizes the importance of reconstructing historical land conversion processes and projecting future land distribution to reveal the underlying mechanisms driving land expansion and conversion patterns. The study recognizes the need to explore different land conversion scenarios to support sustainable land management and ecological protection.

The primary objective of the study is to develop and apply a dynamic analysis framework for land use and land cover change in the Yellow River Basin of Shaanxi Province. This framework aims to analyze the driving forces behind land expansion, establish multiple land conversion scenarios, and perform spatial simulations of land conversion to assess their impacts on ecosystem services. The goal is to provide land allocation schemes that balance ecological protection with regional development needs, thereby offering practical solutions for rational land management.","The research addresses the challenge of understanding and simulating the spatial and temporal changes in land use and land cover, which are closely linked to variations in ecosystem services. It emphasizes the importance of reconstructing historical land conversion processes and projecting future land distribution to reveal the underlying mechanisms driving land expansion and conversion patterns. The study recognizes the need to explore different land conversion scenarios to support sustainable land management and ecological protection.

The primary objective of the study is to develop and apply a dynamic analysis framework for land use and land cover change in the Yellow River Basin of Shaanxi Province. This framework aims to analyze the driving forces behind land expansion, establish multiple land conversion scenarios, and perform spatial evaluations of land conversion to assess their impacts on ecosystem services. The goal is to provide land allocation schemes that balance ecological protection with regional development needs, thereby offering practical solutions for rational land management.",True
Engineering,Utilizing Hybrid Machine Learning and Soft Computing Techniques for Landslide Susceptibility Mapping in a Drainage Basin,"The hydrological system of thebasin of Lake Urmia is complex, deriving its supply from a network comprising 13 perennial rivers, along withnumerous small springs and direct precipitation onto the lake’s surface. Among these contributors, approximately half of the inflow is attributed to the Zarrineh River and the Simineh River. Remarkably, Lake Urmia lacks a natural outlet, with its water loss occurring solely through evaporation processes. This study employed a comprehensive methodology integrating ground surveys, remote sensing analyses, and meticulous documentation of historical landslides within the basin as primary information sources. Through this investigative approach, we preciselyidentified and geolocated a total of 512 historical landslide occurrences across the Urmia Lake drainage basin, leveraging GPS technology for precision. Thisarticle introduces a suite of hybrid machine learning predictive models, such as support-vector machine (SVM), random forest (RF), decision trees (DT), logistic regression (LR), fuzzy logic (FL), and the technique for order of preference by similarity to the ideal solution (TOPSIS). These models were strategically deployed to assess landslide susceptibility within the region. The outcomes of the landslide susceptibility assessment reveal that the main high susceptible zones for landslide occurrence are concentrated in the northwestern, northern, northeastern, and some southern and southeastern areas of the region. Moreover, when considering the implementation of predictions using different algorithms, it became evident that SVM exhibited superior performance regardingboth accuracy (0.89) and precision (0.89), followed by RF, with and accuracy of 0.83 and a precision of 0.83. However, it is noteworthy that TOPSIS yielded the lowest accuracy value among the algorithms assessed.","['support-vector machine (SVM)', 'random forest (RF)', 'decision trees (DT)', 'logistic regression (LR)']","The hydrological system of the Lake Urmia basin is complex, receiving water from 13 perennial rivers, numerous small springs, and direct precipitation, with about half of the inflow coming from the Zarrineh and Simineh Rivers. Lake Urmia has no natural outlet, and water loss occurs solely through evaporation, making the understanding of factors influencing the basin’s stability critical. This study aims to identify and geolocate historical landslides within the Urmia Lake drainage basin to better understand landslide susceptibility in the region. The primary objective is to assess and map the areas most susceptible to landslides across the basin, particularly focusing on the northwestern, northern, northeastern, southern, and southeastern zones, to inform risk management and mitigation efforts.","The hydrological system of the Lake Urmia basin is complex, receiving water from 13 perennial rivers, numerous small springs, and direct precipitation, with about half of the inflow coming from the Zarrineh and Simineh Rivers. Lake Urmia has no natural outlet, and water loss occurs solely through evaporation, making the understanding of factors influencing the basin's stability critical. This study aims to identify and geolocate historical landslides within the Urmia Lake drainage basin to better understand landslide susceptibility in the region. The primary objective is to assess and map the areas most susceptible to landslides across the basin, particularly focusing on the northwestern, northern, northeastern, southern, and southeastern zones, to inform risk management and mitigation efforts.",True
Engineering,Comparison of Random Forest and XGBoost Classifiers Using Integrated Optical and SAR Features for Mapping Urban Impervious Surface,"The integration of optical and SAR datasets through ensemble machine learning models shows promising results in urban remote sensing applications. The integration of multi-sensor datasets enhances the accuracy of information extraction. This research presents a comparison of two ensemble machine learning classifiers (random forest and extreme gradient boost (XGBoost)) classifiers using an integration of optical and SAR features and simple layer stacking (SLS) techniques. Therefore, Sentinel-1 (SAR) and Landsat 8 (optical) datasets were used with SAR textures and enhanced modified indices to extract features for the year 2023. The classification process utilized two machine learning algorithms, random forest and XGBoost, for urban impervious surface extraction. The study focused on three significant East Asian cities with diverse urban dynamics: Jakarta, Manila, and Seoul. This research proposed a novel index called the Normalized Blue Water Index (NBWI), which distinguishes water from other features and was utilized as an optical feature. Results showed an overall accuracy of 81% for UIS classification using XGBoost and 77% with RF while classifying land use land cover into four major classes (water, vegetation, bare soil, and urban impervious). However, the proposed framework with the XGBoost classifier outperformed the RF algorithm and Dynamic World (DW) data product and comparatively showed higher classification accuracy. Still, all three results show poor separability with bare soil class compared to ground truth data. XGBoost outperformed random forest and Dynamic World in classification accuracy, highlighting its potential use in urban remote sensing applications.","['ensemble machine learning models', 'random forest', 'extreme gradient boost (XGBoost)']","The study addresses the challenge of accurately extracting urban impervious surfaces by integrating optical and synthetic aperture radar (SAR) datasets, which enhances the precision of information retrieval in urban remote sensing. The motivation stems from the need to improve land use and land cover classification in rapidly changing urban environments, particularly in diverse East Asian cities. The primary objective of the research is to compare the effectiveness of different classification approaches using combined optical and SAR features for urban impervious surface extraction. Additionally, the study aims to introduce and utilize a novel index, the Normalized Blue Water Index (NBWI), to better distinguish water bodies from other land cover types in the classification process.","The study addresses the challenge of accurately extracting urban impervious surfaces by integrating optical and synthetic aperture radar (SAR) datasets, which enhances the precision of information retrieval in urban remote sensing. The motivation stems from the need to improve land use and land cover classification in rapidly changing urban environments, particularly in diverse East Asian cities. The primary objective of the research is to compare the effectiveness of different analytical methods using combined optical and SAR features for urban impervious surface extraction. Additionally, the study aims to introduce and utilize a novel index, the Normalized Blue Water Index (NBWI), to better distinguish water bodies from other land cover types in the classification process.",True
Engineering,Firefly algorithm based WSN-IoT security enhancement with machine learning for intrusion detection,"Abstract A Wireless Sensor Network (WSN) aided by the Internet of Things (IoT) is a collaborative system of WSN systems and IoT networks are work to exchange, gather, and handle data. The primary objective of this collaboration is to enhance data analysis and automation to facilitate improved decision-making. Securing IoT with the assistance of WSN necessitates the implementation of protective measures to confirm the safety and reliability of the interconnected WSN and IoT components. This research significantly advances the current state of the art in IoT and WSN security by synergistically harnessing the potential of machine learning and the Firefly Algorithm. The contributions of this work are twofold: firstly, the proposed FA-ML technique exhibits an exceptional capability to enhance intrusion detection accuracy within the WSN-IoT landscape. Secondly, the amalgamation of the Firefly Algorithm and machine learning introduces a novel dimension to the domain of security-oriented optimization techniques. The implications of this research resonate across various sectors, ranging from critical infrastructure protection to industrial automation and beyond, where safeguarding the integrity of interconnected systems are of paramount importance. The amalgamation of cutting-edge machine learning and bio-inspired algorithms marks a pivotal step forward in crafting robust and intelligent security measures for the evolving landscape of IoT-driven technologies. For intrusion detection in the WSN-IoT, the FA-ML method employs a support vector machine (SVM) machine model for classification with parameter tuning accomplished using a Grey Wolf Optimizer (GWO) algorithm. The experimental evaluation is simulated using NSL-KDD Dataset, revealing the remarkable enhancement of the FA-ML technique, achieving a maximum accuracy of 99.34%. In comparison, the KNN-PSO and XGBoost models achieved lower accuracies of 96.42% and 95.36%, respectively. The findings validate the potential of the FA-ML technique as an active security solution for WSN-IoT systems, harnessing the power of machine learning and the Firefly Algorithm to bolster intrusion detection capabilities.","['Firefly Algorithm', 'machine learning', 'support vector machine (SVM)', 'Grey Wolf Optimizer (GWO)', 'XGBoost']","The research idea centers on the need to secure the interconnected components of Wireless Sensor Networks (WSN) and Internet of Things (IoT) networks to ensure their safety and reliability. This collaboration between WSN and IoT aims to enhance data exchange and automation, which are critical for improved decision-making across various sectors such as critical infrastructure protection and industrial automation. The study addresses the challenge of protecting these integrated systems from intrusions that could compromise their integrity. The primary objective of the study is to improve intrusion detection accuracy within the WSN-IoT environment by developing advanced security measures that safeguard the integrity of these interconnected systems. The research aims to introduce innovative approaches that significantly enhance the detection of unauthorized access or attacks, thereby strengthening the overall security framework of WSN-IoT applications.","The research idea centers on the need to secure the interconnected components of Wireless Sensor Networks (WSN) and Internet of Things (IoT) networks to ensure their safety and reliability. This collaboration between WSN and IoT aims to enhance data exchange and automation, which are critical for improved decision-making across various sectors such as critical infrastructure protection and industrial automation. The study addresses the challenge of protecting these integrated systems from intrusions that could compromise their integrity. The primary objective of the study is to improve intrusion detection accuracy within the WSN-IoT environment by developing robust security measures that safeguard the integrity of these interconnected systems. The research aims to introduce innovative approaches that significantly enhance the detection of unauthorized access or attacks, thereby strengthening the overall security framework of WSN-IoT applications.",True
Engineering,Peak and ultimate stress-strain model of confined ultra-high-performance concrete (UHPC) using hybrid machine learning model with conditional tabular generative adversarial network,"Ultra-high-performance concrete (UHPC) has gained prominence owing to its exceptional physical and mechanical properties and improved sustainability, making it ideal for large-scale structural applications. While numerous analytical studies have focused on predicting the stress-strain response of unconfined UHPC, there remains a lack of a reliable model for predicting the stress-strain response of confined UHPC, which poses challenges to efficient design and broader adoption, particularly in seismically active regions. To bridge this gap, the present study introduces a framework that implements machine learning (ML) models augmented by a state-of-the-art conditional tabular generative adversarial network (CTGAN) and Optuna, which a next-generation optimization framework, to accurately predict the peak and ultimate axial stress-strain responses of UHPC confined with either normal-strength steel or high-strength steel. The Optuna-optimized CTGAN is employed to address the issue of limited data by generating synthetic datasets of hypothetical confined UHPC specimens. A comprehensive database of confined UHPC stress-strain responses was compiled from existing literature and used to condition the CTGAN. The augmented database is then leveraged to develop a hybrid ML model that integrates extreme gradient boosting, gradient boosting machine, support vector regression, and K-nearest neighbors for predicting peak and ultimate stress-strain responses of confined UHPC. The predictive accuracy of the proposed hybrid ML model is evaluated and compared with a diverse set of ML models of varying complexity, and the results demonstrate its superior performance in predicting the peak and ultimate stress-strain response of confined UHPC. Furthermore, a graphical user interface of the proposed model is developed to facilitate its practical implementation and provide a rapid, autonomous, and accurate prediction of the stress-strain response of confined UHPC at both peak and ultimate states.","['conditional tabular generative adversarial network (CTGAN)', 'extreme gradient boosting', 'gradient boosting machine', 'support vector regression', 'K-nearest neighbors']","The research addresses the challenge of accurately predicting the stress-strain response of ultra-high-performance concrete (UHPC) when it is confined, which is critical for efficient design and wider application in large-scale structural projects, especially in seismically active regions. Despite extensive studies on unconfined UHPC, there is a notable lack of reliable approaches for confined UHPC, limiting its practical use and safety assurance under axial loading conditions. The primary aim of the study is to develop a reliable predictive framework for the peak and ultimate axial stress-strain responses of UHPC confined with either normal-strength or high-strength steel. This objective seeks to enhance the understanding and design capabilities for confined UHPC, thereby supporting its broader adoption in structural engineering applications.","The research addresses the challenge of accurately predicting the stress-strain response of ultra-high-performance concrete (UHPC) when it is confined, which is critical for efficient design and wider application in large-scale structural projects, especially in seismically active regions. Despite extensive studies on unconfined UHPC, there is a notable lack of reliable approaches for confined UHPC, limiting its practical use and safety assurance under axial loading conditions. The primary aim of the study is to develop a reliable methodology for the peak and ultimate axial stress-strain responses of UHPC confined with either normal-strength or high-strength steel. This objective seeks to enhance the understanding and design capabilities for confined UHPC, thereby supporting its broader adoption in structural engineering applications.",True
Engineering,Sentiment Analysis in the Age of Generative AI,"Abstract In the rapidly advancing age of Generative AI, Large Language Models (LLMs) such as ChatGPT stand at the forefront of disrupting marketing practice and research. This paper presents a comprehensive exploration of LLMs’ proficiency in sentiment analysis, a core task in marketing research for understanding consumer emotions, opinions, and perceptions. We benchmark the performance of three state-of-the-art LLMs, i.e., GPT-3.5, GPT-4, and Llama 2, against established, high-performing transfer learning models. Despite their zero-shot nature, our research reveals that LLMs can not only compete with but in some cases also surpass traditional transfer learning methods in terms of sentiment classification accuracy. We investigate the influence of textual data characteristics and analytical procedures on classification accuracy, shedding light on how data origin, text complexity, and prompting techniques impact LLM performance. We find that linguistic features such as the presence of lengthy, content-laden words improve classification performance, while other features such as single-sentence reviews and less structured social media text documents reduce performance. Further, we explore the explainability of sentiment classifications generated by LLMs. The findings indicate that LLMs, especially Llama 2, offer remarkable classification explanations, highlighting their advanced human-like reasoning capabilities. Collectively, this paper enriches the current understanding of sentiment analysis, providing valuable insights and guidance for the selection of suitable methods by marketing researchers and practitioners in the age of Generative AI.","['GPT-3.5', 'GPT-4', 'Llama 2', 'transfer learning models']","The research addresses the challenge of accurately understanding consumer emotions, opinions, and perceptions through sentiment analysis, which is a fundamental task in marketing research. It focuses on evaluating the effectiveness of different approaches in classifying sentiment from textual data, considering factors such as text complexity and data origin that influence classification accuracy. The primary aim of the study is to benchmark the performance of various advanced language-based approaches against established methods in sentiment classification, examining how linguistic features and text characteristics affect their accuracy. Additionally, the study seeks to provide insights into the interpretability of sentiment classifications to enhance the understanding and application of sentiment analysis in marketing contexts.","The research addresses the challenge of accurately understanding consumer emotions, opinions, and perceptions through sentiment analysis, which is a fundamental task in marketing research. It focuses on evaluating the effectiveness of different approaches in classifying sentiment from textual data, considering factors such as text complexity and data origin that influence classification accuracy. The primary aim of the study is to benchmark the performance of various advanced language-based methodologies against established techniques in sentiment classification, examining how linguistic features and text characteristics affect their accuracy. Additionally, the study seeks to provide insights into the interpretability of sentiment classifications to enhance the understanding and application of sentiment analysis in marketing contexts.",True
Engineering,A vehicular network based intelligent transport system for smart cities using machine learning algorithms,"Abstract Smart cities and the Internet of Things have enabled the integration of communicating devices for efficient decision-making. Notably, traffic congestion is one major problem faced by daily commuters in urban cities. In developed countries, specialized sensors are deployed to gather traffic information to predict traffic patterns. Any traffic updates are shared with the commuters via the Internet. Such solutions become impracticable when physical infrastructure and Internet connectivity are either non-existent or very limited. In case of developing countries, no roadside units are available and Internet connectivity is still an issue in remote areas. Internet traffic analysis is a thriving field of study due to the myriad ways in which it may be put to practical use. In the intelligent Internet-of-Vehicles (IOVs), traffic congestion can be predicted and identified using cutting-edge technologies. Using tree-based decision-tree, random-forest, extra-tree, and XGBoost machine learning (ML) strategies, this research proposes an intelligent-transport-system for the IOVs-based vehicular network traffic in a smart city set-up. The suggested system uses ensemble learning and averages the selection of crucial features to give high detection accuracy at minimal computational costs, as demonstrated by the simulation results. For IOV-based vehicular network traffic, the tree-based ML approaches with feature-selection (FS) outperformed those without FS. When contrasted to the lowest KNN accuracy of 96.6% and the highest SVM accuracy of 98.01%, the Stacking approach demonstrates superior accuracy as 99.05%.","['decision-tree', 'random-forest', 'extra-tree', 'XGBoost', 'ensemble learning', 'feature-selection (FS)', 'KNN', 'SVM', 'Stacking']","The research addresses the significant challenge of traffic congestion faced by daily commuters in urban cities, particularly highlighting the limitations of existing traffic monitoring solutions in developing countries where physical infrastructure and Internet connectivity are inadequate or absent. The study is motivated by the need for effective traffic information gathering and dissemination methods that can function in environments with limited roadside units and poor Internet access. The primary aim of the study is to develop an intelligent transport approach for vehicular network traffic within a smart city context that enhances the detection and prediction of traffic congestion. This objective focuses on improving traffic management by utilizing advanced strategies to achieve high accuracy in traffic condition identification while minimizing resource requirements.","The research addresses the significant challenge of traffic congestion faced by daily commuters in urban cities, particularly highlighting the limitations of existing traffic monitoring solutions in developing countries where physical infrastructure and Internet connectivity are inadequate or absent. The study is motivated by the need for effective traffic information gathering and dissemination methods that can function in environments with limited roadside units and poor Internet access. The primary aim of the study is to develop an improved transport approach for vehicular network traffic within a city context that enhances the detection and analysis of traffic congestion. This objective focuses on improving traffic management by utilizing effective strategies to achieve high accuracy in traffic condition identification while minimizing resource requirements.",True
Engineering,Conventional to Deep Ensemble Methods for Hyperspectral Image Classification: A Comprehensive Survey,"Hyperspectral image classification has become a hot research topic. HSI has been widely used in a wide range of real-world application areas due to the in-depth spectral information stored within each pixel. Noticeably, the detailed features - i.e., a nonlinear correlation between the obtained spectral data and the correlating HSI data object, generate efficient classification results that are complex for traditional techniques. Deep Learning (DL) has recently been validated as an influential feature extractor that efficiently identifies the nonlinear issues that have arisen in various computer vision challenges. This motivates using DL for Hyperspectral Image Classification (HSIC), which shows promising results. This survey provides a brief description of DL for HSIC and compares cutting-edge methodologies in the field. We will first summarize the key challenges for HSIC, and then we will discuss the superiority of DL and DL-ensemble in addressing these issues. In this article, we divide the state-of-the-art DL methodologies and DL with ensemble into spectral features, spatial features, and combined spatial-spectral features in order to comprehensively and critically evaluate the progress (future research directions as well) of such methodologies for HSIC. Furthermore, we will take into account that DL involves a substantial percentage of labeled training images, whereas obtaining such a number for HSI is time and cost-consuming. As a result, this survey describes some methodologies for improving the classification performance of DL techniques, which can serve as future recommendations.",['Deep Learning (DL)'],"The research idea centers on the challenge of classifying hyperspectral images, which contain detailed spectral information within each pixel that is crucial for various real-world applications. Traditional techniques struggle to effectively handle the complex nonlinear relationships present in the spectral data, making accurate classification difficult. The study is motivated by the need to address these challenges and improve the classification performance of hyperspectral images. The primary objective of the study is to provide a comprehensive overview of current methodologies for hyperspectral image classification, summarize the key challenges in the field, and evaluate the progress and future directions for improving classification accuracy, particularly considering the limitations related to the availability of labeled training images.","The research idea centers on the challenge of classifying hyperspectral images, which contain detailed spectral information within each pixel that is crucial for various real-world applications. Conventional analysis methods struggle to effectively handle the complex nonlinear relationships present in the spectral data, making accurate classification difficult. The study is motivated by the need to address these challenges and improve the classification performance of hyperspectral images. The primary objective of the study is to provide a comprehensive overview of current methodologies for hyperspectral image classification, summarize the key challenges in the field, and evaluate the progress and future directions for improving classification accuracy, particularly considering the limitations related to the availability of labeled training images.",True
Engineering,NAVIGATING THE FUTURE: INTEGRATING AI AND MACHINE LEARNING IN HR PRACTICES FOR A DIGITAL WORKFORCE,"As organizations navigate the complexities of the digital age, the role of Human Resources (HR) is evolving to meet the demands of a digital workforce. This review explores the integration of Artificial Intelligence (AI) and Machine Learning (ML) in HR practices to enhance efficiency, effectiveness, and employee satisfaction in the digital era. AI and ML technologies offer HR departments the opportunity to streamline operations, improve decision-making processes, and enhance employee experiences. By leveraging AI and ML, HR professionals can automate routine tasks such as recruitment, onboarding, training, and performance evaluation, allowing them to focus on more strategic initiatives that drive organizational success. One of the key advantages of integrating AI and ML in HR practices is the ability to personalize employee experiences. These technologies can analyze large volumes of data to identify patterns and trends, enabling HR professionals to tailor programs and policies to meet the unique needs of individual employees. This personalization can lead to higher levels of employee engagement, satisfaction, and retention. Furthermore, AI and ML can help HR departments make more informed decisions by providing data-driven insights. These technologies can analyze employee data to identify areas for improvement, predict future trends, and develop strategies to address challenges proactively. By leveraging these insights, HR professionals can make strategic decisions that align with the organization's goals and objectives. However, integrating AI and ML in HR practices also presents challenges, such as data privacy concerns, ethical considerations, and the need for upskilling HR professionals to use these technologies effectively. Organizations must address these challenges to realize the full potential of AI and ML in HR practices. In conclusion, integrating AI and ML in HR practices offers organizations the opportunity to enhance efficiency, effectiveness, and employee satisfaction in the digital age. By leveraging these technologies, HR departments can streamline operations, personalize employee experiences, and make more informed decisions that drive organizational success. As organizations increasingly turn to digital solutions, the role of artificial intelligence (AI) and machine learning (ML) in Human Resources becomes pivotal. This paper will focus on how AI and ML are being integrated into HR functions such as recruitment, onboarding, and employee engagement. It will also discuss the ethical implications and the challenges of maintaining human touch in an increasingly automated workplace. Case studies of companies leading in digital HR practices will be highlighted to provide real-world insights. Keywords: Digital Force, HR Practices, AI, Machine Learning, Future.",['Machine Learning (ML)'],"The research addresses the evolving role of Human Resources (HR) in response to the demands of a digital workforce, focusing on how HR practices can be enhanced to improve efficiency, effectiveness, and employee satisfaction in the digital era. It highlights the need for HR departments to streamline operations, personalize employee experiences, and make more informed decisions to better support organizational success. The primary aim of the study is to explore the integration of advanced technologies in HR functions such as recruitment, onboarding, training, and performance evaluation, while also examining the ethical considerations and challenges involved in maintaining a human-centered approach within increasingly automated HR environments. The study seeks to provide insights into how these integrations can drive strategic initiatives and improve overall HR outcomes in organizations adapting to digital transformation.","The research addresses the evolving role of Human Resources (HR) in response to the demands of a digital workforce, focusing on how HR practices can be enhanced to improve efficiency, effectiveness, and employee satisfaction in the digital era. It highlights the need for HR departments to streamline operations, personalize employee experiences, and make more informed decisions to better support organizational success. The primary aim of the study is to explore the integration of advanced technologies in HR functions such as recruitment, onboarding, training, and performance evaluation, while also examining the ethical considerations and challenges involved in maintaining a human-centered approach within increasingly technology-enabled HR environments. The study seeks to provide insights into how these integrations can drive strategic initiatives and improve overall HR outcomes in organizations adapting to digital transformation.",True
Engineering,AI-Driven Digital Twin Model for Reliable Lithium-Ion Battery Discharge Capacity Predictions,"The present study proposes a novel method for predicting the discharge capabilities of lithium-ion (Li-ion) batteries using a digital twin model in practice. By combining cutting-edge machine learning techniques, such as AdaBoost and long short-term memory (LSTM) network, with a semiempirical mathematical structure, the digital twin (DT)—a virtual representation that mimics the behavior of actual batteries in real time is constructed. Various metaheuristic optimization methods, such as antlion, grey wolf optimization (GWO), and improved grey wolf optimization (IGWO), are used to adjust hyperparameters in order to optimize the models. As indicators of performance, mean absolute error (MAE) and root-mean-square error (RMSE) are applied to the models after they have undergone extensive training and ten-fold cross-validation. The models are rigorously trained and cross-validated using the NASA battery aging dataset, a widely accepted benchmark dataset for battery research. The IGWO-AdaBoost digital twin model emerges as the standout performer, achieving exceptional accuracy in predicting the discharge capacity. This model demonstrates the lowest mean absolute error (MAE) of 0.01, showcasing its superior precision in estimating discharge capabilities. Additionally, the root mean square error (RMSE) for the IGWO-AdaBoost DT model is also the lowest at 0.01. The findings of this study offer insightful information about the potential utilization of the digital twin model to accurately predict the discharge capacity of batteries.","['AdaBoost', 'long short-term memory (LSTM) network', 'antlion optimization', 'grey wolf optimization (GWO)', 'improved grey wolf optimization (IGWO)']","The study addresses the challenge of accurately predicting the discharge capabilities of lithium-ion batteries, which is crucial for enhancing battery performance and reliability in practical applications. Understanding and forecasting battery discharge behavior is essential for optimizing battery usage and extending its lifespan. The primary aim of the study is to develop a method that can precisely estimate the discharge capacity of lithium-ion batteries by constructing a virtual representation that closely mimics the real-time behavior of actual batteries. This approach seeks to provide accurate and reliable predictions of battery discharge performance to support better battery management and utilization.","The study addresses the challenge of accurately predicting the discharge capabilities of lithium-ion batteries, which is crucial for enhancing battery performance and reliability in practical applications. Understanding and forecasting battery discharge behavior is essential for optimizing battery usage and extending its lifespan. The primary aim of the study is to develop a method that can precisely estimate the discharge capacity of lithium-ion batteries by creating a systematic representation that closely mimics the real-time behavior of actual batteries. This approach seeks to provide accurate and reliable predictions of battery discharge performance to support better battery management and utilization.",True
Engineering,Multi-USV Task Planning Method Based on Improved Deep Reinforcement Learning,"A safe and reliable task planning method is a prerequisite for the collaborative execution of ocean observation data collection tasks by multiple unmanned surface vessels (multi-USVs). Deep Reinforcement Learning (DRL) combines the powerful nonlinear function-fitting capabilities of deep neural networks with the decision-making and control abilities of reinforcement learning, providing a novel approach to solving the multi-USV task planning problem. However, when applied to the field of multi-USV task planning, it faces challenges such as a vast exploration space, extended training times, and unstable training process. To this end, this paper proposes a multi-USV task planning method based on improved deep reinforcement learning. The proposed method draws on the idea of a value decomposition network, breaking down the multi-USV task planning problem into two subproblems: task allocation and autonomous collision avoidance. Different state spaces, action spaces, and reward functions are designed for the various subproblems. Based on this, a deep neural network is used to map the state space of each subproblem to the action space of each USV, and the generated strategy of the deep neural network is assessed based on the corresponding reward function. This successfully integrates task allocation and path planning into a comprehensive task planning framework. Deep neural networks consist of the Actor networks and the Critic networks. During the training phase of the Critic network, different methods are used to train different Critic networks to improve the convergence speed of the algorithm. An improved temporal difference error method is specifically applied to train the Critic network for evaluating autonomous collision avoidance strategies, resulting in improving the autonomous collision avoidance ability of USVs. At the same time, to improve the efficiency of task allocation, hierarchical mechanisms, and regional division mechanisms are introduced to construct sub-system task planning models, which further decompose the task planning problem. A combination of successor features and an improved temporal difference error method is specifically applied to train another Critic network for evaluating the sub-systems task allocation schemes and collaborative motion trajectories, aiming to enhance the allocation efficiency of the sub-systems. Furthermore, transfer learning is employed to merge the sub-system task planning, using it as a constraint to direct the exploration and assessment of both the cluster task allocation schemes and the cluster collaborative motion trajectories. This enables rapid and accurate learning for task allocation within the multi-USV cluster. During the training phase of the Actor network, the introduction of the experience replay method and target network technique is employed to enhance the proximal policy optimization algorithm. This facilitates distributed joint training of the Actor network, thereby improving the accuracy of the algorithm. Simulation results validate the effectiveness and superiority of this method.","['Deep Reinforcement Learning (DRL)', 'deep neural network', 'Actor networks', 'Critic networks', 'successor features', 'transfer learning', 'experience replay method', 'target network technique', 'proximal policy optimization algorithm']","The research addresses the challenge of safely and reliably planning tasks for multiple unmanned surface vessels (multi-USVs) engaged in collaborative ocean observation data collection. It focuses on overcoming difficulties such as vast exploration spaces, extended training times, and instability in the task planning process to ensure effective coordination among the vessels. The primary aim of the study is to develop a comprehensive task planning approach that integrates task allocation and autonomous collision avoidance for multi-USVs. This approach seeks to improve the efficiency and accuracy of task distribution and path planning within multi-USV clusters, ultimately enhancing their collaborative execution capabilities.","The research addresses the challenge of safely and reliably planning tasks for multiple unmanned surface vessels (multi-USVs) engaged in collaborative ocean observation data collection. It focuses on overcoming difficulties such as vast exploration spaces, extended operational preparation, and instability in the task planning process to ensure effective coordination among the vessels. The primary aim of the study is to develop a comprehensive task planning approach that integrates task allocation and autonomous collision avoidance for multi-USVs. This approach seeks to improve the efficiency and accuracy of task distribution and path planning within multi-USV clusters, ultimately enhancing their collaborative execution capabilities.",True
Engineering,Automated data processing and feature engineering for deep learning and big data applications: A survey,"Modern approach to artificial intelligence (AI) aims to design algorithms that learn directly from data. This approach has achieved impressive results and has contributed significantly to the progress of AI, particularly in the sphere of supervised deep learning. It has also simplified the design of machine learning systems as the learning process is highly automated. However, not all data processing tasks in conventional deep learning pipelines have been automated. In most cases data has to be manually collected, preprocessed and further extended through data augmentation before they can be effective for training. Recently, special techniques for automating these tasks have emerged. The automation of data processing tasks is driven by the need to utilize large volumes of complex, heterogeneous data for machine learning and big data applications. Today, end-to-end automated data processing systems based on automated machine learning (AutoML) techniques are capable of taking raw data and transforming them into useful features for Big Data tasks by automating all intermediate processing stages. In this work, we present a thorough review of approaches for automating data processing tasks in deep learning pipelines, including automated data preprocessing– e.g., data cleaning, labeling, missing data imputation, and categorical data encoding–as well as data augmentation (including synthetic data generation using generative AI methods) and feature engineering–specifically, automated feature extraction, feature construction and feature selection. In addition to automating specific data processing tasks, we discuss the use of AutoML methods and tools to simultaneously optimize all stages of the machine learning pipeline.","['supervised deep learning', 'automated machine learning (AutoML)', 'synthetic data generation using generative AI methods', 'feature construction', 'feature selection']","The research addresses the challenge of managing and preparing large volumes of complex and heterogeneous data for effective use in advanced data processing tasks. It highlights the need to automate various stages of data handling, such as collection, preprocessing, and augmentation, which are traditionally performed manually and can be time-consuming and labor-intensive. The study is motivated by the importance of streamlining these processes to improve efficiency and effectiveness in handling extensive datasets. The primary objective of the study is to review and evaluate approaches for automating data processing tasks, including data cleaning, labeling, missing data imputation, categorical data encoding, data augmentation, and feature engineering. Additionally, the study aims to explore methods that optimize all stages of the data preparation pipeline to enhance the overall workflow in managing complex data.","The research addresses the challenge of managing and preparing large volumes of complex and heterogeneous data for effective use in advanced data processing tasks. It highlights the need to automate various stages of data handling, such as collection, preprocessing, and augmentation, which are traditionally performed manually and can be time-consuming and labor-intensive. The study is motivated by the importance of streamlining these processes to improve efficiency and effectiveness in handling extensive datasets. The primary objective of the study is to review and evaluate approaches for automating data processing tasks, including data cleaning, labeling, missing data imputation, categorical data encoding, data augmentation, and feature selection. Additionally, the study aims to explore methods that optimize all stages of the data preparation pipeline to enhance the overall workflow in managing complex data.",True
Engineering,Data-driven evolution of water quality models: An in-depth investigation of innovative outlier detection approaches-A case study of Irish Water Quality Index (IEWQI) model,"Recently, there has been a significant advancement in the water quality index (WQI) models utilizing data-driven approaches, especially those integrating machine learning and artificial intelligence (ML/AI) technology. Although, several recent studies have revealed that the data-driven model has produced inconsistent results due to the data outliers, which significantly impact model reliability and accuracy. The present study was carried out to assess the impact of data outliers on a recently developed Irish Water Quality Index (IEWQI) model, which relies on data-driven techniques. To the author's best knowledge, there has been no systematic framework for evaluating the influence of data outliers on such models. For the purposes of assessing the outlier impact of the data outliers on the water quality (WQ) model, this was the first initiative in research to introduce a comprehensive approach that combines machine learning with advanced statistical techniques. The proposed framework was implemented in Cork Harbour, Ireland, to evaluate the IEWQI model's sensitivity to outliers in input indicators to assess the water quality. In order to detect the data outlier, the study utilized two widely used ML techniques, including Isolation Forest (IF) and Kernel Density Estimation (KDE) within the dataset, for predicting WQ with and without these outliers. For validating the ML results, the study used five commonly used statistical measures. The performance metric (R2) indicates that the model performance improved slightly (R2 increased from 0.92 to 0.95) in predicting WQ after removing the data outlier from the input. But the IEWQI scores revealed that there were no statistically significant differences among the actual values, predictions with outliers, and predictions without outliers, with a 95% confidence interval at p < 0.05. The results of model uncertainty also revealed that the model contributed <1% uncertainty to the final assessment results for using both datasets (with and without outliers). In addition, all statistical measures indicated that the ML techniques provided reliable results that can be utilized for detecting outliers and their impacts on the IEWQI model. The findings of the research reveal that although the data outliers had no significant impact on the IEWQI model architecture, they had moderate impacts on the rating schemes' of the model. This finding indicated that detecting the data outliers could improve the accuracy of the IEWQI model in rating WQ as well as be helpful in mitigating the model eclipsing problem. In addition, the results of the research provide evidence of how the data outliers influenced the data-driven model in predicting WQ and reliability, particularly since the study confirmed that the IEWQI model's could be effective for accurately rating WQ despite the presence of the data outliers in the input. It could occur due to the spatio-temporal variability inherent in WQ indicators. However, the research assesses the influence of data input outliers on the IEWQI model and underscores important areas for future investigation. These areas include expanding temporal analysis using multi-year data, examining spatial outlier patterns, and evaluating detection methods. Moreover, it is essential to explore the real-world impacts of revised rating categories, involve stakeholders in outlier management, and fine-tune model parameters. Analysing model performance across varying temporal and spatial resolutions and incorporating additional environmental data can significantly enhance the accuracy of WQ assessment. Consequently, this study offers valuable insights to strengthen the IEWQI model's robustness and provides avenues for enhancing its utility in broader WQ assessment applications. Moreover, the study successfully adopted the framework for evaluating how data input outliers affect the data-driven model, such as the IEWQI model. The current study has been carried out in Cork Harbour for only a single year of WQ data. The framework should be tested across various domains for evaluating the response of the IEWQI model's in terms of the spatio-temporal resolution of the domain. Nevertheless, the study recommended that future research should be conducted to adjust or revise the IEWQI model's rating schemes and investigate the practical effects of data outliers on updated rating categories. However, the study provides potential recommendations for enhancing the IEWQI model's adaptability and reveals its effectiveness in expanding its applicability in more general WQ assessment scenarios.",['Isolation Forest (IF)'],"The research addresses the challenge of inconsistent results in water quality assessment due to the presence of data outliers, which affect the reliability and accuracy of the Irish Water Quality Index (IEWQI) model. Despite advancements in water quality evaluation, there has been no systematic approach to understanding how these outliers influence the model’s performance and rating schemes. The study aims to assess the impact of data outliers on the IEWQI model’s ability to rate water quality accurately. It seeks to evaluate the sensitivity of the IEWQI model to outliers in input indicators and to provide insights into improving the model’s robustness and rating accuracy for water quality assessment, particularly within the context of Cork Harbour, Ireland.","The research addresses the challenge of inconsistent results in water quality assessment due to the presence of data outliers, which affect the reliability and accuracy of the Irish Water Quality Index (IEWQI) model. Despite advancements in water quality evaluation, there has been no systematic approach to understanding how these outliers influence the model's performance and rating schemes. The study aims to assess the impact of data outliers on the IEWQI model's ability to rate water quality accurately. It seeks to evaluate the sensitivity of the IEWQI model to outliers in input indicators and to provide insights into improving the model's robustness and rating accuracy for water quality assessment, particularly within the context of Cork Harbour, Ireland.",True
Engineering,Predicting the mechanical properties of plastic concrete: An optimization method by using genetic programming and ensemble learners,"This study presents a comparative analysis of individual and ensemble learning algorithms (ELAs) to predict the compressive strength (CS) and flexural strength (FS) of plastic concrete. Multilayer perceptron neuron network (MLPNN), Support vector machine (SVM), random forest (RF), and decision tree (DT) were used as base learners, which were then combined with bagging and Adaboost methods to improve the predictive performance. In addition, gene expression programming (GEP) was used to develop computational equations that can be used to predict the CS and FS of plastic concrete. An extensive database containing 357 and 125 data points was obtained from the literature, and the eight most impactful ingredients were used in the model's development. The accuracy of all models was assessed using several statistical measures, including an error matrix, Akaike information criterion (AIC), K-fold cross-validation, and other external validation equations. Furthermore, sensitivity and SHAP analysis were performed to evaluate input variables' relative significance and impact on the anticipated CS and FS. Based on statistical measures and other validation criteria, GEP outpaces all other individual models, whereas, in ELAs, the SVR ensemble with Adaboost and RF modified with the Bagging technique demonstrated superior performance. SHapley Additive exPlanations (SHAP) and sensitivity analysis reveal that plastic, cement, water, and the age of the specimens have the highest influence, while superplasticizer has the lowest impact, which is consistent with experimental studies. Moreover, GUI and GEP-based simple mathematical correlation can enhance the practical scope of this study and be an effective tool for the pre-mix design of plastic concrete.","['Multilayer perceptron neuron network (MLPNN)', 'Support vector machine (SVM)', 'random forest (RF)', 'decision tree (DT)', 'bagging', 'Adaboost', 'gene expression programming (GEP)', 'SHapley Additive exPlanations (SHAP)']","The research addresses the challenge of accurately predicting the compressive strength and flexural strength of plastic concrete, which are critical properties for ensuring the material's performance and durability in construction applications. Understanding the influence of various ingredients on these strength parameters is essential for optimizing the mix design and improving the quality of plastic concrete. The primary aim of the study is to develop reliable predictive equations and comparative assessments that can estimate the compressive and flexural strength of plastic concrete based on key ingredient proportions and specimen age. This objective seeks to provide practical tools that enhance the pre-mix design process and support better decision-making in concrete formulation.","The research addresses the challenge of accurately determining the compressive strength and flexural strength of plastic concrete, which are critical properties for ensuring the material's performance and durability in construction applications. Understanding the influence of various ingredients on these strength parameters is essential for optimizing the mix design and improving the quality of plastic concrete. The primary aim of the study is to develop reliable equations and comparative assessments that can estimate the compressive and flexural strength of plastic concrete based on key ingredient proportions and specimen age. This objective seeks to provide practical tools that enhance the pre-mix design process and support better decision-making in concrete formulation.",True
Engineering,A comprehensive evaluation of large Language models on benchmark biomedical text processing tasks,"Recently, Large Language Models (LLMs) have demonstrated impressive capability to solve a wide range of tasks. However, despite their success across various tasks, no prior work has investigated their capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of LLMs on benchmark biomedical tasks. For this purpose, a comprehensive evaluation of 4 popular LLMs in 6 diverse biomedical tasks across 26 datasets has been conducted. To the best of our knowledge, this is the first work that conducts an extensive evaluation and comparison of various LLMs in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot LLMs even outperform the current state-of-the-art models when they were fine-tuned only on the training set of these datasets. This suggests that pre-training on large text corpora makes LLMs quite specialized even in the biomedical domain. We also find that not a single LLM can outperform other LLMs in all tasks, with the performance of different LLMs may vary depending on the task. While their performance is still quite poor in comparison to the biomedical models that were fine-tuned on large training sets, our findings demonstrate that LLMs have the potential to be a valuable tool for various biomedical tasks that lack large annotated data.","['zero-shot learning', 'fine-tuning']","The research idea addresses the challenge of evaluating the effectiveness of recently developed large language models within the biomedical domain, an area that has not been previously explored despite their success in other fields. The study is motivated by the need to understand how these models perform on diverse biomedical tasks, especially given the limited availability of large annotated datasets in this field. The primary objective of the study is to conduct a comprehensive evaluation of multiple large language models across various biomedical tasks and datasets to assess their performance and potential utility. This evaluation aims to identify the strengths and limitations of these models in biomedical applications, particularly in scenarios with smaller training sets, and to compare their effectiveness relative to existing specialized biomedical approaches.","The research idea addresses the challenge of evaluating the effectiveness of recently developed text processing systems within the biomedical domain, an area that has not been previously explored despite their success in other fields. The study is motivated by the need to understand how these systems perform on diverse biomedical tasks, especially given the limited availability of large annotated datasets in this field. The primary objective of the study is to conduct a comprehensive evaluation of multiple advanced text processing systems across various biomedical tasks and datasets to assess their performance and potential utility. This evaluation aims to identify the strengths and limitations of these systems in biomedical applications, particularly in scenarios with smaller training sets, and to compare their effectiveness relative to existing specialized biomedical approaches.",True
Engineering,Performance assessment of machine learning algorithms for mapping of land use/land cover using remote sensing data,"The rapid increase in population accelerates the rate of change of Land use/Land cover (LULC) in various parts of the world. This phenomenon caused a huge strain for natural resources. Hence, continues monitoring of LULC changes gained a significant importance for management of natural resources and assessing the climate change impacts. Recently, application of machine learning algorithms on RS (remote sensing) data for rapid and accurate mapping of LULC gained significant importance due to growing need of LULC estimation for ecosystem services, natural resource management and environmental management. Hence, it is crucial to access and compare the performance of different machine learning classifiers for accurate mapping of LULC. The primary objective of this study was to compare the performance of CART (Classification and Regression Tree), RF (Random Forest) and SVM (Support Vector Machine) for LULC estimation by processing RS data on Google Earth Engine (GEE). In total four classes of LULC (Water Bodies, Vegetation Cover, Urban Land and Barren Land) for city of Lahore were extracted using satellite images from Landsat-7, Landsat-8 and Landsat-9 for years 2008, 2015 and 2022, respectively. According to results, RF is the best performing classifier with maximum overall accuracy of 95.2% and highest Kappa coefficient value of 0.87, SVM achieved maximum accuracy of 89.8% with highest Kappa of 0.84 and CART showed maximum overall accuracy of 89.7% with Kappa value of 0.79. Results from this study can give assistance for decision makers, planners and RS experts to choose a suitable machine learning algorithm for LULC classification in an unplanned urbanized city like Lahore.","['Classification and Regression Tree (CART)', 'Random Forest (RF)', 'Support Vector Machine (SVM)']","The rapid increase in population has accelerated the rate of Land Use/Land Cover (LULC) changes in various parts of the world, placing significant strain on natural resources. Continuous monitoring of these changes is essential for effective management of natural resources and for assessing the impacts of climate change. The primary objective of this study was to compare the performance of different classification approaches for accurate mapping of LULC in the city of Lahore. Specifically, the study aimed to evaluate and contrast the effectiveness of three classification methods in extracting four LULC classes—Water Bodies, Vegetation Cover, Urban Land, and Barren Land—using satellite images from multiple years to support decision-making in urban and environmental planning.","The rapid increase in population has accelerated the rate of Land Use/Land Cover (LULC) changes in various parts of the world, placing significant strain on natural resources. Continuous monitoring of these changes is essential for effective management of natural resources and for assessing the impacts of climate change. The primary objective of this study was to compare the performance of different categorization approaches for accurate mapping of LULC in the city of Lahore. Specifically, the study aimed to evaluate and contrast the effectiveness of three analytical methods in extracting four LULC classes—Water Bodies, Vegetation Cover, Urban Land, and Barren Land—using satellite images from multiple years to support decision-making in urban and environmental planning.",True
Engineering,Transformer and Graph Convolution-Based Unsupervised Detection of Machine Anomalous Sound Under Domain Shifts,"Thanks to the development of deep learning, machine abnormal sound detection (MASD) based on unsupervised learning has exhibited excellent performance. However, in the task of unsupervised MASD, there are discrepancies between the acoustic characteristics of the test set and the training set under the physical parameter changes (domain shifts) of the same machine's operating conditions. Existing methods not only struggle to stably learn the sound signal features under various domain shifts but also inevitably increase computational overhead. To address these issues, we propose an unsupervised machine abnormal sound detection model based on Transformer and Dynamic Graph Convolution (Unsuper-TDGCN) in this paper. Firstly, we design a network that models time-frequency domain features to capture both global and local spatial and time-frequency interactions, thus improving the model's stability under domain shifts. Then, we introduce a Dynamic Graph Convolutional Network (DyGCN) to model the dependencies between features under domain shifts, enhancing the model's ability to perceive changes in domain features. Finally, a Domain Self-adaptive Network (DSN) is employed to compensate for the performance decline caused by domain shifts, thereby improving the model's adaptive ability for detecting anomalous sounds in MASD tasks under domain shifts. The effectiveness of our proposed model has been validated on multiple datasets.","['unsupervised learning', 'Transformer', 'Dynamic Graph Convolutional Network (DyGCN)']","The research addresses the challenge of detecting abnormal sounds in machines when there are changes in the physical operating conditions that cause variations in acoustic characteristics between training and testing environments. These variations, known as domain shifts, make it difficult to consistently identify sound anomalies and maintain stable detection performance. The study aims to improve the stability and adaptability of abnormal sound detection in machines under varying operating conditions by enhancing the ability to capture and respond to changes in sound features caused by domain shifts. The primary objective is to develop a method that can effectively detect anomalous machine sounds despite differences in acoustic characteristics due to changes in the machine’s physical parameters, thereby improving detection reliability across diverse operating scenarios.","The research addresses the challenge of detecting abnormal sounds in machines when there are changes in the physical operating conditions that cause variations in acoustic characteristics between training and testing environments. These variations, known as domain shifts, make it difficult to consistently identify sound anomalies and maintain stable detection performance. The study aims to improve the stability and adaptability of abnormal sound detection in machines under varying operating conditions by enhancing the ability to capture and respond to changes in sound features caused by domain shifts. The primary objective is to develop a method that can effectively detect anomalous machine sounds despite differences in acoustic characteristics due to changes in the machine's physical parameters, thereby improving detection reliability across diverse operating scenarios.",True
Engineering,Artificial intelligence for literature reviews: opportunities and challenges,"Abstract This paper presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous and organised methodology that assesses and integrates prior research on a given topic. Numerous tools have been developed to assist and partially automate the SLR process. The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews. Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases. We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features. We also analyse 11 recent tools that leverage large language models for searching the literature and assisting academic writing. Finally, the paper discusses current trends in the field, outlines key research challenges, and suggests directions for future research. We highlight three primary research challenges: integrating advanced AI solutions, such as large language models and knowledge graphs, improving usability, and developing a standardised evaluation framework. We also propose best practices to ensure more robust evaluations in terms of performance, usability, and transparency. Overall, this review offers a detailed overview of AI-enhanced SLR tools for researchers and practitioners, providing a foundation for the development of next-generation AI solutions in this field.",['knowledge graphs'],"The research idea addresses the need for more effective support in conducting systematic literature reviews (SLRs), which are rigorous and organized methodologies for assessing and integrating prior research on a given topic. The study recognizes the growing importance of semi-automating the SLR process, particularly in the screening and extraction phases, to enhance efficiency and accuracy. It highlights the challenges involved in improving usability and establishing standardized evaluation frameworks for tools that assist in literature reviews. The research objective is to examine and evaluate existing tools designed to support the semi-automation of systematic literature reviews, focusing on their features and capabilities in the screening and extraction stages. The study aims to identify current trends, key challenges, and best practices to guide the development of more robust and user-friendly solutions for conducting systematic literature reviews in the future.","The research idea addresses the need for more effective support in conducting systematic literature reviews (SLRs), which are rigorous and organized methodologies for assessing and integrating prior research on a given topic. The study recognizes the growing importance of assistance tools in the SLR process, particularly in the screening and extraction phases, to enhance efficiency and accuracy. It highlights the challenges involved in improving usability and establishing standardized evaluation frameworks for tools that assist in literature reviews. The research objective is to examine and evaluate existing tools designed to support the systematic literature review process, focusing on their features and capabilities in the screening and extraction stages. The study aims to identify current trends, key challenges, and best practices to guide the development of more robust and user-friendly solutions for conducting systematic literature reviews in the future.",True
Engineering,Artificial intelligence (AI) in renewable energy: A review of predictive maintenance and energy optimization,"The integration of Artificial Intelligence (AI) in the renewable energy sector has emerged as a transformative force, enhancing the efficiency and sustainability of energy systems. This paper provides a comprehensive review of the application of AI in two critical aspects of renewable energy in relation to predictive maintenance and energy optimization. Predictive maintenance, enabled by AI, has revolutionized the renewable energy landscape by predicting and preventing equipment failures before they occur. Utilizing machine learning algorithms, AI analyzes vast amounts of data from sensors and historical performance to identify patterns indicative of potential faults. This proactive approach not only minimizes downtime but also extends the lifespan of renewable energy infrastructure, resulting in substantial cost savings and improved reliability. Furthermore, AI plays a pivotal role in optimizing the energy output of renewable sources. Through advanced data analytics and real-time monitoring, AI algorithms can adapt to changing environmental conditions, predicting energy production patterns and optimizing resource allocation. This ensures maximum energy yield from renewable sources, making them more competitive with traditional energy sources. The paper delves into specific AI techniques such as deep learning, neural networks, and predictive analytics employed for predictive maintenance and energy optimization in various renewable energy systems like solar, wind, and hydropower. Challenges and opportunities associated with implementing AI in renewable energy are discussed, including data security, interoperability, and the need for standardized frameworks. The synthesis of AI technologies with renewable energy not only addresses operational challenges but also contributes to the global transition towards sustainable and clean energy solutions. This review serves as a valuable resource for researchers, practitioners, and policymakers seeking insights into the evolving landscape of AI applications in the renewable energy sector. As technology continues to advance, the synergies between AI and renewable energy are poised to shape the future of the global energy paradigm.","['deep learning', 'neural networks']","The research idea centers on improving the efficiency and sustainability of renewable energy systems by addressing critical challenges related to equipment maintenance and energy output optimization. The study highlights the importance of proactive maintenance to prevent equipment failures and extend the lifespan of renewable energy infrastructure, thereby reducing downtime and costs. Additionally, it emphasizes the need to maximize energy yield from renewable sources by adapting to changing environmental conditions and optimizing resource allocation. The research objective is to comprehensively review the advancements in enhancing predictive maintenance and energy optimization within renewable energy systems, focusing on how these improvements contribute to operational reliability and increased competitiveness of renewable energy compared to traditional sources. The study aims to provide insights into overcoming challenges such as data security and interoperability while supporting the global transition toward sustainable and clean energy solutions.","The research idea centers on improving the efficiency and sustainability of renewable energy systems by addressing critical challenges related to equipment maintenance and energy output optimization. The study highlights the importance of proactive maintenance to prevent equipment failures and extend the lifespan of renewable energy infrastructure, thereby reducing downtime and costs. Additionally, it emphasizes the need to maximize energy yield from renewable sources by adapting to changing environmental conditions and optimizing resource allocation. The research objective is to comprehensively review the advancements in enhancing maintenance planning and energy optimization within renewable energy systems, focusing on how these improvements contribute to operational reliability and increased competitiveness of renewable energy compared to traditional sources. The study aims to provide insights into overcoming challenges such as data security and interoperability while supporting the global transition toward sustainable and clean energy solutions.",True
Engineering,Generative Pre-Trained Transformer (GPT) in Research: A Systematic Review on Data Augmentation,"GPT (Generative Pre-trained Transformer) represents advanced language models that have significantly reshaped the academic writing landscape. These sophisticated language models offer invaluable support throughout all phases of research work, facilitating idea generation, enhancing drafting processes, and overcoming challenges like writer’s block. Their capabilities extend beyond conventional applications, contributing to critical analysis, data augmentation, and research design, thereby elevating the efficiency and quality of scholarly endeavors. Strategically narrowing its focus, this review explores alternative dimensions of GPT and LLM applications, specifically data augmentation and the generation of synthetic data for research. Employing a meticulous examination of 412 scholarly works, it distills a selection of 77 contributions addressing three critical research questions: (1) GPT on Generating Research data, (2) GPT on Data Analysis, and (3) GPT on Research Design. The systematic literature review adeptly highlights the central focus on data augmentation, encapsulating 48 pertinent scholarly contributions, and extends to the proactive role of GPT in critical analysis of research data and shaping research design. Pioneering a comprehensive classification framework for “GPT’s use on Research Data”, the study classifies existing literature into six categories and 14 sub-categories, providing profound insights into the multifaceted applications of GPT in research data. This study meticulously compares 54 pieces of literature, evaluating research domains, methodologies, and advantages and disadvantages, providing scholars with profound insights crucial for the seamless integration of GPT across diverse phases of their scholarly pursuits.",['Generative Pre-trained Transformer (GPT)'],"The research addresses the evolving role of advanced language models in enhancing various phases of academic research, particularly focusing on their impact on data augmentation and the generation of synthetic research data. It highlights the significance of these models in improving the efficiency and quality of scholarly work by supporting critical analysis and research design. The primary aim of the study is to systematically review and classify existing literature on the use of these language models in research data, specifically examining their applications in generating research data, data analysis, and research design. By developing a comprehensive classification framework and comparing numerous scholarly contributions, the study seeks to provide detailed insights into the advantages, disadvantages, and diverse applications of these models within the research process.","The research addresses the evolving role of computational resources in enhancing various phases of academic research, particularly focusing on their impact on data augmentation and the generation of synthetic research data. It highlights the significance of these tools in improving the efficiency and quality of scholarly work by supporting critical analysis and research design. The primary aim of the study is to systematically review and classify existing literature on the use of these resources in research data, specifically examining their applications in generating research data, data analysis, and research design. By developing a comprehensive classification framework and comparing numerous scholarly contributions, the study seeks to provide detailed insights into the advantages, disadvantages, and diverse applications of these tools within the research process.",True
Engineering,Learning to Aggregate Multi-Scale Context for Instance Segmentation in Remote Sensing Images,"The task of instance segmentation in remote sensing images, aiming at performing per-pixel labeling of objects at the instance level, is of great importance for various civil applications. Despite previous successes, most existing instance segmentation methods designed for natural images encounter sharp performance degradations when they are directly applied to top-view remote sensing images. Through careful analysis, we observe that the challenges mainly come from the lack of discriminative object features due to severe scale variations, low contrasts, and clustered distributions. In order to address these problems, a novel context aggregation network (CATNet) is proposed to improve the feature extraction process. The proposed model exploits three lightweight plug-and-play modules, namely, dense feature pyramid network (DenseFPN), spatial context pyramid (SCP), and hierarchical region of interest extractor (HRoIE), to aggregate global visual context at feature, spatial, and instance domains, respectively. DenseFPN is a multi-scale feature propagation module that establishes more flexible information flows by adopting interlevel residual connections, cross-level dense connections, and feature reweighting strategy. Leveraging the attention mechanism, SCP further augments the features by aggregating global spatial context into local regions. For each instance, HRoIE adaptively generates RoI features for different downstream tasks. Extensive evaluations of the proposed scheme on iSAID, DIOR, NWPU VHR-10, and HRSID datasets demonstrate that the proposed approach outperforms state-of-the-arts under similar computational costs. Source code and pretrained models are available at https://github.com/yeliudev/CATNet.","['dense feature pyramid network (DenseFPN)', 'attention mechanism']","The research addresses the challenge of accurately identifying and labeling individual objects in top-view remote sensing images, which is crucial for various civil applications. Existing methods that work well on natural images often perform poorly on remote sensing images due to issues such as severe scale variations, low contrasts, and clustered object distributions that reduce the distinctiveness of object features. The primary objective of the study is to enhance the feature extraction process in remote sensing images to overcome these challenges. This is aimed at improving the accuracy and reliability of instance-level object labeling in remote sensing imagery across multiple benchmark datasets.","The research addresses the challenge of accurately identifying and labeling individual objects in top-view remote sensing images, which is crucial for various civil applications. Existing methods that work well on natural images often perform poorly on remote sensing images due to issues such as severe scale variations, low contrasts, and clustered object distributions that reduce the distinctiveness of object features. The primary objective of the study is to enhance the information extraction process in remote sensing images to overcome these challenges. This is aimed at improving the accuracy and reliability of instance-level object labeling in remote sensing imagery across multiple benchmark datasets.",True
Engineering,"The Convergence of Intelligent Tutoring, Robotics, and IoT in Smart Education for the Transition from Industry 4.0 to 5.0","This review paper provides a comprehensive analysis of the automation of smart education in the context of Industry 5.0 from 78 papers, focusing on the integration of advanced technologies and the development of innovative, effective, and ethical educational solutions for the future workforce. As the world transitions into an era characterized by human–machine collaboration and rapidly evolving technologies, there is an urgent need to recognize the pivotal role of smart education in preparing individuals for the opportunities and challenges presented by the new industrial landscape. The paper examines key components of smart education, including intelligent tutoring systems, adaptive learning environments, learning analytics, and the application of the Internet of Things (IoT) in education. It also discusses the role of advanced technologies such as artificial intelligence (AI), machine learning (ML), robotics, and augmented and virtual reality (AR/VR) in shaping personalized and immersive learning experiences. The review highlights the importance of smart education in addressing the growing demand for upskilling and reskilling, fostering a culture of lifelong learning, and promoting adaptability, resilience, and self-improvement among learners. Furthermore, the paper delves into the challenges and ethical considerations associated with the implementation of smart education, addressing issues such as data privacy, the digital divide, teacher and student readiness, and the potential biases in AI-driven systems. Through a presentation of case studies and examples of successful smart education initiatives, the review aims to inspire educators, policymakers, and industry stakeholders to collaborate and innovate in the design and implementation of effective smart education solutions. Conclusively, the paper outlines emerging trends, future directions, and potential research opportunities in the field of smart education, emphasizing the importance of continuous improvement and the integration of new technologies to ensure that education remains relevant and effective in the context of Industry 5.0. By providing a holistic understanding of the key components, challenges, and potential solutions associated with smart education, this review paper seeks to contribute to the ongoing discourse surrounding the automation of smart education and its role in preparing the workforce for the future of work.",['machine learning (ML)'],"The research idea centers on the critical need to prepare individuals for the evolving industrial landscape characterized by human–machine collaboration and rapidly advancing technologies. It addresses the importance of smart education in equipping the future workforce with the skills required to meet the opportunities and challenges of Industry 5.0, emphasizing upskilling, reskilling, lifelong learning, adaptability, and resilience. The study recognizes the growing demand for innovative and effective educational solutions that integrate emerging technologies to enhance learning experiences while considering ethical and practical challenges. The primary objective of the study is to provide a comprehensive review of the automation of smart education within the context of Industry 5.0 by examining key components, challenges, and potential solutions. It aims to inspire collaboration among educators, policymakers, and industry stakeholders to design and implement effective smart education initiatives that ensure education remains relevant and effective in preparing the workforce for the future of work.","The research idea centers on the critical need to prepare individuals for the evolving industrial landscape characterized by human–machine collaboration and rapidly advancing technologies. It addresses the importance of education innovation in equipping the future workforce with the skills required to meet the opportunities and challenges of Industry 5.0, emphasizing upskilling, reskilling, lifelong learning, adaptability, and resilience. The study recognizes the growing demand for innovative and effective educational solutions that integrate emerging technologies to enhance learning experiences while considering ethical and practical challenges. The primary objective of the study is to provide a comprehensive review of the advancement of education systems within the context of Industry 5.0 by examining key components, challenges, and potential solutions. It aims to inspire collaboration among educators, policymakers, and industry stakeholders to design and implement effective education initiatives that ensure education remains relevant and effective in preparing the workforce for the future of work.",True
Engineering,3DUV-NetR+: A 3D hybrid semantic architecture using transformers for brain tumor segmentation with MultiModal MR images,"Brain tumor segmentation plays a substantial role in Medical Image Analysis (MIS). In this regard, automatic segmentation methods facilitate precise and efficient segmentation, significantly contributing to diagnosis and treatment planning in medical applications. Recently, several Deep Learning-based architectures have been proposed to revolutionize the MIS field. Particularly, the combination of Convolution Neural Networks (CNNs) and Transformers has greatly enhanced and developed segmentation results. Moreover, the Attention mechanism in Transformers allows the modeling of long-range contextual features extracted from CNNs' encoder part. This paper proposes a hybrid advanced 3D model for brain tumor segmentation using multi-modal magnetic resonance images. The model benefits from the features extracted from the encoder of 3DU-Net and V-Net architectures at each depth. Then, a concatenation between these features and their fusion is carried out at each decoder depth to build new significant features followed by a 3D convolution layer and Transformers block for more contextual information. In addition, a final convolution block is applied to get the segmented tumor. To this end, the model is evaluated on the BraTS 2020 dataset to segment different sub-regions of brain tumors. The obtained results demonstrate the effectiveness of the proposed model in terms of dice similarity coefficient (DSC) and Hausdorff Distance (HD). For DSC, 91.95% and 82.80% and 81.70% for Whole Tumor(WT), Tumor Core (TC), and Enhancing Tumor(ET), respectively are archived, while for HD, 4.9 mm, 6.0 mm and 3.8 mm for WT, TC and ET are accomplished.","['Convolution Neural Networks (CNNs)', 'Transformers', 'Attention mechanism in Transformers', '3DU-Net', 'V-Net', '3D convolution layer']","The research addresses the critical need for precise and efficient brain tumor segmentation in medical imaging, which is essential for accurate diagnosis and effective treatment planning. Improving segmentation methods can significantly enhance the identification and delineation of different tumor sub-regions, thereby supporting better clinical outcomes. The study aims to develop an advanced approach for segmenting brain tumors using multi-modal magnetic resonance images. Specifically, the primary objective is to achieve accurate segmentation of various tumor sub-regions, including the whole tumor, tumor core, and enhancing tumor, to improve the reliability and effectiveness of medical image analysis in brain tumor cases.","The research addresses the critical need for precise and efficient brain tumor segmentation in medical imaging, which is essential for accurate diagnosis and effective treatment planning. Improving segmentation methods can significantly enhance the identification and delineation of different tumor sub-regions, thereby supporting better clinical outcomes. The study aims to develop an improved approach for segmenting brain tumors using multi-modal magnetic resonance images. Specifically, the primary objective is to achieve accurate segmentation of various tumor sub-regions, including the whole tumor, tumor core, and enhancing tumor, to improve the reliability and effectiveness of medical image analysis in brain tumor cases.",True
Engineering,The diagnostic and triage accuracy of the GPT-3 artificial intelligence model: an observational study,"BackgroundArtificial intelligence (AI) applications in health care have been effective in many areas of medicine, but they are often trained for a single task using labelled data, making deployment and generalisability challenging. How well a general-purpose AI language model performs diagnosis and triage relative to physicians and laypeople is not well understood.MethodsWe compared the predictive accuracy of Generative Pre-trained Transformer 3 (GPT-3)'s diagnostic and triage ability for 48 validated synthetic case vignettes (<50 words; sixth-grade reading level or below) of both common (eg, viral illness) and severe (eg, heart attack) conditions to a nationally representative sample of 5000 lay people from the USA who could use the internet to find the correct options and 21 practising physicians at Harvard Medical School. There were 12 vignettes for each of four triage categories: emergent, within one day, within 1 week, and self-care. The correct diagnosis and triage category (ie, ground truth) for each vignette was determined by two general internists at Harvard Medical School. For each vignette, human respondents and GPT-3 were prompted to list diagnoses in order of likelihood, and the vignette was marked as correct if the ground-truth diagnosis was in the top three of the listed diagnoses. For triage accuracy, we examined whether the human respondents' and GPT-3's selected triage was exactly correct according to the four triage categories, or matched a dichotomised triage variable (emergent or within 1 day vs within 1 week or self-care). We estimated GPT-3's diagnostic and triage confidence on a given vignette using a modified bootstrap resampling procedure, and examined how well calibrated GPT-3's confidence was by computing calibration curves and Brier scores. We also performed subgroup analysis by case acuity, and an error analysis for triage advice to characterise how its advice might affect patients using this tool to decide if they should seek medical care immediately.FindingsAmong all cases, GPT-3 replied with the correct diagnosis in its top three for 88% (42/48, 95% CI 75–94) of cases, compared with 54% (2700/5000, 53–55) for lay individuals (p<0.0001) and 96% (637/666, 94–97) for physicians (p=0·012). GPT-3 triaged 70% correct (34/48, 57–82) versus 74% (3706/5000, 73–75; p=0.60) for lay individuals and 91% (608/666, 89–93%; p<0.0001) for physicians. As measured by the Brier score, GPT-3 confidence in its top prediction was reasonably well calibrated for diagnosis (Brier score=0·18) and triage (Brier score=0·22). We observed an inverse relationship between case acuity and GPT-3 accuracy (p<0·0001) with a fitted trend line of –8·33% decrease in accuracy for every level of increase in case acuity. For triage error analysis, GPT-3 deprioritised truly emergent cases in seven instances.InterpretationA general-purpose AI language model without any content-specific training could perform diagnosis at levels close to, but below, physicians and better than lay individuals. We found that GPT-3's performance was inferior to physicians for triage, sometimes by a large margin, and its performance was closer to that of lay individuals. Although the diagnostic performance of GPT-3 was comparable to physicians, it was significantly better than a typical person using a search engine.FundingThe National Heart, Lung, and Blood Institute.",['Generative Pre-trained Transformer 3 (GPT-3)'],"The research idea addresses the challenge of evaluating how well a general-purpose diagnostic and triage approach performs in comparison to physicians and laypeople, particularly given the difficulty of deploying tools that are typically trained for single tasks and may lack generalizability across different medical conditions. The study is motivated by the need to understand the accuracy and reliability of such an approach in identifying both common and severe health conditions and providing appropriate triage recommendations. The primary objective of the study is to compare the diagnostic and triage accuracy of a general-purpose approach against that of practicing physicians and lay individuals using a set of validated synthetic medical case vignettes. The aim is to assess the correctness of diagnoses and triage decisions across varying levels of case acuity and to evaluate how well confidence in these decisions corresponds to actual accuracy.","The research idea addresses the challenge of evaluating how well a general-purpose diagnostic and triage approach performs in comparison to physicians and laypeople, particularly given the difficulty of deploying tools that are typically designed for single tasks and may lack generalizability across different medical conditions. The study is motivated by the need to understand the accuracy and reliability of such an approach in identifying both common and severe health conditions and providing appropriate triage recommendations. The primary objective of the study is to compare the diagnostic and triage accuracy of a general-purpose approach against that of practicing physicians and lay individuals using a set of validated synthetic medical case vignettes. The aim is to assess the correctness of diagnoses and triage decisions across varying levels of case acuity and to evaluate how well confidence in these decisions corresponds to actual accuracy.",True
Social Sciences,Systematic literature review: Quantum machine learning and its applications,"Quantum physics has changed the way we understand our environment, and one of its branches, quantum mechanics, has demonstrated accurate and consistent theoretical results. Quantum computing is the process of performing calculations using quantum mechanics. This field studies the quantum behavior of certain subatomic particles (photons, electrons, etc.) for subsequent use in performing calculations, as well as for large-scale information processing. These advantages are achieved through the use of quantum features, such as entanglement or superposition. These capabilities can give quantum computers an advantage in terms of computational time and cost over classical computers. Nowadays, scientific challenges are impossible to perform by classical computation due to computational complexity (more bytes than atoms in the observable universe) or the time it would take (thousands of years), and quantum computation is the only known answer. However, current quantum devices do not have yet the necessary qubits and are not fault-tolerant enough to achieve these goals. Nonetheless, there are other fields like machine learning, finance, or chemistry where quantum computation could be useful with current quantum devices. This manuscript aims to present a review of the literature published between 2017 and 2023 to identify, analyze, and classify the different types of algorithms used in quantum machine learning and their applications. The methodology follows the guidelines related to Systematic Literature Review methods, such as the one proposed by Kitchenham and other authors in the software engineering field. Consequently, this study identified 94 articles that used quantum machine learning techniques and algorithms and shows their implementation using computational quantum circuits or ansatzs. The main types of found algorithms are quantum implementations of classical machine learning algorithms, such as support vector machines or the k-nearest neighbor model, and classical deep learning algorithms, like quantum neural networks. One of the most relevant applications in the machine learning field is image classification. Many articles, especially within the classification, try to solve problems currently answered by classical machine learning but using quantum devices and algorithms. Even though results are promising, quantum machine learning is far from achieving its full potential. An improvement in quantum hardware is required for this potential to be achieved since the existing quantum computers lack enough quality, speed, and scale to allow quantum computing to achieve its full potential.","['support vector machines', 'k-nearest neighbor model', 'quantum neural networks']","The research idea centers on the limitations of current computational methods in addressing complex scientific challenges due to their extensive time and resource requirements, highlighting the potential of emerging technologies to overcome these barriers. It recognizes that while traditional approaches struggle with problems of immense complexity, alternative computational paradigms offer promising advantages that could transform various fields such as finance and chemistry. The research objective is to review and categorize the existing literature from 2017 to 2023 to identify and analyze different approaches and their applications within this emerging computational paradigm. The study aims to provide a comprehensive overview of the types of techniques employed and their practical uses, emphasizing the current state of development and the need for advancements in underlying technology to fully realize their potential.","The research idea centers on the limitations of current conventional methods in addressing complex scientific challenges due to their extensive time and resource requirements, highlighting the potential of emerging technologies to overcome these barriers. It recognizes that while traditional approaches struggle with problems of immense complexity, alternative computational paradigms offer promising advantages that could transform various fields such as finance and chemistry. The research objective is to review and categorize the existing literature from 2017 to 2023 to identify and analyze different approaches and their applications within this emerging computational paradigm. The study aims to provide a comprehensive overview of the types of techniques employed and their practical uses, emphasizing the current state of development and the need for advancements in underlying technology to fully realize their potential.",True
Social Sciences,Larger and more instructable language models become less reliable,"Abstract The prevailing methods to make large language models more powerful and amenable have been based on continuous scaling up (that is, increasing their size, data volume and computational resources 1 ) and bespoke shaping up (including post-filtering 2,3 , fine tuning or use of human feedback 4,5 ). However, larger and more instructable large language models may have become less reliable. By studying the relationship between difficulty concordance, task avoidance and prompting stability of several language model families, here we show that easy instances for human participants are also easy for the models, but scaled-up, shaped-up models do not secure areas of low difficulty in which either the model does not err or human supervision can spot the errors. We also find that early models often avoid user questions but scaled-up, shaped-up models tend to give an apparently sensible yet wrong answer much more often, including errors on difficult questions that human supervisors frequently overlook. Moreover, we observe that stability to different natural phrasings of the same question is improved by scaling-up and shaping-up interventions, but pockets of variability persist across difficulty levels. These findings highlight the need for a fundamental shift in the design and development of general-purpose artificial intelligence, particularly in high-stakes areas for which a predictable distribution of errors is paramount.","['post-filtering', 'fine tuning', 'use of human feedback']","The research idea centers on the challenges associated with improving the reliability and accuracy of large language models, particularly as they become larger and more complex. The study addresses the problem that despite advancements aimed at enhancing these models, they may still produce errors that are difficult for human supervisors to detect, especially on tasks of varying difficulty. The research objective is to investigate the relationship between task difficulty, error occurrence, and response consistency in different language model versions. The study aims to understand how scaling and refinement efforts impact the models’ ability to handle easy versus difficult tasks and to highlight the implications for designing more dependable systems in contexts where error predictability is critical.","The research idea centers on the challenges associated with improving the reliability and accuracy of language processing systems, particularly as they become larger and more complex. The study addresses the problem that despite advancements aimed at enhancing these systems, they may still produce errors that are difficult for human supervisors to detect, especially on tasks of varying difficulty. The research objective is to investigate the relationship between task difficulty, error occurrence, and response consistency in different system versions. The study aims to understand how scaling and refinement efforts impact the systems' ability to handle easy versus difficult tasks and to highlight the implications for designing more dependable systems in contexts where error predictability is critical.",True
Social Sciences,Artificial intelligence-driven virtual rehabilitation for people living in the community: A scoping review,"Abstract Virtual Rehabilitation (VRehab) is a promising approach to improving the physical and mental functioning of patients living in the community. The use of VRehab technology results in the generation of multi-modal datasets collected through various devices. This presents opportunities for the development of Artificial Intelligence (AI) techniques in VRehab, namely the measurement, detection, and prediction of various patients’ health outcomes. The objective of this scoping review was to explore the applications and effectiveness of incorporating AI into home-based VRehab programs. PubMed/MEDLINE, Embase, IEEE Xplore, Web of Science databases, and Google Scholar were searched from inception until June 2023 for studies that applied AI for the delivery of VRehab programs to the homes of adult patients. After screening 2172 unique titles and abstracts and 51 full-text studies, 13 studies were included in the review. A variety of AI algorithms were applied to analyze data collected from various sensors and make inferences about patients’ health outcomes, most involving evaluating patients’ exercise quality and providing feedback to patients. The AI algorithms used in the studies were mostly fuzzy rule-based methods, template matching, and deep neural networks. Despite the growing body of literature on the use of AI in VRehab, very few studies have examined its use in patients’ homes. Current research suggests that integrating AI with home-based VRehab can lead to improved rehabilitation outcomes for patients. However, further research is required to fully assess the effectiveness of various forms of AI-driven home-based VRehab, taking into account its unique challenges and using standardized metrics.","['fuzzy rule-based methods', 'deep neural networks']","The research idea centers on the potential of virtual rehabilitation (VRehab) to enhance the physical and mental functioning of patients living in the community, particularly through home-based programs. Despite the increasing interest in VRehab, there is limited understanding of its application and effectiveness when delivered in patients’ homes. The study aims to explore how integrating advanced techniques into home-based VRehab programs can improve rehabilitation outcomes for adult patients. The primary objective of the study was to examine the applications and effectiveness of incorporating these techniques into home-based virtual rehabilitation programs by reviewing existing research on their use with adult patients in community settings.","The research idea centers on the potential of virtual rehabilitation (VRehab) to enhance the physical and mental functioning of patients living in the community, particularly through home-based programs. Despite the increasing interest in VRehab, there is limited understanding of its application and effectiveness when delivered in patients' homes. The study aims to explore how integrating innovative approaches into home-based VRehab programs can improve rehabilitation outcomes for adult patients. The primary objective of the study was to examine the applications and effectiveness of incorporating these approaches into home-based virtual rehabilitation programs by reviewing existing research on their use with adult patients in community settings.",True
Social Sciences,Applications and challenges of neural networks in otolaryngology (Review),"Artificial Intelligence (AI) has become a topic of interest that is frequently debated in all research fields. The medical field is no exception, where several unanswered questions remain. When and how this field can benefit from AI support in daily routines are the most frequently asked questions. The present review aims to present the types of neural networks (NNs) available for development, discussing their advantages, disadvantages and how they can be applied practically. In addition, the present review summarizes how NNs (combined with various other features) have already been applied in studies in the ear nose throat research field, from assisting diagnosis to treatment management. Although the answer to this question regarding AI remains elusive, understanding the basics and types of applicable NNs can lead to future studies possibly using more than one type of NN. This approach may bypass the actual limitations in accuracy and relevance of information generated by AI. The proposed studies, the majority of which used convolutional NNs, obtained accuracies varying 70-98%, with a number of studies having the AI trained on a limited number of cases (<100 patients). The lack of standardization in AI protocols for research negatively affects data homogeneity and transparency of databases.","['neural networks (NNs)', 'convolutional neural networks (convolutional NNs)']","The research idea centers on the ongoing debate and unresolved questions within the medical field regarding the appropriate timing and manner in which emerging technologies can support daily clinical routines. There is a need to understand the practical applications, advantages, and limitations of these technologies in specific medical specialties, such as ear, nose, and throat research. The study highlights concerns about the lack of standardization in research protocols, which affects the consistency and transparency of information used in this area. The primary objective of the study is to review and summarize the existing types of these technologies available for development, discussing their practical applications, benefits, and drawbacks in the medical field, particularly focusing on their use in diagnosis and treatment management within ear, nose, and throat research. Additionally, the study aims to provide a foundation for future research that may overcome current limitations related to accuracy and relevance of information by exploring combined approaches.","The research idea centers on the ongoing debate and unresolved questions within the medical field regarding the appropriate timing and manner in which emerging technologies can support daily clinical routines. There is a need to understand the practical applications, advantages, and limitations of these technologies in specific medical specialties, such as ear, nose, and throat research. The study highlights concerns about the lack of standardization in research protocols, which affects the consistency and transparency of information used in this area. The primary objective of the study is to review and summarize the existing types of these technologies available for development, discussing their practical applications, benefits, and drawbacks in the medical field, particularly focusing on their use in diagnosis and treatment management within ear, nose, and throat research. Additionally, the study aims to provide a foundation for future research that may overcome current limitations related to accuracy and relevance of information by exploring integrated methodological approaches.",True
Social Sciences,Making Sense of Machine Learning: A Review of Interpretation Techniques and Their Applications,"Transparency in AI models is essential for promoting human–AI collaboration and ensuring regulatory compliance. However, interpreting these models is a complex process influenced by various methods and datasets. This study presents a comprehensive overview of foundational interpretation techniques, meticulously referencing the original authors and emphasizing their pivotal contributions. Recognizing the seminal work of these pioneers is imperative for contextualizing the evolutionary trajectory of interpretation in the field of AI. Furthermore, this research offers a retrospective analysis of interpretation techniques, critically evaluating their inherent strengths and limitations. We categorize these techniques into model-based, representation-based, post hoc, and hybrid methods, delving into their diverse applications. Furthermore, we analyze publication trends over time to see how the adoption of advanced computational methods within various categories of interpretation techniques has shaped the development of AI interpretability over time. This analysis highlights a notable preference shift towards data-driven approaches in the field. Moreover, we consider crucial factors such as the suitability of these techniques for generating local or global insights and their compatibility with different data types, including images, text, and tabular data. This structured categorization serves as a guide for practitioners navigating the landscape of interpretation techniques in AI. In summary, this review not only synthesizes various interpretation techniques but also acknowledges the contributions of their original authors. By emphasizing the origins of these techniques, we aim to enhance AI model explainability and underscore the importance of recognizing biases, uncertainties, and limitations inherent in the methods and datasets. This approach promotes the ethical and practical use of interpretation insights, empowering AI practitioners, researchers, and professionals to make informed decisions when selecting techniques for responsible AI implementation in real-world scenarios.","['model-based methods', 'representation-based methods', 'hybrid methods']","The research idea centers on the importance of transparency to promote effective collaboration and ensure compliance with regulations, highlighting the complexity involved in interpreting models due to the variety of methods and datasets. The study recognizes the significance of acknowledging foundational contributions to understand the development and evolution of interpretation approaches. The primary objective of the study is to provide a comprehensive overview and retrospective evaluation of interpretation techniques, categorizing them and examining their applications, strengths, and limitations. Additionally, the research aims to emphasize the ethical and practical considerations in using these techniques, encouraging informed and responsible decision-making in real-world contexts.","The research idea centers on the importance of transparency to promote effective collaboration and ensure compliance with regulations, highlighting the complexity involved in interpretation due to the variety of methods and datasets. The study recognizes the significance of acknowledging foundational contributions to understand the development and evolution of interpretation approaches. The primary objective of the study is to provide a comprehensive overview and retrospective evaluation of interpretation techniques, categorizing them and examining their applications, strengths, and limitations. Additionally, the research aims to emphasize the ethical and practical considerations in using these techniques, encouraging informed and responsible decision-making in real-world contexts.",True
Social Sciences,Artificial Intelligence to Automate Network Meta-Analyses: Four Case Studies to Evaluate the Potential Application of Large Language Models,"The emergence of artificial intelligence, capable of human-level performance on some tasks, presents an opportunity to revolutionise development of systematic reviews and network meta-analyses (NMAs). In this pilot study, we aim to assess use of a large-language model (LLM, Generative Pre-trained Transformer 4 [GPT-4]) to automatically extract data from publications, write an R script to conduct an NMA and interpret the results. We considered four case studies involving binary and time-to-event outcomes in two disease areas, for which an NMA had previously been conducted manually. For each case study, a Python script was developed that communicated with the LLM via application programming interface (API) calls. The LLM was prompted to extract relevant data from publications, to create an R script to be used to run the NMA and then to produce a small report describing the analysis. The LLM had a > 99% success rate of accurately extracting data across 20 runs for each case study and could generate R scripts that could be run end-to-end without human input. It also produced good quality reports describing the disease area, analysis conducted, results obtained and a correct interpretation of the results. This study provides a promising indication of the feasibility of using current generation LLMs to automate data extraction, code generation and NMA result interpretation, which could result in significant time savings and reduce human error. This is provided that routine technical checks are performed, as recommend for human-conducted analyses. Whilst not currently 100% consistent, LLMs are likely to improve with time.","['large-language model (LLM, Generative Pre-trained Transformer 4 [GPT-4])']","The study addresses the challenge of efficiently conducting systematic reviews and network meta-analyses, which are traditionally time-consuming and prone to human error. It highlights the potential to transform these processes by automating key tasks involved in data extraction and interpretation, thereby improving the speed and accuracy of evidence synthesis in healthcare research. The primary aim of the study is to evaluate the feasibility of automating the extraction of data from publications, the generation of scripts for network meta-analyses, and the interpretation of results across multiple case studies. This pilot investigation seeks to determine whether such automation can produce accurate and reliable outputs that may ultimately enhance the efficiency and quality of systematic reviews and meta-analyses.","The study addresses the challenge of efficiently conducting systematic reviews and network meta-analyses, which are traditionally time-consuming and prone to human error. It highlights the potential to transform these processes by streamlining key tasks involved in data extraction and interpretation, thereby improving the speed and accuracy of evidence synthesis in healthcare research. The primary aim of the study is to evaluate the feasibility of systematizing the extraction of data from publications, the generation of scripts for network meta-analyses, and the interpretation of results across multiple case studies. This pilot investigation seeks to determine whether such systematic approaches can produce accurate and reliable outputs that may ultimately enhance the efficiency and quality of systematic reviews and meta-analyses.",True
Social Sciences,Artificial intelligence for oral squamous cell carcinoma detection based on oral photographs: A comprehensive literature review,"Abstract Introduction Oral squamous cell carcinoma (OSCC) presents a significant global health challenge. The integration of artificial intelligence (AI) and computer vision holds promise for the early detection of OSCC through the analysis of digitized oral photographs. This literature review explores the landscape of AI‐driven OSCC automatic detection, assessing both the performance and limitations of the current state of the art. Materials and Methods An electronic search using several data base was conducted, and a systematic review performed in accordance with PRISMA guidelines (CRD42023441416). Results Several studies have demonstrated remarkable results for this task, consistently achieving sensitivity rates exceeding 85% and accuracy rates surpassing 90%, often encompassing around 1000 images. The review scrutinizes these studies, shedding light on their methodologies, including the use of recent machine learning and pattern recognition approaches coupled with different supervision strategies. However, comparing the results from different papers is challenging due to variations in the datasets used. Discussion Considering these findings, this review underscores the urgent need for more robust and reliable datasets in the field of OSCC detection. Furthermore, it highlights the potential of advanced techniques such as multi‐task learning, attention mechanisms, and ensemble learning as crucial tools in enhancing the accuracy and sensitivity of OSCC detection through oral photographs. Conclusion These insights collectively emphasize the transformative impact of AI‐driven approaches on early OSCC diagnosis, with the potential to significantly improve patient outcomes and healthcare practices.","['machine learning', 'multi-task learning', 'attention mechanisms', 'ensemble learning']","The research addresses the significant global health challenge posed by oral squamous cell carcinoma (OSCC) and the importance of early detection to improve patient outcomes. It highlights the current landscape of efforts aimed at identifying OSCC through the examination of oral photographs, emphasizing the need for more reliable and robust datasets to support these efforts. The primary objective of the study is to review and assess the existing literature on automatic detection of OSCC, focusing on the performance and limitations of current approaches. This review aims to provide insights that could enhance early diagnosis practices and ultimately contribute to better healthcare outcomes for patients affected by OSCC.","The research addresses the significant global health challenge posed by oral squamous cell carcinoma (OSCC) and the importance of early detection to improve patient outcomes. It highlights the current landscape of efforts aimed at identifying OSCC through the examination of oral photographs, emphasizing the need for more reliable and robust datasets to support these efforts. The primary objective of the study is to review and assess the existing literature on detection of OSCC, focusing on the performance and limitations of current approaches. This review aims to provide insights that could enhance early diagnosis practices and ultimately contribute to better healthcare outcomes for patients affected by OSCC.",True
Social Sciences,Diagnostic Performance Comparison between Generative AI and Physicians: A Systematic Review and Meta-Analysis,"Abstract Background The rapid advancement of generative artificial intelligence (AI) has led to the wide dissemination of models with exceptional understanding and generation of human language. Their integration into healthcare has shown potential for improving medical diagnostics, yet a comprehensive diagnostic performance evaluation of generative AI models and the comparison of their diagnostic performance with that of physicians has not been extensively explored. Methods In this systematic review and meta-analysis, a comprehensive search of Medline, Scopus, Web of Science, Cochrane Central, and MedRxiv was conducted for studies published from June 2018 through December 2023, focusing on those that validate generative AI models for diagnostic tasks. The risk of bias was assessed using the Prediction Model Study Risk of Bias Assessment Tool. Meta-regression was performed to summarize the performance of the models and to compare the accuracy of the models with that of physicians. Results The search resulted in 54 studies being included in the meta-analysis. Nine generative AI models were evaluated across 17 medical specialties. The quality assessment indicated a high risk of bias in the majority of studies, primarily due to small sample sizes. The overall accuracy for generative AI models across 54 studies was 56.9% (95% confidence interval [CI]: 51.0–62.7%). The meta-analysis demonstrated that, on average, physicians exceeded the accuracy of the models (difference in accuracy: 14.4% [95% CI: 4.9–23.8%], p-value =0.004). However, both Prometheus (Bing) and GPT-4 showed slightly better performance compared to non-experts (-2.3% [95% CI: -27.0–22.4%], p-value = 0.848 and -0.32% [95% CI: -14.4–13.7%], p-value = 0.962), but slightly underperformed when compared to experts (10.9% [95% CI: -13.1–35.0%], p-value = 0.356 and 12.9% [95% CI: 0.15–25.7%], p-value = 0.048). The sub-analysis revealed significantly improved accuracy in the fields of Gynecology, Pediatrics, Orthopedic surgery, Plastic surgery, and Otolaryngology, while showing reduced accuracy for Neurology, Psychiatry, Rheumatology, and Endocrinology compared to that of General Medicine. No significant heterogeneity was observed based on the risk of bias. Conclusions Generative AI exhibits promising diagnostic capabilities, with accuracy varying significantly by model and medical specialty. Although they have not reached the reliability of expert physicians, the findings suggest that generative AI models have the potential to enhance healthcare delivery and medical education, provided they are integrated with caution and their limitations are well-understood. Key Points Question: What is the diagnostic accuracy of generative AI models and how does this accuracy compare to that of physicians? Findings: This meta-analysis found that generative AI models have a pooled accuracy of 56.9% (95% confidence interval: 51.0–62.7%). The accuracy of expert physicians exceeds that of AI in all specialties, however, some generative AI models are comparable to non-expert physicians. Meaning: The diagnostic performance of generative AI models suggests that they do not match the level of experienced physicians but that they may have potential applications in healthcare delivery and medical education.",['generative AI models'],"The research idea addresses the growing integration of advanced language-based technologies into healthcare, highlighting the need to understand their diagnostic performance compared to human physicians. Despite the potential for improving medical diagnostics, there has been limited comprehensive evaluation of how these technologies perform across various medical specialties and in comparison to expert and non-expert physicians. The study aims to fill this gap by systematically reviewing and synthesizing existing evidence on diagnostic accuracy. The primary objective of the study is to evaluate the diagnostic accuracy of these technologies across multiple medical specialties and to compare their performance directly with that of physicians, both experts and non-experts. This comparison seeks to determine the extent to which these technologies can support or enhance healthcare delivery and medical education while acknowledging their current limitations relative to experienced medical professionals.","The research idea addresses the growing integration of advanced language-based tools into healthcare, highlighting the need to understand their diagnostic performance compared to human physicians. Despite the potential for improving medical diagnostics, there has been limited comprehensive evaluation of how these tools perform across various medical specialties and in comparison to expert and non-expert physicians. The study aims to fill this gap by systematically reviewing and synthesizing existing evidence on diagnostic accuracy. The primary objective of the study is to evaluate the diagnostic accuracy of these technologies across multiple medical specialties and to compare their performance directly with that of physicians, both experts and non-experts. This comparison seeks to determine the extent to which these technologies can support or enhance healthcare delivery and medical education while acknowledging their current limitations relative to experienced medical professionals.",True
Social Sciences,Methodological insights into ChatGPT’s screening performance in systematic reviews,"Abstract Background The screening process for systematic reviews and meta-analyses in medical research is a labor-intensive and time-consuming task. While machine learning and deep learning have been applied to facilitate this process, these methods often require training data and user annotation. This study aims to assess the efficacy of ChatGPT, a large language model based on the Generative Pretrained Transformers (GPT) architecture, in automating the screening process for systematic reviews in radiology without the need for training data. Methods A prospective simulation study was conducted between May 2nd and 24th, 2023, comparing ChatGPT’s performance in screening abstracts against that of general physicians (GPs). A total of 1198 abstracts across three subfields of radiology were evaluated. Metrics such as sensitivity, specificity, positive and negative predictive values (PPV and NPV), workload saving, and others were employed. Statistical analyses included the Kappa coefficient for inter-rater agreement, ROC curve plotting, AUC calculation, and bootstrapping for p-values and confidence intervals. Results ChatGPT completed the screening process within an hour, while GPs took an average of 7–10 days. The AI model achieved a sensitivity of 95% and an NPV of 99%, slightly outperforming the GPs’ sensitive consensus (i.e., including records if at least one person includes them). It also exhibited remarkably low false negative counts and high workload savings, ranging from 40 to 83%. However, ChatGPT had lower specificity and PPV compared to human raters. The average Kappa agreement between ChatGPT and other raters was 0.27. Conclusions ChatGPT shows promise in automating the article screening phase of systematic reviews, achieving high sensitivity and workload savings. While not entirely replacing human expertise, it could serve as an efficient first-line screening tool, particularly in reducing the burden on human resources. Further studies are needed to fine-tune its capabilities and validate its utility across different medical subfields.","['machine learning', 'deep learning', 'Generative Pretrained Transformers (GPT) architecture']","The research addresses the challenge of the labor-intensive and time-consuming screening process for systematic reviews and meta-analyses in medical research, which requires significant human effort and resources. This study is motivated by the need to find more efficient ways to manage the screening workload without compromising accuracy. The primary objective of the study is to evaluate the effectiveness of an automated approach in conducting the screening process for systematic reviews in radiology, aiming to reduce the burden on human reviewers while maintaining high sensitivity and reliability. The study seeks to determine whether this approach can serve as a useful first-line screening tool to improve efficiency in the review process across different medical subfields.","The research addresses the challenge of the labor-intensive and time-consuming screening process for systematic reviews and meta-analyses in medical research, which requires significant human effort and resources. This study is motivated by the need to find more efficient ways to manage the screening workload without compromising accuracy. The primary objective of the study is to evaluate the effectiveness of a computerized assistance system in conducting the screening process for systematic reviews in radiology, aiming to reduce the burden on human reviewers while maintaining high sensitivity and reliability. The study seeks to determine whether this approach can serve as a useful first-line screening tool to improve efficiency in the review process across different medical subfields.",True
Social Sciences,AI-POWERED FRAUD DETECTION IN BANKING: SAFEGUARDING FINANCIAL TRANSACTIONS,"The banking industry's metamorphosis through digitalization has unquestionably revolutionized accessibility and convenience for customers worldwide. However, this paradigm shift has ushered in a new era of challenges, most notably in the realm of cybersecurity. Conventional rule-based fraud detection strategies have struggled to keep pace with the rapid evolution of cyber threats, prompting a surge of interest in more adaptive approaches like unsupervised learning. Furthermore, the COVID-19 pandemic has exacerbated the issue of bank fraud due to the widespread transition to online platforms and the proliferation of charitable funds, which present ripe opportunities for exploitation by cybercriminals. In response to these pressing concerns, this study delves into the realm of machine learning algorithms for the analysis and identification of fraudulent banking transactions. Notably, it contributes scientific novelty by developing models specifically tailored to this purpose and implementing innovative preprocessing techniques to enhance detection accuracy. Utilizing a diverse array of algorithms, including Random Forest, K-Nearest Neighbor (KNN), Naïve Bayes, Decision Trees, and Logistic Regression, the study showcases promising results. In particular, logistic regression and decision tree models exhibit impressive accuracy and Area Under the Curve (AUC) values of approximately 0.98, 0.97 and 0.95, 0.94, respectively. Given the pervasive nature of banking fraud in our digital society, the utilization of artificial intelligence algorithms for fraud detection stands as a critical and timely endeavor, promising enhanced security and trust in the financial ecosystem.","['unsupervised learning', 'Random Forest', 'K-Nearest Neighbor (KNN)', 'Naïve Bayes', 'Decision Trees', 'Logistic Regression']","The banking industry’s transformation through digitalization has significantly improved customer accessibility and convenience but has also introduced new challenges, particularly in the area of cybersecurity. The rapid evolution of cyber threats has rendered traditional fraud detection methods less effective, while the COVID-19 pandemic has intensified the problem of bank fraud due to increased online activity and the rise of charitable fund transactions vulnerable to exploitation. This study aims to address these pressing concerns by focusing on the identification and analysis of fraudulent banking transactions. Its primary objective is to develop and implement approaches specifically designed to enhance the detection of bank fraud, thereby contributing to improved security and trust within the financial sector.","The banking industry's transformation through digitalization has significantly improved customer accessibility and convenience but has also introduced new challenges, particularly in the area of cybersecurity. The rapid evolution of cyber threats has rendered traditional fraud detection methods less effective, while the COVID-19 pandemic has intensified the problem of bank fraud due to increased online activity and the rise of charitable fund transactions vulnerable to exploitation. This study aims to address these pressing concerns by focusing on the identification and analysis of fraudulent banking transactions. Its primary objective is to develop and implement strategies specifically designed to enhance the detection of bank fraud, thereby contributing to improved security and trust within the financial sector.",True
Social Sciences,Do large language models show decision heuristics similar to humans? A case study using GPT-3.5.,"A Large Language Model (LLM) is an artificial intelligence system trained on vast amounts of natural language data, enabling it to generate human-like responses to written or spoken language input. Generative Pre-Trained Transformer (GPT)-3.5 is an example of an LLM that supports a conversational agent called ChatGPT. In this work, we used a series of novel prompts to determine whether ChatGPT shows heuristics and other context-sensitive responses. We also tested the same prompts on human participants. Across four studies, we found that ChatGPT was influenced by random anchors in making estimates (anchoring, Study 1); it judged the likelihood of two events occurring together to be higher than the likelihood of either event occurring alone, and it was influenced by anecdotal information (representativeness and availability heuristic, Study 2); it found an item to be more efficacious when its features were presented positively rather than negatively-even though both presentations contained statistically equivalent information (framing effect, Study 3); and it valued an owned item more than a newly found item even though the two items were objectively identical (endowment effect, Study 4). In each study, human participants showed similar effects. Heuristics and context-sensitive responses in humans are thought to be driven by cognitive and affective processes such as loss aversion and effort reduction. The fact that an LLM-which lacks these processes-also shows such responses invites consideration of the possibility that language is sufficiently rich to carry these effects and may play a role in generating these effects in humans. (PsycInfo Database Record (c) 2024 APA, all rights reserved).",['Generative Pre-Trained Transformer (GPT)-3.5'],"The research idea centers on understanding whether certain cognitive biases and context-sensitive responses, typically observed in human judgment and decision-making, can also be exhibited by a language-based conversational agent. This inquiry is motivated by the observation that humans display heuristics such as anchoring, representativeness, availability, framing effects, and the endowment effect, which are thought to arise from cognitive and affective processes. The study explores the possibility that language itself may carry these effects and contribute to their manifestation in human cognition. The primary objective of the study is to investigate whether a conversational agent demonstrates similar heuristics and context-sensitive responses as humans do across various judgment tasks. By comparing the agent’s responses to those of human participants, the study aims to assess the extent to which language alone might account for these cognitive biases.","The research idea centers on understanding whether certain cognitive biases and context-sensitive responses, typically observed in human judgment and decision-making, can also be exhibited by a language-based conversational system. This inquiry is motivated by the observation that humans display heuristics such as anchoring, representativeness, availability, framing effects, and the endowment effect, which are thought to arise from cognitive and affective processes. The study explores the possibility that language itself may carry these effects and contribute to their manifestation in human cognition. The primary objective of the study is to investigate whether a conversational system demonstrates similar heuristics and context-sensitive responses as humans do across various judgment tasks. By comparing the system's responses to those of human participants, the study aims to assess the extent to which language alone might account for these cognitive biases.",True
Social Sciences,Likelihood-based feature representation learning combined with neighborhood information for predicting circRNA–miRNA associations,"Connections between circular RNAs (circRNAs) and microRNAs (miRNAs) assume a pivotal position in the onset, evolution, diagnosis and treatment of diseases and tumors. Selecting the most potential circRNA-related miRNAs and taking advantage of them as the biological markers or drug targets could be conducive to dealing with complex human diseases through preventive strategies, diagnostic procedures and therapeutic approaches. Compared to traditional biological experiments, leveraging computational models to integrate diverse biological data in order to infer potential associations proves to be a more efficient and cost-effective approach. This paper developed a model of Convolutional Autoencoder for CircRNA-MiRNA Associations (CA-CMA) prediction. Initially, this model merged the natural language characteristics of the circRNA and miRNA sequence with the features of circRNA-miRNA interactions. Subsequently, it utilized all circRNA-miRNA pairs to construct a molecular association network, which was then fine-tuned by labeled samples to optimize the network parameters. Finally, the prediction outcome is obtained by utilizing the deep neural networks classifier. This model innovatively combines the likelihood objective that preserves the neighborhood through optimization, to learn the continuous feature representation of words and preserve the spatial information of two-dimensional signals. During the process of 5-fold cross-validation, CA-CMA exhibited exceptional performance compared to numerous prior computational approaches, as evidenced by its mean area under the receiver operating characteristic curve of 0.9138 and a minimal SD of 0.0024. Furthermore, recent literature has confirmed the accuracy of 25 out of the top 30 circRNA-miRNA pairs identified with the highest CA-CMA scores during case studies. The results of these experiments highlight the robustness and versatility of our model.","['Convolutional Autoencoder', 'deep neural networks classifier']","The study addresses the critical role of connections between circular RNAs (circRNAs) and microRNAs (miRNAs) in the onset, progression, diagnosis, and treatment of diseases and tumors. It highlights the importance of identifying the most promising circRNA-related miRNAs to serve as biological markers or drug targets, which could enhance preventive strategies, diagnostic procedures, and therapeutic approaches for complex human diseases. The primary aim of the study is to select and predict potential associations between circRNAs and miRNAs that can be utilized as biological markers or targets for disease management. This objective focuses on improving the identification of these associations to support more effective prevention, diagnosis, and treatment of diseases.","The study addresses the critical role of connections between circular RNAs (circRNAs) and microRNAs (miRNAs) in the onset, progression, diagnosis, and treatment of diseases and tumors. It highlights the importance of identifying the most promising circRNA-related miRNAs to serve as biological markers or drug targets, which could enhance preventive strategies, diagnostic procedures, and therapeutic approaches for complex human diseases. The primary aim of the study is to identify potential associations between circRNAs and miRNAs that can be utilized as biological markers or targets for disease management. This objective focuses on improving the identification of these associations to support more effective prevention, diagnosis, and treatment of diseases.",True
Social Sciences,DisenSemi: Semi-Supervised Graph Classification via Disentangled Representation Learning,"Graph classification is a critical task in numerous multimedia applications, where graphs are employed to represent diverse types of multimedia data, including images, videos, and social networks. Nevertheless, in the real world, labeled graph data are always limited or scarce. To address this issue, we focus on the semi-supervised graph classification task, which involves both supervised and unsupervised models learning from labeled and unlabeled data. In contrast to recent approaches that transfer the entire knowledge from the unsupervised model to the supervised one, we argue that an effective transfer should only retain the relevant semantics that align well with the supervised task. We introduce a novel framework termed in this article, which learns disentangled representation for semi-supervised graph classification. Specifically, a disentangled graph encoder is proposed to generate factorwise graph representations for both supervised and unsupervised models. Then, we train two models via supervised objective and mutual information (MI)-based constraints, respectively. To ensure the meaningful transfer of knowledge from the unsupervised encoder to the supervised one, we further define an MI-based disentangled consistency regularization between two models and identify the corresponding rationale that aligns well with the current graph classification task. Experiments conducted on various publicly available datasets demonstrate the effectiveness of our .","['semi-supervised graph classification', 'disentangled graph encoder']","The research addresses the challenge of limited labeled data in graph classification tasks, which are important for representing various types of multimedia data such as images, videos, and social networks. The study is motivated by the need to improve the learning process when both labeled and unlabeled data are available, emphasizing the importance of transferring only relevant semantic information that aligns with the supervised classification task. The primary objective of the study is to develop a framework that effectively leverages both labeled and unlabeled data for graph classification by ensuring meaningful knowledge transfer that aligns with the supervised task, thereby enhancing classification performance in scenarios with scarce labeled data.","The research addresses the challenge of limited labeled data in graph classification tasks, which are important for representing various types of multimedia data such as images, videos, and social networks. The study is motivated by the need to improve the analysis process when both labeled and unlabeled data are available, emphasizing the importance of transferring only relevant semantic information that aligns with the supervised classification task. The primary objective of the study is to develop a framework that effectively leverages both labeled and unlabeled data for graph classification by ensuring meaningful knowledge transfer that aligns with the supervised task, thereby enhancing classification performance in scenarios with scarce labeled data.",True
Social Sciences,Smart energy management in residential buildings: the impact of knowledge and behavior,"Abstract A new technology called smart energy management makes use of IoT concepts to enhance energy efficiency and lower waste in structures. The goal of this study is to comprehend how household energy management knowledge affects energy usage, user behavior, related expenses, and environmental effect. Through a survey of 100 valid replies in Palestine, the research model assessed the knowledge and consumption habits of building occupants. Smart PLS software was used to analyze the research model using partial least squares structural equation modeling (PLS-SEM). Using path coefficients and behavior as a mediating variable, the structural model connected the latent variables. The mediation hypotheses were tested using the Preacher and Hayes method, and the indirect effect and confidence intervals were estimated and calculated using bootstrapping. The findings demonstrated that by lowering energy use and enhancing overall building performance, residential buildings that implement smart energy consumption management systems may move toward a more sustainable future. Furthermore, the study found that education and awareness campaigns are necessary to increase residents’ knowledge of these systems to promote energy savings. The results also indicated statistically significant indirect effects, supporting the existence of mediation of the behavior construct. Path coefficient values and P -values were presented to further support the study’s hypotheses. Such smart energy management systems represent an important innovation in building management and can help create more sustainable and efficient buildings.",['bootstrapping'],"The research idea centers on understanding how household energy management knowledge influences energy consumption, user behavior, associated costs, and environmental impact within residential buildings. The study addresses the broader motivation of promoting sustainability by reducing energy waste and improving building performance through informed occupant behavior. The primary objective of the study is to examine the relationship between residents’ knowledge of energy management and their energy usage patterns, as well as to assess the role of behavior as a mediating factor in this relationship. Additionally, the study aims to highlight the importance of education and awareness campaigns in enhancing residents’ understanding of energy management to foster energy savings and support sustainable building practices.","The research idea centers on understanding how household energy management knowledge influences energy consumption, user behavior, associated costs, and environmental impact within residential buildings. The study addresses the broader motivation of promoting sustainability by reducing energy waste and improving building performance through informed occupant behavior. The primary objective of the study is to examine the relationship between residents' knowledge of energy management and their energy usage patterns, as well as to assess the role of behavior as a mediating factor in this relationship. Additionally, the study aims to highlight the importance of education and awareness campaigns in enhancing residents' understanding of energy management to foster energy savings and support sustainable building practices.",True
Social Sciences,Variation in social media sensitivity across people and contexts,"Abstract Social media impacts people’s wellbeing in different ways, but relatively little is known about why this is the case. Here we introduce the construct of “social media sensitivity” to understand how social media and wellbeing associations differ across people and the contexts in which these platforms are used. In a month-long large-scale intensive longitudinal study (total n = 1632; total number of observations = 120,599), we examined for whom and under which circumstances social media was associated with positive and negative changes in social and affective wellbeing. Applying a combination of frequentist and Bayesian multilevel models, we found a small negative average association between social media use AND subsequent wellbeing, but the associations were heterogenous across people. People with psychologically vulnerable dispositions (e.g., those who were depressed, lonely, not satisfied with life) tended to experience heightened negative social media sensitivity in comparison to people who were not psychologically vulnerable. People also experienced heightened negative social media sensitivity when in certain types of places (e.g., in social places, in nature) and while around certain types of people (e.g., around family members, close ties), as compared to using social media in other contexts. Our results suggest that an understanding of the effects of social media on wellbeing should account for the psychological dispositions of social media users, and the physical and social contexts surrounding their use. We discuss theoretical and practical implications of social media sensitivity for scholars, policymakers, and those in the technology industry.",['Bayesian multilevel models'],"The research idea addresses the varying impact of social media on individuals' wellbeing and the limited understanding of why these effects differ across people and contexts. It introduces the concept of “social media sensitivity” to explore how associations between social media use and wellbeing change depending on individual psychological dispositions and the environments in which social media is used. The study’s primary objective is to examine for whom and under which circumstances social media use is linked to positive or negative changes in social and affective wellbeing. It aims to highlight the role of psychological vulnerability and contextual factors, such as physical locations and social surroundings, in shaping the relationship between social media and wellbeing.","The research idea addresses the varying impact of social media on individuals' wellbeing and the limited understanding of why these effects differ across people and contexts. It introduces the concept of ""social media sensitivity"" to explore how associations between social media use and wellbeing change depending on individual psychological dispositions and the environments in which social media is used. The study's primary objective is to examine for whom and under which circumstances social media use is linked to positive or negative changes in social and affective wellbeing. It aims to highlight the role of psychological vulnerability and contextual factors, such as physical locations and social surroundings, in shaping the relationship between social media and wellbeing.",True
Social Sciences,Machine learning models for predicting preeclampsia: a systematic review,"Abstract Background This systematic review provides an overview of machine learning (ML) approaches for predicting preeclampsia. Method This review adhered to the Preferred Reporting Items for Systematic Reviews and Meta-Analyzes (PRISMA) guidelines. We searched the Cochrane Central Register, PubMed, EMBASE, ProQuest, Scopus, and Google Scholar up to February 2023. Search terms were limited to “preeclampsia” AND “artificial intelligence” OR “machine learning” OR “deep learning.” All studies that used ML-based analysis for predicting preeclampsia in pregnant women were considered. Non-English articles and those that are unrelated to the topic were excluded. The PROBAST was used to assess the risk of bias and applicability of each included study. Results The search strategy yielded 128 citations; after duplicates were removed and title and abstract screening was completed, 18 full-text articles were evaluated for eligibility. Four studies were included in this review. Two studies were at low risk of bias, and two had low to moderate risk. All of the study designs included were retrospective cohort studies. Nine distinct models were chosen as ML models from the four studies. Maternal characteristics, medical history, medication intake, obstetrical history, and laboratory and ultrasound findings obtained during prenatal care visits were candidate predictors to train the ML model. Elastic net, stochastic gradient boosting, extreme gradient boosting, and Random forest were among the best models to predict preeclampsia. All four studies used metrics such as the area under the curve, true positive rate, negative positive rate, accuracy, precision, recall, and F1 score. The AUC of ML models varied from 0.860 to 0.973 in four studies. Conclusion The results of studies yielded high prediction performance of ML models for preeclampsia risk from routine early pregnancy information.","['Elastic net', 'stochastic gradient boosting', 'extreme gradient boosting', 'Random forest']","The research idea centers on addressing the challenge of predicting preeclampsia in pregnant women by utilizing information routinely collected during early pregnancy. Preeclampsia is a significant health concern that can lead to serious complications for both the mother and the fetus, making early identification of risk crucial for improving prenatal care and outcomes. The study aims to provide an overview of existing approaches that leverage maternal characteristics, medical history, medication intake, obstetrical history, and prenatal clinical findings to enhance the prediction of preeclampsia risk. The primary objective of the study is to systematically review and evaluate the effectiveness of current predictive approaches for preeclampsia, focusing on their ability to identify high-risk pregnancies based on early pregnancy information. This review seeks to assess the performance and applicability of these approaches to inform future research and clinical practice in prenatal risk assessment.","The research idea centers on addressing the challenge of predicting preeclampsia in pregnant women by utilizing information routinely collected during early pregnancy. Preeclampsia is a significant health concern that can lead to serious complications for both the mother and the fetus, making early identification of risk crucial for improving prenatal care and outcomes. The study aims to provide an overview of existing approaches that leverage maternal characteristics, medical history, medication intake, obstetrical history, and prenatal clinical findings to enhance the identification of preeclampsia risk. The primary objective of the study is to systematically review and evaluate the effectiveness of current methods for preeclampsia risk assessment, focusing on their ability to identify high-risk pregnancies based on early pregnancy information. This review seeks to assess the performance and applicability of these approaches to inform future research and clinical practice in prenatal risk assessment.",True
Social Sciences,Risk predictions of surgical wound complications based on a machine learning algorithm: A systematic review,"Abstract Surgical wounds may arise due to harm inflicted upon soft tissue during surgical intervention, and many complications and injuries may accompany them. These complications can lead to prolonged hospitalization and poorer clinical outcomes. Also, Machine learning (ML) is a Section of artificial intelligence (AI) that has emerged in medical care and is increasingly used for diagnosis, complications, prognosis and recurrence prediction. This study aims to investigate surgical wound risk predictions and management using a ML algorithm by R programming language analysis. The systematic review, following PRISMA guidelines, spanned electronic databases using search terms like ‘machine learning’, ‘surgical’ and ‘wound’. Inclusion criteria covered experimental studies from 1990 to the present on ML's application in surgical wound evaluation. Exclusion criteria included studies lacking full text, focusing on ML in all surgeries, neglecting wound assessment and duplications. Two authors rigorously assessed titles, abstracts and full texts, excluding reviews and guidelines. Ultimately, relevant articles were then analysed. The present study identified nine articles employing ML for surgical wound management. The analysis encompassed various surgical procedures, including Cardiothoracic, Caesarean total abdominal colectomy, Burn plastic surgery, facial plastic surgery, laparotomy, minimal invasive surgery, hernia repair and unspecified surgeries. ML was skillful in evaluating surgical site infections (SSI) in seven studies, while two extended its use to burn‐grade diagnosis and wound classification. Support Vector Machine (SVM) and Convolutional Neural Network (CNN) were the most utilized algorithms. ANN achieved a 96% accuracy in facial plastic surgery wound management. CNN demonstrated commendable accuracies in various surgeries, and SVM exhibited high accuracy in multiple surgeries and burn plastic surgery. In sum, these findings underscore ML's potential for significant improvements in postoperative management and the development of enhanced care techniques, particularly in surgical wound management.","['Support Vector Machine (SVM)', 'Convolutional Neural Network (CNN)', 'Artificial Neural Network (ANN)']","The research idea centers on the challenges posed by surgical wounds, which result from tissue damage during surgical procedures and are often accompanied by complications that can lead to extended hospital stays and poorer clinical outcomes. Addressing these complications is crucial for improving patient recovery and overall healthcare quality. The primary objective of the study is to investigate the prediction and management of surgical wound risks, aiming to enhance postoperative care and develop better techniques specifically for surgical wound management. This involves examining existing approaches to evaluating surgical wounds across various types of surgeries to identify opportunities for improving clinical outcomes.","The research idea centers on the challenges posed by surgical wounds, which result from tissue damage during surgical procedures and are often accompanied by complications that can lead to extended hospital stays and poorer clinical outcomes. Addressing these complications is crucial for improving patient recovery and overall healthcare quality. The primary objective of the study is to investigate the assessment and management of surgical wound risks, aiming to enhance postoperative care and develop better techniques specifically for surgical wound management. This involves examining existing approaches to evaluating surgical wounds across various types of surgeries to identify opportunities for improving clinical outcomes.",True
Social Sciences,CARLA: Self-supervised contrastive representation learning for time series anomaly detection,"One main challenge in time series anomaly detection (TSAD) is the lack of labelled data in many real-life scenarios. Most of the existing anomaly detection methods focus on learning the normal behaviour of unlabelled time series in an unsupervised manner. The normal boundary is often defined tightly, resulting in slight deviations being classified as anomalies, consequently leading to a high false positive rate and a limited ability to generalise normal patterns. To address this, we introduce a novel end-to-end self-supervised ContrAstive Representation Learning approach for time series Anomaly detection (CARLA). While existing contrastive learning methods assume that augmented time series windows are positive samples and temporally distant windows are negative samples, we argue that these assumptions are limited as augmentation of time series can transform them to negative samples, and a temporally distant window can represent a positive sample. Existing approaches to contrastive learning for time series have directly copied methods developed for image analysis. We argue that these methods do not transfer well. Instead, our contrastive approach leverages existing generic knowledge about time series anomalies and injects various types of anomalies as negative samples. Therefore, CARLA not only learns normal behaviour but also learns deviations indicating anomalies. It creates similar representations for temporally close windows and distinct ones for anomalies. Additionally, it leverages the information about representations' neighbours through a self-supervised approach to classify windows based on their nearest/furthest neighbours to further enhance the performance of anomaly detection. In extensive tests on seven major real-world TSAD datasets, CARLA shows superior performance (F1 and AU-PR) over state-of-the-art self-supervised, semi-supervised, and unsupervised TSAD methods for univariate time series and multivariate time series. Our research highlights the immense potential of contrastive representation learning in advancing the TSAD field, thus paving the way for novel applications and in-depth exploration.","['self-supervised ContrAstive Representation Learning', 'contrastive learning', 'self-supervised approach', 'semi-supervised methods', 'unsupervised methods']","The research addresses the challenge of accurately identifying unusual patterns in time-based data when there is a lack of labeled examples, which often leads to misclassifying minor variations as anomalies and limits the understanding of normal behavior. Existing approaches tend to define normal behavior too narrowly, resulting in a high rate of false alarms and poor generalization across different scenarios. The study aims to improve the detection of anomalies in time series data by developing a method that better distinguishes between normal variations and true anomalies. Specifically, the objective is to create a framework that not only learns what constitutes normal behavior but also incorporates knowledge about various types of anomalies to enhance the identification of deviations, thereby reducing false positives and improving overall detection performance.","The research addresses the challenge of accurately identifying unusual patterns in time-based data when there is a lack of labeled examples, which often leads to misclassifying minor variations as anomalies and limits the understanding of normal behavior. Existing approaches tend to define normal behavior too narrowly, resulting in a high rate of false alarms and poor generalization across different scenarios. The study aims to improve the detection of anomalies in time series data by developing a method that better distinguishes between normal variations and true anomalies. Specifically, the objective is to create a framework that not only characterizes what constitutes normal behavior but also incorporates knowledge about various types of anomalies to enhance the identification of deviations, thereby reducing false positives and improving overall detection performance.",True
Social Sciences,Toward Improving Breast Cancer Classification Using an Adaptive Voting Ensemble Learning Algorithm,"Over the past decade, breast cancer has been the most common type of cancer in women. Different methods were proposed for breast cancer detection. These methods mainly classify and categorize malignant and Benign tumors. Machine learning is a practical approach for breast cancer classification. Data mining and classification are effective methods to predict and categorize breast cancer. The optimum classification for detecting Breast Cancer (BC) is ensemble-based. The ensemble approach involves using multiple ways to find the best possible solution. This study used the Wisconsin Breast Cancer Diagnostic (WBCD) dataset. We created a voting ensemble classifier that combines four different machine learning models: Extra Trees Classifier (ETC), Light Gradient Boosting Machine (LightGBM), Ridge Classifier (RC), and Linear Discriminant Analysis (LDA). The proposed ELRL-E approach achieved an accuracy of 97.6%, a precision of 96.4%, a recall of 100%, and an F1 score of 98.1%. Various output evaluations are used to evaluate the performance and efficiency of the proposed model and other classifiers. Overall, the recommended strategy performed better. Results are directly compared with the individual classifier and different recognized state-of-the-art classifiers. The primary objective of this study is to identify the most influential ensemble machine learning classifier for breast cancer detection and diagnosis in terms of accuracy and AUC score.","['Machine learning', 'ensemble-based classification', 'voting ensemble classifier', 'Extra Trees Classifier (ETC)', 'Light Gradient Boosting Machine (LightGBM)', 'Ridge Classifier (RC)', 'Linear Discriminant Analysis (LDA)']","The research idea centers on addressing the critical issue of breast cancer detection, which remains the most common type of cancer among women. The study highlights the importance of accurately classifying and categorizing malignant and benign tumors to improve diagnosis and treatment outcomes. Given the variety of existing methods, there is a need to identify the most effective approach for breast cancer classification. The research objective is to determine the most influential classification method for breast cancer detection and diagnosis by evaluating its accuracy and effectiveness. Specifically, the study aims to identify which approach provides the highest performance in distinguishing between malignant and benign tumors to enhance diagnostic precision.","The research idea centers on addressing the critical issue of breast cancer detection, which remains the most common type of cancer among women. The study highlights the importance of accurately classifying and categorizing malignant and benign tumors to improve diagnosis and treatment outcomes. Given the variety of existing methods, there is a need to identify the most effective approach for breast cancer classification. The research objective is to determine the most influential method for breast cancer detection and diagnosis by evaluating its accuracy and effectiveness. Specifically, the study aims to identify which approach provides the highest performance in distinguishing between malignant and benign tumors to enhance diagnostic precision.",True
Social Sciences,From explainable to interpretable deep learning for natural language processing in healthcare: How far from reality?,"Deep learning (DL) has substantially enhanced natural language processing (NLP) in healthcare research. However, the increasing complexity of DL-based NLP necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review of explainable and interpretable DL in healthcare NLP. The term ""eXplainable and Interpretable Artificial Intelligence"" (XIAI) is introduced to distinguish XAI from IAI. Different models are further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms are the most prevalent emerging IAI technique. The use of IAI is growing, distinguishing it from XAI. The major challenges identified are that most XIAI does not explore ""global"" modelling processes, the lack of best practices, and the lack of systematic evaluation and benchmarks. One important opportunity is to use attention mechanisms to enhance multi-modal XIAI for personalized medicine. Additionally, combining DL with causal logic holds promise. Our discussion encourages the integration of XIAI in Large Language Models (LLMs) and domain-specific smaller models. In conclusion, XIAI adoption in healthcare requires dedicated in-house expertise. Collaboration with domain experts, end-users, and policymakers can lead to ready-to-use XIAI methods across NLP and medical tasks. While challenges exist, XIAI techniques offer a valuable foundation for interpretable NLP algorithms in healthcare.","['deep learning (DL)', 'attention mechanisms']","The research addresses the growing need for transparency and interpretability in the use of advanced language processing techniques within healthcare research to ensure reliable decision-making. It highlights challenges such as the limited exploration of comprehensive modeling processes, the absence of established best practices, and the lack of systematic evaluation and benchmarks in this area. The study aims to provide a comprehensive overview of explainable and interpretable approaches in healthcare language processing, categorizing existing methods and identifying prevailing trends and challenges. It seeks to encourage collaboration among domain experts, end-users, and policymakers to develop practical and accessible interpretability methods that can be effectively applied across various healthcare and medical tasks.","The research addresses the growing need for transparency and interpretability in the use of advanced language processing within healthcare research to ensure reliable decision-making. It highlights challenges such as the limited exploration of comprehensive processing approaches, the absence of established best practices, and the lack of systematic evaluation and benchmarks in this area. The study aims to provide a comprehensive overview of explainable and interpretable approaches in healthcare language processing, categorizing existing methods and identifying prevailing trends and challenges. It seeks to encourage collaboration among domain experts, end-users, and policymakers to develop practical and accessible interpretability methods that can be effectively applied across various healthcare and medical tasks.",True
Social Sciences,PEGA: A Privacy-Preserving Genetic Algorithm for Combinatorial Optimization,"Evolutionary algorithms (EAs), such as the genetic algorithm (GA), offer an elegant way to handle combinatorial optimization problems (COPs). However, limited by expertise and resources, most users lack the capability to implement EAs for solving COPs. An intuitive and promising solution is to outsource evolutionary operations to a cloud server, however, it poses privacy concerns. To this end, this article proposes a novel computing paradigm called evolutionary computation as a service (ECaaS), where a cloud server renders evolutionary computation services for users while ensuring their privacy. Following the concept of ECaaS, this article presents privacy-preserving genetic algorithm (PEGA), a privacy-preserving GA designed specifically for COPs. PEGA enables users, regardless of their domain expertise or resource availability, to outsource COPs to the cloud server that holds a competitive GA and approximates the optimal solution while safeguarding privacy. Notably, PEGA features the following characteristics. First, PEGA empowers users without domain expertise or sufficient resources to solve COPs effectively. Second, PEGA protects the privacy of users by preventing the leakage of optimization problem details. Third, PEGA performs comparably to the conventional GA when approximating the optimal solution. To realize its functionality, we implement PEGA falling in a twin-server architecture and evaluate it on two widely known COPs: 1) the traveling Salesman problem (TSP) and 2) the 0/1 knapsack problem (KP). Particularly, we utilize encryption cryptography to protect users' privacy and carefully design a suite of secure computing protocols to support evolutionary operators of GA on encrypted chromosomes. Privacy analysis demonstrates that PEGA successfully preserves the confidentiality of COP contents. Experimental evaluation results on several TSP datasets and KP datasets reveal that PEGA performs equivalently to the conventional GA in approximating the optimal solution.","['evolutionary algorithms (EAs)', 'genetic algorithm (GA)']","The research idea addresses the challenge faced by users who lack the expertise and resources to solve complex combinatorial optimization problems, highlighting the potential of outsourcing such tasks to external services while raising concerns about privacy protection. The study is motivated by the need to enable users to effectively solve these problems without compromising the confidentiality of their sensitive information. The primary objective of the study is to develop a solution that allows users, regardless of their domain knowledge or resource availability, to outsource combinatorial optimization problems to an external service that can approximate optimal solutions while ensuring the privacy of the users’ problem details. The study aims to demonstrate that this approach can protect user privacy and perform comparably to traditional methods in solving these problems.","The research idea addresses the challenge faced by users who lack the expertise and resources to solve complex combinatorial optimization problems, highlighting the potential of outsourcing such tasks to external services while raising concerns about privacy protection. The study is motivated by the need to enable users to effectively solve these problems without compromising the confidentiality of their sensitive information. The primary objective of the study is to develop a solution that allows users, regardless of their domain knowledge or resource availability, to outsource combinatorial optimization problems to an external service that can approximate optimal solutions while ensuring the privacy of the users' problem details. The study aims to demonstrate that this approach can protect user privacy and perform comparably to traditional methods in solving these problems.",True
Social Sciences,Towards development of functional climate-driven early warning systems for climate-sensitive infectious diseases: Statistical models and recommendations,"Climate, weather and environmental change have significantly influenced patterns of infectious disease transmission, necessitating the development of early warning systems to anticipate potential impacts and respond in a timely and effective way. Statistical modelling plays a pivotal role in understanding the intricate relationships between climatic factors and infectious disease transmission. For example, time series regression modelling and spatial cluster analysis have been employed to identify risk factors and predict spatial and temporal patterns of infectious diseases. Recently advanced spatio-temporal models and machine learning offer an increasingly robust framework for modelling uncertainty, which is essential in climate-driven disease surveillance due to the dynamic and multifaceted nature of the data. Moreover, Artificial Intelligence (AI) techniques, including deep learning and neural networks, excel in capturing intricate patterns and hidden relationships within climate and environmental data sets. Web-based data has emerged as a powerful complement to other datasets encompassing climate variables and disease occurrences. However, given the complexity and non-linearity of climate-disease interactions, advanced techniques are required to integrate and analyse these diverse data to obtain more accurate predictions of impending outbreaks, epidemics or pandemics. This article presents an overview of an approach to creating climate-driven early warning systems with a focus on statistical model suitability and selection, along with recommendations for utilizing spatio-temporal and machine learning techniques. By addressing the limitations and embracing the recommendations for future research, we could enhance preparedness and response strategies, ultimately contributing to the safeguarding of public health in the face of evolving climate challenges.","['spatio-temporal models', 'machine learning', 'deep learning', 'neural networks']","The study addresses the significant influence of climate, weather, and environmental change on the patterns of infectious disease transmission, highlighting the need for early warning systems to anticipate potential impacts and enable timely and effective responses. It emphasizes the complexity and dynamic nature of climate-disease interactions, which pose challenges for understanding and predicting outbreaks, epidemics, or pandemics. The primary aim of the study is to present an overview of approaches to developing climate-driven early warning systems, focusing on the suitability and selection of statistical models. Additionally, it seeks to provide recommendations to improve preparedness and response strategies, thereby contributing to the protection of public health amid evolving climate challenges.","The study addresses the significant influence of climate, weather, and environmental change on the patterns of infectious disease transmission, highlighting the need for early warning systems to anticipate potential impacts and enable timely and effective responses. It emphasizes the complexity and dynamic nature of climate-disease interactions, which pose challenges for understanding and predicting outbreaks, epidemics, or pandemics. The primary aim of the study is to present an overview of approaches to developing climate-driven early warning systems, focusing on the suitability and selection of analytical methods. Additionally, it seeks to provide recommendations to improve preparedness and response strategies, thereby contributing to the protection of public health amid evolving climate challenges.",True
Social Sciences,A sentiment analysis approach for understanding users’ perception of metaverse marketplace,"This research explores the user perceptions of the Metaverse Marketplace, analyzing a substantial dataset of over 860,000 Twitter posts through sentiment analysis and topic modeling techniques. The study aims to uncover the driving factors behind user engagement and sentiment in this novel digital trading space. Key findings highlight a predominantly positive user sentiment, with significant enthusiasm for the marketplace's revenue generation and entertainment potential, particularly within the gaming sector. Users express appreciation for the innovative opportunities the Metaverse Marketplace offers for artists, designers, and traders in handling and trading digital assets. This positive outlook is tempered by notable concerns regarding security and privacy within the Metaverse, pointing to a critical area for development and assurance. The study also reveals a substantial neutral sentiment, reflecting users' cautious but interested stance, particularly regarding the marketplace's role in investment and passive income opportunities. This balanced view underscores the evolving nature of user perceptions in this emerging field. Theoretically, the research enriches the discourse on technology adoption, particularly in virtual environments, by highlighting perceived benefits and enjoyment as significant adoption drivers. These insights are invaluable for stakeholders in the Metaverse Marketplace, guiding the development of more secure, engaging, and user-friendly platforms. While providing a pioneering perspective on Metaverse user perceptions, the study acknowledges its limitation to Twitter data, suggesting the need for broader research methodologies for a more holistic understanding.",['topic modeling'],"The research addresses the evolving user perceptions of the Metaverse Marketplace, focusing on understanding the factors that influence engagement and sentiment within this emerging digital trading environment. It highlights the balance between enthusiasm for the marketplace’s revenue and entertainment potential, especially in gaming, and concerns about security and privacy, reflecting the complexities of user attitudes in this novel context. The primary objective of the study is to uncover the driving factors behind user engagement and sentiment in the Metaverse Marketplace, emphasizing the perceived benefits, enjoyment, and cautious interest users express regarding investment and passive income opportunities. By doing so, the research aims to provide insights that can guide stakeholders in developing more secure, engaging, and user-friendly platforms within this virtual environment.","The research addresses the evolving user perceptions of the Metaverse Marketplace, focusing on understanding the factors that influence engagement and sentiment within this emerging digital trading environment. It highlights the balance between enthusiasm for the marketplace's revenue and entertainment potential, especially in gaming, and concerns about security and privacy, reflecting the complexities of user attitudes in this novel context. The primary objective of the study is to uncover the driving factors behind user engagement and sentiment in the Metaverse Marketplace, emphasizing the perceived benefits, enjoyment, and cautious interest users express regarding investment and passive income opportunities. By doing so, the research aims to provide insights that can guide stakeholders in developing more secure, engaging, and user-friendly platforms within this virtual environment.",True
Social Sciences,BEVSOC: Self-Supervised Contrastive Learning for Calibration-Free BEV 3-D Object Detection,"3D object detection based on multi-view cameras and bird's-eye view (BEV) representation is a key task for autonomous driving, as it enables the perception systems to understand the surrounding scenes. However, most existing BEV representation methods rely on the projection matrix of camera intrinsic and extrinsic parameters, which requires a complex and time-consuming calibration process that may introduce errors and degrade the detection performance. Moreover, the calibration results may vary due to environmental changes and affect the stability of the detection system. To address this problem, we propose a calibration-free 3D object detection method that leverages a group-equivariant convolutional network to extract features from multi-view images and a projection network module to learn the implicit 3D-to-2D projection relationship for obtaining BEV representation. Furthermore, we employ contrastive learning to pre-train the projection network module without using manually annotated data. By exploiting the multi-view camera data through contrastive learning, our proposed method eliminates the need for tedious calibration, avoids calibration errors, and reduces the dependence on a large amount of annotated data for calibration-free 3D object detection. We evaluate our method on the nuScenes dataset and demonstrate its competitive performance. Our method improves the stability and reliability of 3D object detection in long-term autonomous driving.","['group-equivariant convolutional network', 'contrastive learning']","The research idea addresses the challenge of accurately perceiving surrounding scenes in autonomous driving, focusing on the difficulties caused by complex and error-prone calibration processes required for existing methods. These calibration procedures are time-consuming and susceptible to environmental changes, which can negatively impact the stability and reliability of detection systems. The study is motivated by the need to improve the consistency and effectiveness of scene understanding without relying on such calibration. The primary objective of the study is to develop a method for 3D object detection that eliminates the need for manual calibration by learning the relationship between different camera views and spatial representation. This approach aims to enhance the stability and reliability of object detection over time, reducing dependence on extensive manual annotation and improving performance in autonomous driving contexts.","The research idea addresses the challenge of accurately perceiving surrounding scenes in autonomous driving, focusing on the difficulties caused by complex and error-prone calibration processes required for existing methods. These calibration procedures are time-consuming and susceptible to environmental changes, which can negatively impact the stability and reliability of detection systems. The study is motivated by the need to improve the consistency and effectiveness of scene understanding without relying on such calibration. The primary objective of the study is to develop a method for 3D object detection that eliminates the need for manual calibration by establishing the relationship between different camera views and spatial representation. This approach aims to enhance the stability and reliability of object detection over time, reducing dependence on extensive manual annotation and improving performance in autonomous driving contexts.",True
Social Sciences,CrossHAR: Generalizing Cross-dataset Human Activity Recognition via Hierarchical Self-Supervised Pretraining,"The increasing availability of low-cost wearable devices and smartphones has significantly advanced the field of sensor-based human activity recognition (HAR), attracting considerable research interest. One of the major challenges in HAR is the domain shift problem in cross-dataset activity recognition, which occurs due to variations in users, device types, and sensor placements between the source dataset and the target dataset. Although domain adaptation methods have shown promise, they typically require access to the target dataset during the training process, which might not be practical in some scenarios. To address these issues, we introduce CrossHAR, a new HAR model designed to improve model performance on unseen target datasets. CrossHAR involves three main steps: (i) CrossHAR explores the sensor data generation principle to diversify the data distribution and augment the raw sensor data. (ii) CrossHAR then employs a hierarchical self-supervised pretraining approach with the augmented data to develop a generalizable representation. (iii) Finally, CrossHAR fine-tunes the pretrained model with a small set of labeled data in the source dataset, enhancing its performance in cross-dataset HAR. Our extensive experiments across multiple real-world HAR datasets demonstrate that CrossHAR outperforms current state-of-the-art methods by 10.83% in accuracy, demonstrating its effectiveness in generalizing to unseen target datasets.","['domain adaptation methods', 'fine-tuning']","The research addresses the challenge of recognizing human activities across different datasets, which is complicated by variations in users, device types, and sensor placements. This domain shift problem limits the effectiveness of activity recognition when applied to new or unseen datasets, posing practical difficulties in real-world applications. The study aims to improve the performance of human activity recognition on unseen target datasets by developing an approach that enhances the generalizability of activity recognition models. Specifically, the objective is to create a method that can better handle variations between datasets without requiring access to the target dataset during the training process, thereby making activity recognition more robust and applicable across diverse settings.","The research addresses the challenge of recognizing human activities across different datasets, which is complicated by variations in users, device types, and sensor placements. This domain shift problem limits the effectiveness of activity recognition when applied to new or unseen datasets, posing practical difficulties in real-world applications. The study aims to improve the performance of human activity recognition on unseen target datasets by developing an approach that enhances the generalizability of recognition systems. Specifically, the objective is to create a method that can better handle variations between datasets without requiring access to the target dataset during the training process, thereby making activity recognition more robust and applicable across diverse settings.",True
Social Sciences,Transferable deep generative modeling of intrinsically disordered protein conformations,"Intrinsically disordered proteins have dynamic structures through which they play key biological roles. The elucidation of their conformational ensembles is a challenging problem requiring an integrated use of computational and experimental methods. Molecular simulations are a valuable computational strategy for constructing structural ensembles of disordered proteins but are highly resource-intensive. Recently, machine learning approaches based on deep generative models that learn from simulation data have emerged as an efficient alternative for generating structural ensembles. However, such methods currently suffer from limited transferability when modeling sequences and conformations absent in the training data. Here, we develop a novel generative model that achieves high levels of transferability for intrinsically disordered protein ensembles. The approach, named idpSAM, is a latent diffusion model based on transformer neural networks. It combines an autoencoder to learn a representation of protein geometry and a diffusion model to sample novel conformations in the encoded space. IdpSAM was trained on a large dataset of simulations of disordered protein regions performed with the ABSINTH implicit solvent model. Thanks to the expressiveness of its neural networks and its training stability, idpSAM faithfully captures 3D structural ensembles of test sequences with no similarity in the training set. Our study also demonstrates the potential for generating full conformational ensembles from datasets with limited sampling and underscores the importance of training set size for generalization. We believe that idpSAM represents a significant progress in transferable protein ensemble modeling through machine learning.","['deep generative models', 'latent diffusion model', 'transformer neural networks', 'autoencoder', 'diffusion model']","The study addresses the challenge of understanding the dynamic structures of intrinsically disordered proteins, which play key biological roles but are difficult to characterize due to their flexible conformational ensembles. Accurately elucidating these ensembles is important for advancing knowledge of protein behavior and function. The primary aim of the study is to develop a method that can reliably generate structural ensembles of intrinsically disordered proteins, including those with sequences and conformations not previously observed, thereby improving the ability to represent diverse protein structures and enhance generalization from limited experimental data.","The study addresses the challenge of understanding the dynamic structures of intrinsically disordered proteins, which play key biological roles but are difficult to characterize due to their flexible conformational ensembles. Accurately elucidating these ensembles is important for advancing knowledge of protein behavior and function. The primary aim of the study is to develop a methodology that can reliably generate structural ensembles of intrinsically disordered proteins, including those with sequences and conformations not previously observed, thereby improving the ability to represent diverse protein structures and enhance interpretation from limited experimental data.",True
Social Sciences,A machine learning-based classification model to support university students with dyslexia with personalized tools and strategies,"Abstract Dyslexia is a specific learning disorder that causes issues related to reading, which affects around 10% of the worldwide population. This can compromise comprehension and memorization skills, and result in anxiety and lack of self-esteem, if no support is provided. Moreover, this support should be highly personalized, to be actually helpful. In this paper, a model to classify the most useful methodologies to support students with dyslexia has been created, with a focus on university alumni. The prediction algorithm is based on supervised machine learning techniques; starting from the issues that dyslexic students experience during their career, it is capable of suggesting customized support digital tools and learning strategies for each of them. The algorithm was trained and tested on data acquired through a self-evaluation questionnaire, which was designed and then spread to more than 1200 university students. It allowed 17 useful tools and 22 useful strategies to be detected. The results of the testing showed an average prediction accuracy higher than 90%, which rises to 94% by renouncing to guess the less-predictable 8 tools/strategies. In the light of this, it is possible to state that the implemented algorithm can achieve the set goal and, thus, reduce the gap between dyslexic and non-dyslexic students. This achievement paves the way for a new modality of facing the problem of dyslexia by university institutions, which aims at modifying teaching activities toward students’ needs, instead of simply reducing their study load or duties. This complies with the definition and the aims of inclusivity.",['supervised machine learning techniques'],"The study addresses the challenges faced by individuals with dyslexia, a specific learning disorder affecting reading abilities and impacting around 10% of the global population. Dyslexia can hinder comprehension and memorization skills, leading to anxiety and low self-esteem if adequate support is not provided. The research highlights the importance of highly personalized support to effectively assist students with dyslexia, particularly focusing on university alumni. The primary aim of the study is to identify and classify the most useful methodologies and strategies to support dyslexic students during their academic careers. By understanding the specific difficulties these students encounter, the study seeks to suggest customized tools and learning approaches that can help reduce the educational gap between dyslexic and non-dyslexic students, promoting inclusivity within higher education institutions.","The study addresses the challenges faced by individuals with dyslexia, a specific learning disorder affecting reading abilities and impacting around 10% of the global population. Dyslexia can hinder comprehension and memorization skills, leading to anxiety and low self-esteem if adequate support is not provided. The research highlights the importance of highly personalized support to effectively assist students with dyslexia, particularly focusing on university alumni. The primary aim of the study is to identify and categorize the most useful methodologies and strategies to support dyslexic students during their academic careers. By understanding the specific difficulties these students encounter, the study seeks to suggest customized tools and learning approaches that can help reduce the educational gap between dyslexic and non-dyslexic students, promoting inclusivity within higher education institutions.",True
Social Sciences,Machine learning study using 2020 SDHS data to determine poverty determinants in Somalia,"Abstract Extensive research has been conducted on poverty in developing countries using conventional regression analysis, which has limited prediction capability. This study aims to address this gap by applying advanced machine learning (ML) methods to predict poverty in Somalia. Utilizing data from the first-ever 2020 Somalia Demographic and Health Survey (SDHS), a cross-sectional study design is considered. ML methods, including random forest (RF), decision tree (DT), support vector machine (SVM), and logistic regression, are tested and applied using R software version 4.1.2, while conventional methods are analyzed using STATA version 17. Evaluation metrics, such as confusion matrix, accuracy, precision, sensitivity, specificity, recall, F1 score, and area under the receiver operating characteristic (AUROC), are employed to assess the performance of predictive models. The prevalence of poverty in Somalia is notable, with approximately seven out of ten Somalis living in poverty, making it one of the highest rates in the region. Among nomadic pastoralists, agro-pastoralists, and internally displaced persons (IDPs), the poverty average stands at 69%, while urban areas have a lower poverty rate of 60%. The accuracy of prediction ranged between 67.21% and 98.36% for the advanced ML methods, with the RF model demonstrating the best performance. The results reveal geographical region, household size, respondent age group, husband employment status, age of household head, and place of residence as the top six predictors of poverty in Somalia. The findings highlight the potential of ML methods to predict poverty and uncover hidden information that traditional statistical methods cannot detect, with the RF model identified as the best classifier for predicting poverty in Somalia.","['random forest (RF)', 'decision tree (DT)', 'support vector machine (SVM)', 'logistic regression']","The study addresses the significant issue of poverty in Somalia, where approximately seven out of ten individuals live in poverty, representing one of the highest rates in the region. It highlights the limitations of conventional approaches in effectively predicting poverty and the need for improved methods to better understand the factors contributing to poverty in different population groups, including nomadic pastoralists, agro-pastoralists, internally displaced persons, and urban residents. The primary aim of the study is to enhance the prediction of poverty in Somalia by utilizing data from the 2020 Somalia Demographic and Health Survey to identify key predictors of poverty. The study seeks to uncover important socioeconomic and demographic factors influencing poverty, such as geographical region, household size, age groups, employment status, and place of residence, to provide deeper insights into poverty dynamics within the country.","The study addresses the significant issue of poverty in Somalia, where approximately seven out of ten individuals live in poverty, representing one of the highest rates in the region. It highlights the limitations of conventional approaches in effectively understanding poverty and the need for improved methods to better understand the factors contributing to poverty in different population groups, including nomadic pastoralists, agro-pastoralists, internally displaced persons, and urban residents. The primary aim of the study is to enhance the understanding of poverty in Somalia by utilizing data from the 2020 Somalia Demographic and Health Survey to identify key determinants of poverty. The study seeks to uncover important socioeconomic and demographic factors influencing poverty, such as geographical region, household size, age groups, employment status, and place of residence, to provide deeper insights into poverty dynamics within the country.",True
Social Sciences,TimesURL: Self-Supervised Contrastive Learning for Universal Time Series Representation Learning,"Learning universal time series representations applicable to various types of downstream tasks is challenging but valuable in real applications. Recently, researchers have attempted to leverage the success of self-supervised contrastive learning (SSCL) in Computer Vision(CV) and Natural Language Processing(NLP) to tackle time series representation. Nevertheless, due to the special temporal characteristics, relying solely on empirical guidance from other domains may be ineffective for time series and difficult to adapt to multiple downstream tasks. To this end, we review three parts involved in SSCL including 1) designing augmentation methods for positive pairs, 2) constructing (hard) negative pairs, and 3) designing SSCL loss. For 1) and 2), we find that unsuitable positive and negative pair construction may introduce inappropriate inductive biases, which neither preserve temporal properties nor provide sufficient discriminative features. For 3), just exploring segment- or instance-level semantics information is not enough for learning universal representation. To remedy the above issues, we propose a novel self-supervised framework named TimesURL. Specifically, we first introduce a frequency-temporal-based augmentation to keep the temporal property unchanged. And then, we construct double Universums as a special kind of hard negative to guide better contrastive learning. Additionally, we introduce time reconstruction as a joint optimization objective with contrastive learning to capture both segment-level and instance-level information. As a result, TimesURL can learn high-quality universal representations and achieve state-of-the-art performance in 6 different downstream tasks, including short- and long-term forecasting, imputation, classification, anomaly detection and transfer learning.",['self-supervised contrastive learning (SSCL)'],"The study addresses the challenge of developing universal representations for time series data that can be effectively applied across various practical tasks. It highlights the difficulty of adapting approaches from other fields due to the unique temporal characteristics of time series, which require preserving temporal properties and capturing sufficient discriminative features. The research aims to improve the understanding and construction of representations that maintain these temporal aspects and are broadly applicable. The primary objective of the study is to propose a framework that enhances the learning of universal time series representations by addressing limitations in current methods, ultimately enabling better performance across multiple downstream tasks such as forecasting, imputation, classification, anomaly detection, and transfer learning.","The study addresses the challenge of developing universal representations for time series data that can be effectively applied across various practical tasks. It highlights the difficulty of adapting approaches from other fields due to the unique temporal characteristics of time series, which require preserving temporal properties and capturing sufficient discriminative features. The research aims to improve the understanding and construction of representations that maintain these temporal aspects and are broadly applicable. The primary objective of the study is to propose a framework that enhances the creation of universal time series representations by addressing limitations in current methods, ultimately enabling better performance across multiple downstream tasks such as forecasting, imputation, classification, anomaly detection, and transfer learning.",True
Social Sciences,Sequential predictive learning is a unifying theory for hippocampal representation and replay,"Abstract The mammalian hippocampus contains a cognitive map that represents an animal’s position in the environment 1 and generates offline “replay” 2,3 for the purposes of recall 4 , planning 5,6 , and forming long term memories 7 . Recently, it’s been found that artificial neural networks trained to predict sensory inputs develop spatially tuned cells 8 , aligning with predictive theories of hippocampal function 9–11 . However, whether predictive learning can also account for the ability to produce offline replay is unknown. Here, we find that spatially-tuned cells, which robustly emerge from all forms of predictive learning, do not guarantee the presence of a cognitive map with the ability to generate replay. Offline simulations only emerged in networks that used recurrent connections and head-direction information to predict multi-step observation sequences, which promoted the formation of a continuous attractor reflecting the geometry of the environment. These offline trajectories were able to show wake-like statistics, autonomously replay recently experienced locations, and could be directed by a virtual head direction signal. Further, we found that networks trained to make cyclical predictions of future observation sequences were able to rapidly learn a cognitive map and produced sweeping representations of future positions reminiscent of hippocampal theta sweeps 12 . These results demonstrate how hippocampal-like representation and replay can emerge in neural networks engaged in predictive learning, and suggest that hippocampal theta sequences reflect a circuit that implements a data-efficient algorithm for sequential predictive learning. Together, this framework provides a unifying theory for hippocampal functions and hippocampal-inspired approaches to artificial intelligence.","['artificial neural networks', 'recurrent connections']","The research idea centers on understanding how the mammalian hippocampus forms a cognitive map that represents spatial position and generates offline replay, which is crucial for recall, planning, and long-term memory formation. The study addresses the question of whether predictive learning alone can explain the hippocampus's ability to produce offline replay, a key feature of spatial cognition. The research objective is to investigate the conditions under which spatially-tuned cells contribute to the formation of a cognitive map capable of generating offline replay. Specifically, the study aims to determine how certain factors, such as recurrent connections and directional information, influence the emergence of continuous spatial representations and replay phenomena that resemble hippocampal activity patterns observed during navigation and memory processes.","The research idea centers on understanding how the mammalian hippocampus forms a cognitive map that represents spatial position and generates offline replay, which is crucial for recall, planning, and long-term memory formation. The study addresses the question of whether predictive processes alone can explain the hippocampus's ability to produce offline replay, a key feature of spatial cognition. The research objective is to investigate the conditions under which spatially-tuned cells contribute to the formation of a cognitive map capable of generating offline replay. Specifically, the study aims to determine how certain factors, such as recurrent connections and directional information, influence the emergence of continuous spatial representations and replay phenomena that resemble hippocampal activity patterns observed during navigation and memory processes.",True
Social Sciences,Combating the Challenges of False Positives in AI-Driven Anomaly Detection Systems and Enhancing Data Security in the Cloud,"Anomaly detection is critical for network security, fraud detection, and system health monitoring applications. Traditional methods like statistical approaches and distance-based techniques often struggle with high-dimensional and complex data, leading to high false positive rates. This study addresses the challenge by investigating advanced AI-driven techniques to reduce false positives and enhance data security within cloud computing environments. This study employs deep learning models, integrates contextual data, and incorporates comprehensive security measures to enhance anomaly detection performance. Data from synthetic sources, such as the NSL-KDD dataset and real-world cloud environments, were utilized to capture user behavior logs, system states, and network traffic. Over 50 academic journals were reviewed, and 21 were selected based on inclusion criteria, such as relevance to AI-driven anomaly detection, empirical performance metrics, and the focus on cloud environments, and exclusion criteria that filtered out studies lacking empirical data or not specific to cloud-based systems. Methodologically, the research involves a comparative analysis of different AI techniques and their impact on false positive rates, accuracy, precision, and recall. The findings demonstrate that deep learning techniques significantly outperform traditional methods, achieving a lower false positive rate and higher accuracy. The results underscore the importance of contextual data and robust security protocols in reliable anomaly detection. This research fills a gap by thoroughly evaluating advanced AI techniques for reducing false positives in cloud environments. The study's significance lies in guiding the development of more effective anomaly detection systems, thereby enhancing security and reliability across various applications. Additionally, organizations should invest in continuously developing and integrating AI-driven anomaly detection systems with comprehensive security measures to improve their effectiveness the study suggests that further study be conducted with large datasets to evaluate the effectiveness of Hybrid anomaly detection systems in detecting and addressing false positives.",['Hybrid anomaly detection systems'],"The study addresses the critical challenge of improving anomaly detection to enhance network security, fraud detection, and system health monitoring, particularly within cloud computing environments. Traditional approaches often face difficulties handling complex and high-dimensional data, resulting in high false positive rates that undermine security effectiveness. The primary aim of the research is to investigate advanced techniques to reduce false positives and improve the reliability of anomaly detection in cloud environments. By thoroughly evaluating these approaches, the study seeks to guide the development of more effective security measures that enhance overall system reliability and protection.","The study addresses the critical challenge of improving anomaly detection to enhance network security, fraud detection, and system health monitoring, particularly within cloud computing environments. Traditional approaches often face difficulties handling complex and high-dimensional data, resulting in high false positive rates that undermine security effectiveness. The primary aim of the research is to investigate improved methodologies to reduce false positives and enhance the reliability of anomaly detection in cloud environments. By thoroughly evaluating these approaches, the study seeks to guide the development of more effective security measures that enhance overall system reliability and protection.",True
Social Sciences,Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model,"Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation & memory efficiency. For example, Vim is 2.8$\times$ faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248$\times$1248. The results demonstrate that Vim is capable of overcoming the computation & memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models. Code is available at https://github.com/hustvl/Vim.","['state space models (SSMs)', 'vision transformers', 'DeiT']","The research idea centers on addressing the challenges of representing visual data effectively, particularly the position-sensitivity of such data and the need for capturing global context in visual understanding. There is a motivation to develop efficient and versatile frameworks for visual representation that do not rely on conventional approaches typically used in this domain. The study aims to propose a new approach for building a generic vision backbone that improves performance in image classification, object detection, and semantic segmentation tasks. The primary objective is to demonstrate that this new vision backbone can achieve higher accuracy and greater efficiency in computation and memory usage compared to existing well-established methods, thereby overcoming current limitations in processing high-resolution images for visual understanding.","The research idea centers on addressing the challenges of representing visual data effectively, particularly the position-sensitivity of such data and the need for capturing global context in visual understanding. There is a motivation to develop efficient and versatile frameworks for visual representation that explore alternative methodological approaches. The study aims to propose a new approach for building a generic vision backbone that improves performance in image classification, object detection, and semantic segmentation tasks. The primary objective is to demonstrate that this new vision backbone can achieve higher accuracy and greater efficiency in computation and memory usage compared to existing well-established methods, thereby overcoming current limitations in processing high-resolution images for visual understanding.",True
Social Sciences,Comprehensive systematic review of information fusion methods in smart cities and urban environments,"Smart cities result from integrating advanced technologies and intelligent sensors into modern urban infrastructure. The Internet of Things (IoT) and data integration are pivotal in creating interconnected and intelligent urban spaces. In this literature review, we explore the different methods of information fusion used in smart cities, along with their advantages and challenges. However, there are notable challenges in managing diverse data sources, handling large data volumes, and meeting the near-real-time demands of various smart city applications. The review aims to examine smart city applications in detail, incorporating quality evaluation and information fusion techniques and identifying critical issues while outlining promising research directions. In order to accomplish our goal, we conducted a comprehensive search of literature and applied selective criteria. We identified 59 recent studies addressing machine learning (ML) and deep learning (DL) techniques in smart city applications. These studies were obtained from various databases such as ScienceDirect (SD), Scopus, Web of Science (WoS), and IEEE Xplore. The main objective of this study is to provide more detailed insights into smart cities by supplementing existing research. The word cloud visualisation of machine learning/deep learning and information fusion in smart cities papers shows a diverse landscape, covering both technical aspects of artificial intelligence and practical applications in urban settings. Apart from technical exploration, the study also delves into the ethical and privacy implications arising in smart cities. Moreover, it thoroughly examines the challenges that must be addressed to realise this urban revolution's potential fully.","['machine learning (ML)', 'deep learning (DL)']","The research idea centers on the development of smart cities through the integration of advanced technologies into urban infrastructure, highlighting the challenges in managing diverse information sources, large volumes of data, and the demands of real-time urban applications. This study recognizes the complexity of creating interconnected and intelligent urban spaces while addressing critical issues such as ethical and privacy implications. The primary objective of the study is to provide a detailed examination of smart city applications by reviewing existing research, evaluating quality aspects, and identifying key challenges and promising directions for future exploration. Additionally, the study aims to supplement current knowledge by offering deeper insights into the practical and societal dimensions of smart cities.","The research idea centers on the development of smart cities through the integration of advanced technologies into urban infrastructure, highlighting the challenges in managing diverse information sources, large volumes of data, and the demands of real-time urban applications. This study recognizes the complexity of creating interconnected urban spaces while addressing critical issues such as ethical and privacy implications. The primary objective of the study is to provide a detailed examination of smart city applications by reviewing existing research, evaluating quality aspects, and identifying key challenges and promising directions for future exploration. Additionally, the study aims to supplement current knowledge by offering deeper insights into the practical and societal dimensions of smart cities.",True
Social Sciences,Systematic literature review: Quantum machine learning and its applications,"Quantum physics has changed the way we understand our environment, and one of its branches, quantum mechanics, has demonstrated accurate and consistent theoretical results. Quantum computing is the process of performing calculations using quantum mechanics. This field studies the quantum behavior of certain subatomic particles (photons, electrons, etc.) for subsequent use in performing calculations, as well as for large-scale information processing. These advantages are achieved through the use of quantum features, such as entanglement or superposition. These capabilities can give quantum computers an advantage in terms of computational time and cost over classical computers. Nowadays, scientific challenges are impossible to perform by classical computation due to computational complexity (more bytes than atoms in the observable universe) or the time it would take (thousands of years), and quantum computation is the only known answer. However, current quantum devices do not have yet the necessary qubits and are not fault-tolerant enough to achieve these goals. Nonetheless, there are other fields like machine learning, finance, or chemistry where quantum computation could be useful with current quantum devices. This manuscript aims to present a review of the literature published between 2017 and 2023 to identify, analyze, and classify the different types of algorithms used in quantum machine learning and their applications. The methodology follows the guidelines related to Systematic Literature Review methods, such as the one proposed by Kitchenham and other authors in the software engineering field. Consequently, this study identified 94 articles that used quantum machine learning techniques and algorithms and shows their implementation using computational quantum circuits or ansatzs. The main types of found algorithms are quantum implementations of classical machine learning algorithms, such as support vector machines or the k-nearest neighbor model, and classical deep learning algorithms, like quantum neural networks. One of the most relevant applications in the machine learning field is image classification. Many articles, especially within the classification, try to solve problems currently answered by classical machine learning but using quantum devices and algorithms. Even though results are promising, quantum machine learning is far from achieving its full potential. An improvement in quantum hardware is required for this potential to be achieved since the existing quantum computers lack enough quality, speed, and scale to allow quantum computing to achieve its full potential.","['support vector machines', 'k-nearest neighbor model', 'quantum neural networks']","The research idea centers on the limitations of current computational methods in addressing complex scientific challenges due to their extensive time and resource requirements, highlighting the potential of emerging technologies to overcome these barriers. It recognizes that while traditional approaches struggle with problems of immense complexity, alternative computational paradigms offer promising advantages that could transform various fields such as finance and chemistry. The research objective is to review and categorize the existing literature from 2017 to 2023 concerning the different approaches and their applications within this emerging computational paradigm. The study aims to identify the types of approaches used, analyze their implementation, and assess their current capabilities and limitations in practical applications, emphasizing the need for advancements in underlying technology to fully realize their potential.","The research idea centers on the limitations of current methodologies in addressing complex scientific challenges due to their extensive time and resource requirements, highlighting the potential of emerging technologies to overcome these barriers. It recognizes that while traditional approaches struggle with problems of immense complexity, alternative paradigms offer promising advantages that could transform various fields such as finance and chemistry. The research objective is to review and categorize the existing literature from 2017 to 2023 concerning the different approaches and their applications within this emerging paradigm. The study aims to identify the types of approaches used, analyze their implementation, and assess their current capabilities and limitations in practical applications, emphasizing the need for advancements in underlying technology to fully realize their potential.",True
Social Sciences,"Generative AI for Transformative Healthcare: A Comprehensive Study of Emerging Models, Applications, Case Studies, and Limitations","Generative artificial intelligence (GAI) can be broadly described as an artificial intelligence system capable of generating images, text, and other media types with human prompts. GAI models like ChatGPT, DALL-E, and Bard have recently caught the attention of industry and academia equally. GAI applications span various industries like art, gaming, fashion, and healthcare. In healthcare, GAI shows promise in medical research, diagnosis, treatment, and patient care and is already making strides in real-world deployments. There has yet to be any detailed study concerning the applications and scope of GAI in healthcare. Addressing this research gap, we explore several applications, real-world scenarios, and limitations of GAI in healthcare. We examine how GAI models like ChatGPT and DALL-E can be leveraged to aid in the applications of medical imaging, drug discovery, personalized patient treatment, medical simulation and training, clinical trial optimization, mental health support, healthcare operations and research, medical chatbots, human movement simulation, and a few more applications. Along with applications, we cover four real-world healthcare scenarios that employ GAI: visual snow syndrome diagnosis, molecular drug optimization, medical education, and dentistry. We also provide an elaborate discussion on seven healthcare-customized LLMs like Med-PaLM, BioGPT, DeepHealth, etc.,Since GAI is still evolving, it poses challenges like the lack of professional expertise in decision making, risk of patient data privacy, issues in integrating with existing healthcare systems, and the problem of data bias which are elaborated on in this work along with several other challenges. We also put forward multiple directions for future research in GAI for healthcare.","['healthcare-customized LLMs like Med-PaLM', 'healthcare-customized LLMs like BioGPT']","The research idea centers on the emerging role and potential of generative artificial intelligence in healthcare, highlighting its applications across various medical fields such as medical imaging, drug discovery, personalized treatment, and mental health support. Despite its growing use, there is a notable lack of detailed studies examining the scope, real-world applications, and limitations of this technology within healthcare settings. The study aims to address this gap by exploring how generative AI can be utilized in diverse healthcare scenarios and by discussing the challenges it presents, including issues related to professional decision-making, patient data privacy, system integration, and data bias. The primary objective of the study is to investigate and document the applications, real-world scenarios, and limitations of generative AI in healthcare, providing a comprehensive overview of its current use and proposing directions for future research to enhance its integration and effectiveness in medical practice.","The research idea centers on the emerging role and potential of advanced computational systems in healthcare, highlighting their applications across various medical fields such as medical imaging, drug discovery, personalized treatment, and mental health support. Despite their growing use, there is a notable lack of detailed studies examining the scope, real-world applications, and limitations of these technologies within healthcare settings. The study aims to address this gap by exploring how these technologies can be utilized in diverse healthcare scenarios and by discussing the challenges they present, including issues related to professional decision-making, patient data privacy, system integration, and data bias. The primary objective of the study is to investigate and document the applications, real-world scenarios, and limitations of these advanced technologies in healthcare, providing a comprehensive overview of their current use and proposing directions for future research to enhance their integration and effectiveness in medical practice.",True
Social Sciences,Human-AI collaboration patterns in AI-assisted academic writing,"Artificial Intelligence (AI) has increasingly influenced higher education, notably in academic writing where AI-powered assisting tools offer both opportunities and challenges. Recently, the rapid growth of generative AI (GAI) has brought its impacts into sharper focus, yet the dynamics of its utilisation in academic writing remain largely unexplored. This paper focuses on examining the nature of human-AI interactions in academic writing, specifically investigating the strategies doctoral students employ when collaborating with a GAI-powered assisting tool. This study involves 626 recorded activities on how ten doctoral students interact with GAI-powered assisting tool during academic writing. AI-driven learning analytics approach was adopted for three layered analyses: (1) data pre-processing and analysis with quantitative content analysis, (2) sequence analysis with Hidden Markov Model (HMM) and hierarchical sequence clustering, and (3) pattern analysis with process mining. Findings indicate that doctoral students engaging in iterative, highly interactive processes with the GAI-powered assisting tool generally achieve better performance in the writing task. In contrast, those who use GAI merely as a supplementary information source, maintaining a linear writing approach, tend to get lower writing performance. This study points to the need for further investigations into human-AI collaboration in learning in higher education, with implications for tailored educational strategies and solutions.",['Hidden Markov Model (HMM)'],"The study addresses the growing influence of generative AI on academic writing in higher education, highlighting both the opportunities and challenges it presents. Despite the rapid adoption of these tools, the ways in which doctoral students utilize such assistance in their writing processes remain largely unexplored. The research aims to examine the nature of interactions between doctoral students and generative AI tools during academic writing. Specifically, it investigates the strategies employed by these students when collaborating with AI-assisted tools to understand how different approaches impact their writing performance.","The study addresses the growing influence of emerging writing technologies on academic writing in higher education, highlighting both the opportunities and challenges they present. Despite the rapid adoption of these tools, the ways in which doctoral students utilize such assistance in their writing processes remain largely unexplored. The research aims to examine the nature of interactions between doctoral students and these advanced writing tools during academic writing. Specifically, it investigates the strategies employed by these students when working with technology-assisted tools to understand how different approaches impact their writing performance.",True
Social Sciences,Hallucination Rates and Reference Accuracy of ChatGPT and Bard for Systematic Reviews: Comparative Analysis,"Background Large language models (LLMs) have raised both interest and concern in the academic community. They offer the potential for automating literature search and synthesis for systematic reviews but raise concerns regarding their reliability, as the tendency to generate unsupported (hallucinated) content persist. Objective The aim of the study is to assess the performance of LLMs such as ChatGPT and Bard (subsequently rebranded Gemini) to produce references in the context of scientific writing. Methods The performance of ChatGPT and Bard in replicating the results of human-conducted systematic reviews was assessed. Using systematic reviews pertaining to shoulder rotator cuff pathology, these LLMs were tested by providing the same inclusion criteria and comparing the results with original systematic review references, serving as gold standards. The study used 3 key performance metrics: recall, precision, and F1-score, alongside the hallucination rate. Papers were considered “hallucinated” if any 2 of the following information were wrong: title, first author, or year of publication. Results In total, 11 systematic reviews across 4 fields yielded 33 prompts to LLMs (3 LLMs×11 reviews), with 471 references analyzed. Precision rates for GPT-3.5, GPT-4, and Bard were 9.4% (13/139), 13.4% (16/119), and 0% (0/104) respectively (P&lt;.001). Recall rates were 11.9% (13/109) for GPT-3.5 and 13.7% (15/109) for GPT-4, with Bard failing to retrieve any relevant papers (P&lt;.001). Hallucination rates stood at 39.6% (55/139) for GPT-3.5, 28.6% (34/119) for GPT-4, and 91.4% (95/104) for Bard (P&lt;.001). Further analysis of nonhallucinated papers retrieved by GPT models revealed significant differences in identifying various criteria, such as randomized studies, participant criteria, and intervention criteria. The study also noted the geographical and open-access biases in the papers retrieved by the LLMs. Conclusions Given their current performance, it is not recommended for LLMs to be deployed as the primary or exclusive tool for conducting systematic reviews. Any references generated by such models warrant thorough validation by researchers. The high occurrence of hallucinations in LLMs highlights the necessity for refining their training and functionality before confidently using them for rigorous academic purposes.","['GPT-3.5', 'GPT-4']","The research idea centers on the growing interest and concern within the academic community regarding the use of large language models for automating literature search and synthesis in systematic reviews, particularly focusing on their reliability due to the tendency to generate unsupported or inaccurate content. This issue raises questions about the trustworthiness of such tools in producing valid scientific references. The primary objective of the study is to assess the performance of large language models in generating references within the context of scientific writing, specifically evaluating their ability to replicate the results of human-conducted systematic reviews related to shoulder rotator cuff pathology. The study aims to determine the accuracy and reliability of these models in producing valid references and to highlight any biases or errors that may affect their use in academic research.","The research idea centers on the growing interest and concern within the academic community regarding the use of emerging text synthesis technologies for automating literature search and synthesis in systematic reviews, particularly focusing on their reliability due to the tendency to generate unsupported or inaccurate content. This issue raises questions about the trustworthiness of such tools in producing valid scientific references. The primary objective of the study is to assess the performance of these text generation systems in creating references within the context of scientific writing, specifically evaluating their ability to replicate the results of human-conducted systematic reviews related to shoulder rotator cuff pathology. The study aims to determine the accuracy and reliability of these systems in producing valid references and to highlight any biases or errors that may affect their use in academic research.",True
Social Sciences,Unmasking bias in artificial intelligence: a systematic review of bias detection and mitigation strategies in electronic health record-based models,"Abstract Objectives Leveraging artificial intelligence (AI) in conjunction with electronic health records (EHRs) holds transformative potential to improve healthcare. However, addressing bias in AI, which risks worsening healthcare disparities, cannot be overlooked. This study reviews methods to handle various biases in AI models developed using EHR data. Materials and Methods We conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and Meta-analyses guidelines, analyzing articles from PubMed, Web of Science, and IEEE published between January 01, 2010 and December 17, 2023. The review identified key biases, outlined strategies for detecting and mitigating bias throughout the AI model development, and analyzed metrics for bias assessment. Results Of the 450 articles retrieved, 20 met our criteria, revealing 6 major bias types: algorithmic, confounding, implicit, measurement, selection, and temporal. The AI models were primarily developed for predictive tasks, yet none have been deployed in real-world healthcare settings. Five studies concentrated on the detection of implicit and algorithmic biases employing fairness metrics like statistical parity, equal opportunity, and predictive equity. Fifteen studies proposed strategies for mitigating biases, especially targeting implicit and selection biases. These strategies, evaluated through both performance and fairness metrics, predominantly involved data collection and preprocessing techniques like resampling and reweighting. Discussion This review highlights evolving strategies to mitigate bias in EHR-based AI models, emphasizing the urgent need for both standardized and detailed reporting of the methodologies and systematic real-world testing and evaluation. Such measures are essential for gauging models’ practical impact and fostering ethical AI that ensures fairness and equity in healthcare.","['resampling', 'reweighting']","The research idea centers on the critical issue of bias in healthcare applications, which poses a risk of exacerbating existing disparities in healthcare delivery. The study recognizes the transformative potential of integrating advanced approaches with electronic health records to improve healthcare outcomes but emphasizes that addressing bias is essential to ensure fairness and equity. The primary objective of the study is to review and synthesize existing methods for identifying and mitigating various types of bias in healthcare-related developments using electronic health records. It aims to highlight strategies for bias detection and reduction, assess the effectiveness of these approaches, and underscore the need for standardized reporting and real-world evaluation to promote ethical and equitable healthcare practices.","The research idea centers on the critical issue of bias in healthcare applications, which poses a risk of exacerbating existing disparities in healthcare delivery. The study recognizes the transformative potential of integrating new methodologies with electronic health records to improve healthcare outcomes but emphasizes that addressing bias is essential to ensure fairness and equity. The primary objective of the study is to review and synthesize existing methods for identifying and mitigating various types of bias in healthcare-related developments using electronic health records. It aims to highlight strategies for bias detection and reduction, assess the effectiveness of these approaches, and underscore the need for standardized reporting and real-world evaluation to promote ethical and equitable healthcare practices.",True
Social Sciences,Artificial intelligence for literature reviews: opportunities and challenges,"Abstract This paper presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous and organised methodology that assesses and integrates prior research on a given topic. Numerous tools have been developed to assist and partially automate the SLR process. The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews. Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases. We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features. We also analyse 11 recent tools that leverage large language models for searching the literature and assisting academic writing. Finally, the paper discusses current trends in the field, outlines key research challenges, and suggests directions for future research. We highlight three primary research challenges: integrating advanced AI solutions, such as large language models and knowledge graphs, improving usability, and developing a standardised evaluation framework. We also propose best practices to ensure more robust evaluations in terms of performance, usability, and transparency. Overall, this review offers a detailed overview of AI-enhanced SLR tools for researchers and practitioners, providing a foundation for the development of next-generation AI solutions in this field.",['knowledge graphs'],"The research idea centers on the growing importance and potential of enhancing the process of systematic literature reviews (SLRs), which are rigorous and organized methodologies for assessing and integrating prior research on a given topic. The study addresses the need for more effective support in conducting SLRs, particularly in the phases of screening and extraction, to improve the efficiency and quality of literature reviews. The research objective is to comprehensively review the current tools used to assist in the semi-automation of systematic literature reviews, focusing on their features and capabilities. Additionally, the study aims to identify key challenges in the field, such as improving usability and establishing standardized evaluation frameworks, while proposing best practices to enhance the robustness and transparency of future developments in this area.","The research idea centers on the growing importance and potential of enhancing the process of systematic literature reviews (SLRs), which are rigorous and organized methodologies for assessing and integrating prior research on a given topic. The study addresses the need for more effective support in conducting SLRs, particularly in the phases of screening and extraction, to improve the efficiency and quality of literature reviews. The research objective is to comprehensively review the current tools used to assist in streamlining systematic literature reviews, focusing on their features and capabilities. Additionally, the study aims to identify key challenges in the field, such as improving usability and establishing standardized evaluation frameworks, while proposing best practices to enhance the robustness and transparency of future developments in this area.",True
Social Sciences,Larger and more instructable language models become less reliable,"Abstract The prevailing methods to make large language models more powerful and amenable have been based on continuous scaling up (that is, increasing their size, data volume and computational resources 1 ) and bespoke shaping up (including post-filtering 2,3 , fine tuning or use of human feedback 4,5 ). However, larger and more instructable large language models may have become less reliable. By studying the relationship between difficulty concordance, task avoidance and prompting stability of several language model families, here we show that easy instances for human participants are also easy for the models, but scaled-up, shaped-up models do not secure areas of low difficulty in which either the model does not err or human supervision can spot the errors. We also find that early models often avoid user questions but scaled-up, shaped-up models tend to give an apparently sensible yet wrong answer much more often, including errors on difficult questions that human supervisors frequently overlook. Moreover, we observe that stability to different natural phrasings of the same question is improved by scaling-up and shaping-up interventions, but pockets of variability persist across difficulty levels. These findings highlight the need for a fundamental shift in the design and development of general-purpose artificial intelligence, particularly in high-stakes areas for which a predictable distribution of errors is paramount.","['post-filtering', 'fine tuning', 'use of human feedback']","The research idea centers on the challenges associated with improving the reliability and accuracy of large language models, particularly as they become larger and more complex. The study addresses concerns that despite advancements, these models may still produce errors that are difficult for human supervisors to detect, especially on tasks of varying difficulty. It highlights the importance of understanding how task difficulty and response stability affect the performance and trustworthiness of these models in critical applications. The primary objective of the study is to investigate the relationship between task difficulty, error occurrence, and response consistency in different language model families, with the aim of identifying limitations in current approaches and emphasizing the need for a fundamental change in the development of general-purpose tools used in high-stakes contexts where predictable error patterns are crucial.","The research idea centers on the challenges associated with improving the reliability and accuracy of text processing systems, particularly as they become larger and more complex. The study addresses concerns that despite advancements, these systems may still produce errors that are difficult for human supervisors to detect, especially on tasks of varying difficulty. It highlights the importance of understanding how task difficulty and response stability affect the performance and trustworthiness of these systems in critical applications. The primary objective of the study is to investigate the relationship between task difficulty, error occurrence, and response consistency in different text processing methodologies, with the aim of identifying limitations in current approaches and emphasizing the need for a fundamental change in the development of general-purpose tools used in high-stakes contexts where predictable error patterns are crucial.",True
Social Sciences,Integrating artificial intelligence to assess emotions in learning environments: a systematic literature review,"Introduction Artificial Intelligence (AI) is transforming multiple sectors within our society, including education. In this context, emotions play a fundamental role in the teaching-learning process given that they influence academic performance, motivation, information retention, and student well-being. Thus, the integration of AI in emotional assessment within educational environments offers several advantages that can transform how we understand and address the socio-emotional development of students. However, there remains a lack of comprehensive approach that systematizes advancements, challenges, and opportunities in this field. Aim This systematic literature review aims to explore how artificial intelligence (AI) is used to evaluate emotions within educational settings. We provide a comprehensive overview of the current state of research, focusing on advancements, challenges, and opportunities in the domain of AI-driven emotional assessment within educational settings. Method The review involved a search across the following academic databases: Pubmed, Web of Science, PsycINFO and Scopus. Forty-one articles were selected that meet the established inclusion criteria. These articles were analyzed to extract key insights related to the integration of AI and emotional assessment within educational environments. Results The findings reveal a variety of AI-driven approaches that were developed to capture and analyze students’ emotional states during learning activities. The findings are summarized in four fundamental topics: (1) emotion recognition in education, (2) technology integration and learning outcomes, (3) special education and assistive technology, (4) affective computing. Among the key AI techniques employed are machine learning and facial recognition, which are used to assess emotions. These approaches demonstrate promising potential in enhancing pedagogical strategies and creating adaptive learning environments that cater to individual emotional needs. The review identified emerging factors that, while important, require further investigation to understand their relationships and implications fully. These elements could significantly enhance the use of AI in assessing emotions within educational settings. Specifically, we are referring to: (1) federated learning, (2) convolutional neural network (CNN), (3) recurrent neural network (RNN), (4) facial expression databases, and (5) ethics in the development of intelligent systems. Conclusion This systematic literature review showcases the significance of AI in revolutionizing educational practices through emotion assessment. While advancements are evident, challenges related to accuracy, privacy, and cross-cultural validity were also identified. The synthesis of existing research highlights the need for further research into refining AI models for emotion recognition and emphasizes the importance of ethical considerations in implementing AI technologies within educational contexts.","['machine learning', 'federated learning', 'convolutional neural network (CNN)', 'recurrent neural network (RNN)']","The study addresses the growing importance of emotions in the teaching-learning process, recognizing their influence on academic performance, motivation, information retention, and student well-being. It highlights the transformative potential of integrating emotional assessment within educational environments to better understand and support the socio-emotional development of students. However, there is a lack of a comprehensive approach that systematizes the advancements, challenges, and opportunities in this area. The primary aim of the study is to explore how emotions are evaluated within educational settings, providing a comprehensive overview of the current state of research with a focus on advancements, challenges, and opportunities related to emotional assessment in education. The study seeks to synthesize existing knowledge to inform future research and practice in enhancing pedagogical strategies that address students’ emotional needs.","The study addresses the growing importance of emotions in the teaching-learning process, recognizing their influence on academic performance, motivation, information retention, and student well-being. It highlights the transformative potential of integrating emotional assessment within educational environments to better understand and support the socio-emotional development of students. However, there is a lack of a comprehensive approach that systematizes the advancements, challenges, and opportunities in this area. The primary aim of the study is to explore how emotions are evaluated within educational settings, providing a comprehensive overview of the current state of research with a focus on advancements, challenges, and opportunities related to emotional assessment in education. The study seeks to synthesize existing knowledge to inform future research and practice in enhancing pedagogical strategies that address students' emotional needs.",True
Social Sciences,Artificial intelligence-driven virtual rehabilitation for people living in the community: A scoping review,"Abstract Virtual Rehabilitation (VRehab) is a promising approach to improving the physical and mental functioning of patients living in the community. The use of VRehab technology results in the generation of multi-modal datasets collected through various devices. This presents opportunities for the development of Artificial Intelligence (AI) techniques in VRehab, namely the measurement, detection, and prediction of various patients’ health outcomes. The objective of this scoping review was to explore the applications and effectiveness of incorporating AI into home-based VRehab programs. PubMed/MEDLINE, Embase, IEEE Xplore, Web of Science databases, and Google Scholar were searched from inception until June 2023 for studies that applied AI for the delivery of VRehab programs to the homes of adult patients. After screening 2172 unique titles and abstracts and 51 full-text studies, 13 studies were included in the review. A variety of AI algorithms were applied to analyze data collected from various sensors and make inferences about patients’ health outcomes, most involving evaluating patients’ exercise quality and providing feedback to patients. The AI algorithms used in the studies were mostly fuzzy rule-based methods, template matching, and deep neural networks. Despite the growing body of literature on the use of AI in VRehab, very few studies have examined its use in patients’ homes. Current research suggests that integrating AI with home-based VRehab can lead to improved rehabilitation outcomes for patients. However, further research is required to fully assess the effectiveness of various forms of AI-driven home-based VRehab, taking into account its unique challenges and using standardized metrics.","['fuzzy rule-based methods', 'deep neural networks']","The research idea centers on the potential of virtual rehabilitation (VRehab) to enhance the physical and mental functioning of patients living in the community, particularly through home-based programs. Despite the increasing interest in VRehab, there is limited understanding of its application and effectiveness when delivered in patients’ homes. The study aims to explore how incorporating advanced techniques into home-based VRehab programs can improve rehabilitation outcomes for adult patients. The primary objective of the study was to examine the applications and effectiveness of integrating such approaches into home-based virtual rehabilitation programs by reviewing existing research on their use with adult patients, with a focus on evaluating patient exercise quality and feedback mechanisms.","The research idea centers on the potential of virtual rehabilitation (VRehab) to enhance the physical and mental functioning of patients living in the community, particularly through home-based programs. Despite the increasing interest in VRehab, there is limited understanding of its application and effectiveness when delivered in patients' homes. The study aims to explore how incorporating innovative methods into home-based VRehab programs can improve rehabilitation outcomes for adult patients. The primary objective of the study was to examine the applications and effectiveness of integrating such approaches into home-based virtual rehabilitation programs by reviewing existing research on their use with adult patients, with a focus on evaluating patient exercise quality and feedback mechanisms.",True
Social Sciences,Adaptive Segmentation Enhanced Asynchronous Federated Learning for Sustainable Intelligent Transportation Systems,"The proliferation of advanced embedded and communication technologies has facilitated the possibility of modern Intelligent Transportation System (ITS). The hierarchical nature of such large-scale and distributed systems brings obvious challenges in creating a scalable and sustainable computing environment, and hence the development and application of edge intelligence become critical. Federated learning (FL), as an emerging distributed machine learning paradigm, aims to offer secure knowledge sharing and effective learning across multiple devices. However, conventional FL may fall into trouble when facing large-scale and network-agnostic systems with fast moving devices and changing network attributes. In this study, we propose an Adaptive Segmentation enhanced Asynchronous Federated Learning (AS-AFL) model, aiming to improve the learning efficiency and reliability in sustainable ITS via a decentralized fashion. Specifically, a meta-learning based adaptive segmentation scheme is designed to automatically separate the client nodes (e.g., vehicles) into multiple edge groups according to their homogeneous attributes. An integrated aggregation mechanism is then developed to realize the horizontal FL among a group of similar client nodes via the so-called intra-group synchronous aggregation, while allowing the vertical FL across different groups via the so-called inter-group asynchronous aggregation. Experiment and evaluation results based on an open-source dataset demonstrate the outstanding learning and communication performance of our proposed model, compared with several conventional FL schemes in a distributed ITS application scenario.","['Federated learning (FL)', 'meta-learning based adaptive segmentation']","The research idea addresses the challenges posed by the hierarchical and large-scale nature of modern Intelligent Transportation Systems (ITS), which complicate the creation of scalable and sustainable environments for effective operation. The study recognizes the difficulties in managing secure knowledge sharing and learning across multiple devices within such distributed systems, especially when dealing with fast-moving entities and changing network conditions. The primary objective of the study is to improve the efficiency and reliability of learning processes in sustainable ITS by developing a decentralized approach that organizes client nodes into groups based on shared characteristics. This approach aims to enhance collaboration within and across these groups to better support the dynamic and distributed nature of ITS environments.","The research idea addresses the challenges posed by the hierarchical and large-scale nature of modern Intelligent Transportation Systems (ITS), which complicate the creation of scalable and sustainable environments for effective operation. The study recognizes the difficulties in managing secure knowledge sharing across multiple devices within such distributed systems, especially when dealing with fast-moving entities and changing network conditions. The primary objective of the study is to improve the efficiency and reliability of information processing in sustainable ITS by developing a decentralized approach that organizes client nodes into groups based on shared characteristics. This approach aims to enhance collaboration within and across these groups to better support the dynamic and distributed nature of ITS environments.",True
Social Sciences,CFSSynergy: Combining Feature-Based and Similarity-Based Methods for Drug Synergy Prediction,"Drug synergy prediction plays a vital role in cancer treatment. Because experimental approaches are labor-intensive and expensive, computational-based approaches get more attention. There are two types of computational methods for drug synergy prediction: feature-based and similarity-based. In feature-based methods, the main focus is to extract more discriminative features from drug pairs and cell lines to pass to the task predictor. In similarity-based methods, the similarities among all drugs and cell lines are utilized as features and fed into the task predictor. In this work, a novel approach, called CFSSynergy, that combines these two viewpoints is proposed. First, a discriminative representation is extracted for paired drugs and cell lines as input. We have utilized transformer-based architecture for drugs. For cell lines, we have created a similarity matrix between proteins using the Node2Vec algorithm. Then, the new cell line representation is computed by multiplying the protein–protein similarity matrix and the initial cell line representation. Next, we compute the similarity between unique drugs and unique cells using the learned representation for paired drugs and cell lines. Then, we compute a new representation for paired drugs and cell lines based on the similarity-based features and the learned features. Finally, these features are fed to XGBoost as a task predictor. Two well-known data sets were used to evaluate the performance of our proposed method: DrugCombDB and OncologyScreen. The CFSSynergy approach consistently outperformed existing methods in comparative evaluations. This substantiates the efficacy of our approach in capturing complex synergistic interactions between drugs and cell lines, setting it apart from conventional similarity-based or feature-based methods.","['transformer-based architecture', 'Node2Vec algorithm', 'XGBoost']","The research idea centers on addressing the challenge of predicting drug synergy in cancer treatment, which is crucial due to the labor-intensive and costly nature of experimental approaches. The study highlights the need for more effective methods to understand how different drugs interact synergistically to improve therapeutic outcomes. The primary objective of the study is to develop a novel approach that integrates multiple perspectives to better capture the complex interactions between drug pairs and cancer cell lines. This approach aims to enhance the prediction of drug synergy, thereby contributing to more efficient and effective cancer treatment strategies.","The research idea centers on addressing the challenge of predicting drug synergy in cancer treatment, which is crucial due to the labor-intensive and costly nature of experimental approaches. The study highlights the need for more effective methods to understand how different drugs interact synergistically to improve therapeutic outcomes. The primary objective of the study is to develop a new approach that integrates multiple perspectives to better capture the complex interactions between drug pairs and cancer cell lines. This approach aims to enhance the prediction of drug synergy, thereby contributing to more efficient and effective cancer treatment strategies.",True
Social Sciences,A voting gray wolf optimizer-based ensemble learning models for intrusion detection in the Internet of Things,"Abstract The Internet of Things (IoT) has garnered considerable attention from academic and industrial circles as a pivotal technology in recent years. The escalation of security risks is observed to be associated with the growing interest in IoT applications. Intrusion detection systems (IDS) have been devised as viable instruments for identifying and averting malicious actions in this context. Several techniques described in academic papers are thought to be very accurate, but they cannot be used in the real world because the datasets used to build and test the models do not accurately reflect and simulate the IoT network. Existing methods, on the other hand, deal with these issues, but they are not good enough for commercial use because of their lack of precision, low detection rate, receiver operating characteristic (ROC), and false acceptance rate (FAR). The effectiveness of these solutions is predominantly dependent on individual learners and is consequently influenced by the inherent limitations of each learning algorithm. This study introduces a new approach for detecting intrusion attacks in an IoT network, which involves the use of an ensemble learning technique based on gray wolf optimizer (GWO). The novelty of this study lies in the proposed voting gray wolf optimizer (GWO) ensemble model, which incorporates two crucial components: a traffic analyzer and a classification phase engine. The model employs a voting technique to combine the probability averages of the base learners. Secondly, the combination of feature selection and feature extraction techniques is to reduce dimensionality. Thirdly, the utilization of GWO is employed to optimize the parameters of ensemble models. Similarly, the approach employs the most authentic intrusion detection datasets that are accessible and amalgamates multiple learners to generate ensemble learners. The hybridization of information gain (IG) and principal component analysis (PCA) was employed to reduce dimensionality. The study utilized a novel GWO ensemble learning approach that incorporated a decision tree, random forest, K-nearest neighbor, and multilayer perceptron for classification. To evaluate the efficacy of the proposed model, two authentic datasets, namely, BoT-IoT and UNSW-NB15, were scrutinized. The GWO-optimized ensemble model demonstrates superior accuracy when compared to other machine learning-based and deep learning models. Specifically, the model achieves an accuracy rate of 99.98%, a DR of 99.97%, a precision rate of 99.94%, an ROC rate of 99.99%, and an FAR rate of 1.30 on the BoT-IoT dataset. According to the experimental results, the proposed ensemble model optimized by GWO achieved an accuracy of 100%, a DR of 99.9%, a precision of 99.59%, an ROC of 99.40%, and an FAR of 1.5 when tested on the UNSW-NB15 dataset.","['ensemble learning technique', 'gray wolf optimizer (GWO)', 'voting technique', 'feature selection', 'information gain (IG)', 'principal component analysis (PCA)', 'decision tree', 'random forest', 'K-nearest neighbor', 'multilayer perceptron']","The research idea centers on the increasing security risks associated with the growing use of Internet of Things (IoT) applications, highlighting the challenge of effectively identifying and preventing malicious actions within IoT networks. Existing solutions for intrusion detection are limited in their practical applicability due to issues such as lack of precision, low detection rates, and high false acceptance rates, which hinder their commercial viability. The study is motivated by the need to develop more reliable and accurate methods to address these security concerns in real-world IoT environments. The primary objective of the study is to introduce and evaluate a novel approach for detecting intrusion attacks in IoT networks that improves accuracy and detection performance by combining multiple components, including traffic analysis and classification phases. The study aims to enhance the effectiveness of intrusion detection by optimizing the integration of various techniques and validating the approach using authentic datasets to demonstrate superior performance compared to existing methods.","The research idea centers on the increasing security risks associated with the growing use of Internet of Things (IoT) applications, highlighting the challenge of effectively identifying and preventing malicious actions within IoT networks. Existing solutions for intrusion detection are limited in their practical applicability due to issues such as lack of precision, low detection rates, and high false acceptance rates, which hinder their commercial viability. The study is motivated by the need to develop more reliable and accurate methods to address these security concerns in real-world IoT environments. The primary objective of the study is to introduce and evaluate a new approach for detecting intrusion attacks in IoT networks that improves accuracy and detection performance by combining multiple components, including traffic analysis and evaluation phases. The study aims to enhance the effectiveness of intrusion detection by optimizing the integration of various techniques and validating the approach using authentic datasets to demonstrate superior performance compared to existing methods.",True
Social Sciences,Developing a Multi-Criteria Decision-Making model for nuclear power plant location selection using Fuzzy Analytic Hierarchy Process and Fuzzy VIKOR methods focused on socio-economic factors,"In response to its position as the fourth most populous country globally, Indonesia is exploring constructing nuclear power plants (NPPs) as a sustainable energy solution. A pivotal step in this initiative is selecting an appropriate NPP site. This study employs two Multi-Criteria Decision-Making (MCDM) methods, the Fuzzy Analytic Hierarchy Process (Fuzzy-AHP) and Fuzzy VIKOR, to identify the most suitable location for an NPP, focusing on socio-economic factors. The Fuzzy-AHP method is utilized to prioritize ten sub-criteria: transmission network, operating costs, economic impact, security, transportation network, legal considerations, the impact of tourism, land ownership, historical sites, and public acceptance. Following this, the Fuzzy VIKOR method leverages these prioritized criteria to evaluate two potential sites: East Kalimantan and West Kalimantan. The analysis reveals that security, transmission, and transportation networks emerge as the top priorities. The application of the Fuzzy VIKOR algorithm identifies West Kalimantan as the optimal site for NPP construction, evidenced by its lower VIKOR index of 0.3599, indicating a higher overall preference based on the evaluated criteria. The study demonstrates that the integration of Fuzzy-AHP and Fuzzy VIKOR methods prioritizes critical socio-economic factors and quantitatively assesses potential sites, offering a systematic and objective approach to support decision-making in NPP site selection.",['Fuzzy VIKOR'],"The research idea centers on Indonesia’s need to identify a suitable location for constructing nuclear power plants as part of its strategy to develop sustainable energy solutions, given its status as the fourth most populous country globally. A critical challenge in this initiative is selecting an appropriate site that accounts for various socio-economic factors impacting the feasibility and acceptance of nuclear power plants. The study aims to prioritize these socio-economic criteria to support informed decision-making in site selection. The primary objective of the study is to determine the most suitable location for a nuclear power plant in Indonesia by evaluating and prioritizing key socio-economic factors such as security, transmission and transportation networks, economic impact, legal considerations, and public acceptance. Specifically, the study seeks to compare potential sites in East Kalimantan and West Kalimantan to identify the optimal site based on these prioritized criteria.","The research idea centers on Indonesia's need to identify a suitable location for constructing nuclear power plants as part of its strategy to develop sustainable energy solutions, given its status as the fourth most populous country globally. A critical challenge in this initiative is selecting an appropriate site that accounts for various socio-economic factors impacting the feasibility and acceptance of nuclear power plants. The study aims to prioritize these socio-economic criteria to support informed decision-making in site selection. The primary objective of the study is to determine the most suitable location for a nuclear power plant in Indonesia by evaluating and prioritizing key socio-economic factors such as security, transmission and transportation networks, economic impact, legal considerations, and public acceptance. Specifically, the study seeks to compare potential sites in East Kalimantan and West Kalimantan to identify the optimal site based on these prioritized criteria.",True
Social Sciences,"Foundation models in robotics: Applications, challenges, and the future","We survey applications of pretrained foundation models in robotics. Traditional deep learning models in robotics are trained on small datasets tailored for specific tasks, which limits their adaptability across diverse applications. In contrast, foundation models pretrained on internet-scale data appear to have superior generalization capabilities, and in some instances display an emergent ability to find zero-shot solutions to problems that are not present in the training data. Foundation models may hold the potential to enhance various components of the robot autonomy stack, from perception to decision-making and control. For example, large language models can generate code or provide common sense reasoning, while vision-language models enable open-vocabulary visual recognition. However, significant open research challenges remain, particularly around the scarcity of robot-relevant training data, safety guarantees and uncertainty quantification, and real-time execution. In this survey, we study recent papers that have used or built foundation models to solve robotics problems. We explore how foundation models contribute to improving robot capabilities in the domains of perception, decision-making, and control. We discuss the challenges hindering the adoption of foundation models in robot autonomy and provide opportunities and potential pathways for future advancements. The GitHub project corresponding to this paper can be found here: https://github.com/robotics-survey/Awesome-Robotics-Foundation-Models .","['pretrained foundation models', 'vision-language models']","The research idea centers on addressing the limitations of traditional approaches in robotics, which are typically trained on small, task-specific datasets that restrict their adaptability across diverse applications. There is a growing interest in exploring new methods that demonstrate superior generalization capabilities and the potential to solve problems beyond their initial training scope. The study recognizes significant challenges in the field, including the scarcity of relevant training data, safety concerns, and the need for reliable real-time performance. The research objective is to examine recent developments that enhance robotic capabilities in perception, decision-making, and control by leveraging advanced approaches with broader applicability. The study aims to identify the contributions of these approaches to improving robot autonomy, discuss the obstacles to their widespread adoption, and suggest opportunities and pathways for future progress in robotics.","The research idea centers on addressing the limitations of traditional approaches in robotics, which are typically developed using small, task-specific datasets that restrict their adaptability across diverse applications. There is a growing interest in exploring new methods that demonstrate superior generalization capabilities and the potential to solve problems beyond their initial design scope. The study recognizes significant challenges in the field, including the scarcity of relevant data, safety concerns, and the need for reliable real-time performance. The research objective is to examine recent developments that enhance robotic capabilities in perception, decision-making, and control by leveraging advanced approaches with broader applicability. The study aims to identify the contributions of these approaches to improving robot autonomy, discuss the obstacles to their widespread adoption, and suggest opportunities and pathways for future progress in robotics.",True
Social Sciences,One-Step Multi-View Clustering With Diverse Representation,"Multi-View clustering has attracted broad attention due to its capacity to utilize consistent and complementary information among views. Although tremendous progress has been made recently, most existing methods undergo high complexity, preventing them from being applied to large-scale tasks. Multi-View clustering via matrix factorization is a representative to address this issue. However, most of them map the data matrices into a fixed dimension, limiting the model's expressiveness. Moreover, a range of methods suffers from a two-step process, i.e., multimodal learning and the subsequent <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""> <tex-math notation=""LaTeX"">$k$</tex-math> </inline-formula> -means, inevitably causing a suboptimal clustering result. In light of this, we propose a one-step multi-view clustering with diverse representation (OMVCDR) method, which incorporates multi-view learning and <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""> <tex-math notation=""LaTeX"">$k$</tex-math> </inline-formula> -means into a unified framework. Specifically, we first project original data matrices into various latent spaces to attain comprehensive information and auto-weight them in a self-supervised manner. Then, we directly use the information matrices under diverse dimensions to obtain consensus discrete clustering labels. The unified work of representation learning and clustering boosts the quality of the final results. Furthermore, we develop an efficient optimization algorithm with proven convergence to solve the resultant problem. Comprehensive experiments on various datasets demonstrate the promising clustering performance of our proposed method. The code is publicly available at https://github.com/wanxinhang/OMVCDR.","['Multi-View clustering via matrix factorization', 'k-means', 'multi-view learning', 'representation learning']","The research idea centers on addressing the limitations of existing multi-view clustering approaches, which often face challenges such as high complexity and restricted expressiveness due to fixed-dimensional data representation. Additionally, many current methods rely on a two-step process that can lead to suboptimal clustering outcomes. The study is motivated by the need to improve the integration of information from multiple views to enhance clustering quality. The primary objective of the study is to develop a unified approach that simultaneously incorporates multi-view learning and clustering to achieve more comprehensive and accurate clustering results. This approach aims to utilize diverse representations of data to better capture information and improve the consensus on clustering labels, ultimately enhancing the effectiveness of multi-view clustering tasks.","The research idea centers on addressing the limitations of existing multi-view clustering approaches, which often face challenges such as high complexity and restricted expressiveness due to fixed-dimensional data representation. Additionally, many current methods rely on a two-step process that can lead to suboptimal clustering outcomes. The study is motivated by the need to improve the integration of information from multiple views to enhance clustering quality. The primary objective of the study is to develop a unified approach that simultaneously incorporates information from multiple views and performs clustering to achieve more comprehensive and accurate results. This approach aims to utilize diverse representations of data to better capture information and improve the consensus on clustering labels, ultimately enhancing the effectiveness of multi-view clustering tasks.",True
Social Sciences,Artificial Intelligence in Point-of-Care Biosensing: Challenges and Opportunities,"The integration of artificial intelligence (AI) into point-of-care (POC) biosensing has the potential to revolutionize diagnostic methodologies by offering rapid, accurate, and accessible health assessment directly at the patient level. This review paper explores the transformative impact of AI technologies on POC biosensing, emphasizing recent computational advancements, ongoing challenges, and future prospects in the field. We provide an overview of core biosensing technologies and their use at the POC, highlighting ongoing issues and challenges that may be solved with AI. We follow with an overview of AI methodologies that can be applied to biosensing, including machine learning algorithms, neural networks, and data processing frameworks that facilitate real-time analytical decision-making. We explore the applications of AI at each stage of the biosensor development process, highlighting the diverse opportunities beyond simple data analysis procedures. We include a thorough analysis of outstanding challenges in the field of AI-assisted biosensing, focusing on the technical and ethical challenges regarding the widespread adoption of these technologies, such as data security, algorithmic bias, and regulatory compliance. Through this review, we aim to emphasize the role of AI in advancing POC biosensing and inform researchers, clinicians, and policymakers about the potential of these technologies in reshaping global healthcare landscapes.",['neural networks'],"The study addresses the transformative potential of integrating advanced technologies into point-of-care biosensing to improve diagnostic methodologies by enabling rapid, accurate, and accessible health assessments directly at the patient level. It highlights ongoing challenges and ethical concerns related to the widespread adoption of these innovations, such as data security, bias, and regulatory compliance, which impact the effectiveness and acceptance of point-of-care diagnostics. The primary aim of the study is to provide a comprehensive overview of current biosensing technologies used at the point of care, identify existing issues and challenges, and discuss future prospects that could enhance healthcare delivery. Additionally, the study seeks to inform researchers, clinicians, and policymakers about the opportunities and obstacles in advancing point-of-care biosensing to reshape global healthcare landscapes.","The study addresses the transformative potential of integrating modern technologies into point-of-care biosensing to improve diagnostic methodologies by enabling rapid, accurate, and accessible health assessments directly at the patient level. It highlights ongoing challenges and ethical concerns related to the widespread adoption of these innovations, such as data security, bias, and regulatory compliance, which impact the effectiveness and acceptance of point-of-care diagnostics. The primary aim of the study is to provide a comprehensive overview of current biosensing technologies used at the point of care, identify existing issues and challenges, and discuss future prospects that could enhance healthcare delivery. Additionally, the study seeks to inform researchers, clinicians, and policymakers about the opportunities and obstacles in advancing point-of-care biosensing to reshape global healthcare landscapes.",True
Social Sciences,Evaluating the persuasive influence of political microtargeting with large language models,"Recent advancements in large language models (LLMs) have raised the prospect of scalable, automated, and fine-grained political microtargeting on a scale previously unseen; however, the persuasive influence of microtargeting with LLMs remains unclear. Here, we build a custom web application capable of integrating self-reported demographic and political data into GPT-4 prompts in real-time, facilitating the live creation of unique messages tailored to persuade individual users on four political issues. We then deploy this application in a preregistered randomized control experiment ( n = 8,587) to investigate the extent to which access to individual-level data increases the persuasive influence of GPT-4. Our approach yields two key findings. First, messages generated by GPT-4 were broadly persuasive, in some cases increasing support for an issue stance by up to 12 percentage points. Second, in aggregate, the persuasive impact of microtargeted messages was not statistically different from that of non-microtargeted messages (4.83 vs. 6.20 percentage points, respectively, P = 0.226). These trends hold even when manipulating the type and number of attributes used to tailor the message. These findings suggest—contrary to widespread speculation—that the influence of current LLMs may reside not in their ability to tailor messages to individuals but rather in the persuasiveness of their generic, nontargeted messages. We release our experimental dataset, GPTarget2024 , as an empirical baseline for future research.",['GPT-4'],"The research idea centers on understanding the persuasive influence of political microtargeting at an individual level, a practice that has gained attention due to recent technological advancements enabling highly personalized messaging. Despite the potential for finely tailored political communication, it remains unclear whether such microtargeting actually enhances persuasive impact compared to more generic messaging. The study aims to investigate whether access to individual-level demographic and political information increases the effectiveness of persuasive messages on political issues. Specifically, the primary objective is to assess the extent to which personalized political messages influence individuals’ support for issue stances, and to compare the persuasive impact of microtargeted messages with that of non-microtargeted messages in a large-scale experimental setting.","The research idea centers on understanding the persuasive influence of political microtargeting at an individual level, a practice that has gained attention due to recent technological advancements enabling highly personalized messaging. Despite the potential for finely tailored political communication, it remains unclear whether such microtargeting actually enhances persuasive impact compared to more generic messaging. The study aims to investigate whether access to individual-level demographic and political information increases the effectiveness of persuasive messages on political issues. Specifically, the primary objective is to assess the extent to which personalized political messages influence individuals' support for issue stances, and to compare the persuasive impact of microtargeted messages with that of non-microtargeted messages in a large-scale experimental setting.",True
Social Sciences,Utilisation of Deep Learning (DL) and Neural Networks (NN) Algorithms for Energy Power Generation: A Social Network and Bibliometric Analysis (2004-2022),"The research landscape on the applications of advanced computational tools (ACTs) such as machine/deep learning and neural network algorithms for energy and power generation (EPG) was critically examined through publication trends and bibliometrics data analysis. The Elsevier Scopus database and the PRISMA methodology were employed to identify and screen the published documents, whereas the bibliometric analysis software VOSviewer was used to analyse the co-authorships, citations, and keyword occurrences. The results showed that 152 documents have been published on the topic comprising conference proceedings (58.6%) and articles (41.4%) between 2004 and 2022. Publication trends analysis revealed the number of publications increased from 1 to 31 or by 3,000% over the same period, which was ascribed to the growing scientific interest and research impact of the topic. Stakeholder analysis revealed the top authors/researchers are Anvari M, Ghaderi SF and Saberi M, whereas the most prolific affiliation and nations actively engaged in the topic are the North China Electric Power University, and China, respectively. Conversely, the top funding agency actively backing research on the topic is the National Natural Science Foundation of China (NSFC). Co-authorship analysis revealed high levels of collaboration between researching nations compared to authors and affiliations. Hotspot analysis revealed three major thematic focus areas namely; Energy Grid Forecasting, Power Generation Control, and Intelligent Energy Optimization. In conclusion, the study showed that the application of ACTs in EPG is an active, multidisciplinary, and impact area of research with potential for more impactful contributions to research and society at large.","['machine learning', 'deep learning', 'neural network algorithms']","The research idea centers on understanding the growing scientific interest and impact of advanced computational tools in the field of energy and power generation, as evidenced by increasing publication trends and collaborative efforts among researchers and institutions. The study addresses the need to critically examine the development and dissemination of knowledge within this multidisciplinary area, highlighting key contributors, thematic focus areas, and funding sources. The primary objective of the study is to systematically review and characterize the research landscape on the applications of these tools in energy and power generation by analyzing publication trends, authorship collaborations, and thematic priorities. This aims to provide insights into the current state of research and identify areas with potential for further contributions to both academia and society.","The research idea centers on understanding the growing scientific interest and impact of methodological innovations in the field of energy and power generation, as evidenced by increasing publication trends and collaborative efforts among researchers and institutions. The study addresses the need to critically examine the development and dissemination of knowledge within this multidisciplinary area, highlighting key contributors, thematic focus areas, and funding sources. The primary objective of the study is to systematically review and characterize the research landscape on the applications of these methodologies in energy and power generation by analyzing publication trends, authorship collaborations, and thematic priorities. This aims to provide insights into the current state of research and identify areas with potential for further contributions to both academia and society.",True
Social Sciences,Potential of Large Language Models in Health Care: Delphi Study,"Background A large language model (LLM) is a machine learning model inferred from text data that captures subtle patterns of language use in context. Modern LLMs are based on neural network architectures that incorporate transformer methods. They allow the model to relate words together through attention to multiple words in a text sequence. LLMs have been shown to be highly effective for a range of tasks in natural language processing (NLP), including classification and information extraction tasks and generative applications. Objective The aim of this adapted Delphi study was to collect researchers’ opinions on how LLMs might influence health care and on the strengths, weaknesses, opportunities, and threats of LLM use in health care. Methods We invited researchers in the fields of health informatics, nursing informatics, and medical NLP to share their opinions on LLM use in health care. We started the first round with open questions based on our strengths, weaknesses, opportunities, and threats framework. In the second and third round, the participants scored these items. Results The first, second, and third rounds had 28, 23, and 21 participants, respectively. Almost all participants (26/28, 93% in round 1 and 20/21, 95% in round 3) were affiliated with academic institutions. Agreement was reached on 103 items related to use cases, benefits, risks, reliability, adoption aspects, and the future of LLMs in health care. Participants offered several use cases, including supporting clinical tasks, documentation tasks, and medical research and education, and agreed that LLM-based systems will act as health assistants for patient education. The agreed-upon benefits included increased efficiency in data handling and extraction, improved automation of processes, improved quality of health care services and overall health outcomes, provision of personalized care, accelerated diagnosis and treatment processes, and improved interaction between patients and health care professionals. In total, 5 risks to health care in general were identified: cybersecurity breaches, the potential for patient misinformation, ethical concerns, the likelihood of biased decision-making, and the risk associated with inaccurate communication. Overconfidence in LLM-based systems was recognized as a risk to the medical profession. The 6 agreed-upon privacy risks included the use of unregulated cloud services that compromise data security, exposure of sensitive patient data, breaches of confidentiality, fraudulent use of information, vulnerabilities in data storage and communication, and inappropriate access or use of patient data. Conclusions Future research related to LLMs should not only focus on testing their possibilities for NLP-related tasks but also consider the workflows the models could contribute to and the requirements regarding quality, integration, and regulations needed for successful implementation in practice.","['large language model (LLM)', 'transformer methods']","The research idea centers on understanding how large language models might influence health care by exploring their potential benefits, risks, and overall impact on clinical and administrative tasks. The study addresses the need to gather expert opinions on the strengths, weaknesses, opportunities, and threats associated with the use of these models in health care settings. The primary objective of the study was to collect researchers’ perspectives on the influence of large language models in health care, focusing on identifying use cases, benefits, risks, reliability, adoption factors, and future implications. This was aimed at informing considerations for quality, integration, and regulatory requirements necessary for their successful implementation in health care practice.","The research idea centers on understanding how new text-generation technologies might influence health care by exploring their potential benefits, risks, and overall impact on clinical and administrative tasks. The study addresses the need to gather expert opinions on the strengths, weaknesses, opportunities, and threats associated with the use of these systems in health care settings. The primary objective of the study was to collect researchers' perspectives on the influence of advanced text processing systems in health care, focusing on identifying use cases, benefits, risks, reliability, adoption factors, and future implications. This was aimed at informing considerations for quality, integration, and regulatory requirements necessary for their successful implementation in health care practice.",True
Social Sciences,Unveiling the dynamics of AI applications: A review of reviews using scientometrics and BERTopic modeling,"In a world that has rapidly transformed through the advent of artificial intelligence (AI), our systematic review, guided by the PRISMA protocol, investigates a decade of AI research, revealing insights into its evolution and impact. Our study, examining 3,767 articles, has drawn considerable attention, as evidenced by an impressive 63,577 citations, underscoring the scholarly community's profound engagement. Our study reveals a collaborative landscape with 18,189 contributing authors, reflecting a robust network of researchers advancing AI and machine learning applications. Review categories focus on systematic reviews and bibliometric analyses, indicating an increasing emphasis on comprehensive literature synthesis and quantitative analysis. The findings also suggest an opportunity to explore emerging methodologies such as topic modeling and meta-analysis. We dissect the state of the art presented in these reviews, finding themes throughout the broad scholarly discourse through thematic clustering and BERTopic modeling. Categorization of study articles across fields of research indicates dominance in Information and Computing Sciences, followed by Biomedical and Clinical Sciences. Subject categories reveal interconnected clusters across various sectors, notably in healthcare, engineering, business intelligence, and computational technologies. Semantic analysis via BERTopic revealed nineteen clusters mapped to themes such as AI in health innovations, AI for sustainable development, AI and deep learning, AI in education, and ethical considerations. Future research directions are suggested, emphasizing the need for intersectional bias mitigation, holistic health approaches, AI's role in environmental sustainability, and the ethical deployment of generative AI.","['topic modeling', 'BERTopic modeling']","The research idea centers on understanding the evolution and impact of a rapidly transforming technological field over the past decade, highlighting the extensive scholarly engagement and collaborative efforts within this area. The study addresses the need to synthesize a vast body of literature to reveal thematic trends and the interdisciplinary nature of research across various sectors such as healthcare, business, and sustainability. The primary objective of the study is to systematically review and categorize a large collection of scholarly articles to uncover dominant themes, research clusters, and emerging areas of interest within the field. Additionally, the study aims to identify future research directions that emphasize ethical considerations, bias mitigation, holistic health approaches, and contributions to environmental sustainability.","The research idea centers on understanding the evolution and impact of a rapidly transforming technological field over the past decade, highlighting the extensive scholarly engagement and collaborative efforts within this area. The study addresses the need to synthesize a vast body of literature to reveal thematic trends and the interdisciplinary nature of research across various sectors such as healthcare, business, and sustainability. The primary objective of the study is to systematically review and categorize a large collection of scholarly articles to uncover dominant themes, research clusters, and emerging areas of interest within the field. Additionally, the study aims to identify future research directions that emphasize ethical considerations, responsible development practices, holistic health approaches, and contributions to environmental sustainability.",True
Social Sciences,Beyond Discrimination: Generative AI Applications and Ethical Challenges in Forensic Psychiatry,"The advent and growing popularity of generative artificial intelligence (GenAI) holds the potential to revolutionise AI applications in forensic psychiatry and criminal justice, which traditionally relied on discriminative AI algorithms. Generative AI models mark a significant shift from the previously prevailing paradigm through their ability to generate seemingly new realistic data and analyse and integrate a vast amount of unstructured content from different data formats. This potential extends beyond reshaping conventional practices, like risk assessment, diagnostic support, and treatment and rehabilitation plans, to creating new opportunities in previously underexplored areas, such as training and education. This paper examines the transformative impact of generative artificial intelligence on AI applications in forensic psychiatry and criminal justice. First, it introduces generative AI and its prevalent models. Following this, it reviews the current applications of discriminative AI in forensic psychiatry. Subsequently, it presents a thorough exploration of the potential of generative AI to transform established practices and introduce novel applications through multimodal generative models, data generation and data augmentation. Finally, it provides a comprehensive overview of ethical and legal issues associated with deploying generative AI models, focusing on their impact on individuals as well as their broader societal implications. In conclusion, this paper aims to contribute to the ongoing discourse concerning the dynamic challenges of generative AI applications in forensic contexts, highlighting potential opportunities, risks, and challenges. It advocates for interdisciplinary collaboration and emphasises the necessity for thorough, responsible evaluations of generative AI models before widespread adoption into domains where decisions with substantial life-altering consequences are routinely made.","['generative AI models', 'multimodal generative models']","The study addresses the transformative potential of emerging technologies in forensic psychiatry and criminal justice, fields that have traditionally relied on established approaches for risk assessment, diagnostic support, and treatment planning. It highlights how new developments could reshape conventional practices and open opportunities in areas such as training and education, while also raising important ethical and legal concerns. The primary aim of the study is to examine the impact of these emerging technologies on existing applications within forensic psychiatry and criminal justice, exploring both the opportunities for innovation and the associated risks and challenges. Additionally, the study seeks to contribute to ongoing discussions by emphasizing the need for interdisciplinary collaboration and responsible evaluation before these technologies are widely adopted in contexts involving critical decisions affecting individuals’ lives.","The study addresses the transformative potential of emerging technologies in forensic psychiatry and criminal justice, fields that have traditionally relied on established approaches for risk assessment, diagnostic support, and treatment planning. It highlights how new developments could reshape conventional practices and open opportunities in areas such as training and education, while also raising important ethical and legal concerns. The primary aim of the study is to examine the impact of these emerging technologies on existing applications within forensic psychiatry and criminal justice, exploring both the opportunities for innovation and the associated risks and challenges. Additionally, the study seeks to contribute to ongoing discussions by emphasizing the need for interdisciplinary collaboration and responsible evaluation before these technologies are widely adopted in contexts involving critical decisions affecting individuals' lives.",True
Social Sciences,Artificial Intelligence to Automate Network Meta-Analyses: Four Case Studies to Evaluate the Potential Application of Large Language Models,"The emergence of artificial intelligence, capable of human-level performance on some tasks, presents an opportunity to revolutionise development of systematic reviews and network meta-analyses (NMAs). In this pilot study, we aim to assess use of a large-language model (LLM, Generative Pre-trained Transformer 4 [GPT-4]) to automatically extract data from publications, write an R script to conduct an NMA and interpret the results. We considered four case studies involving binary and time-to-event outcomes in two disease areas, for which an NMA had previously been conducted manually. For each case study, a Python script was developed that communicated with the LLM via application programming interface (API) calls. The LLM was prompted to extract relevant data from publications, to create an R script to be used to run the NMA and then to produce a small report describing the analysis. The LLM had a > 99% success rate of accurately extracting data across 20 runs for each case study and could generate R scripts that could be run end-to-end without human input. It also produced good quality reports describing the disease area, analysis conducted, results obtained and a correct interpretation of the results. This study provides a promising indication of the feasibility of using current generation LLMs to automate data extraction, code generation and NMA result interpretation, which could result in significant time savings and reduce human error. This is provided that routine technical checks are performed, as recommend for human-conducted analyses. Whilst not currently 100% consistent, LLMs are likely to improve with time.","['large-language model (LLM, Generative Pre-trained Transformer 4 [GPT-4])']","The study addresses the challenge of efficiently conducting systematic reviews and network meta-analyses, which are important for synthesizing evidence in healthcare research but are often time-consuming and prone to human error. There is a need to explore new approaches that can streamline the process of data extraction, analysis, and interpretation to improve the reliability and speed of these reviews. The primary aim of the study is to assess the feasibility of automating key steps in the development of network meta-analyses, including data extraction from publications, conducting the analysis, and interpreting the results, in order to achieve significant time savings and reduce human error while maintaining accuracy. This pilot study evaluates the potential of such automation in multiple case studies to determine its effectiveness and reliability compared to traditional manual methods.","The study addresses the challenge of efficiently conducting systematic reviews and network meta-analyses, which are important for synthesizing evidence in healthcare research but are often time-consuming and prone to human error. There is a need to explore new approaches that can streamline the process of data extraction, analysis, and interpretation to improve the reliability and speed of these reviews. The primary aim of the study is to assess the feasibility of systematizing key steps in the development of network meta-analyses, including data extraction from publications, conducting the analysis, and interpreting the results, in order to achieve significant time savings and reduce human error while maintaining accuracy. This pilot study evaluates the potential of such systematized processes in multiple case studies to determine their effectiveness and reliability compared to traditional manual methods.",True
Social Sciences,"Exploring Rich Subjective Quality Information for Image Quality
  Assessment in the Wild","Traditional in the wild image quality assessment (IQA) models are generally trained with the quality labels of mean opinion score (MOS), while missing the rich subjective quality information contained in the quality ratings, for example, the standard deviation of opinion scores (SOS) or even distribution of opinion scores (DOS). In this paper, we propose a novel IQA method named RichIQA to explore the rich subjective rating information beyond MOS to predict image quality in the wild. RichIQA is characterized by two key novel designs: (1) a three-stage image quality prediction network which exploits the powerful feature representation capability of the Convolutional vision Transformer (CvT) and mimics the short-term and long-term memory mechanisms of human brain; (2) a multi-label training strategy in which rich subjective quality information like MOS, SOS and DOS are concurrently used to train the quality prediction network. Powered by these two novel designs, RichIQA is able to predict the image quality in terms of a distribution, from which the mean image quality can be subsequently obtained. Extensive experimental results verify that the three-stage network is tailored to predict rich quality information, while the multi-label training strategy can fully exploit the potentials within subjective quality rating and enhance the prediction performance and generalizability of the network. RichIQA outperforms state-of-the-art competitors on multiple large-scale in the wild IQA databases with rich subjective rating labels. The code of RichIQA will be made publicly available on GitHub.",['Convolutional vision Transformer (CvT)'],"The research idea addresses the limitation of traditional approaches to assessing image quality in natural settings, which typically rely only on average quality ratings and overlook the richer subjective information contained in the variability and distribution of individual quality ratings. This study recognizes that subjective quality assessments encompass more nuanced information beyond mean opinion scores, such as the diversity of opinions among viewers, which has been largely ignored in previous evaluations. The primary objective of the study is to develop a method that incorporates this richer subjective rating information, including measures of variability and distribution of opinion scores, to more accurately predict perceived image quality in real-world conditions. The study aims to leverage these comprehensive subjective quality indicators concurrently to improve the understanding and prediction of image quality as experienced by diverse observers.","The research idea addresses the limitation of traditional approaches to assessing image quality in natural settings, which typically rely only on average quality ratings and overlook the richer subjective information contained in the variability and distribution of individual quality ratings. This study recognizes that subjective quality assessments encompass more nuanced information beyond mean opinion scores, such as the diversity of opinions among viewers, which has been largely ignored in previous evaluations. The primary objective of the study is to develop a method that incorporates this richer subjective rating information, including measures of variability and distribution of opinion scores, to more accurately predict perceived image quality in real-world conditions. The study aims to utilize these comprehensive subjective quality indicators concurrently to improve the understanding and prediction of image quality as experienced by diverse observers.",True
Social Sciences,Artificial intelligence for oral squamous cell carcinoma detection based on oral photographs: A comprehensive literature review,"Abstract Introduction Oral squamous cell carcinoma (OSCC) presents a significant global health challenge. The integration of artificial intelligence (AI) and computer vision holds promise for the early detection of OSCC through the analysis of digitized oral photographs. This literature review explores the landscape of AI‐driven OSCC automatic detection, assessing both the performance and limitations of the current state of the art. Materials and Methods An electronic search using several data base was conducted, and a systematic review performed in accordance with PRISMA guidelines (CRD42023441416). Results Several studies have demonstrated remarkable results for this task, consistently achieving sensitivity rates exceeding 85% and accuracy rates surpassing 90%, often encompassing around 1000 images. The review scrutinizes these studies, shedding light on their methodologies, including the use of recent machine learning and pattern recognition approaches coupled with different supervision strategies. However, comparing the results from different papers is challenging due to variations in the datasets used. Discussion Considering these findings, this review underscores the urgent need for more robust and reliable datasets in the field of OSCC detection. Furthermore, it highlights the potential of advanced techniques such as multi‐task learning, attention mechanisms, and ensemble learning as crucial tools in enhancing the accuracy and sensitivity of OSCC detection through oral photographs. Conclusion These insights collectively emphasize the transformative impact of AI‐driven approaches on early OSCC diagnosis, with the potential to significantly improve patient outcomes and healthcare practices.","['machine learning', 'multi-task learning', 'attention mechanisms', 'ensemble learning']","The research idea addresses the significant global health challenge posed by oral squamous cell carcinoma (OSCC) and the importance of early detection to improve patient outcomes. The study recognizes the current efforts and progress made in detecting OSCC through the analysis of oral photographs, while also acknowledging the limitations and variability in existing research due to differences in datasets. The research objective is to explore and review the current landscape of automatic OSCC detection, assessing the performance and limitations of existing studies. This review aims to highlight the need for more robust and reliable datasets and to emphasize approaches that could enhance the accuracy and sensitivity of early OSCC diagnosis, ultimately contributing to improved healthcare practices.","The research idea addresses the significant global health challenge posed by oral squamous cell carcinoma (OSCC) and the importance of early detection to improve patient outcomes. The study recognizes the current efforts and progress made in detecting OSCC through the analysis of oral photographs, while also acknowledging the limitations and variability in existing research due to differences in datasets. The research objective is to explore and review the current landscape of OSCC detection methodologies, assessing the performance and limitations of existing studies. This review aims to highlight the need for more robust and reliable datasets and to emphasize approaches that could enhance the accuracy and sensitivity of early OSCC diagnosis, ultimately contributing to improved healthcare practices.",True
Social Sciences,Systematic Review of Large Language Models for Patient Care: Current Applications and Challenges,"Abstract The introduction of large language models (LLMs) into clinical practice promises to improve patient education and empowerment, thereby personalizing medical care and broadening access to medical knowledge. Despite the popularity of LLMs, there is a significant gap in systematized information on their use in patient care. Therefore, this systematic review aims to synthesize current applications and limitations of LLMs in patient care using a data-driven convergent synthesis approach. We searched 5 databases for qualitative, quantitative, and mixed methods articles on LLMs in patient care published between 2022 and 2023. From 4,349 initial records, 89 studies across 29 medical specialties were included, primarily examining models based on the GPT-3.5 (53.2%, n=66 of 124 different LLMs examined per study) and GPT-4 (26.6%, n=33/124) architectures in medical question answering, followed by patient information generation, including medical text summarization or translation, and clinical documentation. Our analysis delineates two primary domains of LLM limitations: design and output. Design limitations included 6 second-order and 12 third-order codes, such as lack of medical domain optimization, data transparency, and accessibility issues, while output limitations included 9 second-order and 32 third-order codes, for example, non-reproducibility, non-comprehensiveness, incorrectness, unsafety, and bias. In conclusion, this study is the first review to systematically map LLM applications and limitations in patient care, providing a foundational framework and taxonomy for their implementation and evaluation in healthcare settings.","['GPT-3.5', 'GPT-4']","The research idea centers on the potential of large language models to enhance patient education and empowerment, thereby personalizing medical care and expanding access to medical knowledge. Despite their growing popularity, there is a notable lack of organized information regarding their practical use in patient care. The research objective is to systematically synthesize existing applications and limitations of these models in patient care by reviewing relevant studies across various medical specialties. This study aims to provide a comprehensive framework and taxonomy to guide the implementation and evaluation of these tools within healthcare settings.","The research idea centers on the potential of novel communication tools to enhance patient education and empowerment, thereby personalizing medical care and expanding access to medical knowledge. Despite their growing popularity, there is a notable lack of organized information regarding their practical use in patient care. The research objective is to systematically synthesize existing applications and limitations of these tools in patient care by reviewing relevant studies across various medical specialties. This study aims to provide a comprehensive framework and taxonomy to guide the implementation and evaluation of these tools within healthcare settings.",True
Social Sciences,PathAsst: A Generative Foundation AI Assistant towards Artificial General Intelligence of Pathology,"As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, offering significant applications in interpreting natural images. However, the field of pathology has largely remained untapped, particularly in gathering high-quality data and designing comprehensive model frameworks. To bridge the gap in pathology MLLMs, we present PathAsst, a multimodal generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. The development of PathAsst involves three pivotal steps: data acquisition, CLIP model adaptation, and the training of PathAsst's multimodal generative capabilities. Firstly, we collect over 207K high-quality pathology image-text pairs from authoritative sources. Leveraging the advanced power of ChatGPT, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data specifically tailored for invoking eight pathology-specific sub-models we prepared, allowing the PathAsst to effectively collaborate with these models, enhancing its diagnostic ability. Secondly, by leveraging the collected data, we construct PathCLIP, a pathology-dedicated CLIP, to enhance PathAsst's capabilities in interpreting pathology images. Finally, we integrate PathCLIP with the Vicuna-13b and utilize pathology-specific instruction-tuning data to enhance the multimodal generation capacity of PathAsst and bolster its synergistic interactions with sub-models. The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes. We open-source our dataset, as well as a comprehensive toolkit for extensive pathology data collection and preprocessing at https://github.com/superjamessyx/Generative-Foundation-AI-Assistant-for-Pathology.","['CLIP model adaptation', 'instruction-tuning']","The research idea centers on addressing the underexplored area of pathology in the context of interpreting natural images, highlighting challenges in gathering high-quality pathology data and designing comprehensive frameworks for improved diagnostic and predictive capabilities. The study recognizes a significant gap in pathology applications despite advances in related fields, emphasizing the need to enhance diagnostic processes through better data and tailored approaches. The primary objective of the study is to develop a novel pathology-focused assistant aimed at revolutionizing diagnostic and predictive analytics within pathology by collecting extensive high-quality pathology image-text data and creating specialized resources to improve interpretation and collaboration with pathology-specific sub-models. This effort seeks to improve pathology diagnosis and treatment processes by leveraging these tailored resources and making the dataset and tools openly available for further research and application.","The research idea centers on addressing the underexplored area of pathology in the context of interpreting natural images, highlighting challenges in gathering high-quality pathology data and designing comprehensive frameworks for improved diagnostic and predictive capabilities. The study recognizes a significant gap in pathology applications despite advances in related fields, emphasizing the need to enhance diagnostic processes through better data and tailored approaches. The primary objective of the study is to develop a novel pathology-focused assistant aimed at revolutionizing diagnostic and predictive analytics within pathology by collecting extensive high-quality pathology image-text data and creating specialized resources to improve interpretation and collaboration with pathology-specific components. This effort seeks to improve pathology diagnosis and treatment processes by leveraging these tailored resources and making the dataset and tools openly available for further research and application.",True
Social Sciences,Methodological insights into ChatGPT’s screening performance in systematic reviews,"Abstract Background The screening process for systematic reviews and meta-analyses in medical research is a labor-intensive and time-consuming task. While machine learning and deep learning have been applied to facilitate this process, these methods often require training data and user annotation. This study aims to assess the efficacy of ChatGPT, a large language model based on the Generative Pretrained Transformers (GPT) architecture, in automating the screening process for systematic reviews in radiology without the need for training data. Methods A prospective simulation study was conducted between May 2nd and 24th, 2023, comparing ChatGPT’s performance in screening abstracts against that of general physicians (GPs). A total of 1198 abstracts across three subfields of radiology were evaluated. Metrics such as sensitivity, specificity, positive and negative predictive values (PPV and NPV), workload saving, and others were employed. Statistical analyses included the Kappa coefficient for inter-rater agreement, ROC curve plotting, AUC calculation, and bootstrapping for p-values and confidence intervals. Results ChatGPT completed the screening process within an hour, while GPs took an average of 7–10 days. The AI model achieved a sensitivity of 95% and an NPV of 99%, slightly outperforming the GPs’ sensitive consensus (i.e., including records if at least one person includes them). It also exhibited remarkably low false negative counts and high workload savings, ranging from 40 to 83%. However, ChatGPT had lower specificity and PPV compared to human raters. The average Kappa agreement between ChatGPT and other raters was 0.27. Conclusions ChatGPT shows promise in automating the article screening phase of systematic reviews, achieving high sensitivity and workload savings. While not entirely replacing human expertise, it could serve as an efficient first-line screening tool, particularly in reducing the burden on human resources. Further studies are needed to fine-tune its capabilities and validate its utility across different medical subfields.","['machine learning', 'deep learning', 'Generative Pretrained Transformers (GPT) architecture']","The research addresses the challenge of the labor-intensive and time-consuming nature of the screening process for systematic reviews and meta-analyses in medical research, particularly in radiology. This process requires significant human effort and resources, which can delay the synthesis of medical evidence. The study aims to evaluate the effectiveness of an automated approach in facilitating the screening of abstracts for systematic reviews without relying on extensive prior training or user annotation. The primary objective of the study is to assess the performance of this automated approach in screening abstracts compared to general physicians, focusing on metrics such as sensitivity, workload savings, and agreement with human raters, to determine its potential role in reducing the burden on human resources during systematic reviews.","The research addresses the challenge of the labor-intensive and time-consuming nature of the screening process for systematic reviews and meta-analyses in medical research, particularly in radiology. This process requires significant human effort and resources, which can delay the synthesis of medical evidence. The study aims to evaluate the effectiveness of a computerized assistance tool in facilitating the screening of abstracts for systematic reviews without relying on extensive prior training or user annotation. The primary objective of the study is to assess the performance of this assistance tool in screening abstracts compared to general physicians, focusing on metrics such as sensitivity, workload savings, and agreement with human raters, to determine its potential role in reducing the burden on human resources during systematic reviews.",True
Social Sciences,Navigating the Power of Artificial Intelligence in Risk Management: A Comparative Analysis,"This study presents a responsive analysis of the role of artificial intelligence (AI) in risk management, contrasting traditional approaches with those augmented by AI and highlighting the challenges and opportunities that emerge. AI, intense learning methodologies such as convolutional neural networks (CNNs), have been identified as pivotal in extracting meaningful insights from image data, a form of analysis that holds significant potential in identifying and managing risks across various industries. The research methodology involves a strategic selection and processing of images for analysis and introduces three case studies that serve as benchmarks for evaluation. These case studies showcase the application of AI, in place of image processing capabilities, to identify hazards, evaluate risks, and suggest control measures. The comparative evaluation focuses on the accuracy, relevance, and practicality of the AI-generated findings alongside the system’s response time and comprehensive understanding of the context. Results reveal that AI can significantly enhance risk assessment processes, offering rapid and detailed insights. However, the study also recognises the intrinsic limitations of AI in contextual interpretation, advocating for a synergy between technological and domain-specific expertise. The conclusion underscores the transformative potential of AI in risk management, supporting continued research to further integrate AI effectively into risk assessment frameworks.",['convolutional neural networks (CNNs)'],"The study addresses the evolving role of risk management by examining how traditional approaches compare with newer methods that incorporate advanced techniques for interpreting image data to identify and manage risks across various industries. It highlights the challenges and opportunities that arise from integrating these approaches into existing risk assessment practices. The primary aim of the study is to evaluate the effectiveness of these advanced methods in identifying hazards, assessing risks, and recommending control measures through case studies, while also considering their accuracy, relevance, and practical application. The research seeks to understand how these approaches can enhance risk assessment processes and emphasizes the importance of combining technological capabilities with domain-specific expertise for improved outcomes.","The study addresses the evolving role of risk management by examining how traditional approaches compare with newer methods that incorporate specialized techniques for interpreting image data to identify and manage risks across various industries. It highlights the challenges and opportunities that arise from integrating these approaches into existing risk assessment practices. The primary aim of the study is to evaluate the effectiveness of these advanced methods in identifying hazards, assessing risks, and recommending control measures through case studies, while also considering their accuracy, relevance, and practical application. The research seeks to understand how these approaches can enhance risk assessment processes and emphasizes the importance of combining technological capabilities with domain-specific expertise for improved outcomes.",True
Social Sciences,AI-POWERED FRAUD DETECTION IN BANKING: SAFEGUARDING FINANCIAL TRANSACTIONS,"The banking industry's metamorphosis through digitalization has unquestionably revolutionized accessibility and convenience for customers worldwide. However, this paradigm shift has ushered in a new era of challenges, most notably in the realm of cybersecurity. Conventional rule-based fraud detection strategies have struggled to keep pace with the rapid evolution of cyber threats, prompting a surge of interest in more adaptive approaches like unsupervised learning. Furthermore, the COVID-19 pandemic has exacerbated the issue of bank fraud due to the widespread transition to online platforms and the proliferation of charitable funds, which present ripe opportunities for exploitation by cybercriminals. In response to these pressing concerns, this study delves into the realm of machine learning algorithms for the analysis and identification of fraudulent banking transactions. Notably, it contributes scientific novelty by developing models specifically tailored to this purpose and implementing innovative preprocessing techniques to enhance detection accuracy. Utilizing a diverse array of algorithms, including Random Forest, K-Nearest Neighbor (KNN), Naïve Bayes, Decision Trees, and Logistic Regression, the study showcases promising results. In particular, logistic regression and decision tree models exhibit impressive accuracy and Area Under the Curve (AUC) values of approximately 0.98, 0.97 and 0.95, 0.94, respectively. Given the pervasive nature of banking fraud in our digital society, the utilization of artificial intelligence algorithms for fraud detection stands as a critical and timely endeavor, promising enhanced security and trust in the financial ecosystem.","['unsupervised learning', 'Random Forest', 'K-Nearest Neighbor (KNN)', 'Naïve Bayes', 'Decision Trees', 'Logistic Regression']","The banking industry’s transformation through digitalization has significantly improved customer accessibility and convenience but has also introduced new challenges, particularly in the area of cybersecurity. The rapid evolution of cyber threats has rendered traditional fraud detection methods less effective, while the COVID-19 pandemic has intensified the problem of bank fraud due to increased online activity and the rise of charitable fund exploitation. This study aims to address these pressing concerns by focusing on the identification and analysis of fraudulent banking transactions. Its primary objective is to develop and implement approaches specifically designed to enhance the detection of bank fraud, thereby contributing to improved security and trust within the financial sector.","The banking industry's transformation through digitalization has significantly improved customer accessibility and convenience but has also introduced new challenges, particularly in the area of cybersecurity. The rapid evolution of cyber threats has rendered traditional fraud detection methods less effective, while the COVID-19 pandemic has intensified the problem of bank fraud due to increased online activity and the rise of charitable fund exploitation. This study aims to address these pressing concerns by focusing on the identification and analysis of fraudulent banking transactions. Its primary objective is to develop and implement new methodologies specifically designed to enhance the detection of bank fraud, thereby contributing to improved security and trust within the financial sector.",True
