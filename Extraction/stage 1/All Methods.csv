year,openalex_id,domain,title,abstract,extracted methods
2024,https://openalex.org/W4392173735,Biology,"A Comprehensive Survey of Continual Learning: Theory, Method and Application","To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance drop of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efficiency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative strategies address continual learning, and how they are adapted to particular challenges in various applications. Through an in-depth discussion of promising directions, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this field and beyond.",<method>continual learning</method>
2024,https://openalex.org/W4392984771,Biology,Global prediction of extreme floods in ungauged watersheds,"Abstract Floods are one of the most common natural disasters, with a disproportionate impact in developing countries that often lack dense streamflow gauge networks 1 . Accurate and timely warnings are critical for mitigating flood risks 2 , but hydrological simulation models typically must be calibrated to long data records in each watershed. Here we show that artificial intelligence-based forecasting achieves reliability in predicting extreme riverine events in ungauged watersheds at up to a five-day lead time that is similar to or better than the reliability of nowcasts (zero-day lead time) from a current state-of-the-art global modelling system (the Copernicus Emergency Management Service Global Flood Awareness System). In addition, we achieve accuracies over five-year return period events that are similar to or better than current accuracies over one-year return period events. This means that artificial intelligence can provide flood warnings earlier and over larger and more impactful events in ungauged basins. The model developed here was incorporated into an operational early warning system that produces publicly available (free and open) forecasts in real time in over 80 countries. This work highlights a need for increasing the availability of hydrological data to continue to improve global access to reliable flood warnings.",<method>artificial intelligence-based forecasting</method>
2024,https://openalex.org/W4392872715,Biology,GLC_FCS30D: the first global 30 m land-cover dynamics monitoring product with a fine classification system for the period from 1985 to 2022 generated using dense-time-series Landsat imagery and the continuous change-detection method,"Abstract. Land-cover change has been identified as an important cause or driving force of global climate change and is a significant research topic. Over the past few decades, global land-cover mapping has progressed; however, long-time-series global land-cover-change monitoring data are still sparse, especially those at 30 m resolution. In this study, we describe GLC_FCS30D, a novel global 30 m land-cover dynamics monitoring dataset containing 35 land-cover subcategories and covering the period 1985–2022 in 26 time steps (maps were updated every 5 years before 2000 and annually after 2000). GLC_FCS30D has been developed using continuous change detection and all available Landsat imagery based on the Google Earth Engine platform. Specifically, we first take advantage of the continuous change-detection model and the full time series of Landsat observations to capture the time points of changed pixels and identify the temporally stable areas. Then, we apply a spatiotemporal refinement method to derive the globally distributed and high-confidence training samples from these temporally stable areas. Next, local adaptive classification models are used to update the land-cover information for the changed pixels, and a temporal-consistency optimization algorithm is adopted to improve their temporal stability and suppress some false changes. Further, the GLC_FCS30D product is validated using 84 526 globally distributed validation samples from 2020. It achieves an overall accuracy of 80.88 % (±0.27 %) for the basic classification system (10 major land-cover types) and 73.04 % (±0.30 %) for the LCCS (Land Cover Classification System) level-1 validation system (17 LCCS land-cover types). Meanwhile, two third-party time-series datasets used for validation from the United States and Europe Union are also collected for analyzing accuracy variations, and the results show that GLC_FCS30D offers significant stability in terms of variation across the accuracy time series and achieves mean accuracies of 79.50 % (±0.50 %) and 81.91 % (±0.09 %) over the two regions. Lastly, we draw conclusions about the global land-cover-change information from the GLC_FCS30D dataset; namely, that forest and cropland variations have dominated global land-cover change over past 37 years, the net loss of forests reached about 2.5 million km2, and the net gain in cropland area is approximately 1.3 million km2. Therefore, the novel dataset GLC_FCS30D is an accurate land-cover-dynamics time-series monitoring product that benefits from its diverse classification system, high spatial resolution, and long time span (1985–2022); thus, it will effectively support global climate change research and promote sustainable development analysis. The GLC_FCS30D dataset is available via https://doi.org/10.5281/zenodo.8239305 (Liu et al., 2023).","<method>continuous change-detection model</method>, <method>spatiotemporal refinement method</method>, <method>local adaptive classification models</method>, <method>temporal-consistency optimization algorithm</method>"
2024,https://openalex.org/W4391925496,Biology,"Selection, optimization and validation of ten chronic disease polygenic risk scores for clinical implementation in diverse US populations","Polygenic risk scores (PRSs) have improved in predictive performance, but several challenges remain to be addressed before PRSs can be implemented in the clinic, including reduced predictive performance of PRSs in diverse populations, and the interpretation and communication of genetic results to both providers and patients. To address these challenges, the National Human Genome Research Institute-funded Electronic Medical Records and Genomics (eMERGE) Network has developed a framework and pipeline for return of a PRS-based genome-informed risk assessment to 25,000 diverse adults and children as part of a clinical study. From an initial list of 23 conditions, ten were selected for implementation based on PRS performance, medical actionability and potential clinical utility, including cardiometabolic diseases and cancer. Standardized metrics were considered in the selection process, with additional consideration given to strength of evidence in African and Hispanic populations. We then developed a pipeline for clinical PRS implementation (score transfer to a clinical laboratory, validation and verification of score performance), and used genetic ancestry to calibrate PRS mean and variance, utilizing genetically diverse data from 13,475 participants of the All of Us Research Program cohort to train and test model parameters. Finally, we created a framework for regulatory compliance and developed a PRS clinical report for return to providers and for inclusion in an additional genome-informed risk assessment. The initial experience from eMERGE can inform the approach needed to implement PRS-based testing in diverse clinical settings.",No methods found.
2024,https://openalex.org/W4390609372,Biology,Investigation of the moderation effect of gender and study level on the acceptance and use of generative <scp>AI</scp> by higher education students: Comparative evidence from Poland and Egypt,"Abstract This study delves into the implications of incorporating AI tools, specifically ChatGPT, in higher education contexts. With a primary focus on understanding the acceptance and utilization of ChatGPT among university students, the research utilizes the Unified Theory of Acceptance and Use of Technology (UTAUT) as the guiding framework. The investigation probes into four crucial constructs of UTAUT—performance expectancy, effort expectancy, social influence and facilitating conditions—to understand their impact on the intent and actual use behaviour of students. The study relies on data collected from six universities in two countries and assessed through descriptive statistics and structural equation modelling techniques, and also takes into account participants' gender and study level. The key findings show that performance expectancy, effort expectancy, and social influence significantly influence behavioural intention. Furthermore, behavioural intention, when considered alongside facilitating conditions, influences actual use behaviour. This research also explores the moderating impact of gender and study level on the relationships among these variables. The results not only augment our comprehension of technology acceptance in the context of AI tools but also provide valuable input for formulating strategies that promote effective incorporation of ChatGPT in higher education. The study underscores the need for effective awareness initiatives, bespoke training programmes, and intuitive tool designs to bolster students' perceptions and foster the wider adoption of AI tools in education. Practitioner notes What is already known about this topic ChatGPT is a tool that is quickly gaining worldwide recognition. ChatGPT helps with writing essays and solving assignments. ChatGPT raises ethical concerns about authorship, plagiarism and ethics. What this paper adds This study explores students' acceptance of ChatGPT as an aid in their education, which has not been studied previously. We used the extended Unified Technology Acceptance and Use of Technology theory to test what factors mostly influence the use of ChatGPT by students. We conducted a multiple study in Poland and Egypt based on sampling strategy from six universities. Implications for practice and/or policy ChatGPT is a global game changer and should be incorporated into study programmes. The limitations of ChatGPT should be well explained and known since it is prone to making mistakes. Higher education teachers should be aware of ChatGPT's capabilities.",No methods found.
2024,https://openalex.org/W1578086085,Biology,Genotype by Environment Interaction and Adaptation in Barley Breeding: Basic Concepts and Methods of Analysis,"Genotype by environment interaction (GE) has important consequences in barley breeding. It often complicates testing and selection of superior genotypes, reducing genetic progress in breeding programs. This drawback may be overcome by a better understanding of the genetic and environmental factors that determine GE and adaptation of genotypes. An important array of statistical techniques is nowadays available to breeders and researchers to cope with the presence of relevant GE in multi-environment trials. This paper begins with a review of recent literature on the latest barley studies on GE and adaptation, including potential biotic and abiotic causes underlying GE. Most studies reported are empirical, describing postdictively genotypic performance across environments. As an alternative, methods allowing a more analytical approach are proposed, in which genotypes and environments are characterized in terms of external variables that affect genotypic performance. These methods are applied to a real barley data set. After data description, a number of selected multiplicative models are developed, namely the additive main effects and multiplicative interaction (AMMI) model, and the factorial regression model. Finally, the implications of GE in barley breeding are discussed. As an appendix, the SAS programs are given for the models described. Key-words: genotype by environment interaction, adaptation, AMMI, factorial regression, breeding programs","<method>additive main effects and multiplicative interaction (AMMI) model</method>, <method>factorial regression model</method>"
2024,https://openalex.org/W4392754729,Biology,"Revolutionizing agriculture with artificial intelligence: plant disease detection methods, applications, and their limitations","Accurate and rapid plant disease detection is critical for enhancing long-term agricultural yield. Disease infection poses the most significant challenge in crop production, potentially leading to economic losses. Viruses, fungi, bacteria, and other infectious organisms can affect numerous plant parts, including roots, stems, and leaves. Traditional techniques for plant disease detection are time-consuming, require expertise, and are resource-intensive. Therefore, automated leaf disease diagnosis using artificial intelligence (AI) with Internet of Things (IoT) sensors methodologies are considered for the analysis and detection. This research examines four crop diseases: tomato, chilli, potato, and cucumber. It also highlights the most prevalent diseases and infections in these four types of vegetables, along with their symptoms. This review provides detailed predetermined steps to predict plant diseases using AI. Predetermined steps include image acquisition, preprocessing, segmentation, feature selection, and classification. Machine learning (ML) and deep understanding (DL) detection models are discussed. A comprehensive examination of various existing ML and DL-based studies to detect the disease of the following four crops is discussed, including the datasets used to evaluate these studies. We also provided the list of plant disease detection datasets. Finally, different ML and DL application problems are identified and discussed, along with future research prospects, by combining AI with IoT platforms like smart drones for field-based disease detection and monitoring. This work will help other practitioners in surveying different plant disease detection strategies and the limits of present systems.","<method>artificial intelligence (AI)</method>, <method>machine learning (ML)</method>, <method>deep learning (DL)</method>"
2024,https://openalex.org/W4390946589,Biology,Deep-STP: a deep learning-based approach to predict snake toxin proteins by using word embeddings,"Snake venom contains many toxic proteins that can destroy the circulatory system or nervous system of prey. Studies have found that these snake venom proteins have the potential to treat cardiovascular and nervous system diseases. Therefore, the study of snake venom protein is conducive to the development of related drugs. The research technologies based on traditional biochemistry can accurately identify these proteins, but the experimental cost is high and the time is long. Artificial intelligence technology provides a new means and strategy for large-scale screening of snake venom proteins from the perspective of computing. In this paper, we developed a sequence-based computational method to recognize snake toxin proteins. Specially, we utilized three different feature descriptors, namely g-gap , natural vector and word 2 vector, to encode snake toxin protein sequences. The analysis of variance (ANOVA), gradient-boost decision tree algorithm (GBDT) combined with incremental feature selection (IFS) were used to optimize the features, and then the optimized features were input into the deep learning model for model training. The results show that our model can achieve a prediction performance with an accuracy of 82.00% in 10-fold cross-validation. The model is further verified on independent data, and the accuracy rate reaches to 81.14%, which demonstrated that our model has excellent prediction performance and robustness.","<method>gradient-boost decision tree algorithm (GBDT)</method>, <method>incremental feature selection (IFS)</method>, <method>deep learning model</method>"
2024,https://openalex.org/W4392791588,Biology,Can large language models replace humans in systematic reviews? Evaluating <scp>GPT</scp>‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages,"Systematic reviews are vital for guiding practice, research and policy, although they are often slow and labour-intensive. Large language models (LLMs) could speed up and automate systematic reviews, but their performance in such tasks has yet to be comprehensively evaluated against humans, and no study has tested Generative Pre-Trained Transformer (GPT)-4, the biggest LLM so far. This pre-registered study uses a ""human-out-of-the-loop"" approach to evaluate GPT-4's capability in title/abstract screening, full-text review and data extraction across various literature types and languages. Although GPT-4 had accuracy on par with human performance in some tasks, results were skewed by chance agreement and dataset imbalance. Adjusting for these caused performance scores to drop across all stages: for data extraction, performance was moderate, and for screening, it ranged from none in highly balanced literature datasets (~1:1) to moderate in those datasets where the ratio of inclusion to exclusion in studies was imbalanced (~1:3). When screening full-text literature using highly reliable prompts, GPT-4's performance was more robust, reaching ""human-like"" levels. Although our findings indicate that, currently, substantial caution should be exercised if LLMs are being used to conduct systematic reviews, they also offer preliminary evidence that, for certain review tasks delivered under specific conditions, LLMs can rival human performance.","<method>Large language models (LLMs)</method>, <method>Generative Pre-Trained Transformer (GPT)-4</method>"
2024,https://openalex.org/W4399777548,Biology,Feature reduction for hepatocellular carcinoma prediction using machine learning algorithms,"Abstract Hepatocellular carcinoma (HCC) is a highly prevalent form of liver cancer that necessitates accurate prediction models for early diagnosis and effective treatment. Machine learning algorithms have demonstrated promising results in various medical domains, including cancer prediction. In this study, we propose a comprehensive approach for HCC prediction by comparing the performance of different machine learning algorithms before and after applying feature reduction methods. We employ popular feature reduction techniques, such as weighting features, hidden features correlation, feature selection, and optimized selection, to extract a reduced feature subset that captures the most relevant information related to HCC. Subsequently, we apply multiple algorithms, including Naive Bayes, support vector machines (SVM), Neural Networks, Decision Tree, and K nearest neighbors (KNN), to both the original high-dimensional dataset and the reduced feature set. By comparing the predictive accuracy, precision, F Score, recall, and execution time of each algorithm, we assess the effectiveness of feature reduction in enhancing the performance of HCC prediction models. Our experimental results, obtained using a comprehensive dataset comprising clinical features of HCC patients, demonstrate that feature reduction significantly improves the performance of all examined algorithms. Notably, the reduced feature set consistently outperforms the original high-dimensional dataset in terms of prediction accuracy and execution time. After applying feature reduction techniques, the employed algorithms, namely decision trees, Naive Bayes, KNN, neural networks, and SVM achieved accuracies of 96%, 97.33%, 94.67%, 96%, and 96.00%, respectively.","<method>Naive Bayes</method>, <method>support vector machines (SVM)</method>, <method>Neural Networks</method>, <method>Decision Tree</method>, <method>K nearest neighbors (KNN)</method>"
2024,https://openalex.org/W4391019749,Biology,CIFAKE: Image Classification and Explainable Identification of AI-Generated Synthetic Images,"Recent advances in synthetic data have enabled the generation of images with such high quality that human beings cannot tell the difference between real-life photographs and Artificial Intelligence (AI) generated images. Given the critical necessity of data reliability and authentication, this article proposes to enhance our ability to recognise AI-generated images through computer vision. Initially, a synthetic dataset is generated that mirrors the ten classes of the already available CIFAR-10 dataset with latent diffusion, providing a contrasting set of images for comparison to real photographs. The model is capable of generating complex visual attributes, such as photorealistic reflections in water. The two sets of data present as a binary classification problem with regard to whether the photograph is real or generated by AI. This study then proposes the use of a Convolutional Neural Network (CNN) to classify the images into two categories; Real or Fake. Following hyperparameter tuning and the training of 36 individual network topologies, the optimal approach could correctly classify the images with 92.98% accuracy. Finally, this study implements explainable AI via Gradient Class Activation Mapping to explore which features within the images are useful for classification. Interpretation reveals interesting concepts within the image, in particular, noting that the actual entity itself does not hold useful information for classification; instead, the model focuses on small visual imperfections in the background of the images. The complete dataset engineered for this study, referred to as the CIFAKE dataset, is made publicly available to the research community for future work.","<method>latent diffusion</method>, <method>Convolutional Neural Network (CNN)</method>, <method>hyperparameter tuning</method>, <method>Gradient Class Activation Mapping</method>"
2024,https://openalex.org/W4395050972,Biology,Optimization of hepatological clinical guidelines interpretation by large language models: a retrieval augmented generation-based framework,"Abstract Large language models (LLMs) can potentially transform healthcare, particularly in providing the right information to the right provider at the right time in the hospital workflow. This study investigates the integration of LLMs into healthcare, specifically focusing on improving clinical decision support systems (CDSSs) through accurate interpretation of medical guidelines for chronic Hepatitis C Virus infection management. Utilizing OpenAI’s GPT-4 Turbo model, we developed a customized LLM framework that incorporates retrieval augmented generation (RAG) and prompt engineering. Our framework involved guideline conversion into the best-structured format that can be efficiently processed by LLMs to provide the most accurate output. An ablation study was conducted to evaluate the impact of different formatting and learning strategies on the LLM’s answer generation accuracy. The baseline GPT-4 Turbo model’s performance was compared against five experimental setups with increasing levels of complexity: inclusion of in-context guidelines, guideline reformatting, and implementation of few-shot learning. Our primary outcome was the qualitative assessment of accuracy based on expert review, while secondary outcomes included the quantitative measurement of similarity of LLM-generated responses to expert-provided answers using text-similarity scores. The results showed a significant improvement in accuracy from 43 to 99% ( p &lt; 0.001), when guidelines were provided as context in a coherent corpus of text and non-text sources were converted into text. In addition, few-shot learning did not seem to improve overall accuracy. The study highlights that structured guideline reformatting and advanced prompt engineering (data quality vs. data quantity) can enhance the efficacy of LLM integrations to CDSSs for guideline delivery.","<method>Large language models (LLMs)</method>, <method>OpenAI’s GPT-4 Turbo model</method>, <method>retrieval augmented generation (RAG)</method>, <method>prompt engineering</method>, <method>few-shot learning</method>"
2024,https://openalex.org/W4399885374,Biology,Survival Prediction Across Diverse Cancer Types Using Neural Networks,"Gastric cancer and Colon adenocarcinoma represent widespread and challenging malignancies with high mortality rates and complex treatment landscapes. In response to the critical need for accurate prognosis in cancer patients, the medical community has embraced the 5-year survival rate as a vital metric for estimating patient outcomes. This study introduces a pioneering approach to enhance survival prediction models for gastric and Colon adenocarcinoma patients. Leveraging advanced image analysis techniques, we sliced whole slide images (WSI) of these cancers, extracting comprehensive features to capture nuanced tumor characteristics. Subsequently, we constructed patient-level graphs, encapsulating intricate spatial relationships within tumor tissues. These graphs served as inputs for a sophisticated 4-layer graph convolutional neural network (GCN), designed to exploit the inherent connectivity of the data for comprehensive analysis and prediction. By integrating patients' total survival time and survival status, we computed C-index values for gastric cancer and Colon adenocarcinoma, yielding 0.57 and 0.64, respectively. Significantly surpassing previous convolutional neural network models, these results underscore the efficacy of our approach in accurately predicting patient survival outcomes. This research holds profound implications for both the medical and AI communities, offering insights into cancer biology and progression while advancing personalized treatment strategies. Ultimately, our study represents a significant stride in leveraging AI-driven methodologies to revolutionize cancer prognosis and improve patient outcomes on a global scale.","<method>graph convolutional neural network (GCN)</method>, <method>convolutional neural network</method>"
2024,https://openalex.org/W4390652362,Biology,A comprehensive review of the development of land use regression approaches for modeling spatiotemporal variations of ambient air pollution: A perspective from 2011 to 2023,"Land use regression (LUR) models are widely used in epidemiological and environmental studies to estimate humans' exposure to air pollution within urban areas. However, the early models, developed using linear regressions and data from fixed monitoring stations and passive sampling, were primarily designed to model traditional and criteria air pollutants and had limitations in capturing high-resolution spatiotemporal variations of air pollution. Over the past decade, there has been a notable development of multi-source observations from low-cost monitors, mobile monitoring, and satellites, in conjunction with the integration of advanced statistical methods and spatially and temporally dynamic predictors, which have facilitated significant expansion and advancement of LUR approaches. This paper reviews and synthesizes the recent advances in LUR approaches from the perspectives of the changes in air quality data acquisition, novel predictor variables, advances in model-developing approaches, improvements in validation methods, model transferability, and modeling software as reported in 155 LUR studies published between 2011 and 2023. We demonstrate that these developments have enabled LUR models to be developed for larger study areas and encompass a wider range of criteria and unregulated air pollutants. LUR models in the conventional spatial structure have been complemented by more complex spatiotemporal structures. Compared with linear models, advanced statistical methods yield better predictions when handling data with complex relationships and interactions. Finally, this study explores new developments, identifies potential pathways for further breakthroughs in LUR methodologies, and proposes future research directions. In this context, LUR approaches have the potential to make a significant contribution to future efforts to model the patterns of long- and short-term exposure of urban populations to air pollution.","<method>linear regressions</method>, <method>advanced statistical methods</method>"
2024,https://openalex.org/W4394953389,Biology,Boosting Hydrogen Adsorption via Manipulating the d-Band Center of Ferroferric Oxide for Anion Exchange Membrane-Based Seawater Electrolysis,"Ferroferric oxide-based electrocatalysts are widely applied as hydrogen evolution reaction (HER) catalysts due to their low cost and good electrical conductivity, but they tend to exhibit slow hydrogen adsorption kinetics for HER and poison by corrosive Cl– for alkaline seawater splitting. In this regard, we report a nanosheet-like HER catalyst constructed by decorating Fe3O4 with Ru and P dual doping (Ru/P–Fe3O4@IF). In situ characterization and density functional theory (DFT) calculations demonstrate that the resulting Ru/P–Fe3O4@IF catalyst shows enhanced hydrogen adsorption strength and hydrogen coverage with a thermal neutral free energy of adsorbed H (ΔGH*) due to Ru and P dual doping modulating the d-band center of Fe3O4. Moreover, due to Ru/P doping moving up the d-band center, the weak Cl– adsorption energy makes the poison of Cl– on active sites be avoided in alkaline seawater splitting. Benefiting from the above, the Ru/P–Fe3O4@IF exhibits superior HER performance to commercial Pt/C in alkaline seawater with overpotentials of only −46 and −144 mV to reach 100 and 1000 mA cm–2, respectively. In addition, the AEM electrolyzer assembled from Ru/P–Fe3O4 requires only 1.93 V (cell voltage) to drive a current density of 2 A cm–2 and can maintain stable operation for more than 100 h at 500 mA cm–2 for alkaline seawater splitting.",No methods found.
2024,https://openalex.org/W4400659510,Biology,Aspect-based drug review classification through a hybrid model with ant colony optimization using deep learning,"Abstract The task of aspect-level sentiment analysis is intricately designed to determine the sentiment polarity directed towards a specific target within a sentence. With the increasing availability of online reviews and the growing importance of healthcare decisions, analyzing drug reviews has become a critical task. Traditional sentiment analysis, which categorizes a whole review as positive, negative, or neutral, provides limited insights for consumers and healthcare professionals. Aspect-based sentiment analysis (ABSA) aims to overcome these limitations by identifying and evaluating the sentiment associated with specific aspects or attributes of drugs mentioned in the reviews. Various fields, including business, politics, and medicine, have been explored in the context of sentiment analysis. Automation of online user reviews allows pharmaceutical companies to assess large amounts of user feedback. This helps extract pharmacological efficacy and side effect insights. The data collected could improve pharmacovigilance. Reviewing user comments can provide valuable data that can be used to improve drug safety and efficacy monitoring procedures. This improves pharmacovigilance processes, improving pharmaceutical outcomes understanding and corporate decision-making. Therefore, we propose a pre-trained RoBERTa with a Bi-LSTM model to categorise drug reviews from online sources and pre-process the text data. Ant Colony Optimization can be used in feature selection for ABSA, helping to identify the most relevant aspects and sentiments. Further, RoBERTa is fine-tuned to perform ABSA on the dataset, enabling the system to categorize aspects and determine the associated sentiment. The outcomes reveal that the suggested framework has achieved higher accuracy (96.78%) and F1 score (98.29%) on druglib.com, and 95.02% on the drugs.com dataset, than several prior state-of-the-art methods.","<method>pre-trained RoBERTa</method>, <method>Bi-LSTM model</method>, <method>Ant Colony Optimization</method>, <method>fine-tuned RoBERTa</method>"
2024,https://openalex.org/W4390494339,Biology,"A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions","Automating the monitoring of industrial processes has the potential to enhance efficiency and optimize quality by promptly detecting abnormal events and thus facilitating timely interventions. Deep learning, with its capacity to discern non-trivial patterns within large datasets, plays a pivotal role in this process. Standard deep learning methods are suitable to solve a specific task given a specific type of data. During training, deep learning demands large volumes of labeled data. However, due to the dynamic nature of the industrial processes and environment, it is impractical to acquire large-scale labeled data for standard deep learning training for every slightly different case anew. Deep transfer learning offers a solution to this problem. By leveraging knowledge from related tasks and accounting for variations in data distributions, the transfer learning framework solves new tasks with little or even no additional labeled data. The approach bypasses the need to retrain a model from scratch for every new setup and dramatically reduces the labeled data requirement. This survey first provides an in-depth review of deep transfer learning, examining the problem settings of transfer learning and classifying the prevailing deep transfer learning methods. Moreover, we delve into applications of deep transfer learning in the context of a broad spectrum of time series anomaly detection tasks prevalent in primary industrial domains, e.g., manufacturing process monitoring, predictive maintenance, energy management, and infrastructure facility monitoring. We discuss the challenges and limitations of deep transfer learning in industrial contexts and conclude the survey with practical directions and actionable suggestions to address the need to leverage diverse time series data for anomaly detection in an increasingly dynamic production environment.","<method>deep learning</method>, <method>standard deep learning methods</method>, <method>deep transfer learning</method>, <method>transfer learning framework</method>"
2024,https://openalex.org/W4391490182,Biology,Digital capabilities to manage agri-food supply chain uncertainties and build supply chain resilience during compounding geopolitical disruptions,"Purpose The agricultural supply chain is susceptible to disruptive geopolitical events. Therefore, agri-food firms must devise robust resilience strategies to hasten recovery and mitigate global food security effects. Hence, the central aim of this paper is to investigate how supply chains could leverage digital technologies to design resilience strategies to manage uncertainty stemming from the external environment disrupted by a geopolitical event. The context of the study is the African agri-food supply chain during the Russian invasion of Ukraine. Design/methodology/approach The authors employ strategic contingency and dynamic capabilities theory arguments to explore the scenario and conditions under which African agri-food firms could leverage digital technologies to formulate contingency strategies and devise mitigation countermeasures. Then, the authors used a multi-case-study analysis of 14 African firms of different sizes and tiers within three main agri-food sectors (i.e. livestock farming, food-crop and fisheries-aquaculture) to explore, interpret and present data and their findings. Findings Downstream firms (wholesalers and retailers) of the African agri-food supply chain are found to extensively use digital seizing and transforming capabilities to formulate worst-case assumptions amid geopolitical disruption, followed by proactive mitigation actions. These capabilities are mainly supported by advanced technologies such as blockchain and additive manufacturing. On the other hand, smaller upstream partners (SMEs, cooperatives and smallholders) are found to leverage less advanced technologies, such as mobile apps and cloud-based data analytics, to develop sensing capabilities necessary to formulate a “wait-and-see” strategy, allowing them to reduce perceptions of heightened supply chain uncertainty and take mainly reactive mitigation strategies. Finally, the authors integrate their findings into a conceptual framework that advances the research agenda on managing supply chain uncertainty in vulnerable areas. Originality/value This study is the first that sought to understand the contextual conditions (supply chain characteristics and firm characteristics) under which companies in the African agri-food supply chain could leverage digital technologies to manage uncertainty. The study advances contingency and dynamic capability theories by providing a new way of interacting in one specific context. In practice, this study assists managers in developing suitable strategies to manage uncertainty during geopolitical disruptions.",No methods found.
2024,https://openalex.org/W4399326707,Biology,Enhancing precision agriculture: A comprehensive review of machine learning and AI vision applications in all-terrain vehicle for farm automation,"The automation of all-terrain vehicles (ATVs) through the integration of advanced technologies such as machine learning (ML) and artificial intelligence (AI) vision has significantly changed precision agriculture. This paper aims to analyse and develop trends to provide comprehensive knowledge of the current state of ATV-based precision agriculture and the future possibilities of ML and AI. A bibliometric analysis was conducted through network diagram with keywords taken from previous publications in the domain. This review comprehensively analyses the potential of machine learning and artificial intelligence in transforming farming operations through the automation of tasks and the deployment of all-terrain vehicles. The research extensively analyses how machine learning methods have influenced several aspects of agricultural activities, such as planting, harvesting, spraying, weeding, crop monitoring, and others. AI vision systems are being researched for their ability to enhance precise and prompt decision-making in ATV-driven agricultural automation. These technologies have been thoroughly tested to show how they can improve crop yield, reducing overall investment, and make farming more efficient. Examples include machine learning-based seeding accuracy, AI-enabled crop health monitoring, and the use of AI vision for accurate pesticide application. The assessment examines challenges such as data privacy problems and scalability constraints, along with potential advancements and future prospects in the field. This will assist researchers and practitioners in making well-informed judgments regarding farming practices that are efficient, sustainable, and technologically robust.","<method>machine learning</method>, <method>artificial intelligence vision</method>, <method>machine learning methods</method>, <method>AI vision systems</method>, <method>machine learning-based seeding accuracy</method>, <method>AI-enabled crop health monitoring</method>, <method>AI vision for accurate pesticide application</method>"
2024,https://openalex.org/W4394579747,Biology,An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study,"Background Large language models (LLMs) have shown remarkable capabilities in natural language processing (NLP), especially in domains where labeled data are scarce or expensive, such as the clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. Objective The objective of this study is to assess the effectiveness of various prompt engineering techniques, including 2 newly introduced types—heuristic and ensemble prompts, for zero-shot and few-shot clinical information extraction using pretrained language models. Methods This comprehensive experimental study evaluated different prompt types (simple prefix, simple cloze, chain of thought, anticipatory, heuristic, and ensemble) across 5 clinical NLP tasks: clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, and medication attribute extraction. The performance of these prompts was assessed using 3 state-of-the-art language models: GPT-3.5 (OpenAI), Gemini (Google), and LLaMA-2 (Meta). The study contrasted zero-shot with few-shot prompting and explored the effectiveness of ensemble approaches. Results The study revealed that task-specific prompt tailoring is vital for the high performance of LLMs for zero-shot clinical NLP. In clinical sense disambiguation, GPT-3.5 achieved an accuracy of 0.96 with heuristic prompts and 0.94 in biomedical evidence extraction. Heuristic prompts, alongside chain of thought prompts, were highly effective across tasks. Few-shot prompting improved performance in complex scenarios, and ensemble approaches capitalized on multiple prompt strengths. GPT-3.5 consistently outperformed Gemini and LLaMA-2 across tasks and prompt types. Conclusions This study provides a rigorous evaluation of prompt engineering methodologies and introduces innovative techniques for clinical information extraction, demonstrating the potential of in-context learning in the clinical domain. These findings offer clear guidelines for future prompt-based clinical NLP research, facilitating engagement by non-NLP experts in clinical NLP advancements. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative artificial intelligence, and we hope that it will inspire and inform future research in this area.","<method>in-context learning</method>, <method>prompt engineering</method>, <method>heuristic prompts</method>, <method>ensemble prompts</method>, <method>simple prefix prompts</method>, <method>simple cloze prompts</method>, <method>chain of thought prompts</method>, <method>anticipatory prompts</method>, <method>few-shot prompting</method>, <method>zero-shot prompting</method>"
2024,https://openalex.org/W4397001018,Biology,A systematic review of hyperspectral imaging in precision agriculture: Analysis of its current state and future prospects,"Hyperspectral sensor adaptability in precision agriculture to digital images is still at its nascent stage. Hyperspectral imaging (HSI) is data rich in solving agricultural problems like disease detection, weed detection, stress detection, crop monitoring, nutrient application, soil mineralogy, yield estimation, and sorting applications. With modern precision agriculture, the challenge now is to bring these applications to the field for real-time solutions, where machines are enabled to conduct analyses without expert supervision and communicate the results to users for better management of farmlands; a necessary step to gain complete autonomy in agricultural farmlands. Significant advancements in HSI technology for precision agriculture are required to fully realize its potential. As a wide-ranging collection of the status of HSI and analysis in precision agriculture is lacking, this review endeavors to provide a comprehensive overview of the recent advancements and trends of HSI in precision agriculture for real-time applications. In this study, a systematic review of 163 scientific articles published over the past twenty years (2003–2023) was conducted. Of these, 97 were selected for further analysis based on their relevance to the topic at hand. Topics include conventional data preprocessing techniques, hyperspectral data acquisition, data compression methods, and segmentation methods. The hardware implementation of field-programmable gate arrays (FPGAs) and graphics processing units (GPUs) for high-speed data processing and application of machine learning and deep learning technologies were explored. This review highlights the potential of HSI as a powerful tool for precision agriculture, particularly in real-time applications, discusses limitations, and provides insights into future research directions.","<method>machine learning</method>, <method>deep learning</method>"
2024,https://openalex.org/W4398782611,Biology,Transition versus physical climate risk pricing in European financial markets: a text-based approach,"Under its climate regulation, the EU is expected to become the first continent with a net-zero emissions balance. We study the pricing of climate risks, physical and transition, within European markets. Using text-analysis, we construct two novel (daily) physical and transition risk indicators for the period 2005–2021 and two global climate risk vocabularies. Applying our climate risk indices to an asset pricing test framework, we document the emergence of economically significant transition and physical risk premia post-2015. From a firm-level analysis, using firms' GHG emissions, GHG emissions intensity, environmental, and ESG scores, we find that rises in transition (physical) risk are typically associated with an increase (decrease) in the return of green (brown) stocks. Firm-level information is used by investors to proxy firms' climate-risks exposure, especially for transition risk since 2015, whereas the sectoral classification appears to proxy firms' exposures to physical risk. From a country-level analysis emerges an intensified connection between European stock markets and climate risks post-2015, yet with some heterogeneity. Our results have important economic implications and show that investors demand compensation for their exposure to both climate risk types. Our novel climate risk vocabularies and indicators find several applications in identifying, measuring, and studying climate risks.",<method>text-analysis</method>
2024,https://openalex.org/W4390597725,Biology,Critical review on water quality analysis using IoT and machine learning models,"Water quality and its management are the most precise concerns confronting humanity globally. This article evaluates the various sensors used for water quality monitoring and focuses on the water quality index considering the multiple physical, chemical, and biological parameters. A Review of Internet of Things (IoT) research for water quality monitoring and analysis, sensors used for water quality can help remote monitoring of the water quality parameters using various IoT-based sensors that convey the assembled estimations utilizing Low-Power Wide Area Network innovations. Overall, the IoT system was 95 % accurate in measuring pH, Turbidity, TDS, and Temperature, while the traditional method was only 85 % accurate. Also, this study reviewed the different A.I. techniques used to assess water quality, including conventional machine learning techniques, Support Vector Machines, Deep Neural Networks, and K-nearest neighbors. Compared to traditional methods, machine learning and deep learning can significantly increase the accuracy of measurements of groundwater quality. However, various variables, such as the caliber of the training data, the water quality metrics' complexity, and the monitoring frequency, will affect the accuracy. The geographical information system (GIS) is used for spatial data analysis and managing water resources. The quality of its data is also reviewed in the paper. Based on these analyses, the study has forecasted the future sensors, Geospatial Technology, and machine learning techniques for water quality monitoring and analysis.","<method>conventional machine learning techniques</method>, <method>Support Vector Machines</method>, <method>Deep Neural Networks</method>, <method>K-nearest neighbors</method>"
2024,https://openalex.org/W4391332961,Biology,A novel framework for developing environmentally sustainable and cost-effective ultra-high-performance concrete (UHPC) using advanced machine learning and multi-objective optimization techniques,"This study aims to propose a novel framework for strength prediction and multi-objective optimization (MOO) of economical and environmentally sustainable ultra-high-performance concrete (UHPC) which aids in intelligent, sustainable, and resilient construction. Different tree- and boosting ensemble-based machine learning (ML) models are integrated to form an accurate and reliable prediction model for the uniaxial compressive strength of UHPC. The optimized models are integrated into a super learner model, resulting in a robust predictive model that is used as one of the objective functions in the MOO problem. A total of 19 objective functions are considered, including cost, uniaxial compressive strength, and 17 environmental impact categories that comprehensively evaluate the environmental sustainability of the UHPC mix. The resulting impacts from the mid-point indicators were calculated using the Eco-invent v3.7 Life Cycle Inventory database. The results showed that the super learner model accurately predicted the uniaxial compressive strength of UHPC. The MOO resulted in Pareto fronts, demonstrating the trade-off among the uniaxial compressive strength, cost, and environmental sustainability of the mix and a broad range of solutions that can be obtained for the 19 objectives. The study provides a useful tool for designers and decision-makers to select the optimal UHPC mixture that meets specific project requirements. Finally, for the practical application of the ML predictive model and MOO algorithm for UHPC, a graphical user interface-based software tool, FAI-OSUSCONCRET, was developed. This software tool offers fast, accurate, and intelligent predictions and multi-objective optimizations tailored to specific project requirements, thus resulting in a UHPC mixture that perfectly meets project needs.","<method>tree-based ensemble machine learning models</method>, <method>boosting ensemble-based machine learning models</method>, <method>super learner model</method>, <method>multi-objective optimization (MOO)</method>"
2024,https://openalex.org/W4391168980,Biology,Utilizing Hybrid Machine Learning and Soft Computing Techniques for Landslide Susceptibility Mapping in a Drainage Basin,"The hydrological system of thebasin of Lake Urmia is complex, deriving its supply from a network comprising 13 perennial rivers, along withnumerous small springs and direct precipitation onto the lake’s surface. Among these contributors, approximately half of the inflow is attributed to the Zarrineh River and the Simineh River. Remarkably, Lake Urmia lacks a natural outlet, with its water loss occurring solely through evaporation processes. This study employed a comprehensive methodology integrating ground surveys, remote sensing analyses, and meticulous documentation of historical landslides within the basin as primary information sources. Through this investigative approach, we preciselyidentified and geolocated a total of 512 historical landslide occurrences across the Urmia Lake drainage basin, leveraging GPS technology for precision. Thisarticle introduces a suite of hybrid machine learning predictive models, such as support-vector machine (SVM), random forest (RF), decision trees (DT), logistic regression (LR), fuzzy logic (FL), and the technique for order of preference by similarity to the ideal solution (TOPSIS). These models were strategically deployed to assess landslide susceptibility within the region. The outcomes of the landslide susceptibility assessment reveal that the main high susceptible zones for landslide occurrence are concentrated in the northwestern, northern, northeastern, and some southern and southeastern areas of the region. Moreover, when considering the implementation of predictions using different algorithms, it became evident that SVM exhibited superior performance regardingboth accuracy (0.89) and precision (0.89), followed by RF, with and accuracy of 0.83 and a precision of 0.83. However, it is noteworthy that TOPSIS yielded the lowest accuracy value among the algorithms assessed.","<method>support-vector machine (SVM)</method>, <method>random forest (RF)</method>, <method>decision trees (DT)</method>, <method>logistic regression (LR)</method>, <method>fuzzy logic (FL)</method>, <method>technique for order of preference by similarity to the ideal solution (TOPSIS)</method>"
2024,https://openalex.org/W4391831565,Biology,Comparison of Random Forest and XGBoost Classifiers Using Integrated Optical and SAR Features for Mapping Urban Impervious Surface,"The integration of optical and SAR datasets through ensemble machine learning models shows promising results in urban remote sensing applications. The integration of multi-sensor datasets enhances the accuracy of information extraction. This research presents a comparison of two ensemble machine learning classifiers (random forest and extreme gradient boost (XGBoost)) classifiers using an integration of optical and SAR features and simple layer stacking (SLS) techniques. Therefore, Sentinel-1 (SAR) and Landsat 8 (optical) datasets were used with SAR textures and enhanced modified indices to extract features for the year 2023. The classification process utilized two machine learning algorithms, random forest and XGBoost, for urban impervious surface extraction. The study focused on three significant East Asian cities with diverse urban dynamics: Jakarta, Manila, and Seoul. This research proposed a novel index called the Normalized Blue Water Index (NBWI), which distinguishes water from other features and was utilized as an optical feature. Results showed an overall accuracy of 81% for UIS classification using XGBoost and 77% with RF while classifying land use land cover into four major classes (water, vegetation, bare soil, and urban impervious). However, the proposed framework with the XGBoost classifier outperformed the RF algorithm and Dynamic World (DW) data product and comparatively showed higher classification accuracy. Still, all three results show poor separability with bare soil class compared to ground truth data. XGBoost outperformed random forest and Dynamic World in classification accuracy, highlighting its potential use in urban remote sensing applications.","<method>ensemble machine learning models</method>, <method>random forest</method>, <method>extreme gradient boost (XGBoost)</method>, <method>simple layer stacking (SLS)</method>"
2024,https://openalex.org/W4392034144,Biology,"From Industry 4.0 Digital Manufacturing to Industry 5.0 Digital Society: a Roadmap Toward Human-Centric, Sustainable, and Resilient Production","Abstract The present study addresses two critical controversies surrounding the emerging Industry 5.0 agenda. Firstly, it seeks to elucidate the driving forces behind the accelerated momentum of the Industry 5.0 agenda amidst the ongoing digital industrial transformation. Secondly, it explores how the agenda’s sustainability values can be effectively realised. The study conducted a comprehensive content-centric literature synthesis and identified how Industry 4.0 shortcomings adversely impacted sustainability values. Furthermore, the study implements a novel approach that determines how and in what order the sustainability functions of Industry 4.0 should be leveraged to promote the sustainability objectives of Industry 5.0. Results reveal that Industry 4.0 has benefited economic and environmental sustainability values most at the organisational and supply chain levels. Nonetheless, most micro and meso-social sustainability values have been adversely impacted by Industry 4.0. Similarly, Industry 4.0 has been worryingly detrimental to macro sustainability values like social or economic growth equality. These contradictory implications of Industry 4.0 have pulled the Industry 5.0 agenda. However, the results identified nine sustainability functions of Industry 4.0 that, when leveraged appropriately and in the correct order, can offer important implications for realising the economic and socio-environmental goals of Industry 5.0. For example, under extreme unpredictability of business world uncertainties, the business should first leverage the automation and integration capabilities of Industry 4.0 to gain the necessary cost-saving, resource efficiency, risk management capability, and business antifragility that allow them to introduce sustainable innovation into their business model without jeopardising their survival. Various scenarios for empowering Industry 5.0 sustainability values identified in the present study offer important implications for knowledge and practice.",No methods found.
2024,https://openalex.org/W4391037822,Biology,Estimating compressive strength of concrete containing rice husk ash using interpretable machine learning-based models,"The construction sector is a major contributor to global greenhouse gas emissions. Using recycled and waste materials in concrete is a practical solution to address environmental challenges. Currently, agricultural waste is widely used as a substitute for cement in the production of eco-friendly concrete. However, traditional methods for assessing the strength of such materials are both expensive and time-consuming. Therefore, this study uses machine learning techniques to develop prediction models for the compressive strength (CS) of rice husk ash (RHA) concrete. The ML techniques used in the present study include random forest (RF), light gradient boosting machine (LightGBM), ridge regression, and extreme gradient boosting (XGBoost). A total of 348 values of CS were collected from the experimental studies, and five characteristics of RHA concrete were taken as input variables. For the performance assessment of the models, multiple statistical metrics were used. During the training phase, the correlation coefficients (R) obtained for ridge regression, RF, XGBoost, and LightGBM were 0.943, 0.981, 0.985, and 0.996, respectively. In the testing set, these values demonstrated even higher performance, with correlation coefficients of 0.971, 0.993, 0.992, and 0.998 for ridge regression, RF, XGBoost, and LightGBM, respectively. The statistical analysis revealed that the LightGBM model outperformed other models, whereas the ridge regression model exhibited comparatively lower accuracy. SHapley Additive exPlanation (SHAP) method was employed for the interpretability of the developed model. The SHAP analysis revealed that water-to-cement is a controlling parameter in estimating the CS of RHA concrete. In conclusion, this study provides valuable guidance for builders and researchers to estimate the CS of RHA concrete. However, it is suggested that more input variables be incorporated and hybrid models utilized to further enhance the reliability and precision of the models.","<method>random forest (RF)</method>, <method>light gradient boosting machine (LightGBM)</method>, <method>ridge regression</method>, <method>extreme gradient boosting (XGBoost)</method>, <method>SHapley Additive exPlanation (SHAP)</method>"
2024,https://openalex.org/W4393012885,Biology,Improving Thyroid Disorder Diagnosis via Ensemble Stacking and Bidirectional Feature Selection,"Thyroid disorders represent a significant global health challenge with hypothyroidism and hyperthyroidism as two common conditions arising from dysfunction in the thyroid gland.Accurate and timely diagnosis of these disorders is crucial for effective treatment and patient care.This research introduces a comprehensive approach to improve the accuracy of thyroid disorder diagnosis through the integration of ensemble stacking and advanced feature selection techniques.Sequential forward feature selection, sequential backward feature elimination, and bidirectional feature elimination are investigated in this study.In ensemble learning, random forest, adaptive boosting, and bagging classifiers are employed.The effectiveness of these techniques is evaluated using two different datasets obtained from the University of California Irvine-Machine Learning Repository, both of which undergo preprocessing steps, including outlier removal, addressing missing data, data cleansing, and feature reduction.Extensive experimentation demonstrates the remarkable success of proposed ensemble stacking and bidirectional feature elimination achieving 100% and 99.86% accuracy in identifying hyperthyroidism and hypothyroidism, respectively.Beyond enhancing detection accuracy, the ensemble stacking model also demonstrated a streamlined computational complexity which is pivotal for practical medical applications.It significantly outperformed existing studies with similar objectives underscoring the viability and effectiveness of the proposed scheme.This research offers an innovative perspective and sets the platform for improved thyroid disorder diagnosis with broader implications for healthcare and patient well-being.","<method>ensemble stacking</method>, <method>sequential forward feature selection</method>, <method>sequential backward feature elimination</method>, <method>bidirectional feature elimination</method>, <method>random forest</method>, <method>adaptive boosting</method>, <method>bagging classifiers</method>"
2024,https://openalex.org/W4396877909,Biology,The Capacity and Robustness Trade-off: Revisiting the Channel Independent Strategy for Multivariate Time Series Forecasting,"Multivariate time series data comprises various channels of variables. The multivariate forecasting models need to capture the relationship between the channels to accurately predict future values. However, recently, there has been an emergence of methods that employ the Channel Independent (CI) strategy. These methods view multivariate time series data as separate univariate time series and disregard the correlation between channels. Surprisingly, our empirical results have shown that models trained with the CI strategy outperform those trained with the Channel Dependent (CD) strategy, usually by a significant margin. Nevertheless, the reasons behind this phenomenon have not yet been thoroughly explored in the literature. This paper provides comprehensive empirical and theoretical analyses of the characteristics of multivariate time series datasets and the CI/CD strategy. Our results conclude that the CD approach has higher capacity but often lacks robustness to accurately predict distributionally drifted time series. In contrast, the CI approach trades capacity for robust prediction. Practical measures inspired by these analyses are proposed to address the capacity and robustness dilemma, including a modified CD method called Predict Residuals with Regularization (PRReg) that can surpass the CI strategy. We hope our findings can raise awareness among researchers about the characteristics of multivariate time series and inspire the construction of better forecasting models.","<method>Channel Independent (CI) strategy</method>, <method>Channel Dependent (CD) strategy</method>, <method>Predict Residuals with Regularization (PRReg)</method>"
2024,https://openalex.org/W4396919027,Biology,A harmonized public resource of deeply sequenced diverse human genomes,"Underrepresented populations are often excluded from genomic studies owing in part to a lack of resources supporting their analyses. The 1000 Genomes Project (1kGP) and Human Genome Diversity Project (HGDP), which have recently been sequenced to high coverage, are valuable genomic resources because of the global diversity they capture and their open data sharing policies. Here, we harmonized a high-quality set of 4094 whole genomes from 80 populations in the HGDP and 1kGP with data from the Genome Aggregation Database (gnomAD) and identified over 153 million high-quality SNVs, indels, and SVs. We performed a detailed ancestry analysis of this cohort, characterizing population structure and patterns of admixture across populations, analyzing site frequency spectra, and measuring variant counts at global and subcontinental levels. We also show substantial added value from this data set compared with the prior versions of the component resources, typically combined via liftOver and variant intersection; for example, we catalog millions of new genetic variants, mostly rare, compared with previous releases. In addition to unrestricted individual-level public release, we provide detailed tutorials for conducting many of the most common quality-control steps and analyses with these data in a scalable cloud-computing environment and publicly release this new phased joint callset for use as a haplotype resource in phasing and imputation pipelines. This jointly called reference panel will serve as a key resource to support research of diverse ancestry populations.",No methods found.
2024,https://openalex.org/W4400873377,Biology,"Multi-Scenario Simulation of Land Use Change and Ecosystem Service Value Based on the Markov–FLUS Model in Ezhou City, China","Changes in land use patterns, types, and intensities significantly impact ecosystem services. This study follows the time series logic from history to the expected future to investigate the spatial and temporal characteristics of land use changes in Ezhou and their potential impacts on the ecosystem services value (ESV). The results show that the Markov–FLUS model has strong applicability in predicting the spatial pattern of land use, with a Kappa coefficient of 0.9433 and a FoM value of 0.1080. Between 2000 and 2020, construction land expanded continuously, while water area remained relatively stable, and other land types experienced varying degrees of contraction. Notably, the area of construction land expanded significantly compared to 2000, and it expanded by 70.99% in 2020. Moreover, the watershed area expanded by 9.30% from 2000 to 2010, but there was very little change in the following 10 years. Under the three scenarios, significant differences in land use changes were observed in Ezhou City, driven by human activities, particularly the strong expansion of construction land. In the inertial development scenario, construction land expanded to 313.39 km2 by 2030, representing a 38.30% increase from 2020. Conversely, under the farmland protection scenario, construction land increased to 237.66 km2, a 4.89% rise from 2020. However, in the ecological priority development scenario, the construction land area expanded to 253.59 km2, a 10.13% increase from 2020. Compared to 2020, the ESV losses in the inertia development and farmland protection scenarios were USD 4497.71 and USD 1072.23, respectively, by 2030. Conversely, the ESV under the ecological protection scenario increased by USD 2749.09, emphasizing the importance of prioritizing ecological protection in Ezhou City’s development. This study may provide new clues for the formulation of regional strategies for sustainable land use and ecosystem restoration.",<method>Markov–FLUS model</method>
2024,https://openalex.org/W4390738897,Biology,Enhancing crop recommendation systems with explainable artificial intelligence: a study on agricultural decision-making,"Abstract Crop Recommendation Systems are invaluable tools for farmers, assisting them in making informed decisions about crop selection to optimize yields. These systems leverage a wealth of data, including soil characteristics, historical crop performance, and prevailing weather patterns, to provide personalized recommendations. In response to the growing demand for transparency and interpretability in agricultural decision-making, this study introduces XAI-CROP an innovative algorithm that harnesses eXplainable artificial intelligence (XAI) principles. The fundamental objective of XAI-CROP is to empower farmers with comprehensible insights into the recommendation process, surpassing the opaque nature of conventional machine learning models. The study rigorously compares XAI-CROP with prominent machine learning models, including Gradient Boosting (GB), Decision Tree (DT), Random Forest (RF), Gaussian Naïve Bayes (GNB), and Multimodal Naïve Bayes (MNB). Performance evaluation employs three essential metrics: Mean Squared Error (MSE), Mean Absolute Error (MAE), and R-squared (R2). The empirical results unequivocally establish the superior performance of XAI-CROP. It achieves an impressively low MSE of 0.9412, indicating highly accurate crop yield predictions. Moreover, with an MAE of 0.9874, XAI-CROP consistently maintains errors below the critical threshold of 1, reinforcing its reliability. The robust R 2 value of 0.94152 underscores XAI-CROP's ability to explain 94.15% of the data's variability, highlighting its interpretability and explanatory power.","<method>XAI-CROP</method>, <method>Gradient Boosting (GB)</method>, <method>Decision Tree (DT)</method>, <method>Random Forest (RF)</method>, <method>Gaussian Naïve Bayes (GNB)</method>, <method>Multimodal Naïve Bayes (MNB)</method>"
2024,https://openalex.org/W4391783116,Biology,Assessing water quality of an ecologically critical urban canal incorporating machine learning approaches,"This study assessed water quality (WQ) in Tongi Canal, an ecologically critical and economically important urban canal in Bangladesh. The researchers employed the Root Mean Square Water Quality Index (RMS-WQI) model, utilizing seven WQ indicators, including temperature, dissolve oxygen, electrical conductivity, lead, cadmium, and iron to calculate the water quality index (WQI) score. The results showed that most of the water sampling locations showed poor WQ, with many indicators violating Bangladesh's environmental conservation regulations. This study employed eight machine learning algorithms, where the Gaussian process regression (GPR) model demonstrated superior performance (training RMSE = 1.77, testing RMSE = 0.0006) in predicting WQI scores. To validate the GPR model's performance, several performance measures, including the coefficient of determination (R2), the Nash-Sutcliffe efficiency (NSE), the model efficiency factor (MEF), Z statistics, and Taylor diagram analysis, were employed. The GPR model exhibited higher sensitivity (R2 = 1.0) and efficiency (NSE = 1.0, MEF = 0.0) in predicting WQ. The analysis of model uncertainty (standard uncertainty = 7.08 ± 0.9025; expanded uncertainty = 7.08 ± 1.846) indicates that the RMS-WQI model holds potential for assessing the WQ of inland waterbodies. These findings indicate that the RMS-WQI model could be an effective approach for assessing inland waters across Bangladesh. The study's results showed that most of the WQ indicators did not meet the recommended guidelines, indicating that the water in the Tongi Canal is unsafe and unsuitable for various purposes. The study's implications extend beyond the Tongi Canal and could contribute to WQ management initiatives across Bangladesh.",<method>Gaussian process regression (GPR)</method>
2024,https://openalex.org/W4392454900,Biology,Compressive strength prediction of sustainable concrete incorporating rice husk ash (RHA) using hybrid machine learning algorithms and parametric analyses,"The construction industry is making efforts to reduce the environmental impact of cement production in concrete by incorporating alternative and supplementary cementitious materials, as well as lowering carbon emissions. One such material that has gained popularity in this context is rice husk ash (RHA) due to its pozzolanic reactions. This study aims to forecast the compressive strength (CS) of RHA-based concrete (RBC) by examining the effects of several factors such as cement, RHA content, curing age, water usage, aggregate amount, and superplasticizer content. To accomplish this, the study collected and analyzed data from literature, resulting in a dataset of 1404 observations. Several machine learning (ML) models, such as light gradient boosting (LGB), extreme gradient boosting (XGB), and random forest (RF), as well as hybrid machine learning (HML) approaches like XGB-LGB and XGB-RF were employed to thoroughly analyze these parameters and assess their impact on strength. The dataset was split into training and testing groups, and statistical analyses were performed to determine the relationships between the input parameters and CS. Moreover, the performance of all the models was evaluated using various statistical evaluation criteria, including mean absolute percentage error (MAPE), coefficient of efficiency (CE), root mean square error (RMSE), and coefficient of determination (R2). The hybrid XGB-LGB model was found to have higher precision (R2 = 0.95, and RMSE = 5.255 MPa) as compared to other models. SHAP (SHapley Additive exPlanations) analysis revealed that cement, RHA, and superplasticizer had a positive effect on strength. Overall, the study's findings suggest that the hybrid XGB-LGB model with the identified input parameters can be used to accurately predict the CS of RBC. The application of such technologies in the construction sector can facilitate the rapid and low-cost identification of material qualities and the impact of input parameters.","<method>light gradient boosting (LGB)</method>, <method>extreme gradient boosting (XGB)</method>, <method>random forest (RF)</method>, <method>hybrid machine learning (HML) approaches like XGB-LGB</method>, <method>hybrid machine learning (HML) approaches like XGB-RF</method>"
2024,https://openalex.org/W4392816775,Biology,Virtual reality and augmented reality in medical education: an umbrella review,"Objective This umbrella review aims to ascertain the extent to which immersive Virtual Reality (VR) and Augmented Reality (AR) technologies improve specific competencies in healthcare professionals within medical education and training, in contrast to traditional educational methods or no intervention. Methods Adhering to PRISMA guidelines and the PICOS approach, a systematic literature search was conducted across major databases to identify studies examining the use of VR and AR in medical education. Eligible studies were screened and categorized based on the PICOS criteria. Descriptive statistics and chi-square tests were employed to analyze the data, supplemented by the Fisher test for small sample sizes or specific conditions. Analysis The analysis involved cross-tabulating the stages of work (Development and Testing, Results, Evaluated) and variables of interest (Performance, Engagement, Performance and Engagement, Effectiveness, no evaluated) against the types of technologies used. Chi-square tests assessed the associations between these categorical variables. Results A total of 28 studies were included, with the majority reporting increased or positive effects from the use of immersive technologies. VR was the most frequently studied technology, particularly in the “Performance” and “Results” stages. The chi-square analysis, with a Pearson value close to significance ( p = 0.052), suggested a non-significant trend toward the association of VR with improved outcomes. Conclusions The results indicate that VR is a prevalent tool in the research landscape of medical education technologies, with a positive trend toward enhancing educational outcomes. However, the statistical analysis did not reveal a significant association, suggesting the need for further research with larger sample sizes. This review underscores the potential of immersive technologies to enhance medical training yet calls for more rigorous studies to establish definitive evidence of their efficacy.",No methods found.
2024,https://openalex.org/W4392131696,Biology,"Assessing Chilgoza Pine (Pinus gerardiana) forest fire severity: Remote sensing analysis, correlations, and predictive modeling for enhanced management strategies","Forest fires represent a critical global threat to both humans and ecosystems. This study examines the intensity and impacts of Chilgoza (Pinus gerardiana) Pine Forest fires by using advanced remote sensing techniques comprising Normalized Burn Ratio (NBR) and Difference Normalized Burn Ratio (dNBR) analyses based on Landsat 9 datasets. The study highlights the severe effect of these fires, resulting in noteworthy losses of livestock and private properties and widespread damage to 10,156.53 acres of the Chilgoza Pine Forest. A comprehensive variable correlation analysis is conducted to gain deeper insights into the influencing factors causing forest fires. Spearman's Rank Correlation Coefficient was used to assess the association between burnt and unburnt areas and various independent factors. The analysis reveals compelling evidence of significant correlations with forest fire prevalence. This study found moderate negative (-0.532, p < 0.05) and positive (0.513, p < 0.05) correlations with elevation and Land Surface Temperature (LST), respectively, and a weak positive correlation (0.252, p < 0.05) with a Wind Speed (V). To predict forest fire susceptibility and better understand the contributing factors, three machine learning models, Random Forest (RF), XGBoost, and logistic regression, are applied to assess variable importance scores. Among the considered factors, LST is the most critical variable, with consistently high variable importance scores (100%, 96%, and 59%) across all three models. Wind Speed (V) also proved influential in all models, with variable importance scores of 78%, 83%, and 61% for RF, XGBoost, and logistic regression, respectively. Moreover, elevation significantly influences the frequency of forest fires, as evidenced by variable importance scores ranging from 26% to 100%. Comparatively, the Random Forest model outperforms XGBoost and Logistic Regression in predicting forest fire vulnerability. During the training stage, the Random Forest (RF) model achieves an impressive classification accuracy of 99.1%, followed by XGBoost with 94.5% and Logistic Regression with 85.6%. On evaluation with the validation dataset, the accuracies remain promising, with RF at 96.4%, XGBoost at 91.1%, and Logistic Regression at 84.6%. Based on the Random Forest model, the identified high-risk sites offer valuable insights for proactive fire management and prevention strategies. This study provides a robust predictive model and a comprehensive understanding of forest fire severity and impacts. Future research should consider climate change scenarios and account for human activities to enhance fire behavior predictions and risk assessment models.","<method>Random Forest (RF)</method>, <method>XGBoost</method>, <method>logistic regression</method>"
2024,https://openalex.org/W4390659289,Biology,Cognition-Driven Structural Prior for Instance-Dependent Label Transition Matrix Estimation,"The label transition matrix has emerged as a widely accepted method for mitigating label noise in machine learning. In recent years, numerous studies have centered on leveraging deep neural networks to estimate the label transition matrix for individual instances within the context of instance-dependent noise. However, these methods suffer from low search efficiency due to the large space of feasible solutions. Behind this drawback, we have explored that the real murderer lies in the invalid class transitions, that is, the actual transition probability between certain classes is zero but is estimated to have a certain value. To mask the <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">invalid class transitions</i> , we introduced a human-cognition-assisted method with structural information from human cognition. Specifically, we introduce a structured transition matrix network ( <bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">STMN</b> ) designed with an adversarial learning process to balance instance features and prior information from human cognition. The proposed method offers two advantages: 1) better estimation effectiveness is obtained by sparing the transition matrix and 2) better estimation accuracy is obtained with the assistance of human cognition. By exploiting these two advantages, our method parametrically estimates a sparse label transition matrix, effectively converting noisy labels into true labels. The efficiency and superiority of our proposed method are substantiated through comprehensive comparisons with state-of-the-art methods on three synthetic datasets and a real-world dataset. Our code will be available at https://github.com/WheatCao/STMN-Pytorch.","<method>label transition matrix</method>, <method>deep neural networks</method>, <method>human-cognition-assisted method</method>, <method>structured transition matrix network (STMN)</method>, <method>adversarial learning process</method>"
2024,https://openalex.org/W4391479301,Biology,Comparative Assessment of Two Global Sensitivity Approaches Considering Model and Parameter Uncertainty,"Abstract Global Sensitivity Analysis (GSA) is key to assisting appraisal of the behavior of hydrological systems through model diagnosis considering multiple sources of uncertainty. Uncertainty sources typically comprise incomplete knowledge in (a) conceptual and mathematical formulation of models and (b) parameters embedded in the models. In this context, there is the need for detailed investigations aimed at a robust quantification of the importance of model and parameter uncertainties in a rigorous multi‐model context. This study aims at evaluating and comparing two modern multi‐model GSA methodologies. These are the first GSA approaches embedding both model and parameter uncertainty sources and encompass the variance‐based framework based on Sobol indices (as derived by Dai &amp; Ye, 2015, https://doi.org/10.1016/j.jhydrol.2015.06.034 ) and the moment‐based approach upon which the formulation of the multi‐model AMA indices (as derived by Dell'Oca et al., 2020, https://doi.org/10.1029/2019wr025754 ) is based. We provide an assessment of various aspects of sensitivity upon considering a joint analysis of these two approaches in a multi‐model context. Our work relies on well‐established scenarios that comprise (a) a synthetic setting related to reactive transport across a groundwater system and (b) an experimentally‐based study considering heavy metal sorption onto a soil. Our study documents that the joint use of these GSA approaches can provide different while complementary information to assess mutual consistency of approaches and to enrich the information content provided by GSA under model and parameter uncertainty. While being related to groundwater settings, our results can be considered as reference for future GSA studies coping with model and parameter uncertainty.",No methods found.
2024,https://openalex.org/W4392980686,Biology,Data-driven evolution of water quality models: An in-depth investigation of innovative outlier detection approaches-A case study of Irish Water Quality Index (IEWQI) model,"Recently, there has been a significant advancement in the water quality index (WQI) models utilizing data-driven approaches, especially those integrating machine learning and artificial intelligence (ML/AI) technology. Although, several recent studies have revealed that the data-driven model has produced inconsistent results due to the data outliers, which significantly impact model reliability and accuracy. The present study was carried out to assess the impact of data outliers on a recently developed Irish Water Quality Index (IEWQI) model, which relies on data-driven techniques. To the author's best knowledge, there has been no systematic framework for evaluating the influence of data outliers on such models. For the purposes of assessing the outlier impact of the data outliers on the water quality (WQ) model, this was the first initiative in research to introduce a comprehensive approach that combines machine learning with advanced statistical techniques. The proposed framework was implemented in Cork Harbour, Ireland, to evaluate the IEWQI model's sensitivity to outliers in input indicators to assess the water quality. In order to detect the data outlier, the study utilized two widely used ML techniques, including Isolation Forest (IF) and Kernel Density Estimation (KDE) within the dataset, for predicting WQ with and without these outliers. For validating the ML results, the study used five commonly used statistical measures. The performance metric (R2) indicates that the model performance improved slightly (R2 increased from 0.92 to 0.95) in predicting WQ after removing the data outlier from the input. But the IEWQI scores revealed that there were no statistically significant differences among the actual values, predictions with outliers, and predictions without outliers, with a 95% confidence interval at p < 0.05. The results of model uncertainty also revealed that the model contributed <1% uncertainty to the final assessment results for using both datasets (with and without outliers). In addition, all statistical measures indicated that the ML techniques provided reliable results that can be utilized for detecting outliers and their impacts on the IEWQI model. The findings of the research reveal that although the data outliers had no significant impact on the IEWQI model architecture, they had moderate impacts on the rating schemes' of the model. This finding indicated that detecting the data outliers could improve the accuracy of the IEWQI model in rating WQ as well as be helpful in mitigating the model eclipsing problem. In addition, the results of the research provide evidence of how the data outliers influenced the data-driven model in predicting WQ and reliability, particularly since the study confirmed that the IEWQI model's could be effective for accurately rating WQ despite the presence of the data outliers in the input. It could occur due to the spatio-temporal variability inherent in WQ indicators. However, the research assesses the influence of data input outliers on the IEWQI model and underscores important areas for future investigation. These areas include expanding temporal analysis using multi-year data, examining spatial outlier patterns, and evaluating detection methods. Moreover, it is essential to explore the real-world impacts of revised rating categories, involve stakeholders in outlier management, and fine-tune model parameters. Analysing model performance across varying temporal and spatial resolutions and incorporating additional environmental data can significantly enhance the accuracy of WQ assessment. Consequently, this study offers valuable insights to strengthen the IEWQI model's robustness and provides avenues for enhancing its utility in broader WQ assessment applications. Moreover, the study successfully adopted the framework for evaluating how data input outliers affect the data-driven model, such as the IEWQI model. The current study has been carried out in Cork Harbour for only a single year of WQ data. The framework should be tested across various domains for evaluating the response of the IEWQI model's in terms of the spatio-temporal resolution of the domain. Nevertheless, the study recommended that future research should be conducted to adjust or revise the IEWQI model's rating schemes and investigate the practical effects of data outliers on updated rating categories. However, the study provides potential recommendations for enhancing the IEWQI model's adaptability and reveals its effectiveness in expanding its applicability in more general WQ assessment scenarios.","<method>Isolation Forest (IF)</method>, <method>Kernel Density Estimation (KDE)</method>"
2024,https://openalex.org/W4391235397,Biology,Real-Time Plant Disease Dataset Development and Detection of Plant Disease Using Deep Learning,"Agriculture plays a significant role in meeting food needs and providing food security for the increasingly growing global population, which has increased by 0.88% since 2022. Plant diseases can reduce food production and affect food security. Worldwide crop loss due to plant disease is estimated to be around 14.1%. The lack of proper identification of plant disease at the early stages of infection can result in inappropriate disease control measures. Therefore, the automatic identification and diagnosis of plant diseases are highly recommended. Lack of availability of large amounts of data that are not processed to a large extent is one of the main challenges in plant disease diagnosis. In the current manuscript, we developed datasets for food grains specifically for rice, wheat, and maize to address the identified challenges. The developed datasets consider the common diseases (two bacterial diseases and two fungal diseases of rice, four fungal diseases of maize, and four fungal diseases of wheat) that affect crop yields and cause damage to the whole plant. The datasets developed were applied to eight fine-tuned deep learning models with the same training hyperparameters. The experimental results based on eight fine-tuned deep learning models show that, while recognizing maize leaf diseases, the models Xception and MobileNet performed best with a testing accuracy of 0.9580 and 0.9464 respectively. Similarly, while recognizing the wheat leaf diseases, the models MobileNetV2 and MobileNet performed best with a testing accuracy of 0.9632 and 0.9628 respectively. The Xception and Inception V3 models performed best, with a testing accuracy of 0.9728 and 0.9620, respectively, for recognizing rice leaf diseases. The research also proposes a new convolutional neural network (CNN) model trained from scratch on all three food grain datasets developed. The proposed model performs well and shows a testing accuracy of 0.9704, 0.9706, and 0.9808 respectively on the maize, rice, and wheat datasets.","<method>fine-tuned deep learning models</method>, <method>Xception</method>, <method>MobileNet</method>, <method>MobileNetV2</method>, <method>Inception V3</method>, <method>convolutional neural network (CNN) model trained from scratch</method>"
2024,https://openalex.org/W4396494945,Biology,Vision–language foundation model for echocardiogram interpretation,"Abstract The development of robust artificial intelligence models for echocardiography has been limited by the availability of annotated clinical data. Here, to address this challenge and improve the performance of cardiac imaging models, we developed EchoCLIP, a vision–language foundation model for echocardiography, that learns the relationship between cardiac ultrasound images and the interpretations of expert cardiologists across a wide range of patients and indications for imaging. After training on 1,032,975 cardiac ultrasound videos and corresponding expert text, EchoCLIP performs well on a diverse range of benchmarks for cardiac image interpretation, despite not having been explicitly trained for individual interpretation tasks. EchoCLIP can assess cardiac function (mean absolute error of 7.1% when predicting left ventricular ejection fraction in an external validation dataset) and identify implanted intracardiac devices (area under the curve (AUC) of 0.84, 0.92 and 0.97 for pacemakers, percutaneous mitral valve repair and artificial aortic valves, respectively). We also developed a long-context variant (EchoCLIP-R) using a custom tokenizer based on common echocardiography concepts. EchoCLIP-R accurately identified unique patients across multiple videos (AUC of 0.86), identified clinical transitions such as heart transplants (AUC of 0.79) and cardiac surgery (AUC 0.77) and enabled robust image-to-text search (mean cross-modal retrieval rank in the top 1% of candidate text reports). These capabilities represent a substantial step toward understanding and applying foundation models in cardiovascular imaging for preliminary interpretation of echocardiographic findings.","<method>vision–language foundation model</method>, <method>custom tokenizer</method>"
2024,https://openalex.org/W4398780590,Biology,Greater cane rat algorithm (GCRA): A nature-inspired metaheuristic for optimization problems,"This paper introduces a new metaheuristic technique known as the Greater Cane Rat Algorithm (GCRA) for addressing optimization problems. The optimization process of GCRA is inspired by the intelligent foraging behaviors of greater cane rats during and off mating season. Being highly nocturnal, they are intelligible enough to leave trails as they forage through reeds and grass. Such trails would subsequently lead to food and water sources and shelter. The exploration phase is achieved when they leave the different shelters scattered around their territory to forage and leave trails. It is presumed that the alpha male maintains knowledge about these routes, and as a result, other rats modify their location according to this information. Also, the males are aware of the breeding season and separate themselves from the group. The assumption is that once the group is separated during this season, the foraging activities are concentrated within areas of abundant food sources, which aids the exploitation. Hence, the smart foraging paths and behaviors during the mating season are mathematically represented to realize the design of the GCR algorithm and carry out the optimization tasks. The performance of GCRA is tested using twenty-two classical benchmark functions, ten CEC 2020 complex functions, and the CEC 2011 real-world continuous benchmark problems. To further test the performance of the proposed algorithm, six classic problems in the engineering domain were used. Furthermore, a thorough analysis of computational and convergence results is presented to shed light on the efficacy and stability levels of GCRA. The statistical significance of the results is compared with ten state-of-the-art algorithms using Friedman's and Wilcoxon's signed rank tests. These findings show that GCRA produced optimal or nearly optimal solutions and evaded the trap of local minima, distinguishing it from the rival optimization algorithms employed to tackle similar problems. The GCRA optimizer source code is publicly available at: https://www.mathworks.com/matlabcentral/fileexchange/165241-greater-cane-rat-algorithm-gcra",<method>Greater Cane Rat Algorithm (GCRA)</method>
2024,https://openalex.org/W4391855187,Biology,Machine learning-assisted in-situ adaptive strategies for the control of defects and anomalies in metal additive manufacturing,"In metal additive manufacturing (AM), the material microstructure and part geometry are formed incrementally. Consequently, the resulting part could be defect- and anomaly-free if sufficient care is taken to deposit each layer under optimal process conditions. Conventional closed-loop control (CLC) engineering solutions which sought to achieve this were deterministic and rule-based, thus resulting in limited success in the stochastic environment experienced in the highly dynamic AM process. On the other hand, emerging machine learning (ML) based strategies are better suited to providing the robustness, scope, flexibility, and scalability required for process control in an uncertain environment. Offline ML models that help optimise AM process parameters before a build begins and online ML models that efficiently processed in-situ sensory data to detect and diagnose flaws in real-time (or near-real-time) have been developed. However, ML models that enable a process to take evasive or corrective actions in relation to flaws via on the fly decision-making are only emerging. These models must possess prognostic capabilities to provide context-sensitive recommendations for in-situ process control based on real-time diagnostics. In this article, we pinpoint the shortcomings in traditional CLC strategies, and provide a framework for defect and anomaly control through ML-assisted CLC in AM. We discuss flaws in terms of their causes, in-situ detectability, and controllability, and examine their management under three scenarios: avoidance, mitigation, and repair. Then, we summarise the research into ML models developed for offline optimisation and in-situ diagnosis before initiating a detailed conversation on the implementation of ML-assisted in-situ process control. We found that researchers favoured reinforcement learning approaches or inverse ML models for making rapid, situation-aware control decisions. We also observed that, to-date, the defects addressed were those that may be quantified relatively easily autonomously, and that mitigation (rather than avoidance or repair) was the aim of ML-assisted in-situ control strategies. Additionally, we highlight the various technologies that must seamlessly combine to advance the field of autonomous in-situ control so that it becomes a reality in industrial settings. Finally, we raise awareness of seldom discussed, yet highly pertinent, topics relevant to adaptive control. Our work closes a significant gap in the current AM literature by broaching wide-ranging discussions on matters relevant to in-situ adaptive control in AM.","<method>machine learning (ML) based strategies</method>, <method>offline ML models</method>, <method>online ML models</method>, <method>reinforcement learning approaches</method>, <method>inverse ML models</method>"
2024,https://openalex.org/W4392640075,Biology,Performance assessment of machine learning algorithms for mapping of land use/land cover using remote sensing data,"The rapid increase in population accelerates the rate of change of Land use/Land cover (LULC) in various parts of the world. This phenomenon caused a huge strain for natural resources. Hence, continues monitoring of LULC changes gained a significant importance for management of natural resources and assessing the climate change impacts. Recently, application of machine learning algorithms on RS (remote sensing) data for rapid and accurate mapping of LULC gained significant importance due to growing need of LULC estimation for ecosystem services, natural resource management and environmental management. Hence, it is crucial to access and compare the performance of different machine learning classifiers for accurate mapping of LULC. The primary objective of this study was to compare the performance of CART (Classification and Regression Tree), RF (Random Forest) and SVM (Support Vector Machine) for LULC estimation by processing RS data on Google Earth Engine (GEE). In total four classes of LULC (Water Bodies, Vegetation Cover, Urban Land and Barren Land) for city of Lahore were extracted using satellite images from Landsat-7, Landsat-8 and Landsat-9 for years 2008, 2015 and 2022, respectively. According to results, RF is the best performing classifier with maximum overall accuracy of 95.2% and highest Kappa coefficient value of 0.87, SVM achieved maximum accuracy of 89.8% with highest Kappa of 0.84 and CART showed maximum overall accuracy of 89.7% with Kappa value of 0.79. Results from this study can give assistance for decision makers, planners and RS experts to choose a suitable machine learning algorithm for LULC classification in an unplanned urbanized city like Lahore.","<method>Classification and Regression Tree (CART)</method>, <method>Random Forest (RF)</method>, <method>Support Vector Machine (SVM)</method>"
2024,https://openalex.org/W4390754233,Biology,Groundwater Quality Assessment and Irrigation Water Quality Index Prediction Using Machine Learning Algorithms,"The evaluation of groundwater quality is crucial for irrigation purposes; however, due to financial constraints in developing countries, such evaluations suffer from insufficient sampling frequency, hindering comprehensive assessments. Therefore, associated with machine learning approaches and the irrigation water quality index (IWQI), this research aims to evaluate the groundwater quality in Naama, a region in southwest Algeria. Hydrochemical parameters (cations, anions, pH, and EC), qualitative indices (SAR,RSC,Na%,MH,and PI), as well as geospatial representations were used to determine the groundwater’s suitability for irrigation in the study area. In addition, efficient machine learning approaches for forecasting IWQI utilizing Extreme Gradient Boosting (XGBoost), Support vector regression (SVR), and K-Nearest Neighbours (KNN) models were implemented. In this research, 166 groundwater samples were used to calculate the irrigation index. The results showed that 42.18% of them were of excellent quality, 34.34% were of very good quality, 6.63% were good quality, 9.64% were satisfactory, and 4.21% were considered unsuitable for irrigation. On the other hand, results indicate that XGBoost excels in accuracy and stability, with a low RMSE (of 2.8272 and a high R of 0.9834. SVR with only four inputs (Ca2+, Mg2+, Na+, and K) demonstrates a notable predictive capability with a low RMSE of 2.6925 and a high R of 0.98738, while KNN showcases robust performance. The distinctions between these models have important implications for making informed decisions in agricultural water management and resource allocation within the region.","<method>Extreme Gradient Boosting (XGBoost)</method>, <method>Support vector regression (SVR)</method>, <method>K-Nearest Neighbours (KNN)</method>"
2024,https://openalex.org/W4391347933,Biology,CCL-DTI: contributing the contrastive loss in drug–target interaction prediction,"Abstract Background The Drug–Target Interaction (DTI) prediction uses a drug molecule and a protein sequence as inputs to predict the binding affinity value. In recent years, deep learning-based models have gotten more attention. These methods have two modules: the feature extraction module and the task prediction module. In most deep learning-based approaches, a simple task prediction loss (i.e., categorical cross entropy for the classification task and mean squared error for the regression task) is used to learn the model. In machine learning, contrastive-based loss functions are developed to learn more discriminative feature space. In a deep learning-based model, extracting more discriminative feature space leads to performance improvement for the task prediction module. Results In this paper, we have used multimodal knowledge as input and proposed an attention-based fusion technique to combine this knowledge. Also, we investigate how utilizing contrastive loss function along the task prediction loss could help the approach to learn a more powerful model. Four contrastive loss functions are considered: (1) max-margin contrastive loss function, (2) triplet loss function, (3) Multi-class N-pair Loss Objective, and (4) NT-Xent loss function. The proposed model is evaluated using four well-known datasets: Wang et al. dataset, Luo's dataset, Davis, and KIBA datasets. Conclusions Accordingly, after reviewing the state-of-the-art methods, we developed a multimodal feature extraction network by combining protein sequences and drug molecules, along with protein–protein interaction networks and drug–drug interaction networks. The results show it performs significantly better than the comparable state-of-the-art approaches.","<method>deep learning-based models</method>, <method>attention-based fusion technique</method>, <method>contrastive loss function</method>, <method>max-margin contrastive loss function</method>, <method>triplet loss function</method>, <method>Multi-class N-pair Loss Objective</method>, <method>NT-Xent loss function</method>, <method>multimodal feature extraction network</method>"
2024,https://openalex.org/W4391248672,Biology,Pedestrian 3D Shape Understanding for Person Re-Identification via Multi-View Learning,"Recent development in computing power has resulted in performance improvements on holistic(none-occluded) person Re-Identification (ReID) tasks. Nevertheless, the precision of the recent research will diminish when a pedestrian is obstructed by obstacles. Within the realm of 2D space, the loss of information from obstructed objects continues to pose significant challenges in the context of person ReID. Person is a 3D non-grid object, and thus semantic representation learning in only 2D space limits the understanding of occluded person. In the present work, we propose a network based on 3D multi-view learning, allowing it to acquire geometric and shape details of an occluded pedestrian from 3D space. Simultaneously, it capitalizes on advancements in 2D-based networks to extract semantic representations from 3D multi-views. Specifically, the surface random selection strategy is proposed to convert images of 2D RGB into 3D multi-views. Using this strategy, we build four extensive 3D multi-view data collections for person ReID. After that, Pedestrian 3D Shape Understanding for Person Re-Identification via Multi-View Learning(MV-3DSReID), is proposed for identifying the person by learning person geometry and structure representation from the groups of multi-view images. In comparison to alternative data formats (e.g., 2D RGB, 3D point cloud), multi-view images complement each other's detailed features of the 3D object by adjusting rendering viewpoints, thus facilitating a more comprehensive understanding of the person for both holistic and occluded ReID situations. Experiments on occluded and holistic ReID tasks demonstrate performance levels comparable to state-of-the-art methods, validating the effectiveness of our proposed approach in tackling challenges related to occlusion. The code is available at https://github.com/hangjiaqi1/MV-TransReID.","<method>3D multi-view learning</method>, <method>2D-based networks</method>, <method>surface random selection strategy</method>, <method>Pedestrian 3D Shape Understanding for Person Re-Identification via Multi-View Learning (MV-3DSReID)</method>"
2024,https://openalex.org/W4392077419,Biology,Selection of sustainable food suppliers using the Pythagorean fuzzy CRITIC-MARCOS method,"Sustainable food supplier selection (SFSS) can be handled as an uncertain decision-making issue. The Pythagorean fuzzy set (PFS), a type of non-standard fuzzy set, offers an expanded description space for articulating fuzzy and uncertain data. Accordingly, this paper proposes a Pythagorean fuzzy synthetic decision method-based selection framework for solving the SFSS problem within a subjective context. Then, the weighted distance measures for the PFS are introduced to derive the importance degrees of the experts, which can provide a more objective decision result. Then, an information fusion method with a PFS-weighted power average (WPA) operator is introduced to form a group decision matrix competent to accommodate the deviation effect. Next, an extended PF-measurement of alternatives and ranking according to compromise solution (MARCOS) method integrating PF-criteria importance through inter-criteria correlation (CRITIC) is presented to calculate the priority of each supplier, which can capture the inter-correlations between criteria. Finally, a numerical example of SFSS is implemented to show the application of the proposed synthetic decision approach. Subsequently, the sensitivity analysis of distance parameters and comparison analysis among different SFSS approaches were conducted to test the rationality and advantages of the proposed framework for resolving the SFSS problem. The results show that the reported method can provide a practical way to resolve the SFSS problems with uncertain data.","<method>Pythagorean fuzzy synthetic decision method</method>, <method>weighted distance measures for the Pythagorean fuzzy set (PFS)</method>, <method>information fusion method with a PFS-weighted power average (WPA) operator</method>, <method>extended PF-measurement of alternatives and ranking according to compromise solution (MARCOS) method integrating PF-criteria importance through inter-criteria correlation (CRITIC)</method>"
2024,https://openalex.org/W4399578149,Biology,Impact of groundwater nitrogen legacy on water quality,"Abstract The loss of agricultural nitrogen (N) is a leading cause of global eutrophication and freshwater and coastal hypoxia. Despite regulatory efforts, such as the European Union’s Nitrogen Directive, high concentrations of N persist in freshwaters. Excessive N leaching and accumulation in groundwater has created a substantial N reservoir as groundwater travel times are orders-of-magnitude slower than those of surface waters. In this study we reconstructed past and projected future N dynamics in groundwater for four major river basins, the Rhine, Mississippi, Yangtze and Pearl, showcasing different N trajectories. The Rhine and Mississippi river basins have accumulated N since the 1950s and although strategies to reduce excess agricultural N have worked well in the Rhine, groundwater legacy N persists in the Mississippi. The Yangtze and Pearl river basins entered the N accumulation phase in the 1970s and the accumulation is expected to continue until 2050. Policies to reduce N pollution from fertilizers have not halted N accumulation, highlighting the importance of accounting for the N legacy in groundwater. Restoring groundwater N storage to 1970 levels by diminishing N leaching will therefore take longer in the Yangtze and Pearl (&gt;35 years) than in the Rhine (9 years) and Mississippi (15 years). Sustainable watershed management requires long-term strategies that address the impacts of legacy N and promote sustainable agricultural practices aligned with the Sustainable Development Goals to balance agricultural productivity with water conservation.",No methods found.
2024,https://openalex.org/W4391097427,Biology,"Quantifying the direct and indirect effects of terrain, climate and human activity on the spatial pattern of kNDVI-based vegetation growth: A case study from the Minjiang River Basin, Southeast China","In the context of global change, it is vital to comprehensively understand the spatial pattern and driving mechanism of vegetation growth to maintain the stability of watershed ecosystems. Previous research has focused mainly on identifying the main drivers of vegetation growth, while the direct and indirect effects of climate, terrain, and human activity on vegetation growth have rarely been explored. This study used the Minjiang River Basin (MRB), an important ecological barrier and the largest watershed in southeastern China, as an example. The kernel normalized difference vegetation index (kNDVI) was calculated on the Google Earth Engine (GEE) platform to examine the spatial pattern and evolution characteristics of vegetation growth. The optimal parameter-based geographical detector (OPGD) and partial least squares structural equation modeling (PLS-SEM) were used to analyze how terrain, climate, and human activity influenced the spatial pattern of the kNDVI. (1) From 2001 to 2020, vegetation growth in the MRB was predominantly rated as excellent or good, and 88.93% of the area showed an increasing trend of vegetation growth. (2) The OPGD revealed that the primary drivers influencing the spatial distribution of the kNDVI in the MRB included population density, nighttime light, elevation and temperature, which explained >40% of the variation in the kNDVI. The interaction of all paired drivers enhanced the explanatory power of the kNDVI, among which the strongest interaction was between population density and elevation, and the second interaction was between population density and temperature. (3) PLS-SEM revealed that human activity had a direct negative effect on the kNDVI, while terrain and climate had direct and indirect positive effects on the kNDVI. Overall, the total effects of terrain, climate and human activity on the kNDVI were 0.594, 0.233 and − 0.495, respectively, indicating that the positive effect of terrain outweighed the negative effect of human activity on vegetation growth in the MRB. These findings not only provide scientific evidence for ecological conservation and management in the MRB but also offer a useful reference for other regions exploring the complex causes of spatial patterns of vegetation growth.","<method>optimal parameter-based geographical detector (OPGD)</method>, <method>partial least squares structural equation modeling (PLS-SEM)</method>"
2024,https://openalex.org/W4391796054,Biology,Optical remote sensing of crop biophysical and biochemical parameters: An overview of advances in sensor technologies and machine learning algorithms for precision agriculture,"This paper provides an overview of the recent developments in remote sensing technology and machine learning algorithms for estimating important biophysical and biochemical parameters for precision farming. The objectives are (i) to provide an overview of recent advances in remotely sensed retrieval of biophysical and biochemical parameters brought by the developments in sensor technologies and robust machine learning algorithms and (ii) to identify the sources of uncertainty in retrieving biophysical and biochemical parameters and implications for precision agriculture. The review revealed that developments in crop biophysical and biochemical parameters retrieval techniques were mainly driven by announcements and the availability of new sensors. Two ground-breaking events can be identified, i.e., the availability of Sentinel-2 and the SuperDove constellation. The two provide high temporal-high spatial resolution data relevant for site-specific management and super-spectral configuration, enabling retrieval of crop growth and health parameters. The free availability of Sentinel-2 triggered the testing of its spectral configurations and upscaling of retrieval approaches using simulated data from field spectrometers and airborne hyperspectral sensors. SuperDoves will likely reduce the cost of very high-resolution data while providing unprecedented capabilities for detailed, accurate and frequent characterisation of field variability. Studies showed that the red-edge bands and hybrid models coupling Radiative Transfer Model (RTM) and machine learning regression algorithms (MLRA) are promising for operational and accurate monitoring of stress-related crop parameters to aid time-sensitive agronomic decisions. However, such models were tested in Mediterranean climates and performed poorly in African semi-arid areas and China's temperate continental semi-humid monsoon climates. Therefore, locally-calibrated RTM models incorporating crop-type maps and other spatio-temporal constraints may reduce uncertainties when adapted to data-scarce regions. Generally, permanent experimental sites and a lack of systematic calibration data on various crops are some limiting factors to using remote sensing technologies for PA in Sub-Saharan Africa. Other complexities arise from farm configurations, such as small field sizes and mixed cropping practices. Therefore, future studies should develop generic, scalable and transferable models, especially within under-studied areas.",<method>machine learning regression algorithms (MLRA)</method>
2024,https://openalex.org/W4393167823,Biology,Risk analysis and assessment of water resource carrying capacity based on weighted gray model with improved entropy weighting method in the central plains region of China,"The issue of global water shortage is a serious concern. The scientific evaluation of water resource carrying capacity (WRCC) serves as the foundation for implementing measures to protect water resources. In addition, most of the studies are based on the analysis and research of regional WRCC from the aspects of water quantity and water quality. There are few studies on the four aspects of water resources endowment conditions, society, economy and ecological environment, which is difficult to scientifically and accurately reflect the analysis and evaluation of regional WRCC by the four systems. Therefore, it is necessary to conduct a deeper discussion and Analysis on this topic. This study presents a WRCC index system and corresponding ranking criteria based on 20 influencing factors from four aspects: water resources endowment (WRE), economy, society, and ecological environment. In addition, by combining the improved entropy weighting method (EWM) with gray correlation analysis, the weighted gray technique for order preference by similarity to an ideal solution (TOPSIS) model is proposed for analyzing and assessing WRCC risk. Finally, the WRCC of the study area from 2012 to 2021 is comprehensively evaluated in the central plains region of China (CPROC) as an example. The results show that the comprehensive evaluation obtained a multi-year average value of 0.2935, and the water resources shortage in the CPROC is generally in grade III status. The comprehensive average value of Beijing is 0.345, and the comprehensive average value of Henan is 0.397. The overall degree of water resources shortage is in the state of grade V shortage, Shaanxi is in the state of grade IV shortage, and the degree of water resources in Tianjin and Shanxi is relatively good. This study provides corresponding scientific basis and methodological guidance for the sustainable utilization of water resources and healthy socio-economic performance in the CPROC.","<method>improved entropy weighting method (EWM)</method>, <method>gray correlation analysis</method>, <method>weighted gray technique for order preference by similarity to an ideal solution (TOPSIS) model</method>"
2024,https://openalex.org/W4394822945,Biology,A Critical Review of Artificial Intelligence Based Approaches in Intrusion Detection: A Comprehensive Analysis,"Intrusion detection (ID) is critical in securing computer networks against various malicious attacks. Recent advancements in machine learning (ML), deep learning (DL), federated learning (FL), and explainable artificial intelligence (XAI) have drawn significant attention as potential approaches for ID. DL-based approaches have shown impressive performance in ID by automatically learning relevant features from data but require significant labelled data and computational resources to train complex models. ML-based approaches require fewer computational resources and labelled data, but their ability to generalize to unseen data is limited. FL is a relatively new approach that enables multiple entities to train a model collectively without exchanging their data, providing privacy and security benefits, making it an attractive option for ID. However, FL-based approaches require more communication resources and additional computation to aggregate models from different entities. XAI is critical for understanding how AI models make decisions, improving interpretability and transparency. While existing literature has explored the strengths and weaknesses of DL, ML, FL, and XAI-based approaches for ID, a significant gap exists in providing a comprehensive analysis of the specific use cases and scenarios where each approach is most suitable. This paper seeks to fill this void by delivering an in-depth review that not only highlights strengths and weaknesses but also offers guidance for selecting the appropriate approach based on the unique ID context and available resources. The selection of an appropriate approach depends on the specific use case, and this work provides insights into which method is best suited for various network sizes, data availability, privacy, and security concerns, thus aiding practitioners in making informed decisions for their ID needs.","<method>machine learning (ML)</method>, <method>deep learning (DL)</method>, <method>federated learning (FL)</method>, <method>explainable artificial intelligence (XAI)</method>"
2024,https://openalex.org/W4395011414,Biology,"admetSAR3.0: a comprehensive platform for exploration, prediction and optimization of chemical ADMET properties","Abstract Absorption, distribution, metabolism, excretion and toxicity (ADMET) properties play a crucial role in drug discovery and chemical safety assessment. Built on the achievements of admetSAR and its successor, admetSAR2.0, this paper introduced the new version of the series, admetSAR3.0, as a comprehensive platform for chemical ADMET assessment, including search, prediction and optimization modules. In the search module, admetSAR3.0 hosted over 370 000 high-quality experimental ADMET data for 104 652 unique compounds, and supplemented chemical structure similarity search function to facilitate read-across. In the prediction module, we introduced comprehensive ADMET endpoints and two new sections for environmental and cosmetic risk assessments, empowering admetSAR3.0 to provide prediction for 119 endpoints, more than double numbers compared to the previous version. Furthermore, the advanced multi-task graph neural network framework offered robust and reliable support for ADMET prediction. In particular, a module named ADMETopt was added to automatically optimize the ADMET properties of query molecules through transformation rules or scaffold hopping. Finally, admetSAR3.0 provides user-friendly interfaces for multiple types of input data, such as SMILES string, chemical structure and batch molecule file, and supports various output types, including digital, chart displays and file downloads. In summary, admetSAR3.0 is anticipated to be a valuable and powerful tool in drug discovery and chemical safety assessment at http://lmmd.ecust.edu.cn/admetsar3/.",<method>multi-task graph neural network framework</method>
2024,https://openalex.org/W4401163187,Biology,Monthly climate prediction using deep convolutional neural network and long short-term memory,"Climate change affects plant growth, food production, ecosystems, sustainable socio-economic development, and human health. The different artificial intelligence models are proposed to simulate climate parameters of Jinan city in China, include artificial neural network (ANN), recurrent NN (RNN), long short-term memory neural network (LSTM), deep convolutional NN (CNN), and CNN-LSTM. These models are used to forecast six climatic factors on a monthly ahead. The climate data for 72 years (1 January 1951–31 December 2022) used in this study include monthly average atmospheric temperature, extreme minimum atmospheric temperature, extreme maximum atmospheric temperature, precipitation, average relative humidity, and sunlight hours. The time series of 12 month delayed data are used as input signals to the models. The efficiency of the proposed models are examined utilizing diverse evaluation criteria namely mean absolute error, root mean square error (RMSE), and correlation coefficient (R). The modeling result inherits that the proposed hybrid CNN-LSTM model achieves a greater accuracy than other compared models. The hybrid CNN-LSTM model significantly reduces the forecasting error compared to the models for the one month time step ahead. For instance, the RMSE values of the ANN, RNN, LSTM, CNN, and CNN-LSTM models for monthly average atmospheric temperature in the forecasting stage are 2.0669, 1.4416, 1.3482, 0.8015 and 0.6292 °C, respectively. The findings of climate simulations shows the potential of CNN-LSTM models to improve climate forecasting. Climate prediction will contribute to meteorological disaster prevention and reduction, as well as flood control and drought resistance.","<method>artificial neural network (ANN)</method>, <method>recurrent NN (RNN)</method>, <method>long short-term memory neural network (LSTM)</method>, <method>deep convolutional NN (CNN)</method>, <method>CNN-LSTM</method>"
2024,https://openalex.org/W4399303474,Biology,Improving Forest Above-Ground Biomass Estimation by Integrating Individual Machine Learning Models,"The accurate estimation of forest above-ground biomass (AGB) is crucial for sustainable forest management and tracking the carbon cycle of forest ecosystem. Machine learning algorithms have been proven to have great potential in forest AGB estimation with remote sensing data. Though many studies have demonstrated that a single machine learning model can produce highly accurate estimations of forest AGB in many situations, efforts are still required to explore the possible improvement in forest AGB estimation for a specific scenario under study. This study aims to investigate the performance of novel ensemble machine learning methods for forest AGB estimation and analyzes whether these methods are affected by forest types, independent variables, and spatial autocorrelation. Four well-known machine learning models (CatBoost, LightGBM, random forest (RF), and XGBoost) were compared for forest AGB estimation in the study using eight scenarios devised on the basis of two study regions, two variable types, and two validation strategies. Subsequently, a hybrid model combining the strengths of these individual models was proposed for forest AGB estimation. The findings indicated that no individual model outperforms the others in all scenarios. The RF model demonstrates superior performance in scenarios 5, 6, and 7, while the CatBoost model shows the best performance in the remaining scenarios. Moreover, the proposed hybrid model consistently has the best performance in all scenarios in spite of some uncertainties. The ensemble strategy developed in this study for the hybrid model substantially improves estimation accuracy and exhibits greater stability, effectively addressing the challenge of model selection encountered in the forest AGB forecasting process.","<method>CatBoost</method>, <method>LightGBM</method>, <method>random forest (RF)</method>, <method>XGBoost</method>, <method>ensemble machine learning methods</method>, <method>hybrid model</method>"
2024,https://openalex.org/W4401384485,Biology,GAN based augmentation using a hybrid loss function for dermoscopy images,"Dermatology is the most appropriate field to utilize pattern recognition-based automated techniques for objective, accurate, and rapid diagnosis because diagnosis mainly relies on visual examinations of skin lesions. Recent approaches utilizing deep learning techniques have shown remarkable results in this field. However, they necessitate a substantial quantity of images and the availability of dermoscopy images is often limited. Also, even if enough images are available, their labeling requires expert knowledge and is time-consuming. To overcome these issues, an efficient augmentation approach is needed to expand training datasets from input images. Therefore, in this work, a generative adversarial network has been developed using a new hybrid loss function constructed with traditional loss functions to enhance the generation power of the architecture. Also, the effect of the proposed approach and different generative network-based augmentations, which have been used with dermoscopy images in the literature, on the classification of skin lesions has been investigated. Therefore, the main contributions of this work are: (i) introducing a new generative model for the augmentation of dermoscopy images; (ii) presenting the effect of the proposed model on the classification of the images; (iii) comparative evaluations of the effectiveness of different generative network-based augmentations in the classification of seven forms of skin lesions. The classification accuracy when the proposed augmentation is used is 93.12%, which is higher than its counterparts. Experimental results indicate the significance of augmentation techniques in the classification of skin lesions and the efficiency of the proposed structure in improving the classification accuracy.","<method>deep learning techniques</method>, <method>generative adversarial network</method>, <method>generative network-based augmentations</method>"
2024,https://openalex.org/W4391943312,Biology,"Machine Learning and Deep Learning in Synthetic Biology: Key Architectures, Applications, and Challenges","Machine learning (ML), particularly deep learning (DL), has made rapid and substantial progress in synthetic biology in recent years. Biotechnological applications of biosystems, including pathways, enzymes, and whole cells, are being probed frequently with time. The intricacy and interconnectedness of biosystems make it challenging to design them with the desired properties. ML and DL have a synergy with synthetic biology. Synthetic biology can be employed to produce large data sets for training models (for instance, by utilizing DNA synthesis), and ML/DL models can be employed to inform design (for example, by generating new parts or advising unrivaled experiments to perform). This potential has recently been brought to light by research at the intersection of engineering biology and ML/DL through achievements like the design of novel biological components, best experimental design, automated analysis of microscopy data, protein structure prediction, and biomolecular implementations of ANNs (Artificial Neural Networks). I have divided this review into three sections. In the first section, I describe predictive potential and basics of ML along with myriad applications in synthetic biology, especially in engineering cells, activity of proteins, and metabolic pathways. In the second section, I describe fundamental DL architectures and their applications in synthetic biology. Finally, I describe different challenges causing hurdles in the progress of ML/DL and synthetic biology along with their solutions.","<method>machine learning (ML)</method>, <method>deep learning (DL)</method>, <method>Artificial Neural Networks (ANNs)</method>"
2024,https://openalex.org/W4391997375,Biology,A machine learning approach to predict the efficiency of corrosion inhibition by natural product-based organic inhibitors,"Abstract This paper presents a quantitative structure–property relationship (QSPR)-based machine learning (ML) framework designed for predicting corrosion inhibition efficiency (CIE) values in natural organic inhibitor compounds. The modeling dataset comprises 50 natural organic compounds, with 11 quantum chemical properties (QCP) serving as input features, and the target variable being the corrosion inhibition efficiency (CIE) value. To enhance the predictive accuracy of the ML model, the kernel density estimation (KDE) function is employed to generate virtual samples during the training process, with the overarching goal of refining the precision of the ML model. Three distinct models, namely random forest (RF), gradient boosting (GB), and k-nearest neighbor (KNN), are tested in the study. The results demonstrate a noteworthy enhancement in the prediction performance of the models, attributable to the incorporation of virtual samples that effectively improve the correlation between input features and target values. Consequently, the accuracy of the predicted CIE values is significantly augmented, aligning more closely with the actual CIE values. Performance improvements were evident across all models after the incorporation of virtual samples. The GB, RF, and KNN models exhibited increments in R 2 values from 0.557 to 0.996, 0.522 to 0.999, and 0.415 to 0.994, respectively, concomitant with the introduction of 500 virtual samples. Additionally, each model demonstrated a notable reduction in RMSE values, transitioning from 1.41 to 0.19, 1.27 to 0.10, and 1.22 to 0.16, respectively. While the GB model initially outperformed others before the addition of virtual samples, the performance of the model exhibited fluctuation as the number of virtual samples varied. This behavior suggests that the KDE function provides a certain level of resilience against model variations. The proposed approach contributes to the effective design and exploration of corrosion inhibitor candidates, offering a reliable and accurate predictive tool that bridges the gap between theoretical studies and experimental synthesis.","<method>random forest (RF)</method>, <method>gradient boosting (GB)</method>, <method>k-nearest neighbor (KNN)</method>, <method>kernel density estimation (KDE)</method>"
2024,https://openalex.org/W4395037579,Biology,Assessing ChatGPT 4.0’s test performance and clinical diagnostic accuracy on USMLE STEP 2 CK and clinical case reports,"Abstract While there is data assessing the test performance of artificial intelligence (AI) chatbots, including the Generative Pre-trained Transformer 4.0 (GPT 4) chatbot (ChatGPT 4.0), there is scarce data on its diagnostic accuracy of clinical cases. We assessed the large language model (LLM), ChatGPT 4.0, on its ability to answer questions from the United States Medical Licensing Exam (USMLE) Step 2, as well as its ability to generate a differential diagnosis based on corresponding clinical vignettes from published case reports. A total of 109 Step 2 Clinical Knowledge (CK) practice questions were inputted into both ChatGPT 3.5 and ChatGPT 4.0, asking ChatGPT to pick the correct answer. Compared to its previous version, ChatGPT 3.5, we found improved accuracy of ChatGPT 4.0 when answering these questions, from 47.7 to 87.2% ( p = 0.035) respectively. Utilizing the topics tested on Step 2 CK questions, we additionally found 63 corresponding published case report vignettes and asked ChatGPT 4.0 to come up with its top three differential diagnosis. ChatGPT 4.0 accurately created a shortlist of differential diagnoses in 74.6% of the 63 case reports (74.6%). We analyzed ChatGPT 4.0’s confidence in its diagnosis by asking it to rank its top three differentials from most to least likely. Out of the 47 correct diagnoses, 33 were the first (70.2%) on the differential diagnosis list, 11 were second (23.4%), and three were third (6.4%). Our study shows the continued iterative improvement in ChatGPT’s ability to answer standardized USMLE questions accurately and provides insights into ChatGPT’s clinical diagnostic accuracy.","<method>Generative Pre-trained Transformer 4.0 (GPT 4) chatbot (ChatGPT 4.0)</method>, <method>ChatGPT 3.5</method>, <method>large language model (LLM)</method>"
2024,https://openalex.org/W4399173789,Biology,Utilizing Deep Learning and the Internet of Things to Monitor the Health of Aquatic Ecosystems to Conserve Biodiversity,"The decline in water conditions contributes to the crisis in clean water biodiversity. The interactions between water conditions indicators and the correlations among these variables and taxonomic groupings are intricate in their impact on biodiversity. However, since there are just a few kinds of Internet of Things (IoT) that are accessible to purchase, many chemical and biological measurements still need laboratory studies. The newest progress in Deep Learning and the IoT allows for the use of this method in the real-time surveillance of water quality, therefore contributing to preserving biodiversity. This paper presents a thorough examination of the scientific literature about the water quality factors that have a significant influence on the variety of freshwater ecosystems. It selected the ten most crucial water quality criteria. The connections between the quantifiable and valuable aspects of the IoT are assessed using a Generalized Regression-based Neural Networks (G-RNN) framework and a multi-variational polynomial regression framework. These models depend on historical data from the monitoring of water quality. The projected findings in an urbanized river were validated using a combination of traditional field water testing, in-lab studies, and the created IoT-depend water condition management system. The G-RNN effectively differentiates abnormal increases in variables from typical scenarios. The assessment coefficients for the system for degree 8 are as follows: 0.87, 0.73, 0.89, and 0.79 for N-O3-N, BO-D5, P-O4, and N-H3-N. The suggested methods and prototypes were verified against laboratory findings to assess their efficacy and effectiveness. The general efficacy was deemed suitable, with most forecasting mistakes smaller than 0.3 mg/L. This validation offers valuable insights into IoT methods' usage in pollutants released observation and additional water quality regulating usage, specifically for freshwater biodiversity preservation.","<method>Generalized Regression-based Neural Networks (G-RNN)</method>, <method>multi-variational polynomial regression framework</method>"
2024,https://openalex.org/W4401593044,Biology,Overcoming the Limits of Cross-Sensitivity: Pattern Recognition Methods for Chemiresistive Gas Sensor Array,"Abstract As information acquisition terminals for artificial olfaction, chemiresistive gas sensors are often troubled by their cross-sensitivity, and reducing their cross-response to ambient gases has always been a difficult and important point in the gas sensing area. Pattern recognition based on sensor array is the most conspicuous way to overcome the cross-sensitivity of gas sensors. It is crucial to choose an appropriate pattern recognition method for enhancing data analysis, reducing errors and improving system reliability, obtaining better classification or gas concentration prediction results. In this review, we analyze the sensing mechanism of cross-sensitivity for chemiresistive gas sensors. We further examine the types, working principles, characteristics, and applicable gas detection range of pattern recognition algorithms utilized in gas-sensing arrays. Additionally, we report, summarize, and evaluate the outstanding and novel advancements in pattern recognition methods for gas identification. At the same time, this work showcases the recent advancements in utilizing these methods for gas identification, particularly within three crucial domains: ensuring food safety, monitoring the environment, and aiding in medical diagnosis. In conclusion, this study anticipates future research prospects by considering the existing landscape and challenges. It is hoped that this work will make a positive contribution towards mitigating cross-sensitivity in gas-sensitive devices and offer valuable insights for algorithm selection in gas recognition applications.",<method>pattern recognition</method>
2024,https://openalex.org/W4390492164,Biology,Efficient Camouflaged Object Detection Network Based on Global Localization Perception and Local Guidance Refinement,"Camouflaged Object Detection (COD) is a challenging visual task due to its complex contour, diverse scales, and high similarity to the background. Existing COD methods encounter two predicaments: One is that they are prone to falling into local perception, resulting in inaccurate object localization; Another issue is the difficulty in achieving precise object segmentation due to a lack of detailed information. In addition, most COD methods typically require larger parameter amounts and higher computational complexity in pursuit of better performance. To this end, we propose a global localization perception and local guidance refinement network (PRNet), that simultaneously addresses performance and computational costs. Through effective aggregation and use of semantic and details information, the PRNet can achieve accurate localization and refined segmentation of camouflaged objects. Specifically, with the help of a Cascaded Attention Perceptron (CAP) designed, we can effectively integrate and perceive multi-scale information to localize camouflaged objects. We also design a Guided Refinement Decoder (GRD) in a top-down manner to extract context information and aggregate details to further refine camouflaged prediction results. Extensive experimental results demonstrate that our PRNet outperforms 12 state-of-the-art models on 4 challenging datasets. Meanwhile, the PRNet has a smaller number of parameters (12.74M), lower computational complexity (10.24G), and real-time inference speed (105FPS). Source codes are available at https://github.com/hu-xh/PRNet.","<method>global localization perception and local guidance refinement network (PRNet)</method>, <method>Cascaded Attention Perceptron (CAP)</method>, <method>Guided Refinement Decoder (GRD)</method>"
2024,https://openalex.org/W4390501772,Biology,Remote sensing based forest cover classification using machine learning,"Abstract Pakistan falls significantly below the recommended forest coverage level of 20 to 30 percent of total area, with less than 6 percent of its land under forest cover. This deficiency is primarily attributed to illicit deforestation for wood and charcoal, coupled with a failure to embrace advanced techniques for forest estimation, monitoring, and supervision. Remote sensing techniques leveraging Sentinel-2 satellite images were employed. Both single-layer stacked images and temporal layer stacked images from various dates were utilized for forest classification. The application of an artificial neural network (ANN) supervised classification algorithm yielded notable results. Using a single-layer stacked image from Sentinel-2, an impressive 91.37% training overall accuracy and 0.865 kappa coefficient were achieved, along with 93.77% testing overall accuracy and a 0.902 kappa coefficient. Furthermore, the temporal layer stacked image approach demonstrated even better results. This method yielded 98.07% overall training accuracy, 97.75% overall testing accuracy, and kappa coefficients of 0.970 and 0.965, respectively. The random forest (RF) algorithm, when applied, achieved 99.12% overall training accuracy, 92.90% testing accuracy, and kappa coefficients of 0.986 and 0.882. Notably, with the temporal layer stacked image of the Sentinel-2 satellite, the RF algorithm reached exceptional performance with 99.79% training accuracy, 96.98% validation accuracy, and kappa coefficients of 0.996 and 0.954. In terms of forest cover estimation, the ANN algorithm identified 31.07% total forest coverage in the District Abbottabad region. In comparison, the RF algorithm recorded a slightly higher 31.17% of the total forested area. This research highlights the potential of advanced remote sensing techniques and machine learning algorithms in improving forest cover assessment and monitoring strategies.","<method>artificial neural network (ANN) supervised classification algorithm</method>, <method>random forest (RF) algorithm</method>"
2024,https://openalex.org/W4390954471,Biology,Traffic Sign Detection and Recognition Using YOLO Object Detection Algorithm: A Systematic Review,"Context: YOLO (You Look Only Once) is an algorithm based on deep neural networks with real-time object detection capabilities. This state-of-the-art technology is widely available, mainly due to its speed and precision. Since its conception, YOLO has been applied to detect and recognize traffic signs, pedestrians, traffic lights, vehicles, and so on. Objective: The goal of this research is to systematically analyze the YOLO object detection algorithm, applied to traffic sign detection and recognition systems, from five relevant aspects of this technology: applications, datasets, metrics, hardware, and challenges. Method: This study performs a systematic literature review (SLR) of studies on traffic sign detection and recognition using YOLO published in the years 2016–2022. Results: The search found 115 primary studies relevant to the goal of this research. After analyzing these investigations, the following relevant results were obtained. The most common applications of YOLO in this field are vehicular security and intelligent and autonomous vehicles. The majority of the sign datasets used to train, test, and validate YOLO-based systems are publicly available, with an emphasis on datasets from Germany and China. It has also been discovered that most works present sophisticated detection, classification, and processing speed metrics for traffic sign detection and recognition systems by using the different versions of YOLO. In addition, the most popular desktop data processing hardwares are Nvidia RTX 2080 and Titan Tesla V100 and, in the case of embedded or mobile GPU platforms, Jetson Xavier NX. Finally, seven relevant challenges that these systems face when operating in real road conditions have been identified. With this in mind, research has been reclassified to address these challenges in each case. Conclusions: This SLR is the most relevant and current work in the field of technology development applied to the detection and recognition of traffic signs using YOLO. In addition, insights are provided about future work that could be conducted to improve the field.",<method>YOLO (You Look Only Once)</method>
2024,https://openalex.org/W4393306481,Biology,Reliable water quality prediction and parametric analysis using explainable AI models,"Abstract The consumption of water constitutes the physical health of most of the living species and hence management of its purity and quality is extremely essential as contaminated water has to potential to create adverse health and environmental consequences. This creates the dire necessity to measure, control and monitor the quality of water. The primary contaminant present in water is Total Dissolved Solids (TDS), which is hard to filter out. There are various substances apart from mere solids such as potassium, sodium, chlorides, lead, nitrate, cadmium, arsenic and other pollutants. The proposed work aims to provide the automation of water quality estimation through Artificial Intelligence and uses Explainable Artificial Intelligence (XAI) for the explanation of the most significant parameters contributing towards the potability of water and the estimation of the impurities. XAI has the transparency and justifiability as a white-box model since the Machine Learning (ML) model is black-box and unable to describe the reasoning behind the ML classification. The proposed work uses various ML models such as Logistic Regression, Support Vector Machine (SVM), Gaussian Naive Bayes, Decision Tree (DT) and Random Forest (RF) to classify whether the water is drinkable. The various representations of XAI such as force plot, test patch, summary plot, dependency plot and decision plot generated in SHAPELY explainer explain the significant features, prediction score, feature importance and justification behind the water quality estimation. The RF classifier is selected for the explanation and yields optimum Accuracy and F1-Score of 0.9999, with Precision and Re-call of 0.9997 and 0.998 respectively. Thus, the work is an exploratory analysis of the estimation and management of water quality with indicators associated with their significance. This work is an emerging research at present with a vision of addressing the water quality for the future as well.","<method>Artificial Intelligence</method>, <method>Explainable Artificial Intelligence (XAI)</method>, <method>Logistic Regression</method>, <method>Support Vector Machine (SVM)</method>, <method>Gaussian Naive Bayes</method>, <method>Decision Tree (DT)</method>, <method>Random Forest (RF)</method>"
2024,https://openalex.org/W4396609541,Biology,Robust Drone Delivery with Weather Information,"Problem definition: Drone delivery has recently garnered significant attention due to its potential for faster delivery at a lower cost than other delivery options. When scheduling drones from a depot for delivery to various destinations, the dispatcher must take into account the uncertain wind conditions, which affect the delivery times of drones to their destinations, leading to late deliveries. Methodology/results: To mitigate the risk of delivery delays caused by wind uncertainty, we propose a two-period drone scheduling model to robustly optimize the delivery schedule. In this framework, the scheduling decisions are made in the morning, with the provision for different delivery schedules in the afternoon that adapt to updated weather information available by midday. Our approach minimizes the essential riskiness index, which can simultaneously account for the probability of tardy delivery and the magnitude of lateness. Using wind observation data, we characterize the uncertain flight times via a cluster-wise ambiguity set, which has the benefit of tractability while avoiding overfitting the empirical distribution. A branch-and-cut (B&amp;C) algorithm is developed for this adaptive distributionally framework to improve its scalability. Our adaptive distributionally robust model can effectively reduce lateness in out-of-sample tests compared with other classical models. The proposed B&amp;C algorithm can solve instances to optimality within a shorter time frame than a general modeling toolbox. Managerial implications: Decision makers can use the adaptive robust model together with the cluster-wise ambiguity set to effectively reduce service lateness at customers for drone delivery systems. Funding: This work was supported by the National Natural Science Foundation of China [Grants 72101049 and 72232001], the Natural Science Foundation of Liaoning Province [Grant 2023-BS-091], the Fundamental Research Funds for the Central Universities [Grant DUT23RC(3)045], and the Major Project of the National Social Science Foundation [Grant 22&amp;ZD151]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/msom.2022.0339 .","<method>branch-and-cut (B&C) algorithm</method>, <method>adaptive distributionally robust model</method>"
2024,https://openalex.org/W4399319394,Biology,Multi-task aquatic toxicity prediction model based on multi-level features fusion,"With the escalating menace of organic compounds in environmental pollution imperiling the survival of aquatic organisms, the investigation of organic compound toxicity across diverse aquatic species assumes paramount significance for environmental protection. Understanding how different species respond to these compounds helps assess the potential ecological impact of pollution on aquatic ecosystems as a whole. Compared with traditional experimental methods, deep learning methods have higher accuracy in predicting aquatic toxicity, faster data processing speed and better generalization ability. This article presents ATFPGT-multi, an advanced multi-task deep neural network prediction model for organic toxicity. The model integrates molecular fingerprints and molecule graphs to characterize molecules, enabling the simultaneous prediction of acute toxicity for the same organic compound across four distinct fish species. Furthermore, to validate the advantages of multi-task learning, we independently construct prediction models, named ATFPGT-single, for each fish species. We employ cross-validation in our experiments to assess the performance and generalization ability of ATFPGT-multi. The experimental results indicate, first, that ATFPGT-multi outperforms ATFPGT-single on four fish datasets with AUC improvements of 9.8%, 4%, 4.8%, and 8.2%, respectively, demonstrating the superiority of multi-task learning over single-task learning. Furthermore, in comparison with previous algorithms, ATFPGT-multi outperforms comparative methods, emphasizing that our approach exhibits higher accuracy and reliability in predicting aquatic toxicity. Moreover, ATFPGT-multi utilizes attention scores to identify molecular fragments associated with fish toxicity in organic molecules, as demonstrated by two organic molecule examples in the main text, demonstrating the interpretability of ATFPGT-multi. In summary, ATFPGT-multi provides important support and reference for the further development of aquatic toxicity assessment. All of codes and datasets are freely available online at https://github.com/zhaoqi106/ATFPGT-multi.","<method>deep learning methods</method>, <method>multi-task deep neural network prediction model</method>, <method>multi-task learning</method>, <method>single-task learning</method>, <method>cross-validation</method>"
2024,https://openalex.org/W4391321561,Biology,A survey on training challenges in generative adversarial networks for biomedical image analysis,"Abstract In biomedical image analysis, the applicability of deep learning methods is directly impacted by the quantity of image data available. This is due to deep learning models requiring large image datasets to provide high-level performance. Generative Adversarial Networks (GANs) have been widely utilized to address data limitations through the generation of synthetic biomedical images. GANs consist of two models. The generator, a model that learns how to produce synthetic images based on the feedback it receives. The discriminator, a model that classifies an image as synthetic or real and provides feedback to the generator. Throughout the training process, a GAN can experience several technical challenges that impede the generation of suitable synthetic imagery. First, the mode collapse problem whereby the generator either produces an identical image or produces a uniform image from distinct input features. Second, the non-convergence problem whereby the gradient descent optimizer fails to reach a Nash equilibrium. Thirdly, the vanishing gradient problem whereby unstable training behavior occurs due to the discriminator achieving optimal classification performance resulting in no meaningful feedback being provided to the generator. These problems result in the production of synthetic imagery that is blurry, unrealistic, and less diverse. To date, there has been no survey article outlining the impact of these technical challenges in the context of the biomedical imagery domain. This work presents a review and taxonomy based on solutions to the training problems of GANs in the biomedical imaging domain. This survey highlights important challenges and outlines future research directions about the training of GANs in the domain of biomedical imagery.","<method>deep learning</method>, <method>Generative Adversarial Networks (GANs)</method>, <method>generator</method>, <method>discriminator</method>, <method>gradient descent optimizer</method>"
2024,https://openalex.org/W4392017386,Biology,Vegetable disease detection using an improved YOLOv8 algorithm in the greenhouse plant environment,"Abstract This study introduces YOLOv8n-vegetable, a model designed to address challenges related to imprecise detection of vegetable diseases in greenhouse plant environment using existing network models. The model incorporates several improvements and optimizations to enhance its effectiveness. Firstly, a novel C2fGhost module replaces partial C2f. with GhostConv based on Ghost lightweight convolution, reducing the model’s parameters and improving detection performance. Second, the Occlusion Perception Attention Module (OAM) is integrated into the Neck section to better preserve feature information after fusion, enhancing vegetable disease detection in greenhouse settings. To address challenges associated with detecting small-sized objects and the depletion of semantic knowledge due to varying scales, an additional layer for detecting small-sized objects is included. This layer improves the amalgamation of extensive and basic semantic knowledge, thereby enhancing overall detection accuracy. Finally, the HIoU boundary loss function is introduced, leading to improved convergence speed and regression accuracy. These improvement strategies were validated through experiments using a self-built vegetable disease detection dataset in a greenhouse environment. Multiple experimental comparisons have demonstrated the model's effectiveness, achieving the objectives of improving detection speed while maintaining accuracy and real-time detection capability. According to experimental findings, the enhanced model exhibited a 6.46% rise in mean average precision (mAP) over the original model on the self-built vegetable disease detection dataset under greenhouse conditions. Additionally, the parameter quantity and model size decreased by 0.16G and 0.21 MB, respectively. The proposed model demonstrates significant advancements over the original algorithm and exhibits strong competitiveness when compared with other advanced object detection models. The lightweight and fast detection of vegetable diseases offered by the proposed model presents promising applications in vegetable disease detection tasks.","<method>YOLOv8n-vegetable</method>, <method>C2fGhost module</method>, <method>GhostConv</method>, <method>Occlusion Perception Attention Module (OAM)</method>, <method>HIoU boundary loss function</method>"
2024,https://openalex.org/W4393055891,Biology,A new intelligently optimized model reference adaptive controller using GA and WOA-based MPPT techniques for photovoltaic systems,"Recently, the integration of renewable energy sources, specifically photovoltaic (PV) systems, into power networks has grown in significance for sustainable energy generation. Researchers have investigated different control algorithms for maximum power point tracking (MPPT) to enhance the efficiency of PV systems. This article presents an innovative method to address the problem of maximum power point tracking in photovoltaic systems amidst swiftly changing weather conditions. MPPT techniques supply maximum power to the load during irradiance fluctuations and ambient temperatures. A novel optimal model reference adaptive controller is developed and designed based on the MIT rule to seek global maximum power without ripples rapidly. The suggested controller is also optimized through two popular meta-heuristic algorithms: The genetic algorithm (GA) and the whale optimization algorithm (WOA). These meta-heuristic approaches have been exploited to overcome the difficulty of selecting the adaptation gain of the MRAC controller. The reference voltage for MPPT is generated in the study through an adaptive neuro-fuzzy inference system. The suggested controller's performance is tested via MATLAB/Simulink software under varying temperature and radiation circumstances. Simulation is carried out using a Soltech 1sth-215-p module coupled to a boost converter, which powers a resistive load. Furthermore, to emphasize the recommended algorithm's performance, a comparative study was done between the optimal MRAC using GA and WOA and the conventional incremental conductance (INC) method.","<method>genetic algorithm (GA)</method>, <method>whale optimization algorithm (WOA)</method>, <method>adaptive neuro-fuzzy inference system</method>"
2024,https://openalex.org/W4399144385,Biology,Assessment of technical water quality in mining based on machine learning methods,"Introduction. Mining requires water treatment and wastewater processing, abstraction and discharge during mining increases consumption several times. Since water consumption in mining and processing is usually associated with domestic, industrial and technical needs, the need for water supply systems required for water treatment increases. Water from different sources can be used for treatment: incoming water, process and reused water, and wastewater. But the water obtained from any of the sources must meet all the norms and requirements. Water quality is determined by physical, chemical and bacteriological properties. The main directions for improving water consumption by mining enterprises are to reduce the consumption of drinking water from rivers, lakes and municipal water supply, as well as to expand the use of mine and quarry water for domestic and technical needs. Materials and methods. As training data for training the neural network, a dataset that includes water quality data obtained from fresh water sources was selected for the methods work, and using machine learning, develops a model that predicts whether the water is suitable for technical use in mines. This dataset includes 2293 values (samples) as well as 9 attributes. Correlation, neural network, and decision tree methods were used to build the models in this study. Results. Various machine learning methods (neural network and decision trees) were used to build a predictive model to assess the quality of water that would be suitable for use in the mining industry for technical purposes. With the help of the built models were processed data obtained from public sources, when analyzing which it was found that the method of decision trees was more accurate. The constructed model, for determining dependencies, thus, has high accuracy (small error). To increase the practical significance of the study, a number of transformations of the initial data set were carried out, in particular, an experiment with the division of attributes into groups of importance, in relation to the data, taking into account the subject area. The results obtained made it clear that checking only for hazardous impurities does not guarantee the suitability of water, but almost completely excludes (low significance factor) samples with impurities that do not meet the requirements, and the model can have practical significance. Allocation of the group for rapid quality determination, showed that for the express test, in an emergency situation or under time constraints, the possibility of practical use of the obtained model, has a justification, due to the small error. In general, the conducted experiments have shown that when taking into account the costs (total) for data collection, it makes sense to use models, taking into account the reduction of collected data, on the parameters (factors) of technical water. Discussion. In general, on the basis of the conducted research, we can talk about the successful application of machine learning methods in determining the suitability of technical water in the mining industry. During the experiments, the decision tree method performed particularly well, with the lowest error values. In addition, further work can be carried out to reduce the error in the models, in particular, by possibly increasing the number of attributes, as well as more fine-tuning of the applied machine learning methods. Conclusions. The authors conclude that machine learning techniques can be successfully integrated to determine the quality and suitability of process water in the mining industry in today’s world. Resume. The paper compares machine learning methods such as decision trees and neural network method. The comparative analysis of these methods and their quality of information processing is shown on the example of a set of data on water quality in the mining industry. With the help of built models were processed data obtained from open sources, when analyzing which it was found that the method of decision trees was more accurate. The constructed model for determining dependencies has high accuracy (small error). Suggestions for practical applications and future research directions. This study can form the basis for research in this or related fields to conduct further studies on the reliability and accuracy of using machine learning to predict the quality of water used in the mining industry. Continued work in the above direction may be the rationale for wider use of the above methods to improve various meaningful production performance in this or related areas.","<method>neural network</method>, <method>decision tree</method>"
2024,https://openalex.org/W4401490202,Biology,Cultural dimensions and sustainability disclosure in the banking sector: Insights from a qualitative comparative analysis approach,"Abstract This study adopts an innovative, holistic research approach based on fuzzy set qualitative comparative analysis (fs‐QCA) to deeply delve into national cultural dimensions' role in affecting banks' sustainability disclosure practices in the Eastern European (EE) region. Accordingly, this study aims to identify whether one or more configurations of cultural dimensions derived from Hofstede's national culture framework are conducive to higher levels of sustainability disclosure, using a sample including the five largest banks in each country of the ‘Bucharest Nine’ (B9) area over 2018–2022 period. Results evidence that sustainability disclosure patterns are not homogeneous among the banks operating in B9 countries. After the introduction of Directive 95/2014/EU, banks in some countries maintained relatively constant levels of sustainability disclosure, while others experienced steady growth rates. No cultural dimension alone would likely determine higher sustainability disclosure levels among B9 banks, confirming that normative pressures influencing EE banks' sustainability disclosure practices result from a combination of more cultural facets. In particular, fs‐QCA highlights a bundle of cultural dimension configurations that mould stakeholders' expectations in investigated countries, exerting pressures on banks to enhance their transparency on sustainability issues. The presence of power distance recurs in most configurations as a factor enabling higher sustainability disclosure levels. On the other hand, in most cases, the presence of uncertainty avoidance and long‐term orientation is conducive to higher banks' sustainability and transparency.",<method>fuzzy set qualitative comparative analysis (fs‐QCA)</method>
2024,https://openalex.org/W4390483836,Biology,Ranking Factors Affecting Sustainable Competitive Advantage from The Business Intelligence Perspective: Using Content Analysis And F-TOPSIS,"Sustainable competitive advantage, as a key factor in business success, ensures that the company is able to dominate the market with differentiated products and services over a long period of time. This advantage is especially achieved through business intelligence, since smart decisions, leveraging meaningful data and analytics, and continuous process improvement help the company maintain this advantage and experience sustainable growth. The aim of this study is to rank the factors influencing sustainable competitive advantage from a business intelligence standpoint. The research methodology consists of two stages: qualitative and quantitative. In the first step, content analysis was performed to extract indicators from previous studies. In the second step, indicators were ranked using the F-TOPSIS method. Factors affecting sustainable competitive advantage from the business intelligence viewpoint were categorized into 5 criteria, including 27 sub-criteria. The 5 main criteria are customer relationship management, smart marketing, soft and hard organizational factors, and the mental image of the product, respectively. In the second step, the sub-criteria in each criterion were ranked. In customer relationship management, the most important sub-criterion is effective interaction with customers. In smart marketing, the most important sub-criterion is feedback and continuous improvement. Among the soft and hard organizational factors, the most important sub-criteria are support from senior management and technology and infrastructure. In the mental image of the product, the most important sub-criterion is social responsibility.",<method>F-TOPSIS</method>
2024,https://openalex.org/W4391093770,Biology,A Convolutional Neural Network approach for image-based anomaly detection in smart agriculture,"The recent technological advances and their applications to agriculture provide leverage for the new paradigm of smart agriculture. Remote sensing applications can help optimize resources, making agriculture more ecological, increasing productivity and helping farmers to anticipate events that could not otherwise be avoided. Considering that losses caused by anomalies such as diseases, weeds and pests account for 20-40 % of overall agricultural productivity, a successful research effort in this area would be a breakthrough for agriculture. In this paper, we propose a methodology with which to discover and classify anomalies in images of crops, taken from a wide range of distances, using different Convolutional Neural Network architectures. This methodology also deals with several difficulties that usually appear in this kind of problems, such as class imbalance, the insufficient and small variety of images, overtraining or lack of models generalisation. We have implemented four convolutional neural network architectures in a high-performance computing environment, and propose a methodology based on data augmentation with the addition of Gaussian noise to the images to solve the above problems. Our approach was tested using two well-established open datasets that are unalike: DeepWeeds, which provides a classification of 8 weed species native to Australia using images that were taken at a distance of 1 m, and Agriculture-Vision, which classifies 6 types of crop anomalies using multispectral satellite imagery. Our methodology attained accuracies of 98 % and 95.3% respectively, improving the state-of-the-art by several points. In order to ease reproducibility and model selection, we have provided a comparison in terms of computational time and other metrics, thus enabling the choice between architectures to be made according to the resources available. The complete code is available in an open repository in order to encourage reproducibility and promote scientific advances in sustainable agriculture.","<method>Convolutional Neural Network architectures</method>, <method>data augmentation with the addition of Gaussian noise</method>"
2024,https://openalex.org/W4391612257,Biology,Machine learning for the management of biochar yield and properties of biomass sources for sustainable energy,"Abstract Biochar is emerging as a potential solution for biomass conversion to meet the ever increasing demand for sustainable energy. Efficient management systems are needed in order to exploit fully the potential of biochar. Modern machine learning (ML) techniques, and in particular ensemble approaches and explainable AI methods, are valuable for forecasting the properties and efficiency of biochar properly. Machine‐learning‐based forecasts, optimization, and feature selection are critical for improving biomass management techniques. In this research, we explore the influences of these techniques on the accurate forecasting of biochar yield and properties for a range of biomass sources. We emphasize the importance of the interpretability of a model, as this improves human comprehension and trust in ML predictions. Sensitivity analysis is shown to be an effective technique for finding crucial biomass characteristics that influence the synthesis of biochar. Precision prognostics have far‐reaching ramifications, influencing industries such as biomass logistics, conversion technologies, and the successful use of biomass as renewable energy. These advances can make a substantial contribution to a greener future and can encourage the development of a circular biobased economy. This work emphasizes the importance of using sophisticated data‐driven methodologies such as ML in biochar synthesis, to usher in ecologically friendly energy solutions. These breakthroughs hold the key to a more sustainable and environmentally friendly future.","<method>ensemble approaches</method>, <method>explainable AI methods</method>, <method>machine-learning-based forecasts</method>, <method>optimization</method>, <method>feature selection</method>, <method>sensitivity analysis</method>"
2024,https://openalex.org/W4391878291,Biology,Effective lung nodule detection using deep CNN with dual attention mechanisms,"Abstract Novel methods are required to enhance lung cancer detection, which has overtaken other cancer-related causes of death as the major cause of cancer-related mortality. Radiologists have long-standing methods for locating lung nodules in patients with lung cancer, such as computed tomography (CT) scans. Radiologists must manually review a significant amount of CT scan pictures, which makes the process time-consuming and prone to human error. Computer-aided diagnosis (CAD) systems have been created to help radiologists with their evaluations in order to overcome these difficulties. These systems make use of cutting-edge deep learning architectures. These CAD systems are designed to improve lung nodule diagnosis efficiency and accuracy. In this study, a bespoke convolutional neural network (CNN) with a dual attention mechanism was created, which was especially crafted to concentrate on the most important elements in images of lung nodules. The CNN model extracts informative features from the images, while the attention module incorporates both channel attention and spatial attention mechanisms to selectively highlight significant features. After the attention module, global average pooling is applied to summarize the spatial information. To evaluate the performance of the proposed model, extensive experiments were conducted using benchmark dataset of lung nodules. The results of these experiments demonstrated that our model surpasses recent models and achieves state-of-the-art accuracy in lung nodule detection and classification tasks.","<method>convolutional neural network (CNN)</method>, <method>dual attention mechanism</method>, <method>channel attention</method>, <method>spatial attention</method>, <method>global average pooling</method>"
2024,https://openalex.org/W4392432727,Biology,Plant disease recognition in a low data scenario using few-shot learning,"Plant disease is one of the major problems in agriculture. Diseases damage plants, reduce yields and lower the quality of the produce. Traditional approaches to detecting plant diseases are usually based on visual inspection and laboratory testing, which can be expensive and time-consuming. They require trained plant pathologists as well as specialised equipment. Several studies demonstrate that artificial intelligence (AI) methods can produce promising results. However, AI methods are generally data-hungry and require large annotated datasets, and the collection and annotation of such datasets can be a limiting factor. It often appears that only a small amount of data is available for certain disease types. Whereas the performance of typical AI methods drops significantly when they are trained with inadequate data. This paper proposes a novel few-shot learning (FSL) method to detect plant diseases and alleviate the data scarcity problem. The proposed method uses as few as five images per class in the machine learning process. Our method is based on a state-of-the-art FSL pipeline called pre-training, meta-learning, and fine-tuning (PMF), integrated with a novel feature attention (FA) module; we call the overall method PMF+FA. The FA module emphasises the discriminative parts in the image and reduces the impact of complicated backgrounds and undesired objects. We used ResNet50 and Vision Transformers (ViT) as the feature learner. Two publicly available plant disease datasets were repurposed to meet the FSL requirements. We thoroughly evaluated the proposed method on the PlantDoc dataset, which contains disease samples in field environments with complex backgrounds and unwanted objects. The PMF+FA method with ViT achieved an average accuracy of 90.12% in disease recognition. The results demonstrate that the PMF+FA pipeline consistently outperforms the baseline PMF. The results also highlight that the method using ViT generates better results than ResNet50 for diagnosing complex data. ViT and ResNet50 implementations are computationally efficient, taking 1.11 and 0.57 ms on average per image to evaluate the test set respectively. The high throughput and high-quality performance with only a small training dataset indicate that the proposed technique can be used for real-time disease detection in digital farming systems.","<method>few-shot learning (FSL)</method>, <method>pre-training, meta-learning, and fine-tuning (PMF)</method>, <method>feature attention (FA) module</method>, <method>ResNet50</method>, <method>Vision Transformers (ViT)</method>"
2024,https://openalex.org/W4394808249,Biology,An ensemble penalized regression method for multi-ancestry polygenic risk prediction,"Abstract Great efforts are being made to develop advanced polygenic risk scores (PRS) to improve the prediction of complex traits and diseases. However, most existing PRS are primarily trained on European ancestry populations, limiting their transferability to non-European populations. In this article, we propose a novel method for generating multi-ancestry Polygenic Risk scOres based on enSemble of PEnalized Regression models (PROSPER). PROSPER integrates genome-wide association studies (GWAS) summary statistics from diverse populations to develop ancestry-specific PRS with improved predictive power for minority populations. The method uses a combination of $${{{{{{\mathscr{L}}}}}}}_{1}$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:msub> <mml:mrow> <mml:mi>L</mml:mi> </mml:mrow> <mml:mrow> <mml:mn>1</mml:mn> </mml:mrow> </mml:msub> </mml:math> (lasso) and $${{{{{{\mathscr{L}}}}}}}_{2}$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:msub> <mml:mrow> <mml:mi>L</mml:mi> </mml:mrow> <mml:mrow> <mml:mn>2</mml:mn> </mml:mrow> </mml:msub> </mml:math> (ridge) penalty functions, a parsimonious specification of the penalty parameters across populations, and an ensemble step to combine PRS generated across different penalty parameters. We evaluate the performance of PROSPER and other existing methods on large-scale simulated and real datasets, including those from 23andMe Inc., the Global Lipids Genetics Consortium, and All of Us. Results show that PROSPER can substantially improve multi-ancestry polygenic prediction compared to alternative methods across a wide variety of genetic architectures. In real data analyses, for example, PROSPER increased out-of-sample prediction R 2 for continuous traits by an average of 70% compared to a state-of-the-art Bayesian method (PRS-CSx) in the African ancestry population. Further, PROSPER is computationally highly scalable for the analysis of large SNP contents and many diverse populations.","<method>lasso</method>, <method>ridge</method>, <method>ensemble</method>, <method>Bayesian method (PRS-CSx)</method>"
2024,https://openalex.org/W4399651788,Biology,Semantic segmentation of microbial alterations based on SegFormer,"Introduction Precise semantic segmentation of microbial alterations is paramount for their evaluation and treatment. This study focuses on harnessing the SegFormer segmentation model for precise semantic segmentation of strawberry diseases, aiming to improve disease detection accuracy under natural acquisition conditions. Methods Three distinct Mix Transformer encoders - MiT-B0, MiT-B3, and MiT-B5 - were thoroughly analyzed to enhance disease detection, targeting diseases such as Angular leaf spot, Anthracnose rot, Blossom blight, Gray mold, Leaf spot, Powdery mildew on fruit, and Powdery mildew on leaves. The dataset consisted of 2,450 raw images, expanded to 4,574 augmented images. The Segment Anything Model integrated into the Roboflow annotation tool facilitated efficient annotation and dataset preparation. Results The results reveal that MiT-B0 demonstrates balanced but slightly overfitting behavior, MiT-B3 adapts rapidly with consistent training and validation performance, and MiT-B5 offers efficient learning with occasional fluctuations, providing robust performance. MiT-B3 and MiT-B5 consistently outperformed MiT-B0 across disease types, with MiT-B5 achieving the most precise segmentation in general. Discussion The findings provide key insights for researchers to select the most suitable encoder for disease detection applications, propelling the field forward for further investigation. The success in strawberry disease analysis suggests potential for extending this approach to other crops and diseases, paving the way for future research and interdisciplinary collaboration.","<method>SegFormer segmentation model</method>, <method>Mix Transformer encoders - MiT-B0</method>, <method>Mix Transformer encoders - MiT-B3</method>, <method>Mix Transformer encoders - MiT-B5</method>, <method>Segment Anything Model</method>"
2024,https://openalex.org/W4390590855,Biology,A Survey of Text Classification With Transformers: How Wide? How Large? How Long? How Accurate? How Expensive? How Safe?,"Text classification is a basic task in natural language processing (NLP) with applications from sentiment analysis to question-answering with chat bots. In recent years, transformer-based models have emerged as the prevailing framework in NLP, demonstrating excellent results across many benchmarks. This paper recommends an expanded taxonomy of applications and provides a review of the performance of different models across these applications. The use of traditional research techniques plus co-citation and bibliographic coupling provides a comprehensive view of the current and past research in this area. The study begins by providing an overview of the history of transformer-based models with an emphasis on recent large language models (LLM). Next, uni-modal (text only) inputs and the emerging area of multi-modal classification are discussed to provide a comparison of current and emerging research in this area. Gaps are highlighted in the use of multi-modal text/numeric/columnar data and recommendations for future research are provided. Finally, the length of text input variables (tokens) is reviewed to explore the evolution from short-text to longer document applications. Furthermore, the accuracy on 358 datasets across 20 applications is reviewed and unexpected results emerge which show that LLMs are not always the most accurate or least expensive option. In addition to model performance, the safety implications of transformer-based models are reviewed, and a summary of issues related to ethics, bias, social implications, and copyright are explored.","<method>transformer-based models</method>, <method>large language models (LLM)</method>"
2024,https://openalex.org/W4391289138,Biology,"A stacking ANN ensemble model of ML models for stream water quality prediction of Godavari River Basin, India","The importance of water quality models has increased as their inputs are critical to the development of risk assessment framework for environmental management and monitoring of rivers. However, with the advent of a plethora of recent advances in ML algorithms better predictions are possible. This study proposes a causal and effect model by considering climatological such as temperature and precipitation along with geospatial information related to the agricultural land use factor (ALUF), the forest land use factor (FLUF), the grassland usage factor (GLUF), the shrub land use factor (SLUF), and the urban land use factor (ULUF). All these factors are included in the input data, whereas four Stream Water Quality parameters (SWQPs) such as Electrical Conductivity (EC), Biochemical Oxygen Demand (BOD), Nitrate, and Dissolved Oxygen (DO) from 2019 to 2021 are taken as outputs to predict the Godavari River Basin water quality. In the preliminary investigation, out of these four SWQPs, nitrate's coefficient of variation (CV) is high, revealing a close association with climate parameters and land use practices across the sampling stations. In the authors' earlier study, a model using a single-layer Feed-Forward Neural Network (FFNN) showed improved performance in predicting cause and effect factors linked to water quality metrics. To achieve better prediction, a stacked ANN meta-model and nine conventional machine learning (ML) models, including Extreme Gradient Boosting (XGB), Extra Trees (ET), Bagging (BG), Random Forest (RF), AdaBoost or Adaptive Boosting (ADB), Decision Tree (DT), Highest Gradient Boosting (HGB), Light Gradient Boosting Method (LGBM), and Gradient Boosting (GB), were compared in this study. According to the study's findings, Bagging and Boosting models outperformed stand-alone earlier FFNN for the same dataset and showed superior predictive capabilities in terms of accuracy in forecasting the variable of interest. For instance, during testing, the coefficient of determination (R2) of Biochemical Oxygen Demand (BOD) increased from 0.72 to 0.87. Furthermore, a stacked Artificial Neural Network (ANN) meta model that was reinforced using Extreme Gradient Boosting (XGB), Random Forest (RF), and Extra Trees (ET) as base models performed better than the individual ML models (from R2 = 0.87 to 0.91 for BOD in testing). By using this new framework, the effort for hyperparameter tuning can be minimized.","<method>Feed-Forward Neural Network (FFNN)</method>, <method>stacked ANN meta-model</method>, <method>Extreme Gradient Boosting (XGB)</method>, <method>Extra Trees (ET)</method>, <method>Bagging (BG)</method>, <method>Random Forest (RF)</method>, <method>AdaBoost or Adaptive Boosting (ADB)</method>, <method>Decision Tree (DT)</method>, <method>Highest Gradient Boosting (HGB)</method>, <method>Light Gradient Boosting Method (LGBM)</method>, <method>Gradient Boosting (GB)</method>, <method>stacked Artificial Neural Network (ANN) meta model</method>"
2024,https://openalex.org/W4391392820,Biology,Advancements in Imaging Sensors and AI for Plant Stress Detection: A Systematic Literature Review,"Integrating imaging sensors and artificial intelligence (AI) have contributed to detecting plant stress symptoms, yet data analysis remains a key challenge. Data challenges include standardized data collection, analysis protocols, selection of imaging sensors and AI algorithms, and finally, data sharing. Here, we present a systematic literature review (SLR) scrutinizing plant imaging and AI for identifying stress responses. We performed a scoping review using specific keywords, namely abiotic and biotic stress, machine learning, plant imaging and deep learning. Next, we used programmable bots to retrieve relevant papers published since 2006. In total, 2,704 papers from 4 databases (Springer, ScienceDirect, PubMed, and Web of Science) were found, accomplished by using a second layer of keywords (e.g., hyperspectral imaging and supervised learning). To bypass the limitations of search engines, we selected OneSearch to unify keywords. We carefully reviewed 262 studies, summarizing key trends in AI algorithms and imaging sensors. We demonstrated that the increased availability of open-source imaging repositories such as PlantVillage or Kaggle has strongly contributed to a widespread shift to deep learning, requiring large datasets to train in stress symptom interpretation. Our review presents current trends in AI-applied algorithms to develop effective methods for plant stress detection using image-based phenotyping. For example, regression algorithms have seen substantial use since 2021. Ultimately, we offer an overview of the course ahead for AI and imaging technologies to predict stress responses. Altogether, this SLR highlights the potential of AI imaging in both biotic and abiotic stress detection to overcome challenges in plant data analysis.","<method>machine learning</method>, <method>deep learning</method>, <method>supervised learning</method>, <method>regression algorithms</method>"
2024,https://openalex.org/W4391715819,Biology,The impact of a blockchain-based food traceability system on the online purchase intention of organic agricultural products,"The issue of food safety has become a crucial public health issue worldwide. Blockchain technology allows consumers to track the flow of food and effectively record the sources and processes of their products, and reduces the elimination of associated food fraud phenomena, such as counterfeiting and dilution. Using signaling theory, structural equation model method and an online sample of 415 Chinese participants, this study investigates the impact of how blockchain-based food traceability system influences consumers' online purchase intentions of organic agricultural products. The results illustrate that: (1) blockchain-based food traceability system can increase consumers' willingness to buy organic agricultural products. (2) Blockchain-based food traceability system can increase consumers' product quality perceptions, product trust and environmental information transparency perceptions. (3) Product quality perceptions, product trust and environmental information transparency perceptions can also promote consumers' purchase intentions. (4) Product quality perceptions, product trust and environmental information transparency perceptions all have significant mediation effects in the in the relationship between blockchain-based food traceability system and consumer purchase intention. The findings of the research are valuable information for business managers and retailers who want to increase sales of their products, as well as helping relevant policy makers to achieve the ultimate goal of green and sustainable development in the food industry.",No methods found.
2024,https://openalex.org/W4392783914,Biology,CAR-Toner: an AI-driven approach for CAR tonic signaling prediction and optimization,"2][3] Our previous work has elucidated that positively charged patches (PCPs) on the surface of the CAR antigenbinding domain facilitate CAR clustering, thereby triggering CAR tonic signals.To quantify these PCPs, which are indicative of CAR tonic signaling, we previously developed a bioinformatic method to determine the PCP score. 1 This calculation method starts with constructing three-dimensional (3D) homology models for CAR's single-chain variable fragments (scFvs) using the SWISS homology modeler.Subsequently, the BindUP web server is used to determine the total count of residues within the top three largest patches containing continuous positively charged residues on the surface of CAR scFv.However, this PCP score calculation method has several limitations: 1. reliance on two external servers; 2. each calculation taking a few days, significantly hindering efficiency; 3. lack of batch calculation capability; 4. no optimization strategies provided for finetuning PCP scores.Given these constraints, we aimed to develop an artificial intelligence (AI)-based PCP score calculator and optimizer to overcome these bottlenecks.Protein databases, structural biology, and advanced deep learning models are all integrated into our AI-based PCP score calculator (Fig. 1a).A comprehensive protein structure database consisting of over 170,000 entries was established by extracting 3D structural information from the Protein Data Bank (PDB) and AlphaFold predictions, followed by stringent quality control procedures.We further developed an in-house algorithm tailored for calculating PCP scores based on the obtained 3D structure information (Supplementary Information), subsequently generating a dataset comprising approximately 170,000 protein sequences along with their associated PCP scores.For model training and evaluation, 70% of the data are allocated as the training dataset, while the remaining 30% serve as the test dataset.The ESM2 model, developed by the FAIR (Meta Fundamental AI Research Protein Team), is utilized for fine-tuning tasks related to PCP prediction. 4,5SM2 is a transformer-based language model using an attention mechanism to learn interaction patterns between pairs of amino acids in the input sequence.Pre-trained on over 60 million protein sequences from the UniProt Reference Clusters (UniRef) database, ESM2 demonstrates strong adaptability to downstream protein structure-related tasks. 5The ESM2-8M model was used to fine-tune the training dataset.Following updating parameters, the ESM2 model was transformed into the PCP-AI prediction model, referred to as CAR-Tonic Signal Tuner (abbreviated as CAR-Toner; http://cartfitness.slst.shanghaitech.edu.cn/CAR-fitness/).This model encompasses three key functionalities: proficient PCP calculation for individual proteins, streamlined batch processing, and an integrated optimization strategy for refining PCP scores (Fig. 1b).","<method>deep learning models</method>, <method>ESM2 model</method>, <method>transformer-based language model using an attention mechanism</method>, <method>fine-tuning</method>"
2024,https://openalex.org/W4390708138,Biology,Accuracy of GPT-4 in histopathological image detection and classification of colorectal adenomas,"Aims To evaluate the accuracy of Chat Generative Pre-trained Transformer (ChatGPT) powered by GPT-4 in histopathological image detection and classification of colorectal adenomas using the diagnostic consensus provided by pathologists as a reference standard. Methods A study was conducted with 100 colorectal polyp photomicrographs, comprising an equal number of adenomas and non-adenomas, classified by two pathologists. These images were analysed by classic GPT-4 for 1 time in October 2023 and custom GPT-4 for 20 times in December 2023. GPT-4’s responses were compared against the reference standard through statistical measures to evaluate its proficiency in histopathological diagnosis, with the pathologists further assessing the model’s descriptive accuracy. Results GPT-4 demonstrated a median sensitivity of 74% and specificity of 36% for adenoma detection. The median accuracy of polyp classification varied, ranging from 16% for non-specific changes to 36% for tubular adenomas. Its diagnostic consistency, indicated by low kappa values ranging from 0.06 to 0.11, suggested only poor to slight agreement. All of the microscopic descriptions corresponded with their diagnoses. GPT-4 also commented about the limitations in its diagnoses (eg, slide diagnosis best done by pathologists, the inadequacy of single-image diagnostic conclusions, the need for clinical data and a higher magnification view). Conclusions GPT-4 showed high sensitivity but low specificity in detecting adenomas and varied accuracy for polyp classification. However, its diagnostic consistency was low. This artificial intelligence tool acknowledged its diagnostic limitations, emphasising the need for a pathologist’s expertise and additional clinical context.","<method>Chat Generative Pre-trained Transformer (ChatGPT) powered by GPT-4</method>, <method>classic GPT-4</method>, <method>custom GPT-4</method>"
2024,https://openalex.org/W4390721566,Biology,Thousands of AI Authors on the Future of AI,"In the largest survey of its kind, 2,778 researchers who had published in top-tier artificial intelligence (AI) venues gave predictions on the pace of AI progress and the nature and impacts of advanced AI systems The aggregate forecasts give at least a 50% chance of AI systems achieving several milestones by 2028, including autonomously constructing a payment processing site from scratch, creating a song indistinguishable from a new song by a popular musician, and autonomously downloading and fine-tuning a large language model. If science continues undisrupted, the chance of unaided machines outperforming humans in every possible task was estimated at 10% by 2027, and 50% by 2047. The latter estimate is 13 years earlier than that reached in a similar survey we conducted only one year earlier [Grace et al., 2022]. However, the chance of all human occupations becoming fully automatable was forecast to reach 10% by 2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey). Most respondents expressed substantial uncertainty about the long-term value of AI progress: While 68.3% thought good outcomes from superhuman AI are more likely than bad, of these net optimists 48% gave at least a 5% chance of extremely bad outcomes such as human extinction, and 59% of net pessimists gave 5% or more to extremely good outcomes. Between 38% and 51% of respondents gave at least a 10% chance to advanced AI leading to outcomes as bad as human extinction. More than half suggested that ""substantial"" or ""extreme"" concern is warranted about six different AI-related scenarios, including misinformation, authoritarian control, and inequality. There was disagreement about whether faster or slower AI progress would be better for the future of humanity. However, there was broad agreement that research aimed at minimizing potential risks from AI systems ought to be prioritized more.",No methods found.
2024,https://openalex.org/W4390870882,Biology,A domain adaptation approach to damage classification with an application to bridge monitoring,"Data-driven machine-learning algorithms generally suffer from a lack of labelled health-state data, mainly those referring to damage conditions. To address such an issue, population-based structural health monitoring seeks to enrich the original dataset by transferring knowledge from a population of monitored structures. Within this context, this paper presents a transfer learning approach, based on domain adaptation, to leverage information from completely-labelled bridge structure data to accurately predict new instances of an unknown target domain. Since intrinsic structural differences may cause distribution shifts, domain adaptation attempts to minimise the distance between the domains and to learn a mapping within a shared feature space. Specifically, the methodology involves the long-term acquisition of natural frequencies from several structural scenarios. Such damage-sensitive features are then aligned via domain adaptation so that a machine-learning algorithm can effectively utilise the labelled source domain data and generalise well to the unlabelled target-domain data. The described procedure is applied to two case studies, including the Z24 and the S101 benchmark bridges and their finite element models, respectively. The results demonstrate the successful exchange of health-state labels to identify the damage class within a population of bridges equipped with SHM systems, showing potential to reduce computational efforts and to deal with scarce or poor data sets in application to bridge network monitoring.","<method>transfer learning</method>, <method>domain adaptation</method>, <method>machine-learning algorithm</method>"
2024,https://openalex.org/W4391112512,Biology,Incorporating biological and clinical insights into variant choice for Mendelian randomisation: examples and principles,"Mendelian randomisation is an accessible and valuable epidemiological approach to provide insight into the causal nature of relationships between risk factor exposures and disease outcomes. However, if performed without critical thought, we may simply have replaced one set of implausible assumptions (no unmeasured confounding or reverse causation) with another set of implausible assumptions (no pleiotropy or other instrument invalidity). The most critical decision to avoid pleiotropy is which genetic variants to use as instrumental variables. Two broad strategies for instrument selection are a biologically motivated strategy and a genome-wide strategy; in general, a biologically motivated strategy is preferred. In this review, we discuss various ways of implementing a biologically motivated selection strategy: using variants in a coding gene region for the exposure or a gene region that encodes a regulator of exposure levels, using a positive control variable and using a biomarker as the exposure rather than its behavioural proxy. In some cases, a genome-wide analysis can provide important complementary evidence, even when its reliability is questionable. In other cases, a biologically-motivated analysis may not be possible. The choice of genetic variants must be informed by biological and functional considerations where possible, requiring collaboration to combine biological and clinical insights with appropriate statistical methodology.",No methods found.
2024,https://openalex.org/W4391164184,Biology,Discovering Consensus Regions for Interpretable Identification of RNA N6-Methyladenosine Modification Sites via Graph Contrastive Clustering,"As a pivotal post-transcriptional modification of RNA, N6-methyladenosine (m6A) has a substantial influence on gene expression modulation and cellular fate determination. Although a variety of computational models have been developed to accurately identify potential m6A modification sites, few of them are capable of interpreting the identification process with insights gained from consensus knowledge. To overcome this problem, we propose a deep learning model, namely M6A-DCR, by discovering consensus regions for interpretable identification of m6A modification sites. In particular, M6A-DCR first constructs an instance graph for each RNA sequence by integrating specific positions and types of nucleotides. The discovery of consensus regions is then formulated as a graph clustering problem in light of aggregating all instance graphs. After that, M6A-DCR adopts a motif-aware graph reconstruction optimization process to learn high-quality embeddings of input RNA sequences, thus achieving the identification of m6A modification sites in an end-to-end manner. Experimental results demonstrate the superior performance of M6A-DCR by comparing it with several state-of-the-art identification models. The consideration of consensus regions empowers our model to make interpretable predictions at the motif level. The analysis of cross validation through different species and tissues further verifies the consistency between the identification results of M6A-DCR and the evolutionary relationships among species","<method>deep learning model</method>, <method>graph clustering</method>, <method>motif-aware graph reconstruction optimization</method>"
2024,https://openalex.org/W4391667625,Biology,"How organizational readiness for green innovation, green innovation performance and knowledge integration affects sustainability performance of exporting firms","Purpose Consumers and businesses are becoming increasingly conscious of sustainable business practices and are often willing to pay a premium for responsibly sourced and manufactured products. Many countries and organizations have implemented regulations and standards for sustainability and companies face penalties or are barred from exporting for not meeting the requirements. Rooted in the resource-based view theory, this study aims to test a moderated mediation model to improve the sustainability performance of exporting firms. Design/methodology/approach Textile firms generating more than 25% of export revenues were targeted for this research. The data collected from 245 middle management-level employees were tested for reliability and validity. The structural equation modelling in AMOS 26 was used to test hypotheses. Findings Organizational readiness for green innovation (ORGI) has a direct positive effect on sustainability performance. The mediation analysis implies that ORGI translates into sustainability performance through improvement in green innovation performance. The moderating effect of knowledge integration highlights the importance of being prepared internally and actively seeking and incorporating external knowledge to improve green innovation performance. Originality/value The findings offer a solid foundation for informed decision-making, policy development and strategies to improve sustainability performance while aligning with the global nature of the textile industry and its inherent challenges. The proposed model and practical implications guide policymakers and managers of exporting firms to foster a culture of green innovation to leverage the effect of their readiness for green innovation on sustainability performance.",No methods found.
2024,https://openalex.org/W4392241969,Biology,All models are wrong and yours are useless: making clinical prediction models impactful for patients,"All models are wrong and yours are useless: making clinical prediction models impactful for patients Florian MarkowetzCheck for updates Most published clinical prediction models are never used in clinical practice and there is a huge gap between academic research and clinical implementation.Here, I propose ways for academic researchers to be proactive partners in improving clinical practice and to design models in ways that ultimately benefit patients.""All models are wrong, but some are useful"" is an aphorism attributed to the statistician George Box.There is humility in claiming your model is wrong, but there is also bravado in implying your model might be useful.And, honestly, I don't think it is.I think your model is useless.How would I know?I don't even know who you are.Well, it is a bet.A bet I am willing to take because the odds are ridiculously in my favour.I will explain what I mean in the context of clinical prediction models.My points apply to a wide range of preclinical models, both computational and biological, but my own core expertise is with clinical prediction tools.These are computational models from statistics, machine learning or AI that try to predict clinically relevant variables and ultimately aim to help doctors to treat patients better.The papers describing them make claims like ""this model can be used in the clinic""; generally softened with words like ""might"", ""could"", ""potential"", ""promise"", or other techniques to reduce accountability.The Box quote offers a yardstick to measure the success of these models; not by how correctly they describe reality but by how useful they are in helping patients.And in general, almost none of these tools ever help anyone.There is a wealth of systematic reviews in different fields to show how many models have been proposed and how few have even been validated, let alone been adopted in the clinic.For example, 408(!) models for chronic obstructive pulmonary disease were systematically reviewed 1 and as a summary the authors bleakly note ""several methodological pitfalls in their development and a low rate of external validation"".And whatever biomedical area you work in, your experiences will mirror this resultmany novel prediction models, little help for patients.I believe that a model designed to be used for patients is useless unless it is actually used for patients.",No methods found.
2024,https://openalex.org/W4392356867,Biology,The SAFE procedure: a practical stopping heuristic for active learning-based screening in systematic reviews and meta-analyses,"Abstract Active learning has become an increasingly popular method for screening large amounts of data in systematic reviews and meta-analyses. The active learning process continually improves its predictions on the remaining unlabeled records, with the goal of identifying all relevant records as early as possible. However, determining the optimal point at which to stop the active learning process is a challenge. The cost of additional labeling of records by the reviewer must be balanced against the cost of erroneous exclusions. This paper introduces the SAFE procedure, a practical and conservative set of stopping heuristics that offers a clear guideline for determining when to end the active learning process in screening software like ASReview. The eclectic mix of stopping heuristics helps to minimize the risk of missing relevant papers in the screening process. The proposed stopping heuristic balances the costs of continued screening with the risk of missing relevant records, providing a practical solution for reviewers to make informed decisions on when to stop screening. Although active learning can significantly enhance the quality and efficiency of screening, this method may be more applicable to certain types of datasets and problems. Ultimately, the decision to stop the active learning process depends on careful consideration of the trade-off between the costs of additional record labeling against the potential errors of the current model for the specific dataset and context.",<method>active learning</method>
2024,https://openalex.org/W4392661577,Biology,"Machine learning models for gully erosion susceptibility assessment in the Tensift catchment, Haouz Plain, Morocco for sustainable development","Gully erosion is a widespread environmental danger, threatening global socio-economic stability and sustainable development. This study comprehensively applied seven machine learning (ML) models including SVM, KNN, RF, XGBoost, ANN, DT, and LR, and evaluated gully erosion susceptibility in the Tensift catchment and predict it within the Haouz plain, Morocco. To ensure the reliability of the findings, the study employed a robust combination of gully erosion inventory, sentinel images, and Digital Surface Model. Eighteen predictors, encompassing topographical, geomorphological, environmental, and hydrological factors, were selected after multicollinearity analyses. The gully erosion susceptibility of the study revealed that approximately 28.18% of the Tensift catchment is at a very high risk of erosion. Furthermore, 15.13% and 31.28% of the catchment are categorized as low and very low respectively. These findings extend to the Haouz plain, where 7.84% of the surface area are very highly risking erosion, while 18.25% and 55.18% are characterized as low and very low risk areas. To gauge the performance of the ML models, an array of metrics including specificity, precision, sensitivity, and accuracy were employed. The study highlights XGBoost and KNN as the most promising models, achieving AUC ROC values of 0.96 and 0.93 in the test phase. The remaining models namely RF (AUC ROC = 0.89), LR (AUC ROC = 0.80), SVM (AUC ROC = 0.81), DT (AUC ROC = 0.86), and ANN (AUC ROC = 0.78), also displayed commendable performance. The novelty of this research is its innovative approach to combat gully erosion through cutting edge ML models, offering practical solutions for watershed conservation, sustainable management, and the prevention of land degradation. These insights are invaluable for addressing the challenges posed by gully erosion within the region, and beyond its geographical boundaries and can be used for defining appropriate mitigation strategies at local to national scale.","<method>SVM</method>, <method>KNN</method>, <method>RF</method>, <method>XGBoost</method>, <method>ANN</method>, <method>DT</method>, <method>LR</method>"
2024,https://openalex.org/W4390576295,Biology,Dynamic-Memory Event-Triggered Sliding-Mode Secure Control for Nonlinear Semi-Markov Jump Systems With Stochastic Cyber Attacks,"This paper presents an investigation of the sliding-mode secure control for nonlinear semi-Markov jump systems in the presence of non-periodic denial-of-service attacks and false data injection attacks. First of all, we employ Takagi-Sugeno fuzzy model to describe the nonlinear semi-Markov jump control systems. In order to optimize transmission efficiency and control performance, a novel dynamic-memory event-triggered mechanism is developed by incorporating auxiliary dynamic variable and historical transmitted data. Then, a memory-based fuzzy sliding surface is put forward to attenuate the influences of stochastic cyber attacks with the aid of event-triggered state information. Moreover, by utilizing Lyapunov stability theory, sufficient conditions are derived to guarantee the exponentially mean-square stability of the system with an <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""> <tex-math notation=""LaTeX"">$H_{\infty}$</tex-math> </inline-formula> performance index, even in the cases of generally uncertain and unknown transition rates. Furthermore, a memory-based sliding mode secure controller is designed to ensure the reachability of the predefined switching surface and desirable sliding motion within finite time. Finally, the efficacy of the proposed control scheme is demonstrated through a tunnel diode circuit model. <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Note to Practitioners</i> —This study focuses on the issue of secure control for nonlinear semi-Markov jump systems, which holds practical significance across various domains, including applications in robotic manipulators, circuit models, and DC motors. We broaden the scope by considering more general jump parameter matrices to align more closely with the real-world system environment. Moreover, networked environments pose two primary challenges: network bandwidth constraints and the external network attacks. To tackle these issues, this paper introduces an innovative dynamic memory event triggering mechanism to enhance network transmission efficiency and optimize communication resource utilization. Meanwhile, to address cyber attacks resulting from the inherent openness of networks, the current study adopts a defensive sliding mode control strategy to provide robust protection against network attacks and disturbances. The effectiveness of the suggested approach is confirmed through a circuit system model. It is worth mentioning that the proposed method has the potential for broader application within real-world engineering scenarios, particularly those involving network-based nonlinear systems with various practical constraints.",No methods found.
2024,https://openalex.org/W4390703358,Biology,A Deep Bidirectional LSTM Model Enhanced by Transfer-Learning-Based Feature Extraction for Dynamic Human Activity Recognition,"Dynamic human activity recognition (HAR) is a domain of study that is currently receiving considerable attention within the fields of computer vision and pattern recognition. The growing need for artificial-intelligence (AI)-driven systems to evaluate human behaviour and bolster security underscores the timeliness of this research. Despite the strides made by numerous researchers in developing dynamic HAR frameworks utilizing diverse pre-trained architectures for feature extraction and classification, persisting challenges include suboptimal performance accuracy and the computational intricacies inherent in existing systems. These challenges arise due to the vast video-based datasets and the inherent similarity in the data. To address these challenges, we propose an innovative, dynamic HAR technique employing a deep-learning-based, deep bidirectional long short-term memory (Deep BiLSTM) model facilitated by a pre-trained transfer-learning-based feature-extraction approach. Our approach begins with the utilization of Convolutional Neural Network (CNN) models, specifically MobileNetV2, for extracting deep-level features from video frames. Subsequently, these features are fed into an optimized deep bidirectional long short-term memory (Deep BiLSTM) network to discern dependencies and process data, enabling optimal predictions. During the testing phase, an iterative fine-tuning procedure is introduced to update the high parameters of the trained model, ensuring adaptability to varying scenarios. The proposed model’s efficacy was rigorously evaluated using three benchmark datasets, namely UCF11, UCF Sport, and JHMDB, achieving notable accuracies of 99.20%, 93.3%, and 76.30%, respectively. This high-performance accuracy substantiates the superiority of our proposed model, signaling a promising advancement in the domain of activity recognition.","<method>deep bidirectional long short-term memory (Deep BiLSTM)</method>, <method>pre-trained transfer-learning-based feature-extraction approach</method>, <method>Convolutional Neural Network (CNN)</method>, <method>MobileNetV2</method>"
2024,https://openalex.org/W4390881001,Biology,A multimodal graph neural network framework for cancer molecular subtype classification,"Abstract Background The recent development of high-throughput sequencing has created a large collection of multi-omics data, which enables researchers to better investigate cancer molecular profiles and cancer taxonomy based on molecular subtypes. Integrating multi-omics data has been proven to be effective for building more precise classification models. Most current multi-omics integrative models use either an early fusion in the form of concatenation or late fusion with a separate feature extractor for each omic, which are mainly based on deep neural networks. Due to the nature of biological systems, graphs are a better structural representation of bio-medical data. Although few graph neural network (GNN) based multi-omics integrative methods have been proposed, they suffer from three common disadvantages. One is most of them use only one type of connection, either inter-omics or intra-omic connection; second, they only consider one kind of GNN layer, either graph convolution network (GCN) or graph attention network (GAT); and third, most of these methods have not been tested on a more complex classification task, such as cancer molecular subtypes. Results In this study, we propose a novel end-to-end multi-omics GNN framework for accurate and robust cancer subtype classification. The proposed model utilizes multi-omics data in the form of heterogeneous multi-layer graphs, which combine both inter-omics and intra-omic connections from established biological knowledge. The proposed model incorporates learned graph features and global genome features for accurate classification. We tested the proposed model on the Cancer Genome Atlas (TCGA) Pan-cancer dataset and TCGA breast invasive carcinoma (BRCA) dataset for molecular subtype and cancer subtype classification, respectively. The proposed model shows superior performance compared to four current state-of-the-art baseline models in terms of accuracy, F1 score, precision, and recall. The comparative analysis of GAT-based models and GCN-based models reveals that GAT-based models are preferred for smaller graphs with less information and GCN-based models are preferred for larger graphs with extra information.","<method>deep neural networks</method>, <method>graph neural network (GNN)</method>, <method>graph convolution network (GCN)</method>, <method>graph attention network (GAT)</method>"
2024,https://openalex.org/W4390953734,Biology,DIGITAL MARKETING IN THE AGE OF IOT: A REVIEW OF TRENDS AND IMPACTS,"This paper presents a comprehensive review of the evolving landscape of digital marketing in the era of the Internet of Things (IoT). As IoT technology continues to permeate various aspects of daily life, its influence on digital marketing strategies and consumer engagement has become a pivotal area of study. The aim of this review is to elucidate the trends and impacts of digital marketing in the context of IoT, offering insights into how this integration is reshaping marketing practices. Employing a systematic methodology, we analyzed current literature and case studies to identify key trends in IoT-driven digital marketing. Our study reveals that IoT offers unprecedented opportunities for marketers to gather real-time data, enabling more personalized and context-aware marketing strategies. We discuss how IoT devices, such as smart home appliances, wearables, and connected vehicles, are becoming new channels for digital marketing, allowing for more direct and interactive consumer engagement. The paper also delves into the challenges and ethical considerations that arise with IoT in digital marketing, including privacy concerns and data security. We highlight the need for robust regulatory frameworks to safeguard consumer interests in this rapidly evolving landscape. Our findings indicate that IoT is not only enhancing the efficiency and effectiveness of digital marketing but also transforming the consumer experience. This integration is leading to more dynamic, responsive, and personalized marketing approaches, fundamentally altering the relationship between brands and consumers. The paper concludes with recommendations for future research and practical implications for marketers aiming to leverage IoT in their digital strategies.&#x0D; Keywords: Digital Marketing, Internet of Things (IoT), Consumer Engagement, Data-Driven Strategies, IoT Technologies, Marketing Innovations.",No methods found.
2024,https://openalex.org/W4391280050,Biology,Benchmarking of computational methods for m6A profiling with Nanopore direct RNA sequencing,"Abstract N6-methyladenosine (m6A) is the most abundant internal eukaryotic mRNA modification, and is involved in the regulation of various biological processes. Direct Nanopore sequencing of native RNA (dRNA-seq) emerged as a leading approach for its identification. Several software were published for m6A detection and there is a strong need for independent studies benchmarking their performance on data from different species, and against various reference datasets. Moreover, a computational workflow is needed to streamline the execution of tools whose installation and execution remains complicated. We developed NanOlympicsMod, a Nextflow pipeline exploiting containerized technology for comparing 14 tools for m6A detection on dRNA-seq data. NanOlympicsMod was tested on dRNA-seq data generated from in vitro (un)modified synthetic oligos. The m6A hits returned by each tool were compared to the m6A position known by design of the oligos. In addition, NanOlympicsMod was used on dRNA-seq datasets from wild-type and m6A-depleted yeast, mouse and human, and each tool’s hits were compared to reference m6A sets generated by leading orthogonal methods. The performance of the tools markedly differed across datasets, and methods adopting different approaches showed different preferences in terms of precision and recall. Changing the stringency cut-offs allowed for tuning the precision-recall trade-off towards user preferences. Finally, we determined that precision and recall of tools are markedly influenced by sequencing depth, and that additional sequencing would likely reveal additional m6A sites. Thanks to the possibility of including novel tools, NanOlympicsMod will streamline the benchmarking of m6A detection tools on dRNA-seq data, improving future RNA modification characterization.",No methods found.
2024,https://openalex.org/W4392450360,Biology,Geographically weighted machine learning for modeling spatial heterogeneity in traffic crash frequency and determinants in US,"Spatial analyses of traffic crashes have drawn much interest due to the nature of the spatial dependence and spatial heterogeneity in the crash data. This study makes the best of Geographically Weighted Random Forest (GW-RF) model to explore the local associations between crash frequency and various influencing factors in the US, including road network attributes, socio-economic characteristics, and land use factors collected from multiple data sources. Special emphasis is put on modeling the spatial heterogeneity in the effects of a factor on crash frequency in different geographical areas in a data-driven way. The GW-RF model outperforms global models (e.g. Random Forest) and conventional geographically weighted regression, demonstrating superior predictive accuracy and elucidating spatial variations. The GW-RF model reveals spatial distinctions in the effects of certain factors on crash frequency. For example, the importance of intersection density varies significantly across regions, with high significance in the southern and northeastern areas. Low-grade road density emerges as influential in specific cities. The findings highlight the significance of different factors in influencing crash frequency across zones. Road network factors, particularly intersection density, exhibit high importance universally, while socioeconomic variables demonstrate moderate effects. Interestingly, land use variables show relatively lower importance. The outcomes could help to allocate resources and implement tailored interventions to reduce the likelihood of crashes.","<method>Geographically Weighted Random Forest (GW-RF)</method>, <method>Random Forest</method>, <method>geographically weighted regression</method>"
2024,https://openalex.org/W4399649641,Biology,An artificial intelligence-assisted microfluidic colorimetric wearable sensor system for monitoring of key tear biomarkers,"Abstract The precise, simultaneous, and rapid detection of essential biomarkers in human tears is imperative for monitoring both ocular and systemic health. The utilization of a wearable colorimetric biochemical sensor exhibits potential in achieving swift and concurrent detection of pivotal biomarkers in tears. Nevertheless, challenges arise in the collection, interpretation, and sharing of data from the colorimetric sensor, thereby restricting the practical implementation of this technology. To overcome these challenges, this research introduces an artificial intelligence-assisted wearable microfluidic colorimetric sensor system (AI-WMCS) for rapid, non-invasive, and simultaneous detection of key biomarkers in human tears, including vitamin C, H + (pH), Ca 2+ , and proteins. The sensor consists of a flexible microfluidic epidermal patch that collects tears and facilitates the colorimetric reaction, and a deep-learning neural network-based cloud server data analysis system (CSDAS) embedded in a smartphone enabling color data acquisition, interpretation, auto-correction, and display. To enhance accuracy, a well-trained multichannel convolutional recurrent neural network (CNN-GRU) corrects errors in the interpreted concentration data caused by varying pH and color temperature in different measurements. The test set determination coefficients (R 2 ) of 1D-CNN-GRU for predicting pH and 3D-CNN-GRU for predicting the other three biomarkers were as high as 0.998 and 0.994, respectively. This correction significantly improves the accuracy of the predicted concentration, enabling accurate, simultaneous, and quick detection of four critical tear biomarkers using only minute amounts of tears ( ~ 20 μL). This research demonstrates the powerful integration of a flexible microfluidic colorimetric biosensor and deep-learning algorithm, which holds immense potential to revolutionize the fields of health monitoring.","<method>deep-learning neural network-based cloud server data analysis system (CSDAS)</method>, <method>multichannel convolutional recurrent neural network (CNN-GRU)</method>, <method>1D-CNN-GRU</method>, <method>3D-CNN-GRU</method>"
2024,https://openalex.org/W4391071022,Biology,Weaving a greener future: The impact of green human resources management and green supply chain management on sustainable performance in Bangladesh's textile industry,"The purpose of this study is to investigate the impact of Green Human Resource Management (GHRM) and Green Supply Chain Management (GSCM) on the sustainable performance of the Bangladeshi textile sector. Specifically, the study focuses on environmental and employee-related aspects. Additionally, we examine how environmental performance and employee performance mediate the relationship between GHRM and GSCM. This study draws upon data collected from 450 employees across various textile enterprises in Bangladesh. Structural Equation Modeling is employed using the Amos 24 software to analyze the relationships and interactions among these variables. These findings demonstrate that using environmentally sustainable practices in human resource management and supply chain management results in enhanced sustainability. The study indicates that environmental performance significantly influences the relationship between GHRM and GSCM regarding sustainable performance. The study findings indicate that firms operating in the textile industry should implement GHRM and GSCM practices to enhance their sustainability performance. Additionally, it is recommended that these organizations prioritize the well-being and engagement of their employees. Implementing such a strategy can bolster the organization's comprehensive sustainability initiatives and raise its standing among stakeholders. This study contributes to the expanding body of literature on textile sustainability by investigating the mediating role of employee and environmental performance. It emphasizes the significance of GHRM and GSCM techniques in improving sustainable performance. The findings provide valuable insights for firms seeking to develop more effective sustainability initiatives.",<method>Structural Equation Modeling</method>
2024,https://openalex.org/W4391243967,Biology,Reviews and syntheses: Remotely sensed optical time series for monitoring vegetation productivity,"Abstract. Vegetation productivity is a critical indicator of global ecosystem health and is impacted by human activities and climate change. A wide range of optical sensing platforms, from ground-based to airborne and satellite, provide spatially continuous information on terrestrial vegetation status and functioning. As optical Earth observation (EO) data are usually routinely acquired, vegetation can be monitored repeatedly over time, reflecting seasonal vegetation patterns and trends in vegetation productivity metrics. Such metrics include gross primary productivity, net primary productivity, biomass, or yield. To summarize current knowledge, in this paper we systematically reviewed time series (TS) literature for assessing state-of-the-art vegetation productivity monitoring approaches for different ecosystems based on optical remote sensing (RS) data. As the integration of solar-induced fluorescence (SIF) data in vegetation productivity processing chains has emerged as a promising source, we also include this relatively recent sensor modality. We define three methodological categories to derive productivity metrics from remotely sensed TS of vegetation indices or quantitative traits: (i) trend analysis and anomaly detection, (ii) land surface phenology, and (iii) integration and assimilation of TS-derived metrics into statistical and process-based dynamic vegetation models (DVMs). Although the majority of used TS data streams originate from data acquired from satellite platforms, TS data from aircraft and unoccupied aerial vehicles have found their way into productivity monitoring studies. To facilitate processing, we provide a list of common toolboxes for inferring productivity metrics and information from TS data. We further discuss validation strategies of the RS data derived productivity metrics: (1) using in situ measured data, such as yield; (2) sensor networks of distinct sensors, including spectroradiometers, flux towers, or phenological cameras; and (3) inter-comparison of different productivity metrics. Finally, we address current challenges and propose a conceptual framework for productivity metrics derivation, including fully integrated DVMs and radiative transfer models here labelled as “Digital Twin”. This novel framework meets the requirements of multiple ecosystems and enables both an improved understanding of vegetation temporal dynamics in response to climate and environmental drivers and enhances the accuracy of vegetation productivity monitoring.","<method>trend analysis and anomaly detection</method>, <method>land surface phenology</method>, <method>integration and assimilation of time series-derived metrics into statistical and process-based dynamic vegetation models (DVMs)</method>, <method>fully integrated dynamic vegetation models (DVMs) and radiative transfer models (“Digital Twin” framework)</method>"
2024,https://openalex.org/W4391665000,Biology,A comprehensive review of critical analysis of biodegradable waste PCM for thermal energy storage systems using machine learning and deep learning to predict dynamic behavior,"This article explores the use of phase change materials (PCMs) derived from waste, in energy storage systems. It emphasizes the potential of these PCMs in addressing concerns related to fossil fuel usage and environmental impact. This article also highlights the aspects of these PCMs including reduced reliance on renewable resources minimized greenhouse gas emissions and waste reduction. The study also discusses approaches such as integrating nanotechnology to enhance thermal conductivity and utilizing machine learning and deep learning techniques for predicting dynamic behavior. The article provides an overall view of research on biodegradable waste-based PCMs and how they can play a promising role in achieving energy-efficient and sustainable thermal storage systems. However, specific conclusions drawn from the presented results are not explicitly outlined, leaving room, for investigation and exploration in this evolving field. Artificial neural network (ANN) predictive models for thermal energy storage devices perform differently. With a 4% adjusted mean absolute error, the Gaussian radial basis function kernel Support Vector Regression (SVR) model captured heat-related charging and discharging issues. The ANN model predicted finned tube heat and heat flux better than the numerical model. SVM models outperformed ANN and ANFIS in some datasets. Material property predictions favored gradient boosting, but Linear Regression and SVR models performed better, emphasizing application- and dataset-specific model selection. These predictive models provide insights into the complex thermal performance of building structures, aiding in the design and operation of energy-efficient systems. Biodegradable waste-based PCMs' sustainability includes carbon footprint, waste reduction, biodegradability, and circular economy alignment. Nanotechnology, machine learning, and deep learning improve thermal conductivity and prediction. Circular economy principles include waste reduction and carbon footprint reduction. Specific results-based conclusions are not stated. Presenting a comprehensive overview of current research highlights biodegradable waste-based PCMs' potential for energy-efficient and sustainable thermal storage systems.","<method>machine learning</method>, <method>deep learning</method>, <method>Artificial neural network (ANN)</method>, <method>Gaussian radial basis function kernel Support Vector Regression (SVR)</method>, <method>Support Vector Machine (SVM)</method>, <method>Adaptive Neuro-Fuzzy Inference System (ANFIS)</method>, <method>gradient boosting</method>, <method>Linear Regression</method>"
2024,https://openalex.org/W4392112102,Biology,Coupling Deep Learning and Physically Based Hydrological Models for Monthly Streamflow Predictions,"Abstract This study proposes a new hybrid model for monthly streamflow predictions by coupling a physically based distributed hydrological model with a deep learning (DL) model. Specifically, a simplified hydrological model is first developed by optimally selecting grid cells from a distributed hydrological model according to their soil moisture characteristics. It is then driven by bias corrected general circulation model (GCM) predictions to generate soil moistures for the forecasting months. Finally, model‐simulated soil moisture along with other predictors from multiple sources are used as inputs of the DL model to predict future monthly streamflows. The proposed hybrid model, using the simplified Variable Infiltration Capacity (VIC) as the hydrological model and the combination of Convolutional Neural Network and Gated Recurrent Unit (CNN‐GRU) as the DL model, is applied to predict 1‐, 3‐, and 6‐month ahead reservoir inflows for the Danjiangkou Reservoir in China. The results show that the hybrid model consistently performs better than VIC and CNN‐GRU models with great improvement in Kling‐Gupta efficiency (KGE) values for lead times up to 6 months. Additional tests indicate that hybrid models based on CNN‐GRU outperform those based on LASSO, XGBoost, CNN, and GRU models. Moreover, compared with the distributed hydrological model, the hybrid model greatly reduces the computation burden of rolling prediction. It also saves decision‐makers the time and effort of trying different combinations of predictors, which is indispensable when building DL models. Overall, the new hybrid model demonstrates great potential for monthly streamflow prediction where training data are limited.","<method>deep learning (DL) model</method>, <method>Convolutional Neural Network (CNN)</method>, <method>Gated Recurrent Unit (GRU)</method>, <method>CNN-GRU</method>, <method>LASSO</method>, <method>XGBoost</method>, <method>CNN</method>, <method>GRU</method>"
2024,https://openalex.org/W4392124217,Biology,A structural equation modeling framework for exploring the industry 5.0 and sustainable supply chain determinants,"Sustainable Supply Chain and Industry 5.0 are two important concepts reshaping how businesses operate in the modern world. Together, these two concepts drive the advancement of a highly sustainable and robust worldwide economy. Companies are now becoming more sustainable in supply chain management, using technologies like blockchain and co-bots to track the origin of goods, ensure ethical and sustainable sourcing, and work with humans safely and effectively. This study develops a theoretical model highlighting the determinants of Industry 5.0, Sustainable Supply Chain Practices, by combining theoretical frameworks from the manufacturing, supply chain, and information systems literature. The study's analytic sample comprises 342 responses collected from professionals working in the electronics industry's supply chain. Hypotheses were constructed employing deductive reasoning, leveraging insights gleaned from prior research. The study is conducted utilizing the Structural Equation Modeling (SEM) to substantiate the presumed connections among various constructs, namely, Industry 5.0 innovations, Sustainable Supply Chain Practices (SSCP), Sustainable Supply Chain Performance (SCP), and Supply Chain Risks (SCR). The Structural Equation Modeling analysis results show a direct impact of Industry 5.0 technologies through Sustainable Supply Chain Practices can enhance Supply Chain Performance and mitigate Supply Chain Risks. Combining the two paradigms can foster the development of new business models that prioritize sustainability and contribute to a more equitable and environmentally friendly economy that brings positive change for both businesses and society.",<method>Structural Equation Modeling (SEM)</method>
2024,https://openalex.org/W4400061043,Biology,MTMol-GPT: De novo multi-target molecular generation with transformer-based generative adversarial imitation learning,"De novo drug design is crucial in advancing drug discovery, which aims to generate new drugs with specific pharmacological properties. Recently, deep generative models have achieved inspiring progress in generating drug-like compounds. However, the models prioritize a single target drug generation for pharmacological intervention, neglecting the complicated inherent mechanisms of diseases, and influenced by multiple factors. Consequently, developing novel multi-target drugs that simultaneously target specific targets can enhance anti-tumor efficacy and address issues related to resistance mechanisms. To address this issue and inspired by Generative Pre-trained Transformers (GPT) models, we propose an upgraded GPT model with generative adversarial imitation learning for multi-target molecular generation called MTMol-GPT. The multi-target molecular generator employs a dual discriminator model using the Inverse Reinforcement Learning (IRL) method for a concurrently multi-target molecular generation. Extensive results show that MTMol-GPT generates various valid, novel, and effective multi-target molecules for various complex diseases, demonstrating robustness and generalization capability. In addition, molecular docking and pharmacophore mapping experiments demonstrate the drug-likeness properties and effectiveness of generated molecules potentially improve neuropsychiatric interventions. Furthermore, our model’s generalizability is exemplified by a case study focusing on the multi-targeted drug design for breast cancer. As a broadly applicable solution for multiple targets, MTMol-GPT provides new insight into future directions to enhance potential complex disease therapeutics by generating high-quality multi-target molecules in drug discovery.","<method>Generative Pre-trained Transformers (GPT)</method>, <method>generative adversarial imitation learning</method>, <method>Inverse Reinforcement Learning (IRL)</method>"
2024,https://openalex.org/W4401209403,Biology,AI-Driven Deep Learning Techniques in Protein Structure Prediction,"Protein structure prediction is important for understanding their function and behavior. This review study presents a comprehensive review of the computational models used in predicting protein structure. It covers the progression from established protein modeling to state-of-the-art artificial intelligence (AI) frameworks. The paper will start with a brief introduction to protein structures, protein modeling, and AI. The section on established protein modeling will discuss homology modeling, ab initio modeling, and threading. The next section is deep learning-based models. It introduces some state-of-the-art AI models, such as AlphaFold (AlphaFold, AlphaFold2, AlphaFold3), RoseTTAFold, ProteinBERT, etc. This section also discusses how AI techniques have been integrated into established frameworks like Swiss-Model, Rosetta, and I-TASSER. The model performance is compared using the rankings of CASP14 (Critical Assessment of Structure Prediction) and CASP15. CASP16 is ongoing, and its results are not included in this review. Continuous Automated Model EvaluatiOn (CAMEO) complements the biennial CASP experiment. Template modeling score (TM-score), global distance test total score (GDT_TS), and Local Distance Difference Test (lDDT) score are discussed too. This paper then acknowledges the ongoing difficulties in predicting protein structure and emphasizes the necessity of additional searches like dynamic protein behavior, conformational changes, and protein-protein interactions. In the application section, this paper introduces some applications in various fields like drug design, industry, education, and novel protein development. In summary, this paper provides a comprehensive overview of the latest advancements in established protein modeling and deep learning-based models for protein structure predictions. It emphasizes the significant advancements achieved by AI and identifies potential areas for further investigation.","<method>homology modeling</method>, <method>ab initio modeling</method>, <method>threading</method>, <method>AlphaFold</method>, <method>AlphaFold2</method>, <method>AlphaFold3</method>, <method>RoseTTAFold</method>, <method>ProteinBERT</method>"
2024,https://openalex.org/W4403839497,Biology,When combinations of humans and AI are useful: A systematic review and meta-analysis,"Abstract Inspired by the increasing use of artificial intelligence (AI) to augment humans, researchers have studied human–AI systems involving different tasks, systems and populations. Despite such a large body of work, we lack a broad conceptual understanding of when combinations of humans and AI are better than either alone. Here we addressed this question by conducting a preregistered systematic review and meta-analysis of 106 experimental studies reporting 370 effect sizes. We searched an interdisciplinary set of databases (the Association for Computing Machinery Digital Library, the Web of Science and the Association for Information Systems eLibrary) for studies published between 1 January 2020 and 30 June 2023. Each study was required to include an original human-participants experiment that evaluated the performance of humans alone, AI alone and human–AI combinations. First, we found that, on average, human–AI combinations performed significantly worse than the best of humans or AI alone (Hedges’ g = −0.23; 95% confidence interval, −0.39 to −0.07). Second, we found performance losses in tasks that involved making decisions and significantly greater gains in tasks that involved creating content. Finally, when humans outperformed AI alone, we found performance gains in the combination, but when AI outperformed humans alone, we found losses. Limitations of the evidence assessed here include possible publication bias and variations in the study designs analysed. Overall, these findings highlight the heterogeneity of the effects of human–AI collaboration and point to promising avenues for improving human–AI systems.",No methods found.
2024,https://openalex.org/W4390660035,Biology,An Empirical Study on Correlations Between Deep Neural Network Fairness and Neuron Coverage Criteria,"Recently, with the widespread use of deep neural networks (DNNs) in high-stakes decision-making systems (such as fraud detection and prison sentencing), concerns have arisen about the fairness of DNNs in terms of the potential negative impact they may have on individuals and society. Therefore, fairness testing has become an important research topic in DNN testing. At the same time, the neural network coverage criteria (such as criteria based on neuronal activation) is considered as an adequacy test for DNN white-box testing. It is implicitly assumed that improving the coverage can enhance the quality of test suites. Nevertheless, the correlation between DNN fairness (a test property) and coverage criteria (a test method) has not been adequately explored. To address this issue, we conducted a systematic empirical study on seven coverage criteria, six fairness metrics, three fairness testing techniques, and five bias mitigation methods on five DNN models and nine fairness datasets to assess the correlation between coverage criteria and DNN fairness. Our study achieved the following findings: 1) with the increase in the size of the test suite, some of the coverage and fairness metrics changed significantly, as the size of the test suite increased; 2) the statistical correlation between coverage criteria and DNN fairness is limited; and 3) after bias mitigation for improving the fairness of DNN, the change pattern in coverage criteria is different; 4) Models debiased by different bias mitigation methods have a lower correlation between coverage and fairness compared to the original models. Our findings cast doubt on the validity of coverage criteria concerning DNN fairness (i.e., increasing the coverage may even have a negative impact on the fairness of DNNs). Therefore, we warn DNN testers against blindly pursuing higher coverage of coverage criteria at the cost of test properties of DNNs (such as fairness).","<method>deep neural networks (DNNs)</method>, <method>neural network coverage criteria</method>, <method>fairness testing techniques</method>, <method>bias mitigation methods</method>"
2024,https://openalex.org/W4391097037,Biology,An Early and Smart Detection of Corn Plant Leaf Diseases Using IoT and Deep Learning Multi-Models,"Plant leaf diseases have various causes, leading to severe disorders. The early and accurate detection and classification of these diseases are fundamental for fostering healthy crop production. In recent years, smart agricultural systems have garnered significant attention due to their capability to enhance efficiency by deploying sensor networks and Internet of Things (IoT) devices that collect and analyze environmental data. However, traditional plant disease detection methods are manual, time-consuming, and often need help handling the data's complexity and dynamism. These manual methods do not use heterogeneous data to make better decisions. Corn holds significant importance yet it faces numerous diseases that include main three diseases such as blight, common rust, and grey leaf spot. The advancement of computer technology has led to a pivotal focus on corn leaf diseases classification application based on deep learning. Convolutional Neural Networks (CNNs) have revealed remarkable achievements within Precision Agriculture (PA) due to their ability to enhance information. To this end, this work introduces a CNN-based architecture, the Multi-Model Fusion Network (MMF-Net). Its primary objective is to classify diseases within the realm of PA. MMF-Net integrates multi-contextual features using RL-block and PL-blocks 1 & 2, thus effectively combining different model streams trained on heterogeneous data. The RL-block uses spatial range to process coarse grained images to convolve the local context, while PL-block 1 extracts fine-grained global context by expanding the perceptual area of images. PL-block 2 deals with real-life environmental parameters as features. The extracted features are syndicated using multiple classifiers that ensemble three individual blocks at the decision level to improve the accuracy. After fusion, it uses adaptively the majority voting scheme to generate the final decision probability score of the base model. Multiple experiments are conducted involving the corn leaf diseases dataset and a real-life numerical dataset, generating an impressive 99.23% accuracy in the classification of corn leaf diseases. Overall, MMF-Net provides a promising and smart solution to identify plant leaf diseases in PA effectively.","<method>Convolutional Neural Networks (CNNs)</method>, <method>Multi-Model Fusion Network (MMF-Net)</method>, <method>RL-block</method>, <method>PL-block 1</method>, <method>PL-block 2</method>, <method>multiple classifiers ensemble</method>, <method>majority voting scheme</method>"
2024,https://openalex.org/W4391708122,Biology,Application of machine learning approaches in supporting irrigation decision making: A review,"Irrigation decision-making has evolved from solely depending on farmers' decisions taken based on the visual analysis of field conditions to making decisions based on crop water need predictions generated using machine learning (ML) techniques. This paper reviews ML related articles to discuss how ML has been used to enhance irrigation decision making. We reviewed 16 studies that used ML approaches for irrigation scheduling prediction and decision-making focusing on the input features, algorithms used and their applicability in real world conditions. ML performances in terms of accuracy, water conservation compared to fixed or threshold-based methods are discussed along with modeling performances. Informed by the 16 research studies, we assessed constraints to the adoption of ML in irrigation decision making at field scale, which include limited data availability coupled with data sharing constraints, and a lack of uncertainty quantification as well as the need for physics informed ML based irrigation scheduling models. To address these limitations, we discussed approaches in future research such as integrating process-based models with ML, incorporating expert knowledge into the modeling procedure, and making data and tools Findable, Accessible, Interoperable, and Reusable (FAIR). These approaches will improve ML modeling outcomes and boost the availability of farm-related data and tools for FAIRer data-driven applications of irrigation modeling.","<method>machine learning (ML) techniques</method>, <method>ML approaches</method>, <method>process-based models integrated with ML</method>, <method>physics informed ML based irrigation scheduling models</method>"
2024,https://openalex.org/W4392359418,Biology,Construction and optimization of watershed-scale ecological network based on complex network method: A case study of Erhai Lake Basin in China,"The ecological network construction and optimization are of great significance in ensuring regional ecological security and optimizing the ecological space of the national territory, therefore constructing the ecological spatial network and proposing optimization countermeasures are conducive to enhancing regional ecological stability. However, current research on ecological networks ignores the ecospatial community structure and the topology characteristics of ecological networks, and lacks a systematic optimization framework. The research scope of the ecological network primarily concentrates on urban administrative units, with less emphasis on the geographic scale of watersheds. This approach is not conducive to the comprehensive management of all elements of ecosystems. Therefore, this research took Erhai Lake Basin as an object, adopted Morphological Spatial Pattern Analysis (MSPA) and landscape connectivity to extract ecological sources, simulate corridors and identify the weak points through the model of Minimum Cumulative Resistance (MCR) and gravity model, constructed the ecological network of Erhai Lake Basin, topologized the ecological network by using the Gephi platform. Based on the results of the analysis of complex network indicators, the optimization strategy of increasing edges was proposed. Priority conservation areas were further identified, an ecological security pattern was designed, and an ecological restoration strategy was planned. Results show that 28 ecological sources, 378 potential ecological corridors, 48 important ecological corridors and 86 ecological weak points formed the complex ecological network in Erhai Lake Basin. The network had clear clustering characteristics and instability, with a high degree concentration in the northeast and uneven betweenness centrality, especially higher in the east. Through topology analysis, 12 increased edge nodes were identified, 9 increased edges were simulated, and 26 weak points were added, significantly improving the network robustness. Based on the ecological security pattern, the restoration strategy with strict control around the lake, conservation of barrier belts and management of priority areas were designed. This study implemented increased edge optimization based on complex network analysis to enhance the stability of the network, which provides a theoretical basis for the optimization of the spatial pattern of Erhai Lake Basin, and is a useful exploration of the ecologically fragile watersheds to protect the environment and achieve high-quality sustainable development.","<method>Morphological Spatial Pattern Analysis (MSPA)</method>, <method>Minimum Cumulative Resistance (MCR) model</method>, <method>gravity model</method>, <method>complex network analysis</method>"
2024,https://openalex.org/W4392404413,Biology,Investigating the Impact of Train / Test Split Ratio on the Performance of Pre-Trained Models with Custom Datasets,"The proper allocation of data between training and testing is a critical factor influencing the performance of deep learning models, especially those built upon pre-trained architectures. Having the suitable training set size is an important factor for the classification model’s generalization performance. The main goal of this study is to find the appropriate training set size for three pre-trained networks using different custom datasets. For this aim, the study presented in this paper explores the effect of varying the train / test split ratio on the performance of three popular pre-trained models, namely MobileNetV2, ResNet50v2 and VGG19, with a focus on image classification task. In this work, three balanced datasets never seen by the models have been used, each containing 1000 images divided into two classes. The train / test split ratios used for this study are: 60-40, 70-30, 80-20 and 90-10. The focus was on the critical metrics of sensitivity, specificity and overall accuracy to evaluate the performance of the classifiers under the different ratios. Experimental results show that, the performance of the classifiers is affected by varying the training / testing split ratio for the three custom datasets. Moreover, with the three pre-trained models, using more than 70% of the dataset images for the training task gives better performance.","<method>MobileNetV2</method>, <method>ResNet50v2</method>, <method>VGG19</method>"
2024,https://openalex.org/W4393044095,Biology,"Comparative performance analysis of Boruta, SHAP, and Borutashap for disease diagnosis: A study with multiple machine learning algorithms","Interpretable machine learning models are instrumental in disease diagnosis and clinical decision-making, shedding light on relevant features. Notably, Boruta, SHAP (SHapley Additive exPlanations), and BorutaShap were employed for feature selection, each contributing to the identification of crucial features. These selected features were then utilized to train six machine learning algorithms, including LR, SVM, ETC, AdaBoost, RF, and LR, using diverse medical datasets obtained from public sources after rigorous preprocessing. The performance of each feature selection technique was evaluated across multiple ML models, assessing accuracy, precision, recall, and F1-score metrics. Among these, SHAP showcased superior performance, achieving average accuracies of 80.17%, 85.13%, 90.00%, and 99.55% across diabetes, cardiovascular, statlog, and thyroid disease datasets, respectively. Notably, the LGBM emerged as the most effective algorithm, boasting an average accuracy of 91.00% for most disease states. Moreover, SHAP enhanced the interpretability of the models, providing valuable insights into the underlying mechanisms driving disease diagnosis. This comprehensive study contributes significant insights into feature selection techniques and machine learning algorithms for disease diagnosis, benefiting researchers and practitioners in the medical field. Further exploration of feature selection methods and algorithms holds promise for advancing disease diagnosis methodologies, paving the way for more accurate and interpretable diagnostic models.","<method>Boruta</method>, <method>SHAP (SHapley Additive exPlanations)</method>, <method>BorutaShap</method>, <method>LR</method>, <method>SVM</method>, <method>ETC</method>, <method>AdaBoost</method>, <method>RF</method>, <method>LGBM</method>"
2024,https://openalex.org/W4395479913,Biology,Machinability investigation of natural fibers reinforced polymer matrix composite under drilling: Leveraging machine learning in bioengineering applications,"The growing demand for fiber-reinforced polymer (FRP) in industrial applications has prompted the exploration of natural fiber-based composites as a viable alternative to synthetic fibers. Using jute–rattan fiber-reinforced composite offers the potential for environmentally sustainable waste material decomposition and cost reduction compared to conventional fiber materials. This article focuses on the impact of different machining constraints on surface roughness and delamination during the drilling process of the jute–rattan FRP composite. Inspired by this unexplored research area, this article emphasizes the influence of various machining constraints on surface roughness and delamination in drilling jute–rattan FRP composite. Response surface methodology designs the experiment using drill bit material, spindle speed, and feed rate as input variables to measure surface roughness and delamination factors. The technique of order of preference by similarity to the ideal solution method is used to optimize the machining parameters, and for predicting surface roughness and delamination, two machine learning-based models named random forest (RF) and support vector machine (SVM) are utilized. To evaluate the accuracy of the predicted values, the correlation coefficient (R2), mean absolute percentage error, and mean squared error were used. RF performed better in comparison with SVM, with a higher value of R2 for both testing and training datasets, which is 0.997, 0.981, and 0.985 for surface roughness, entry delamination, and exit delamination, respectively. Hence, this study presents an innovative methodology for predicting surface roughness and delamination through machine learning techniques.","<method>random forest (RF)</method>, <method>support vector machine (SVM)</method>"
2024,https://openalex.org/W4403085522,Biology,Baryon Acoustic Oscillation Theory and Modelling Systematics for the DESI 2024 results,"ABSTRACT This paper provides a comprehensive overview of how fitting of baryon acoustic oscillations (BAO) is carried out within the upcoming Dark Energy Spectroscopic Instrument’s (DESI) 2024 results using its DR1 data set, and the associated systematic error budget from theory and modelling of the BAO. We derive new results showing how non-linearities in the clustering of galaxies can cause potential biases in measurements of the isotropic ($\alpha _{\mathrm{iso}}$) and anisotropic ($\alpha _{\mathrm{ap}}$) BAO distance scales, and how these can be effectively removed with an appropriate choice of reconstruction algorithm. We then demonstrate how theory leads to a clear choice for how to model the BAO and develop, implement, and validate a new model for the remaining smooth-broad-band (i.e. without BAO) component of the galaxy clustering. Finally, we explore the impact of all remaining modelling choices on the BAO constraints from DESI using a suite of high-precision simulations, arriving at a set of best practices for DESI BAO fits, and an associated theory and modelling systematic error. Overall, our results demonstrate the remarkable robustness of the BAO to all our modelling choices and motivate a combined theory and modelling systematic error contribution to the post-reconstruction DESI BAO measurements of no more than 0.1 per cent (0.2 per cent) for its isotropic (anisotropic) distance measurements. We expect the theory and best practices laid out to here to be applicable to other BAO experiments in the era of DESI and beyond.",No methods found.
2024,https://openalex.org/W4403158403,Biology,Artificial intelligence alphafold model for molecular biology and drug discovery: a machine-learning-driven informatics investigation,"AlphaFold model has reshaped biological research. However, vast unstructured data in the entire AlphaFold field requires further analysis to fully understand the current research landscape and guide future exploration. Thus, this scientometric analysis aimed to identify critical research clusters, track emerging trends, and highlight underexplored areas in this field by utilizing machine-learning-driven informatics methods. Quantitative statistical analysis reveals that the AlphaFold field is enjoying an astonishing development trend (Annual Growth Rate = 180.13%) and global collaboration (International Co-authorship = 33.33%). Unsupervised clustering algorithm, time series tracking, and global impact assessment point out that Cluster 3 (Artificial Intelligence-Powered Advancements in AlphaFold for Structural Biology) has the greatest influence (Average Citation = 48.36 ± 184.98). Additionally, regression curve and hotspot burst analysis highlight ""structure prediction"" (s = 12.40, R2 = 0.9480, p = 0.0051), ""artificial intelligence"" (s = 5.00, R2 = 0.8096, p = 0.0375), ""drug discovery"" (s = 1.90, R2 = 0.7987, p = 0.0409), and ""molecular dynamics"" (s = 2.40, R2 = 0.8000, p = 0.0405) as core hotspots driving the research frontier. More importantly, the Walktrap algorithm further reveals that ""structure prediction, artificial intelligence, molecular dynamics"" (Relevance Percentage[RP] = 100%, Development Percentage[DP] = 25.0%), ""sars-cov-2, covid-19, vaccine design"" (RP = 97.8%, DP = 37.5%), and ""homology modeling, virtual screening, membrane protein"" (RP = 89.9%, DP = 26.1%) are closely intertwined with the AlphaFold model but remain underexplored, which implies a broad exploration space. In conclusion, through the machine-learning-driven informatics methods, this scientometric analysis offers an objective and comprehensive overview of global AlphaFold research, identifying critical research clusters and hotspots while prospectively pointing out underexplored critical areas.","<method>unsupervised clustering algorithm</method>, <method>Walktrap algorithm</method>"
2024,https://openalex.org/W4391202196,Biology,Screening of miRNAs as prognostic biomarkers and their associated hub targets across Hepatocellular carcinoma using survival-based bioinformatics approach,"The hepatocellular carcinoma (HCC) incident rate is gradually increasing yearly despite all the research and efforts taken by scientific communities and governing bodies. Approximately 90% of all liver cancer cases belong to HCC. Usually, HCC patients approach the treatment in the late stages of this malignancy which becomes the primary cause of high mortality rate. The knowledge about molecular pathogenesis of HCC is limited and needs more attention from researchers to identify the driver genes and miRNAs, which causes to translate this information into clinical practice. Therefore, the key regulators identification of miRNA-mRNA regulatory network is essential to identify HCC-associated genes. We extracted microRNA (miRNA) and messenger RNA (mRNA) expression datasets of normal and tumor HCC patient samples from UCSC Xena followed by identifying differentially expressed genes (DEGs) and differentially expressed miRNAs (DEMs). Univariate and multivariate cox-proportional hazard models were utilized to identify DEMs having significant association with overall survival (OS). Kaplan-Meier (KM) plotter was used to validate the presence of prognostic DEMs. A risk-score model was used to evaluate the effectiveness of KM-plotter validated DEMs combination on risk of samples. Target DEGs of prognostic miRNAs were identified via sources such as miRTargetLink and miRWalk followed by their validation in an external microarray cohort and enrichment analysis. 562 DEGs and 388 DEMs were identified followed by seven prognostic miRNAs (i.e., miR-19a, miR-19b, miR-30d-5p, miR-424-5p, miR-3677-5p, miR-3913-5p, miR-7705) post univariate, multivariate, risk-score model evaluation and KM-plotter analyses. ANLN, MRO, CPEB3 were their targets and were also validated in GSE84005 dataset. The findings of this study decipher that most significant miRNAs and their identified target genes have association with apoptosis, inflammation, cell cycle regulation and cancer-related pathways, which appear to contribute to HCC pathogenesis and therefore, the discovery of new targets.","<method>univariate cox-proportional hazard model</method>, <method>multivariate cox-proportional hazard model</method>, <method>Kaplan-Meier (KM) plotter</method>, <method>risk-score model</method>"
2024,https://openalex.org/W4391475057,Biology,Leadership and Environmental Sustainability: An Integrative Conceptual Model of Multilevel Antecedents and Consequences of Leader Green Behavior,"Environmental sustainability is a strategic and ethical imperative for organizations, and numerous studies have investigated associations between leadership and employee pro-environmental or “green” behavior. However, these studies have typically focused on leadership styles that conflate leader behavior with its assumed antecedents or consequences. Moreover, the literature on relations between leadership and environmental sustainability constructs is fragmented and in need of systematic integration to effectively guide future research and practice. Accordingly, we pursue three goals in this conceptual paper. First, after a brief review of key insights from extant theoretical and empirical research, we define leadership in the context of environmental sustainability and leader green behavior based on established theoretical frameworks. Second, based on a systematic integration and extension of the literature, we present an integrative conceptual model of multilevel antecedents and consequences of leader green behavior. We further develop eight propositions on multiple known and novel pathways toward leader and follower green behaviors, as well as multiple known and novel pathways toward consequences related to environmental sustainability at the leader, follower, and organizational levels. Finally, based on our integrative conceptual model and propositions, we outline several recommendations for future research on leadership and environmental sustainability, including theoretical and methodological considerations.",No methods found.
2024,https://openalex.org/W4392351452,Biology,Response of soil erosion to vegetation and terrace changes in a small watershed on the Loess Plateau over the past 85 years,"Land use on the Chinese Loess Plateau has undergone dramatic changes over the past few decades. The implementation of a series of soil and water conservation measures has significantly altered the soil erosion, transportation, and deposition processes on the Loess Plateau. To effectively address and mitigate soil erosion, it is crucial to accurately quantify the soil loss rate and analyze the contributions of soil and water conservation measures over the past several decades. In this study, the Zhifanggou watershed, located in the hilly area of the Chinese Loess Plateau, is utilized as an illustrative example. Using historical data, remote sensing imagery, and on-site data of soil, vegetation, and soil conservation measures, we assessed the soil loss rates from 1938 to 2022 based on long-term land use changes, utilizing the Chinese Soil Loss Equation (CSLE) model. Furthermore, we employed a quantitative evaluation to assess the impacts of vegetation change and terrace construction on soil erosion. The findings of our study reveal significant transformations in land use. Farmland experienced an initial increase followed by a subsequent decline, while the opposite pattern was observed for forest land. The simulated soil loss rate for the entire watershed exhibited an upward trend, rising from 34.86 t·ha−1·yr−1 in 1938 to 104.11 t·ha−1·yr−1 in 1958, before declining to 56.98 t·ha−1·yr−1 in 1999 and reaching 5.87 t·ha−1·yr−1 in 2022. Attribution analysis showed that vegetation change exerted a dominant influence on recent soil erosion dynamics, accounting for 86.10 % of the total contribution in 2022, while terraces contributed 13.90 %. These findings clarify long-term soil erosion mechanisms and provide guidance for watershed soil and water conservation management.",No methods found.
2024,https://openalex.org/W4392499245,Biology,Exploring the role of skin temperature in thermal sensation and thermal comfort: A comprehensive review,"The role of skin temperature as a determinant of human thermal sensation and comfort has gained increasing recognition, prompting a need for a systematic review. This review examines the relationship between skin temperature and thermal sensation, synthesizing insights from 172 studies published since 2000. It uniquely focuses on the indispensable roles of local and mean skin temperatures, a perspective not comprehensively explored in previous literature. The review reveals that the most common measurement points for skin temperature are the face and hands, attributed to their higher thermal sensitivity and the practical ease of measurement. It establishes a clear linear relationship between mean skin temperature and user thermal sensation, though affected by the choice of measurement locations and number of points. A notable finding is the varying impact of local skin temperature on overall thermal sensation in changing environments, with local heating less influential than cooling. The review also uncovers significant demographic variations in thermal sensation, strongly influenced by differing skin temperatures across age groups, genders, and climatic regions. For example, elderly populations exhibit a decreased temperature sensitivity, especially towards warmth. Gender differences are also significant, with females experiencing higher skin temperatures in warmer environments and lower in colder ones. Machine learning (ML)-based methods, especially classification tree-based and support vector machine (SVM) techniques, dominate in predicting thermal sensation and comfort, leveraging skin temperature data. While ML methods are prevalent, statistical regression-based approaches offer valuable empirical insights. Thermo-physiological model-based methods provide reliable results by incorporating detailed skin temperature dynamics. The review identifies a gap in understanding how gender, age, and regional differences influence thermal comfort in diverse environments. The study recommends conducting more nuanced experiments to dissect the impact of these factors and proposes the integration of individual demographic variables into ML models to personalize thermal comfort predictions.","<method>classification tree-based</method>, <method>support vector machine (SVM)</method>, <method>statistical regression-based approaches</method>, <method>thermo-physiological model-based methods</method>"
2024,https://openalex.org/W4393380917,Biology,Motif-Aware miRNA-Disease Association Prediction via Hierarchical Attention Network,"As post-transcriptional regulators of gene expression, micro-ribonucleic acids (miRNAs) are regarded as potential biomarkers for a variety of diseases. Hence, the prediction of miRNA-disease associations (MDAs) is of great significance for an in-depth understanding of disease pathogenesis and progression. Existing prediction models are mainly concentrated on incorporating different sources of biological information to perform the MDA prediction task while failing to consider the fully potential utility of MDA network information at the motif-level. To overcome this problem, we propose a novel motif-aware MDA prediction model, namely MotifMDA, by fusing a variety of high- and low-order structural information. In particular, we first design several motifs of interest considering their ability to characterize how miRNAs are associated with diseases through different network structural patterns. Then, MotifMDA adopts a two-layer hierarchical attention to identify novel MDAs. Specifically, the first attention layer learns high-order motif preferences based on their occurrences in the given MDA network, while the second one learns the final embeddings of miRNAs and diseases through coupling high- and low-order preferences. Experimental results on two benchmark datasets have demonstrated the superior performance of MotifMDA over several state-of-the-art prediction models. This strongly indicates that accurate MDA prediction can be achieved by relying solely on MDA network information. Furthermore, our case studies indicate that the incorporation of motif-level structure information allows MotifMDA to discover novel MDAs from different perspectives. The data and codes are available at <uri xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">https://github.com/stevejobws/MotifMDA.</uri>","<method>motif-aware MDA prediction model (MotifMDA)</method>, <method>two-layer hierarchical attention</method>"
2024,https://openalex.org/W4394011823,Biology,"Artificial intelligence in lung cancer screening: Detection, classification, prediction, and prognosis","Abstract Background The exceptional capabilities of artificial intelligence (AI) in extracting image information and processing complex models have led to its recognition across various medical fields. With the continuous evolution of AI technologies based on deep learning, particularly the advent of convolutional neural networks (CNNs), AI presents an expanded horizon of applications in lung cancer screening, including lung segmentation, nodule detection, false‐positive reduction, nodule classification, and prognosis. Methodology This review initially analyzes the current status of AI technologies. It then explores the applications of AI in lung cancer screening, including lung segmentation, nodule detection, and classification, and assesses the potential of AI in enhancing the sensitivity of nodule detection and reducing false‐positive rates. Finally, it addresses the challenges and future directions of AI in lung cancer screening. Results AI holds substantial prospects in lung cancer screening. It demonstrates significant potential in improving nodule detection sensitivity, reducing false‐positive rates, and classifying nodules, while also showing value in predicting nodule growth and pathological/genetic typing. Conclusions AI offers a promising supportive approach to lung cancer screening, presenting considerable potential in enhancing nodule detection sensitivity, reducing false‐positive rates, and classifying nodules. However, the universality and interpretability of AI results need further enhancement. Future research should focus on the large‐scale validation of new deep learning‐based algorithms and multi‐center studies to improve the efficacy of AI in lung cancer screening.","<method>artificial intelligence (AI)</method>, <method>deep learning</method>, <method>convolutional neural networks (CNNs)</method>"
2024,https://openalex.org/W4394753955,Biology,Distributed Hydrological Modeling With Physics‐Encoded Deep Learning: A General Framework and Its Application in the Amazon,"Abstract While deep learning (DL) models exhibit superior simulation accuracy over traditional distributed hydrological models (DHMs), their main limitations lie in opacity and the absence of underlying physical mechanisms. The pursuit of synergies between DL and DHMs is an engaging research domain, yet a definitive roadmap remains elusive. In this study, a novel framework that seamlessly integrates a process‐based hydrological model encoded as a neural network (NN), an additional NN for mapping spatially distributed and physically meaningful parameters from watershed attributes, and NN‐based replacement models representing inadequately understood processes is developed. Multi‐source observations are used as training data, and the framework is fully differentiable, enabling fast parameter tuning by backpropagation. A hybrid DL model of the Amazon Basin (∼6 × 10 6 km 2 ) was established based on the framework, and HydroPy, a global‐scale DHM, was encoded as its physical backbone. Trained simultaneously with streamflow observations and Gravity Recovery and Climate Experiment satellite data, the hybrid model yielded median Nash‐Sutcliffe efficiencies of 0.83 and 0.77 for dynamic and distributed simulations of streamflow and total water storage, respectively, 41% and 35% higher than those of the original HydroPy model. Replacing the original Penman‒Monteith formulation in HydroPy with a replacement NN produces more plausible potential evapotranspiration (PET) estimates, and unravels the spatial pattern of PET in this giant basin. The NN used for parameterization was interpreted to identify the factors controlling the spatial variability in key parameters. Overall, this study lays out a feasible technical roadmap for distributed hydrological modeling in the big data era.","<method>deep learning (DL) models</method>, <method>neural network (NN)</method>, <method>NN-based replacement models</method>, <method>backpropagation</method>, <method>hybrid DL model</method>"
2024,https://openalex.org/W4395069357,Biology,Characterizing land use/land cover change dynamics by an enhanced random forest machine learning model: a Google Earth Engine implementation,"Abstract Land use and land cover (LULC) analysis is crucial for understanding societal development and assessing changes during the Anthropocene era. Conventional LULC mapping faces challenges in capturing changes under cloud cover and limited ground truth data. To enhance the accuracy and comprehensiveness of the descriptions of LULC changes, this investigation employed a combination of advanced techniques. Specifically, multitemporal 30 m resolution Landsat-8 satellite imagery was utilized, in addition to the cloud computing capabilities of the Google Earth Engine (GEE) platform. Additionally, the study incorporated the random forest (RF) algorithm. This study aimed to generate continuous LULC maps for 2014 and 2020 for the Shrirampur area of Maharashtra, India. A novel multiple composite RF approach based on LULC classification was utilized to generate the final LULC classification maps utilizing the RF-50 and RF-100 tree models. Both RF models utilized seven input bands (B1 to B7) as the dataset for LULC classification. By incorporating these bands, the models were able to influence the spectral information captured by each band to classify the LULC categories accurately. The inclusion of multiple bands enhanced the discrimination capabilities of the classifiers, increasing the comprehensiveness of the assessment of the LULC classes. The analysis indicated that RF-100 exhibited higher training and validation/testing accuracy for 2014 and 2020 (0.99 and 0.79/0.80, respectively). The study further revealed that agricultural land, built-up land, and water bodies have changed adequately and have undergone substantial variation among the LULC classes in the study area. Overall, this research provides novel insights into the application of machine learning (ML) models for LULC mapping and emphasizes the importance of selecting the optimal tree combination for enhancing the accuracy and reliability of LULC maps based on the GEE and different RF tree models. The present investigation further enabled the interpretation of pixel-level LULC interactions while improving image classification accuracy and suggested the best models for the classification of LULC maps through the identification of changes in LULC classes.","<method>random forest (RF) algorithm</method>, <method>multiple composite RF approach</method>, <method>RF-50 tree model</method>, <method>RF-100 tree model</method>"
2024,https://openalex.org/W4395954533,Biology,DeepKEGG: a multi-omics data integration framework with biological insights for cancer recurrence prediction and biomarker discovery,"Abstract Deep learning-based multi-omics data integration methods have the capability to reveal the mechanisms of cancer development, discover cancer biomarkers and identify pathogenic targets. However, current methods ignore the potential correlations between samples in integrating multi-omics data. In addition, providing accurate biological explanations still poses significant challenges due to the complexity of deep learning models. Therefore, there is an urgent need for a deep learning-based multi-omics integration method to explore the potential correlations between samples and provide model interpretability. Herein, we propose a novel interpretable multi-omics data integration method (DeepKEGG) for cancer recurrence prediction and biomarker discovery. In DeepKEGG, a biological hierarchical module is designed for local connections of neuron nodes and model interpretability based on the biological relationship between genes/miRNAs and pathways. In addition, a pathway self-attention module is constructed to explore the correlation between different samples and generate the potential pathway feature representation for enhancing the prediction performance of the model. Lastly, an attribution-based feature importance calculation method is utilized to discover biomarkers related to cancer recurrence and provide a biological interpretation of the model. Experimental results demonstrate that DeepKEGG outperforms other state-of-the-art methods in 5-fold cross validation. Furthermore, case studies also indicate that DeepKEGG serves as an effective tool for biomarker discovery. The code is available at https://github.com/lanbiolab/DeepKEGG.","<method>deep learning-based multi-omics data integration methods</method>, <method>DeepKEGG</method>, <method>biological hierarchical module</method>, <method>pathway self-attention module</method>, <method>attribution-based feature importance calculation method</method>"
2024,https://openalex.org/W4400724905,Biology,Hyperspectral Image Analysis and Machine Learning Techniques for Crop Disease Detection and Identification: A Review,"Originally, the use of hyperspectral images was for military applications, but their use has been extended to precision agriculture. In particular, they are used for activities related to crop classification or disease detection, combining these hyperspectral images with machine learning techniques and algorithms. The study of hyperspectral images has a wide range of wavelengths for observation. These wavelengths allow for monitoring agricultural crops such as cereals, oilseeds, vegetables, and fruits, and other applications. In the ranges of these wavelengths, crop conditions such as maturity index and nutrient status, or the early detection of some diseases that cause losses in crops, can be studied and diagnosed. Therefore, this article proposes a technical review of the main applications of hyperspectral images in agricultural crops and perspectives and challenges that combine artificial intelligence algorithms such as machine learning and deep learning in the classification and detection of diseases of crops such as cereals, oilseeds, fruits, and vegetables. A systematic review of the scientific literature was carried out using a 10-year observation window to determine the evolution of the integration of these technological tools that support sustainable agriculture; among the findings, information on the most documented crops is highlighted, among which are some cereals and citrus fruits due to their high demand and large cultivation areas, as well as information on the main fruits and vegetables that are integrating these technologies. Also, the main artificial intelligence algorithms that are being worked on are summarized and classified, as well as the wavelength ranges for the prediction, disease detection, and analysis of other tasks of physiological characteristics used for sustainable production. This review can be useful as a reference for future research, based mainly on detection, classification, and other tasks in agricultural crops and decision making, to implement the most appropriate artificial intelligence algorithms.","<method>machine learning</method>, <method>deep learning</method>, <method>artificial intelligence algorithms</method>"
2024,https://openalex.org/W4403248109,Biology,Optimizing cancer classification: a hybrid RDO-XGBoost approach for feature selection and predictive insights,"The identification of relevant biomarkers from high-dimensional cancer data remains a significant challenge due to the complexity and heterogeneity inherent in various cancer types. Conventional feature selection methods often struggle to effectively navigate the vast solution space while maintaining high predictive accuracy. In response to these challenges, we introduce a novel feature selection approach that integrates Random Drift Optimization (RDO) with XGBoost, specifically designed to enhance the performance of cancer classification tasks. Our proposed framework not only improves classification accuracy but also offers valuable insights into the underlying biological mechanisms driving cancer progression. Through comprehensive experiments conducted on real-world cancer datasets, including Central Nervous System (CNS), Leukemia, Breast, and Ovarian cancers, we demonstrate the efficacy of our method in identifying a smaller subset of unique and relevant genes. This selection results in significantly improved classification efficiency and accuracy. When compared with popular classifiers such as Support Vector Machine, K-Nearest Neighbor, and Naive Bayes, our approach consistently outperforms these models in terms of both accuracy and F-measure metrics. For instance, our framework achieved an accuracy of 97.24% in the CNS dataset, 99.14% in Leukemia, 95.21% in Ovarian, and 87.62% in Breast cancer, showcasing its robustness and effectiveness across different types of cancer data. These results underline the potential of our RDO-XGBoost framework as a promising solution for feature selection in cancer data analysis, offering enhanced predictive performance and valuable biological insights.","<method>Random Drift Optimization (RDO)</method>, <method>XGBoost</method>, <method>Support Vector Machine</method>, <method>K-Nearest Neighbor</method>, <method>Naive Bayes</method>"
2024,https://openalex.org/W4390604872,Biology,Cross-Domain Class Incremental Broad Network for Continuous Diagnosis of Rotating Machinery Faults Under Variable Operating Conditions,"Machine learning models have been widely successful in the field of intelligent fault diagnosis. Most of the existing machine learning models are deployed in static environments and rely on precollected datasets for offline training, which makes it impossible to update the models further once they are established. However, in the open and dynamic environment in reality, there is always incoming data in the form of streams, including new categories of data that are constantly generated over time. In addition, the operating conditions of mechanical equipment are time-varying, which results in continuous stream data that are nonindependently and homogeneously distributed. In industrial applications, the diagnosis problem of nonindependent and identically distributed continuous streaming data is referred to as the cross-domain class incremental diagnosis problem. To address the cross-domain class incremental problem, a novel cross-domain class incremental broad network (CDCIBN) is proposed. Specifically, to solve the nonindependent identically distributed problem, a novel domain-adaptation learning loss function is first designed, which enables the conventional broad network to handle the category increment task well. Then, a cross-domain class incremental learning mechanism is designed, which learns new categories while retaining the knowledge of old categories well enough without replaying old category data. The effectiveness of the proposed method is evaluated through multiple mechanical failure increment cases. Experimental analysis demonstrates that the designed CDCIBN has significant advantages in the variable working condition class incremental application.","<method>cross-domain class incremental broad network (CDCIBN)</method>, <method>domain-adaptation learning loss function</method>, <method>cross-domain class incremental learning mechanism</method>"
2024,https://openalex.org/W4390686423,Biology,Real-life data-driven model predictive control for building energy systems comparing different machine learning models,"By considering forecasts and exploiting storage effects, model predictive control can achieve significant energy and cost savings in the building sector. However, due to the high individual modeling effort, model predictive control lacks practical applicability. For that reason, data-driven process models, approximating the system behavior based on measurements, have become increasingly popular in recent years. Still, scientific literature lacks consent about the most promising model types and efficient workflows to integrate different machine learning models into a model predictive controller. With this work, we present a workflow to provide efficient model predictive controllers based on measurement data automatically. The main idea is to translate different machine learning models into optimization syntax to enable efficient optimization with full access to gradients. We currently consider artificial neural networks, gaussian process regression, and simple linear regression process models. We use a generic model ontology to automatize the controller generation further and test the methodology on two real-life use cases. The first use case is the application of five office rooms with smart thermostat valves. The second use case is a test hall with an air handling unit and a concrete core activation. Using only two days of initial training data, we deploy controllers based on the different model types for six weeks in the offices and apply online learning to improve the models continuously. We observe only minor differences in controller performance despite the artificial neural networks showing the highest prediction accuracy. The second use case shows that the simple linear models require less controller tuning effort. Thus, for practical applications, we recommend linear regression models.","<method>model predictive control</method>, <method>artificial neural networks</method>, <method>gaussian process regression</method>, <method>linear regression</method>"
2024,https://openalex.org/W4390688548,Biology,Understanding the acceptance of business intelligence from healthcare professionals’ perspective: an empirical study of healthcare organizations,"Purpose Due to its ability to support well-informed decision-making, business intelligence (BI) has grown in popularity among executives across a range of industries. However, given the volume of data collected in health-care organizations, there is a lack of exploration concerning its implementation. Consequently, this research paper aims to investigate the key factors affecting the acceptance and use of BI in healthcare organizations. Design/methodology/approach Leveraging the theoretical lens of the “unified theory of acceptance and use of technology” (UTAUT), a study framework was proposed and integrated with three context-related factors, including “rational decision-making culture” (RDC), “perceived threat to professional autonomy” (PTA) and “medical–legal risk” (MLR). The variables in the study framework were categorized as follows: information systems (IS) perspective; organizational perspective; and user perspective. In Jordan, 434 healthcare professionals participated in a cross-sectional online survey that was used to collect data. Findings The findings of the “structural equation modeling” revealed that professionals’ behavioral intentions toward using BI systems were significantly affected by performance expectancy, social influence, facilitating conditions, MLR, RDC and PTA. Also, an insignificant effect of PTA on PE was found based on the results of statistical analysis. These variables explained 68% of the variance ( R 2 ) in the individuals’ intentions to use BI-based health-care systems. Practical implications To promote the acceptance and use of BI technology in health-care settings, developers, designers, service providers and decision-makers will find this study to have a number of practical implications. Additionally, it will support the development of effective strategies and BI-based health-care systems based on these study results, attracting the interest of many users. Originality/value To the best of the author’s knowledge, this is one of the first studies that integrates the UTAUT model with three contextual factors (RDC, PTA and MLR) in addition to examining the suggested framework in a developing nation (Jordan). This study is one of the few in which the users’ acceptance behavior of BI systems was investigated in a health-care setting. More specifically, to the best of the author’s knowledge, this is the first study that reveals the critical antecedents of individuals’ intention to accept BI for health-care purposes in the Jordanian context.",No methods found.
2024,https://openalex.org/W4391658582,Biology,Interpretation issues with “genomic vulnerability” arise from conceptual issues in local adaptation and maladaptation,"Abstract As climate change causes the environment to shift away from the local optimum that populations have adapted to, fitness declines are predicted to occur. Recently, methods known as genomic offsets (GOs) have become a popular tool to predict population responses to climate change from landscape genomic data. Populations with a high GO have been interpreted to have a high “genomic vulnerability” to climate change. GOs are often implicitly interpreted as a fitness offset, or a change in fitness of an individual or population in a new environment compared to a reference. However, there are several different types of fitness offset that can be calculated, and the appropriate choice depends on the management goals. This study uses hypothetical and empirical data to explore situations in which different types of fitness offsets may or may not be correlated with each other or with a GO. The examples reveal that even when GOs predict fitness offsets in a common garden experiment, this does not necessarily validate their ability to predict fitness offsets to environmental change. Conceptual examples are also used to show how a large GO can arise under a positive fitness offset, and thus cannot be interpreted as a population vulnerability. These issues can be resolved with robust validation experiments that can evaluate which fitness offsets are correlated with GOs.",No methods found.
2024,https://openalex.org/W4391684052,Biology,Enhancing MPPT performance for partially shaded photovoltaic arrays through backstepping control with Genetic Algorithm-optimized gains,"As the significance and complexity of solar panel performance, particularly at their maximum power point (MPP), continue to grow, there is a demand for improved monitoring systems. The presence of variable weather conditions in Maroua, including potential partial shadowing caused by cloud cover or urban buildings, poses challenges to the efficiency of solar systems. This study introduces a new approach to tracking the Global Maximum Power Point (GMPP) in photovoltaic systems within the context of solar research conducted in Cameroon. The system utilizes Genetic Algorithm (GA) and Backstepping Controller (BSC) methodologies. The Backstepping Controller (BSC) dynamically adjusts the duty cycle of the Single Ended Primary Inductor Converter (SEPIC) to align with the reference voltage of the Genetic Algorithm (GA) in Maroua's dynamic environment. This environment, characterized by intermittent sunlight and the impact of local factors and urban shadowing, affects the production of energy. The Genetic Algorithm is employed to enhance the efficiency of BSC gains in Maroua's solar environment. This optimization technique expedites the tracking process and minimizes oscillations in the GMPP. The adaptability of the learning algorithm to specific conditions improves energy generation, even in the challenging environment of Maroua. This study introduces a novel approach to enhance the efficiency of photovoltaic systems in Maroua, Cameroon, by tailoring them to the specific solar dynamics of the region. In terms of performance, our approach surpasses the INC-BSC, P&O-BSC, GA-BSC, and PSO-BSC methodologies. In practice, the stabilization period following shadowing typically requires fewer than three iterations. Additionally, our Maximum Power Point Tracking (MPPT) technology is based on the Global Maximum Power Point (GMPP) methodology, contrasting with alternative technologies that prioritize the Local Maximum Power Point (LMPP). This differentiation is particularly relevant in areas with partial shading, such as Maroua, where the use of LMPP-based technologies can result in power losses. The proposed method demonstrates significant performance by achieving a minimum 33% reduction in power losses.","<method>Genetic Algorithm (GA)</method>, <method>Backstepping Controller (BSC)</method>, <method>INC-BSC</method>, <method>P&O-BSC</method>, <method>GA-BSC</method>, <method>PSO-BSC</method>"
2024,https://openalex.org/W4391995887,Biology,"funspace: An R package to build, analyse and plot functional trait spaces","Abstract Aim Functional trait space analyses are pivotal to describe and compare organisms' functional diversity across the tree of life. Yet, there is no single application that streamlines the many sometimes‐troublesome steps needed to build and analyse functional trait spaces. Innovation To fill this gap, we propose funspace , an R package to easily handle bivariate and multivariate functional trait space analyses. The six functions that constitute the package can be grouped in three modules: ‘Building and exploring’, ‘Mapping’ and ‘Plotting’. The building and exploring module defines the main features of a functional trait space (e.g. functional diversity metrics) by leveraging kernel density‐based methods. The mapping module uses general additive models to map how a target variable distributes within a trait space. The plotting module provides many options for creating flexible and publication‐ready figures representing the outputs obtained from previous modules. We provide a worked example to demonstrate a complete funspace workflow. Main Conclusions funspace will provide researchers working with functional traits across the tree of life with a new tool to easily explore: (i) the main features of any functional trait space, (ii) the relationship between a functional trait space and any other biological or non‐biological factor that might contribute to shaping species' functional diversity.","<method>kernel density-based methods</method>, <method>general additive models</method>"
2024,https://openalex.org/W4392111029,Biology,GAM-MDR: probing miRNA–drug resistance using a graph autoencoder based on random path masking,"Abstract MicroRNAs (miRNAs) are found ubiquitously in biological cells and play a pivotal role in regulating the expression of numerous target genes. Therapies centered around miRNAs are emerging as a promising strategy for disease treatment, aiming to intervene in disease progression by modulating abnormal miRNA expressions. The accurate prediction of miRNA–drug resistance (MDR) is crucial for the success of miRNA therapies. Computational models based on deep learning have demonstrated exceptional performance in predicting potential MDRs. However, their effectiveness can be compromised by errors in the data acquisition process, leading to inaccurate node representations. To address this challenge, we introduce the GAM-MDR model, which combines the graph autoencoder (GAE) with random path masking techniques to precisely predict potential MDRs. The reliability and effectiveness of the GAM-MDR model are mainly reflected in two aspects. Firstly, it efficiently extracts the representations of miRNA and drug nodes in the miRNA–drug network. Secondly, our designed random path masking strategy efficiently reconstructs critical paths in the network, thereby reducing the adverse impact of noisy data. To our knowledge, this is the first time that a random path masking strategy has been integrated into a GAE to infer MDRs. Our method was subjected to multiple validations on public datasets and yielded promising results. We are optimistic that our model could offer valuable insights for miRNA therapeutic strategies and deepen the understanding of the regulatory mechanisms of miRNAs. Our data and code are publicly available at GitHub:https://github.com/ZZCrazy00/GAM-MDR.","<method>deep learning</method>, <method>graph autoencoder (GAE)</method>, <method>random path masking</method>"
2024,https://openalex.org/W4399442306,Biology,Integrative analysis of AI-driven optimization in HIV treatment regimens,"The integration of artificial intelligence (AI) into HIV treatment regimens has revolutionized the approach to personalized care and optimization strategies. This study presents an in-depth analysis of the role of AI in transforming HIV treatment, focusing on its ability to tailor therapy to individual patient needs and enhance treatment outcomes. AI-driven optimization in HIV treatment involves the utilization of advanced algorithms and computational techniques to analyze vast amounts of patient data, including genetic information, viral load measurements, and treatment history. By harnessing the power of machine learning and predictive analytics, AI algorithms can identify patterns and trends in patient data that may not be readily apparent to human clinicians. One of the key benefits of AI-driven optimization is its ability to personalize treatment regimens based on individual patient characteristics and disease progression. By considering factors such as drug resistance profiles, comorbidities, and lifestyle factors, AI algorithms can recommend the most effective and well-tolerated treatment options for each patient, leading to improved adherence and clinical outcomes. Furthermore, AI enables continuous monitoring and adjustment of treatment regimens in real time, allowing healthcare providers to respond rapidly to changes in patient status and evolving viral dynamics. This proactive approach to HIV management can help prevent treatment failure and the development of drug resistance, ultimately leading to better long-term outcomes for patients. Despite its transformative potential, AI-driven optimization in HIV treatment is not without challenges. Ethical considerations, data privacy concerns, and the need for robust validation and regulatory oversight are all important factors that must be addressed to ensure the safe and effective implementation of AI algorithms in clinical practice. In conclusion, the integrative analysis presented in this study underscores the significant impact of AI-driven optimization on the personalization and optimization of HIV treatment regimens. By leveraging AI technologies, healthcare providers can tailor treatment approaches to individual patient needs, leading to improved outcomes and quality of life for people living with HIV. Keywords: Integrative Analysis, AI- Driven, Optimization, HIV Treatment, Regimens.","<method>machine learning</method>, <method>predictive analytics</method>"
2024,https://openalex.org/W4401415934,Biology,Short-Term Load Forecasting: A Comprehensive Review and Simulation Study With CNN-LSTM Hybrids Approach,"Short-term load forecasting (STLF) is vital in effectively managing the reserve requirement in modern power grids. Subsequently, it supports the grid operator in making effective and economical decisions during the power balancing operation. Therefore, this study comprehensively reviews STLF methods, including time series analysis, regression-based frameworks, artificial neural networks (ANNs), and hybrid models that employ different forecasting approaches. Detailed mathematical and graphical analyses and a comparative evaluation of these methods are provided, highlighting their advantages and disadvantages. Further, the study proposes a hybrid CNN-LSTM model comprised of Convolutional neural networks (CNN) for feature extraction of high dimensional data and Long short-term memory (LSTM) networks to boost the model's efficiency for temporal sequence prediction. This study assessed the model using a comprehensive dataset from Pakistan's NTDC national grid. The analysis revealed superior performance in short-term load prediction, achieving enhanced accuracy. For single-step forecasting, the model yielded an RMSE of 538.71, MAE of 371.97, and MAPE of 2.72. In 24-hour forecasting, it achieved an RMSE of 951.94, MAE of 656.35, and MAPE of 4.72 on the NTDC dataset. Moreover, the model has outperformed previous models in comparison using the AEP dataset, demonstrating its superiority in enhancing reserve management and balancing supply and demand in modern electricity networks.","<method>time series analysis</method>, <method>regression-based frameworks</method>, <method>artificial neural networks (ANNs)</method>, <method>hybrid models</method>, <method>Convolutional neural networks (CNN)</method>, <method>Long short-term memory (LSTM) networks</method>, <method>hybrid CNN-LSTM model</method>"
2024,https://openalex.org/W4402137675,Biology,A comprehensive review of model compression techniques in machine learning,"Abstract This paper critically examines model compression techniques within the machine learning (ML) domain, emphasizing their role in enhancing model efficiency for deployment in resource-constrained environments, such as mobile devices, edge computing, and Internet of Things (IoT) systems. By systematically exploring compression techniques and lightweight design architectures, it is provided a comprehensive understanding of their operational contexts and effectiveness. The synthesis of these strategies reveals a dynamic interplay between model performance and computational demand, highlighting the balance required for optimal application. As machine learning (ML) models grow increasingly complex and data-intensive, the demand for computational resources and memory has surged accordingly. This escalation presents significant challenges for the deployment of artificial intelligence (AI) systems in real-world applications, particularly where hardware capabilities are limited. Therefore, model compression techniques are not merely advantageous but essential for ensuring that these models can be utilized across various domains, maintaining high performance without prohibitive resource requirements. Furthermore, this review underscores the importance of model compression in sustainable artificial intelligence (AI) development. The introduction of hybrid methods, which combine multiple compression techniques, promises to deliver superior performance and efficiency. Additionally, the development of intelligent frameworks capable of selecting the most appropriate compression strategy based on specific application needs is crucial for advancing the field. The practical examples and engineering applications discussed demonstrate the real-world impact of these techniques. By optimizing the balance between model complexity and computational efficiency, model compression ensures that the advancements in AI technology remain sustainable and widely applicable. This comprehensive review thus contributes to the academic discourse and guides innovative solutions for efficient and responsible machine learning practices, paving the way for future advancements in the field. Graphical abstract","<method>model compression techniques</method>, <method>lightweight design architectures</method>, <method>hybrid methods</method>"
2024,https://openalex.org/W4390821680,Biology,Application of deep learning to fault diagnosis of rotating machineries,"Abstract Deep learning (DL) has attained remarkable achievements in diagnosing faults for rotary machineries. Capitalizing on the formidable learning capacity of DL, it has the potential to automate human labor and augment the efficiency of fault diagnosis in rotary machinery. These advantages have engendered escalating interest over the past decade. Although recent reviews of the literature have encapsulated the utilization of DL in diagnosing faults in rotating machinery, they no longer encompass the introduction of novel methodologies and emerging directions as DL methodologies continually evolve. Moreover, in practical application, novel issues and trajectories perpetually manifest, demanding a comprehensive exegesis. To rectify this lacuna, this article amalgamates current research trends and avant-garde methodologies while systematizing the utilization of anterior DL techniques. The evolution and extant status of DL in diagnosing faults for rotary machinery were delineated, with the intent of providing orientation for prospective research. Over the bygone decade, archetypal DL theory has empowered the diagnosis of faults in rotating machinery by directly establishing the nexus between mechanical data and fault conditions. In recent years, meta learning methods aimed at solving small sample scenarios and large model transformers aimed at mining big data features have both received widespread attention and development in the field of fault diagnosis of rotating machinery equipment. Although excellent results have been achieved in these two directions, there is no review and summary article yet, so it is necessary to update the review literature in the field of fault diagnosis of rotating machinery equipment. Lastly, predicated on a survey of the literature and the current developmental landscape, the challenges and prospective orientations of DL in rotary machinery fault diagnosis are presented.","<method>deep learning (DL)</method>, <method>meta learning methods</method>, <method>large model transformers</method>"
2024,https://openalex.org/W4390863669,Biology,Determining the financial performance of the firms in the Borsa Istanbul sustainability index: integrating multi criteria decision making methods with simulation,"Abstract Regardless of the industry in which a company operates, evaluating corporate performance is one of the most critical and vital processes; the most essential and prominent performance evaluation is related to financial performance. Appropriate performance analysis is complex and critical for decision-makers in different financial performance factors; thus, a methodological framework is needed to solve such complex decision problems. Therefore, this research aims to rank the companies included in the sustainability index (excluding banks) in Turkey by considering their financial performance. The criteria weights were determined using the full consistency method (FUCOM) by considering the evaluations of four experts. The firms were ranked using nine multi-criteria decision-making methods. The consensus among the nine rankings was ensured with the Copeland technique. The decision matrix includes financial ratios and the stock market performance of the firms; 100,000 FUCOM weights were created with random evaluations to validate the results. The results indicate that the most crucial criterion is the current ratio by considering expert evaluations. Weight simulation indicates that alternative 16 (alternative 21) is superior (inferior) to the other alternatives, even though the weights are determined with random evaluations. Ranking with expert evaluations is similar to the mean of the weight simulation results. The results demonstrate that the proposed framework can be performed as a basis for financial performance ranking.",No methods found.
2024,https://openalex.org/W4390970205,Biology,Empowering Cyberattack Identification in IoHT Networks With Neighborhood-Component-Based Improvised Long Short-Term Memory,"Cybersecurity has become an inevitable concern in the healthcare industry due to the rapid growth of the Internet of Health Things (IoHT). The IoHT is revolutionizing healthcare by enabling remote access to hospital equipment, real-time patient monitoring, and urgent alerts to patients and hospitals. However, the convenience of these systems also makes them vulnerable to cyberattacks, with hackers seeking to disrupt health services or extort money through ransomware attacks. Efficiently detecting multiple threats is a challenging task because IoHT generates large temporal data and system log information. In this paper, we propose time series classification models for the identification of potential cyberattacks in IoHT networks. First, we introduce Neighborhood Component Analysis (NCA) with modifications of the regularization parameter to select the vital input features. With the selected features, we propose two LSTM-based models: Directed Acyclic Graph-based Long Short-Term Memory (DAG-LSTM) and Projected Layer-based Long Short-Term Memory (PL-LSTM) for detecting cyberattacks. We evaluate the existing time series classification models (i.e., GRU, LSTM, and Bi-LSTM) and proposed models (i.e., DAG-LSTM and PL-LSTM) using real-world IoHT data. We also validate the models by applying a non-parametric statistical test, Friedman test. Our evaluation results show that the proposed DAG-LSTM achieves the highest accuracy with 99.89% training and 92.04% an average testing accuracy.","<method>Neighborhood Component Analysis (NCA)</method>, <method>Directed Acyclic Graph-based Long Short-Term Memory (DAG-LSTM)</method>, <method>Projected Layer-based Long Short-Term Memory (PL-LSTM)</method>, <method>Gated Recurrent Unit (GRU)</method>, <method>Long Short-Term Memory (LSTM)</method>, <method>Bidirectional Long Short-Term Memory (Bi-LSTM)</method>"
2024,https://openalex.org/W4391451930,Biology,Advancing real-time plant disease detection: A lightweight deep learning approach and novel dataset for pigeon pea crop,"Plant disease detection and early disease treatment are essential for sustainable crop production. Computer vision for crop science is overgrowing with the advancement in deep learning. Real time plant disease detection poses a challenge due to the unpredictable spread of diseases within the plant, environmental factors, and the scarcity of real field datasets. The proposed work systematically addresses these issues through three key components: (a) Collaboratively generating the novel pigeon pea image dataset from agricultural fields, in partnership with 20 Agricultural Research Centers (ARS) and governmental agencies spanning 18 Indian states. (b) The design of lightweight and high-performance models for real-time plant disease detection in resource-constrained devices. (c) The extraction of multiscale feature of plant diseases using Multi-kernel Depthwise separable Convolutions. The proposed lightweight Lite-MDC architecture uses the Multi-kernel Depthwise separable Convolutions (MDsConv). The MDsConv module captures spatial features across various scales while maintaining a lightweight design. It effectively extract multi-scale information to characterize plant diseases, accommodating their diverse scale. Proposed architectural approach significantly reduces computational complexity, employing only 2.2 million parameters, which is a 62% reduction compared to the standard VGG16 architecture. The proposed method outperforms the state-of-the-art networks such as InceptionV3, VGG16, ResNet50, DenseNet, MobileNet, MobileNetV3, NASNet, and EfficieNetB0 on the proposed pigeon pea dataset with 94.14% accuracy. Notably, the method achieves a 34 Frames Per Second (FPS) inference on an NVIDIA P100 GPU. Furthermore, its performance is validated across publicly available datasets, including the plant village dataset, Cassava, and apple leaf datasets, yielding 99.78%, 86.4%, and 97.2% accuracy, respectively. The Lite-MDC model exhibits the potential for real-time plant disease detection on resource-constrained edge devices such as Agriculture robots and drones.","<method>Multi-kernel Depthwise separable Convolutions (MDsConv)</method>, <method>Lite-MDC architecture</method>, <method>InceptionV3</method>, <method>VGG16</method>, <method>ResNet50</method>, <method>DenseNet</method>, <method>MobileNet</method>, <method>MobileNetV3</method>, <method>NASNet</method>, <method>EfficientNetB0</method>"
2024,https://openalex.org/W4391456824,Biology,Deep Learning-Based Mask Identification System Using ResNet Transfer Learning Architecture,"Recently, the coronavirus disease 2019 has shown excellent attention in the global community regarding health and the economy.World Health Organization (WHO) and many others advised controlling Corona Virus Disease in 2019.The limited treatment resources, medical resources, and unawareness of immunity is an essential horizon to unfold.Among all resources, wearing a mask is the primary non-pharmaceutical intervention to stop the spreading of the virus caused by Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) droplets.All countries made masks mandatory to prevent infection.For such enforcement, automatic and effective face detection systems are crucial.This study presents a face mask identification approach for static photos and real-time movies that distinguishes between images with and without masks.To contribute to society, we worked on mask detection of an individual to adhere to the rule and provide awareness to the public or organization.The paper aims to get detection accuracy using transfer learning from Residual Neural Network 50 (ResNet-50) architecture and works on detection localization.The experiment is tested with other popular pre-trained models such as Deep Convolutional Neural Networks (AlexNet), Residual Neural Networks (ResNet), and Visual Geometry Group Networks (VGG-Net) advanced architecture.The proposed system generates an accuracy of 98.4% when modeled using Residual Neural Network 50 (ResNet-50).Also, the precision and recall values are proved as better when compared to the existing models.This outstanding work also can be used in video surveillance applications.","<method>transfer learning</method>, <method>Residual Neural Network 50 (ResNet-50)</method>, <method>Deep Convolutional Neural Networks (AlexNet)</method>, <method>Residual Neural Networks (ResNet)</method>, <method>Visual Geometry Group Networks (VGG-Net)</method>"
2024,https://openalex.org/W4391509831,Biology,A hyperspectral deep learning attention model for predicting lettuce chlorophyll content,"Abstract Background The phenotypic traits of leaves are the direct reflection of the agronomic traits in the growth process of leafy vegetables, which plays a vital role in the selection of high-quality leafy vegetable varieties. The current image-based phenotypic traits extraction research mainly focuses on the morphological and structural traits of plants or leaves, and there are few studies on the phenotypes of physiological traits of leaves. The current research has developed a deep learning model aimed at predicting the total chlorophyll of greenhouse lettuce directly from the full spectrum of hyperspectral images. Results A CNN-based one-dimensional deep learning model with spectral attention module was utilized for the estimate of the total chlorophyll of greenhouse lettuce from the full spectrum of hyperspectral images. Experimental results demonstrate that the deep neural network with spectral attention module outperformed the existing standard approaches, including partial least squares regression (PLSR) and random forest (RF), with an average R 2 of 0.746 and an average RMSE of 2.018. Conclusions This study unveils the capability of leveraging deep attention networks and hyperspectral imaging for estimating lettuce chlorophyll levels. This approach offers a convenient, non-destructive, and effective estimation method for the automatic monitoring and production management of leafy vegetables.","<method>CNN-based one-dimensional deep learning model with spectral attention module</method>, <method>partial least squares regression (PLSR)</method>, <method>random forest (RF)</method>"
2024,https://openalex.org/W4402061574,Biology,Abstractive Text Summarization Using GAN,"In the field of natural language processing, the task of writing long concepts into short expressions has attracted attention due to its ability to simplify the processing and understanding of information. While traditional transcription techniques are effective to some extent, they often fail to capture the essence and nuances of the original texts. This article explores a new approach to collecting abstract data using artificial neural networks (GANs), a class of deep learning models known for their ability to create patterns of real information. We describe the fundamentals of text collection through a comprehensive review of existing literature and methods and highlight the complexity of GAN-based text. Our goal is to transform complex text into context and meaning by combining the power of GANs with natural language understanding. We detail the design and training of an adaptive GAN model for the text recognition task. We also conduct various experiments and evaluations using established metrics such as ROUGE and BLEU scores to evaluate the effectiveness and efficiency of our approach. The results show that GANs can be used to improve the quality and consistency of generated content, data storage, data analysis paper, etc. It shows its promise in paving the way for advanced applications in fields. Through this research, we aim to contribute to the continued evolution of writing technology, providing insights and innovations that support the field to a new level of well-done.","<method>artificial neural networks (GANs)</method>, <method>GAN-based text</method>, <method>adaptive GAN model</method>"
2024,https://openalex.org/W2986574354,Biology,African Journal of Environmental Science and Technology,"The aim of the present study is to test ESA's Sentinel-2 (S2) satellites (S2A and S2B) for an efficient quantification of land cover (LC) and forest compositions in a tropical environment southwest of Mount Kenya.Furthermore, outcome of the research is used to validate ESA's S2 prototype LC 20 m map of Africa that was produced in 2016.A decision tree that is based on significant altitudinal ranges was used to discriminate four natural tree compositions that occur within the investigation area.In addition, the classification process was supported by Google Earth images, and land use (LU) data that were provided by the local Kenyan Forest Service (KFS).Final classification products include four LC classes and five subclasses of forest (four natural forest subclasses plus one non-natural forest class).Results of the Jeffries-Matusita (JM) distance test show significant differences in spectral separability between all classes.Furthermore, the study identifies spectral signatures and significant wavelengths for a classification of all LC classes and forest subclasses where wavelengths of SWIR and the rededge domain show highest importance for the discrimination of tree compositions.Finally, considerable differences can be seen between the utilized multi-temporal classification set (total of 39 bands from three acquisition dates) and ESA's S2 prototype LC 20 m map of Africa 2016.A visual comparison of ESA's prototype map within the investigation area indicates an overrepresentation of tree cover areas (as confirmed in previous studies) and also an underrepresentation of water.",<method>decision tree</method>
2024,https://openalex.org/W4392173735,Biology,"A Comprehensive Survey of Continual Learning: Theory, Method and Application","To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance drop of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efficiency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative strategies address continual learning, and how they are adapted to particular challenges in various applications. Through an in-depth discussion of promising directions, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this field and beyond.",<method>continual learning</method>
2024,https://openalex.org/W4390658983,Biology,Rapid single-particle chemical imaging of nanoplastics by SRS microscopy,"Plastics are now omnipresent in our daily lives. The existence of microplastics (1 µm to 5 mm in length) and possibly even nanoplastics (<1 μm) has recently raised health concerns. In particular, nanoplastics are believed to be more toxic since their smaller size renders them much more amenable, compared to microplastics, to enter the human body. However, detecting nanoplastics imposes tremendous analytical challenges on both the nano-level sensitivity and the plastic-identifying specificity, leading to a knowledge gap in this mysterious nanoworld surrounding us. To address these challenges, we developed a hyperspectral stimulated Raman scattering (SRS) imaging platform with an automated plastic identification algorithm that allows micro-nano plastic analysis at the single-particle level with high chemical specificity and throughput. We first validated the sensitivity enhancement of the narrow band of SRS to enable high-speed single nanoplastic detection below 100 nm. We then devised a data-driven spectral matching algorithm to address spectral identification challenges imposed by sensitive narrow-band hyperspectral imaging and achieve robust determination of common plastic polymers. With the established technique, we studied the micro-nano plastics from bottled water as a model system. We successfully detected and identified nanoplastics from major plastic types. Micro-nano plastics concentrations were estimated to be about 2.4 ± 1.3 × 10","<method>automated plastic identification algorithm</method>, <method>data-driven spectral matching algorithm</method>"
2024,https://openalex.org/W4391069573,Biology,ChatGPT in healthcare: A taxonomy and systematic review,"The recent release of ChatGPT, a chat bot research project/product of natural language processing (NLP) by OpenAI, stirs up a sensation among both the general public and medical professionals, amassing a phenomenally large user base in a short time. This is a typical example of the 'productization' of cutting-edge technologies, which allows the general public without a technical background to gain firsthand experience in artificial intelligence (AI), similar to the AI hype created by AlphaGo (DeepMind Technologies, UK) and self-driving cars (Google, Tesla, etc.). However, it is crucial, especially for healthcare researchers, to remain prudent amidst the hype. This work provides a systematic review of existing publications on the use of ChatGPT in healthcare, elucidating the 'status quo' of ChatGPT in medical applications, for general readers, healthcare professionals as well as NLP scientists. The large biomedical literature database PubMed is used to retrieve published works on this topic using the keyword 'ChatGPT'. An inclusion criterion and a taxonomy are further proposed to filter the search results and categorize the selected publications, respectively. It is found through the review that the current release of ChatGPT has achieved only moderate or 'passing' performance in a variety of tests, and is unreliable for actual clinical deployment, since it is not intended for clinical applications by design. We conclude that specialized NLP models trained on (bio)medical datasets still represent the right direction to pursue for critical clinical applications.","<method>ChatGPT</method>, <method>natural language processing (NLP)</method>, <method>specialized NLP models trained on (bio)medical datasets</method>"
2024,https://openalex.org/W4390919701,Biology,Chatbots and Large Language Models in Radiology: A Practical Primer for Clinical and Research Applications,"Although chatbots have existed for decades, the emergence of transformer-based large language models (LLMs) has captivated the world through the most recent wave of artificial intelligence chatbots, including ChatGPT. Transformers are a type of neural network architecture that enables better contextual understanding of language and efficient training on massive amounts of unlabeled data, such as unstructured text from the internet. As LLMs have increased in size, their improved performance and emergent abilities have revolutionized natural language processing. Since language is integral to human thought, applications based on LLMs have transformative potential in many industries. In fact, LLM-based chatbots have demonstrated human-level performance on many professional benchmarks, including in radiology. LLMs offer numerous clinical and research applications in radiology, several of which have been explored in the literature with encouraging results. Multimodal LLMs can simultaneously interpret text and images to generate reports, closely mimicking current diagnostic pathways in radiology. Thus, from requisition to report, LLMs have the opportunity to positively impact nearly every step of the radiology journey. Yet, these impressive models are not without limitations. This article reviews the limitations of LLMs and mitigation strategies, as well as potential uses of LLMs, including multimodal models. Also reviewed are existing LLM-based applications that can enhance efficiency in supervised settings.","<method>transformer-based large language models (LLMs)</method>, <method>Transformers</method>, <method>Multimodal LLMs</method>"
2024,https://openalex.org/W4399528455,Biology,Bias and Fairness in Large Language Models: A Survey,"Abstract Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this article, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.","<method>bias evaluation metrics</method>, <method>bias evaluation datasets</method>, <method>bias mitigation techniques</method>, <method>pre-processing intervention</method>, <method>in-training intervention</method>, <method>intra-processing intervention</method>, <method>post-processing intervention</method>"
2024,https://openalex.org/W9567271,Biology,Fingerprint Based Gender Classification,"Male-female classification from a fingerprint is an important step in forensic science, anthropological and medical studies to reduce the efforts required for searching a person.The aim of this research is to establish a relationship between gender and the fingerprint using some special features such as ridge density, ridge thickness to valley thickness ratio (RTVTR) and ridge width.showed that male-female classification can be done correctly up to 88.5% based on white lines count, RTVTR & ridge count using Neural Network as Classifier.We have used RTVTR, ridge width and ridge density for classification and SVM as classifier.We have found male-female can be correctly classified up to 91%.Gender classification plays an active role in several applications such as biometrics, criminology, surveillance, human computer interaction, commercial profiling.Though biometric traits such as face, gait, iris and hand shape are used for gender classification in the past, majority of the work is based on face as it contains more prominent features than others.In this paper we have analyzed fingerprints for gender classification with a hope that it has great potential for future research.We have employed a three convolutional layer CNN with rectified linear and activation functions on NIST database which contains a set of 4000 images and achieved 99% accuracy.Performance of the proposed system demonstrated that fingerprints contains vital features to discriminate gender of a person.Humans have distinctive and unique traits which can be used to distinguish them thus, acting as a form of identification.Biometrics identify people by measuring some aspect of individual's anatomy or physiology such as hand geometry or fingerprint which consists of a pattern of interleaved ridges and valleys.The year 2015 election in Nigeria was greeted by some petitions including under-aged voters.The need for an age and gender detector system is a major concern for organizations at all levels where integrity of information cannot be compromised.This work developed a system that determines human age-range and gender using fingerprint analysis trained with Back Propagation Neural Network (for gender classification) and DWT+PCA (for age classification).A total of 280 fingerprint samples of people with various age and gender were collected.140 of these samples were used for training the system""s Database; 70 males and 70 females respectively.This was done for age groups 1-10, 11-20, 21-30, 31-40, 41-50, 51-60 and 61-70 accordingly.In order to determine the gender of an individual, the Ridge Thickness Valley Thickness Ratio (RTVTR) of the person was put into consideration.Result showed 80.00 % classification accuracy for females and 72.86 % for males while 115 subjects out of 140 (82.14%)were correctly classified in age.","<method>Neural Network</method>, <method>SVM</method>, <method>CNN</method>, <method>Back Propagation Neural Network</method>, <method>DWT+PCA</method>"
2024,https://openalex.org/W4392193191,Biology,Challenges and barriers of using large language models (LLM) such as ChatGPT for diagnostic medicine with a focus on digital pathology – a recent scoping review,"Abstract Background The integration of large language models (LLMs) like ChatGPT in diagnostic medicine, with a focus on digital pathology, has garnered significant attention. However, understanding the challenges and barriers associated with the use of LLMs in this context is crucial for their successful implementation. Methods A scoping review was conducted to explore the challenges and barriers of using LLMs, in diagnostic medicine with a focus on digital pathology. A comprehensive search was conducted using electronic databases, including PubMed and Google Scholar, for relevant articles published within the past four years. The selected articles were critically analyzed to identify and summarize the challenges and barriers reported in the literature. Results The scoping review identified several challenges and barriers associated with the use of LLMs in diagnostic medicine. These included limitations in contextual understanding and interpretability, biases in training data, ethical considerations, impact on healthcare professionals, and regulatory concerns. Contextual understanding and interpretability challenges arise due to the lack of true understanding of medical concepts and lack of these models being explicitly trained on medical records selected by trained professionals, and the black-box nature of LLMs. Biases in training data pose a risk of perpetuating disparities and inaccuracies in diagnoses. Ethical considerations include patient privacy, data security, and responsible AI use. The integration of LLMs may impact healthcare professionals’ autonomy and decision-making abilities. Regulatory concerns surround the need for guidelines and frameworks to ensure safe and ethical implementation. Conclusion The scoping review highlights the challenges and barriers of using LLMs in diagnostic medicine with a focus on digital pathology. Understanding these challenges is essential for addressing the limitations and developing strategies to overcome barriers. It is critical for health professionals to be involved in the selection of data and fine tuning of the models. Further research, validation, and collaboration between AI developers, healthcare professionals, and regulatory bodies are necessary to ensure the responsible and effective integration of LLMs in diagnostic medicine.",<method>large language models (LLMs)</method>
2024,https://openalex.org/W4392872715,Biology,GLC_FCS30D: the first global 30 m land-cover dynamics monitoring product with a fine classification system for the period from 1985 to 2022 generated using dense-time-series Landsat imagery and the continuous change-detection method,"Abstract. Land-cover change has been identified as an important cause or driving force of global climate change and is a significant research topic. Over the past few decades, global land-cover mapping has progressed; however, long-time-series global land-cover-change monitoring data are still sparse, especially those at 30 m resolution. In this study, we describe GLC_FCS30D, a novel global 30 m land-cover dynamics monitoring dataset containing 35 land-cover subcategories and covering the period 1985–2022 in 26 time steps (maps were updated every 5 years before 2000 and annually after 2000). GLC_FCS30D has been developed using continuous change detection and all available Landsat imagery based on the Google Earth Engine platform. Specifically, we first take advantage of the continuous change-detection model and the full time series of Landsat observations to capture the time points of changed pixels and identify the temporally stable areas. Then, we apply a spatiotemporal refinement method to derive the globally distributed and high-confidence training samples from these temporally stable areas. Next, local adaptive classification models are used to update the land-cover information for the changed pixels, and a temporal-consistency optimization algorithm is adopted to improve their temporal stability and suppress some false changes. Further, the GLC_FCS30D product is validated using 84 526 globally distributed validation samples from 2020. It achieves an overall accuracy of 80.88 % (±0.27 %) for the basic classification system (10 major land-cover types) and 73.04 % (±0.30 %) for the LCCS (Land Cover Classification System) level-1 validation system (17 LCCS land-cover types). Meanwhile, two third-party time-series datasets used for validation from the United States and Europe Union are also collected for analyzing accuracy variations, and the results show that GLC_FCS30D offers significant stability in terms of variation across the accuracy time series and achieves mean accuracies of 79.50 % (±0.50 %) and 81.91 % (±0.09 %) over the two regions. Lastly, we draw conclusions about the global land-cover-change information from the GLC_FCS30D dataset; namely, that forest and cropland variations have dominated global land-cover change over past 37 years, the net loss of forests reached about 2.5 million km2, and the net gain in cropland area is approximately 1.3 million km2. Therefore, the novel dataset GLC_FCS30D is an accurate land-cover-dynamics time-series monitoring product that benefits from its diverse classification system, high spatial resolution, and long time span (1985–2022); thus, it will effectively support global climate change research and promote sustainable development analysis. The GLC_FCS30D dataset is available via https://doi.org/10.5281/zenodo.8239305 (Liu et al., 2023).","<method>continuous change-detection model</method>, <method>spatiotemporal refinement method</method>, <method>local adaptive classification models</method>, <method>temporal-consistency optimization algorithm</method>"
2024,https://openalex.org/W4392714200,Biology,Genomic selection in plant breeding: Key factors shaping two decades of progress,"Genomic selection, the application of genomic prediction (GP) models to select candidate individuals, has significantly advanced in the past two decades, effectively accelerating genetic gains in plant breeding.This article provides a holistic overview of key factors that have influenced GP in plant breeding during this period.We delved into the pivotal roles of training population size and genetic diversity, and their relationship with the breeding population, in determining GP accuracy.Special emphasis was placed on optimizing training population size.We explored its benefits and the associated diminishing returns beyond an optimum size.This was done while considering the balance between resource allocation and maximizing prediction accuracy through current optimization algorithms.The density and distribution of single-nucleotide polymorphisms, level of linkage disequilibrium, genetic complexity, trait heritability, statistical machine-learning methods, and non-additive effects are the other vital factors.Using wheat, maize, and potato as examples, we summarize the effect of these factors on the accuracy of GP for various traits.The search for high accuracy in GP-theoretically reaching one when using the Pearson's correlation as a metric-is an active research area as yet far from optimal for various traits.We hypothesize that with ultra-high sizes of genotypic and phenotypic datasets, effective training population optimization methods and support from other omics approaches (transcriptomics, metabolomics and proteomics) coupled with deep-learning algorithms could overcome the boundaries of current limitations to achieve the highest possible prediction accuracy, making genomic selection an effective tool in plant breeding.","<method>genomic prediction (GP) models</method>, <method>statistical machine-learning methods</method>, <method>optimization algorithms</method>, <method>deep-learning algorithms</method>"
2024,https://openalex.org/W4391844002,Biology,Texture Exposure of Unconventional (101)<sub>Zn</sub> Facet: Enabling Dendrite‐Free Zn Deposition on Metallic Zinc Anodes,"Abstract Texturing metallic zinc anodes (MZAs) for selective exposure of (002) Zn plane with high thermodynamical stability is an efficient scheme for dendrite‐free Zn electrodeposition. However, fundamental factors that influence Zn deposition morphology via surface crystallographic texture engineering are not well understood. Herein, different from traditional cognition, MZAs with preferential exposure of (101) Zn facet are demonstrated to be equally effective in promoting dendrite‐free Zn deposition, which is enabled by introducing trace amount (0.01 m ) of theophylline into ZnSO 4 electrolyte. Experimental results and mathematical model corroborate, indicating mechanistically that the theophylline derived cations preferentially adsorb on the (002) Zn crystal plane due to higher adsorption energy, thereby accelerating its growth through increased binding affinity with Zn 2+ ions. Consequently, this phenomenon facilitates the texture exposure of (101) Zn facet to achieve ordered surface crystallographic orientation of MZAs (101‐Zn), thus enabling electrodeposition/dissolution cycling over 650 h under a depth of discharge up to 40% and significantly boosting the rechargeability (76.7% capacity retention after 1000 cycles) of the 101‐Zn||carbon‐cloth@MnO 2 full battery relative to counterpart without theophylline additive (36.3%). The work offers deep insights on the scientific links between the surface crystallographic orientation of MZAs and Zn deposition morphology, while opens up vast untapped opportunities to realize dendrite‐free MZAs.",No methods found.
2024,https://openalex.org/W4391559729,Biology,FFCA-YOLO for Small Object Detection in Remote Sensing Images,"Issues such as insufficient feature representation and background confusion make detection tasks for small object in remote sensing arduous. Particularly when the algorithm will be deployed on board for real-time processing, which requires extensive optimization of accuracy and speed under limited computing resources. To tackle these problems, an efficient detector called FFCA-YOLO(Feature enhancement, Fusion and Context Aware YOLO) is proposed in this paper. FFCA-YOLO includes three innovative lightweight and plug-and-play modules: feature enhancement module(FEM), feature fusion module(FFM) and spatial context aware module(SCAM). These three modules improve the network capabilities of local area awareness, multi-scale feature fusion and global association cross channels and space, respectively, while trying to avoid increasing complexity as possible. Thus the weak feature representations of small objects are enhanced and the confusable backgrounds are suppressed. Two public remote sensing datasets(VEDAI and AI-TOD) for small object detection and one self-built dataset(USOD) are used to validate the effectiveness of FFCA-YOLO. The accuracy of FFCA-YOLO reaches 0.748, 0.617 and 0.909(in terms of mAP <sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">50</sub> ) that exceeds several benchmark models and state-of-the-art methods. Meanwhile, the robustness of FFCA-YOLO is also validated under different simulated degradation conditions. Moreover, to further reduce computational resource consumption while ensuring efficiency, a lite version of FFCA-YOLO(L-FFCA-YOLO) is optimized by reconstructing the backbone and neck of FFCA-YOLO based on partial convolution. L-FFCA-YOLO has faster speed, smaller parameter scale, lower computing power requirement but little accuracy loss compared with FFCA-YOLO. The source code will be available at https://github.com/yemu1138178251/FFCA-YOLO.","<method>FFCA-YOLO (Feature enhancement, Fusion and Context Aware YOLO)</method>, <method>feature enhancement module (FEM)</method>, <method>feature fusion module (FFM)</method>, <method>spatial context aware module (SCAM)</method>, <method>L-FFCA-YOLO (lite version of FFCA-YOLO optimized by partial convolution)</method>"
2024,https://openalex.org/W4394929444,Biology,AI literacy and its implications for prompt engineering strategies,"Artificial intelligence technologies are rapidly advancing. As part of this development, large language models (LLMs) are increasingly being used when humans interact with systems based on artificial intelligence (AI), posing both new opportunities and challenges. When interacting with LLM-based AI system in a goal-directed manner, prompt engineering has evolved as a skill of formulating precise and well-structured instructions to elicit desired responses or information from the LLM, optimizing the effectiveness of the interaction. However, research on the perspectives of non-experts using LLM-based AI systems through prompt engineering and on how AI literacy affects prompting behavior is lacking. This aspect is particularly important when considering the implications of LLMs in the context of higher education. In this present study, we address this issue, introduce a skill-based approach to prompt engineering, and explicitly consider the role of non-experts' AI literacy (students) in their prompt engineering skills. We also provide qualitative insights into students' intuitive behaviors towards LLM-based AI systems. The results show that higher-quality prompt engineering skills predict the quality of LLM output, suggesting that prompt engineering is indeed a required skill for the goal-directed use of generative AI tools. In addition, the results show that certain aspects of AI literacy can play a role in higher quality prompt engineering and targeted adaptation of LLMs within education. We, therefore, argue for the integration of AI educational content into current curricula to enable a hybrid intelligent society in which students can effectively use generative AI tools such as ChatGPT.",No methods found.
2024,https://openalex.org/W4391968719,Biology,Artificial intelligence and IoT driven technologies for environmental pollution monitoring and management,"Detecting hazardous substances in the environment is crucial for protecting human wellbeing and ecosystems. As technology continues to advance, artificial intelligence (AI) has emerged as a promising tool for creating sensors that can effectively detect and analyze these hazardous substances. The increasing advancements in information technology have led to a growing interest in utilizing this technology for environmental pollution detection. AI-driven sensor systems, AI and Internet of Things (IoT) can be efficiently used for environmental monitoring, such as those for detecting air pollutants, water contaminants, and soil toxins. With the increasing concerns about the detrimental impact of legacy and emerging hazardous substances on ecosystems and human health, it is necessary to develop advanced monitoring systems that can efficiently detect, analyze, and respond to potential risks. Therefore, this review aims to explore recent advancements in using AI, sensors and IOTs for environmental pollution monitoring, taking into account the complexities of predicting and tracking pollution changes due to the dynamic nature of the environment. Integrating machine learning (ML) methods has the potential to revolutionize environmental science, but it also poses challenges. Important considerations include balancing model performance and interpretability, understanding ML model requirements, selecting appropriate models, and addressing concerns related to data sharing. Through examining these issues, this study seeks to highlight the latest trends in leveraging AI and IOT for environmental pollution monitoring.","<method>artificial intelligence (AI)</method>, <method>machine learning (ML) methods</method>"
2024,https://openalex.org/W4392754729,Biology,"Revolutionizing agriculture with artificial intelligence: plant disease detection methods, applications, and their limitations","Accurate and rapid plant disease detection is critical for enhancing long-term agricultural yield. Disease infection poses the most significant challenge in crop production, potentially leading to economic losses. Viruses, fungi, bacteria, and other infectious organisms can affect numerous plant parts, including roots, stems, and leaves. Traditional techniques for plant disease detection are time-consuming, require expertise, and are resource-intensive. Therefore, automated leaf disease diagnosis using artificial intelligence (AI) with Internet of Things (IoT) sensors methodologies are considered for the analysis and detection. This research examines four crop diseases: tomato, chilli, potato, and cucumber. It also highlights the most prevalent diseases and infections in these four types of vegetables, along with their symptoms. This review provides detailed predetermined steps to predict plant diseases using AI. Predetermined steps include image acquisition, preprocessing, segmentation, feature selection, and classification. Machine learning (ML) and deep understanding (DL) detection models are discussed. A comprehensive examination of various existing ML and DL-based studies to detect the disease of the following four crops is discussed, including the datasets used to evaluate these studies. We also provided the list of plant disease detection datasets. Finally, different ML and DL application problems are identified and discussed, along with future research prospects, by combining AI with IoT platforms like smart drones for field-based disease detection and monitoring. This work will help other practitioners in surveying different plant disease detection strategies and the limits of present systems.","<method>artificial intelligence (AI)</method>, <method>machine learning (ML)</method>, <method>deep learning (DL)</method>"
2024,https://openalex.org/W4390946589,Biology,Deep-STP: a deep learning-based approach to predict snake toxin proteins by using word embeddings,"Snake venom contains many toxic proteins that can destroy the circulatory system or nervous system of prey. Studies have found that these snake venom proteins have the potential to treat cardiovascular and nervous system diseases. Therefore, the study of snake venom protein is conducive to the development of related drugs. The research technologies based on traditional biochemistry can accurately identify these proteins, but the experimental cost is high and the time is long. Artificial intelligence technology provides a new means and strategy for large-scale screening of snake venom proteins from the perspective of computing. In this paper, we developed a sequence-based computational method to recognize snake toxin proteins. Specially, we utilized three different feature descriptors, namely g-gap , natural vector and word 2 vector, to encode snake toxin protein sequences. The analysis of variance (ANOVA), gradient-boost decision tree algorithm (GBDT) combined with incremental feature selection (IFS) were used to optimize the features, and then the optimized features were input into the deep learning model for model training. The results show that our model can achieve a prediction performance with an accuracy of 82.00% in 10-fold cross-validation. The model is further verified on independent data, and the accuracy rate reaches to 81.14%, which demonstrated that our model has excellent prediction performance and robustness.","<method>gradient-boost decision tree algorithm (GBDT)</method>, <method>incremental feature selection (IFS)</method>, <method>deep learning model</method>"
2024,https://openalex.org/W4400118952,Biology,When large language models meet personalization: perspectives of challenges and opportunities,"Abstract The advent of large language models marks a revolutionary breakthrough in artificial intelligence. With the unprecedented scale of training and model parameters, the capability of large language models has been dramatically improved, leading to human-like performances in understanding, language synthesizing, common-sense reasoning, etc. Such a major leap forward in general AI capacity will fundamentally change the pattern of how personalization is conducted. For one thing, it will reform the way of interaction between humans and personalization systems. Instead of being a passive medium of information filtering, like conventional recommender systems and search engines, large language models present the foundation for active user engagement. On top of such a new foundation, users’ requests can be proactively explored, and users’ required information can be delivered in a natural, interactable, and explainable way. For another thing, it will also considerably expand the scope of personalization, making it grow from the sole function of collecting personalized information to the compound function of providing personalized services. By leveraging large language models as a general-purpose interface, the personalization systems may compile user’s requests into plans, calls the functions of external tools (e.g., search engines, calculators, service APIs, etc.) to execute the plans, and integrate the tools’ outputs to complete the end-to-end personalization tasks. Today, large language models are still being rapidly developed, whereas the application in personalization is largely unexplored. Therefore, we consider it to be right the time to review the challenges in personalization and the opportunities to address them with large language models. In particular, we dedicate this perspective paper to the discussion of the following aspects: the development and challenges for the existing personalization system, the newly emerged capabilities of large language models, and the potential ways of making use of large language models for personalization.",<method>large language models</method>
2024,https://openalex.org/W4391075306,Biology,Conumee 2.0: enhanced copy-number variation analysis from DNA methylation arrays for humans and mice,"Abstract Motivation Copy-number variations (CNVs) are common genetic alterations in cancer and their detection may impact tumor classification and therapeutic decisions. However, detection of clinically relevant large and focal CNVs remains challenging when sample material or resources are limited. This has motivated us to create a software tool to infer CNVs from DNA methylation arrays which are often generated as part of clinical routines and in research settings. Results We present our R package, conumee 2.0, that combines tangent normalization, an adjustable genomic binning heuristic, and weighted circular binary segmentation to utilize DNA methylation arrays for CNV analysis and mitigate technical biases and batch effects. Segmentation results were validated in a lung squamous cell carcinoma dataset from TCGA (n = 367 samples) by comparison to segmentations derived from genotyping arrays (Pearson’s correlation coefficient of 0.91). We further introduce a segmented block bootstrapping approach to detect focal alternations that achieved 60.9% sensitivity and 98.6% specificity for deletions affecting CDKN2A/B (60.0% and 96.9% for RB1, respectively) in a low-grade glioma cohort from TCGA (n = 239 samples). Finally, our tool provides functionality to detect and summarize CNVs across large sample cohorts. Availability and implementation Conumee 2.0 is available under open-source license at: https://github.com/hovestadtlab/conumee2.","<method>tangent normalization</method>, <method>adjustable genomic binning heuristic</method>, <method>weighted circular binary segmentation</method>, <method>segmented block bootstrapping</method>"
2024,https://openalex.org/W4392791588,Biology,Can large language models replace humans in systematic reviews? Evaluating <scp>GPT</scp>‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages,"Systematic reviews are vital for guiding practice, research and policy, although they are often slow and labour-intensive. Large language models (LLMs) could speed up and automate systematic reviews, but their performance in such tasks has yet to be comprehensively evaluated against humans, and no study has tested Generative Pre-Trained Transformer (GPT)-4, the biggest LLM so far. This pre-registered study uses a ""human-out-of-the-loop"" approach to evaluate GPT-4's capability in title/abstract screening, full-text review and data extraction across various literature types and languages. Although GPT-4 had accuracy on par with human performance in some tasks, results were skewed by chance agreement and dataset imbalance. Adjusting for these caused performance scores to drop across all stages: for data extraction, performance was moderate, and for screening, it ranged from none in highly balanced literature datasets (~1:1) to moderate in those datasets where the ratio of inclusion to exclusion in studies was imbalanced (~1:3). When screening full-text literature using highly reliable prompts, GPT-4's performance was more robust, reaching ""human-like"" levels. Although our findings indicate that, currently, substantial caution should be exercised if LLMs are being used to conduct systematic reviews, they also offer preliminary evidence that, for certain review tasks delivered under specific conditions, LLMs can rival human performance.","<method>Large language models (LLMs)</method>, <method>Generative Pre-Trained Transformer (GPT)-4</method>"
2024,https://openalex.org/W4390782971,Biology,MemBrain v2: an end-to-end tool for the analysis of membranes in cryo-electron tomography,"A bstract MemBrain v2 is a deep learning-enabled program aimed at the efficient analysis of membranes in cryo-electron tomography (cryo-ET). The final v2 release of MemBrain will comprise three main modules: 1) MemBrain-seg, which provides automated membrane segmentation, 2) MemBrain-pick, which provides automated picking of particles along segmented membranes, and 3) MemBrain-stats, which provides quantitative statistics of particle distributions and membrane morphometrics. This initial version of the manuscript is focused on the beta release of MemBrain-seg, which combines iterative training with diverse data and specialized Fourier-based data augmentations. These augmentations are specifically designed to enhance the tool’s adaptability to a variety of tomographic data and address common challenges in cryo-ET analysis. A key feature of MemBrain-seg is the implementation of the Surface-Dice loss function, which improves the network’s focus on membrane connectivity and allows for the effective incorporation of manual annotations from different sources. This function is beneficial in handling the variability inherent in membrane structures and annotations. Our ongoing collaboration with the cryo-ET community plays an important role in continually improving MemBrain v2 with a wide array of training data. This collaborative approach ensures that MemBrain v2 remains attuned to the field’s needs, enhancing its robustness and generalizability across different types of tomographic data. The current version of MemBrain-seg is available at https://github.com/teamtomo/membrain-seg , and the predecessor of MemBrain-pick (also called MemBrain v1) is deposited at https://github.com/CellArchLab/MemBrain . This preprint will be updated concomitantly with the code until the three integrated modules of MemBrain v2 are complete.","<method>deep learning</method>, <method>iterative training</method>, <method>Fourier-based data augmentations</method>, <method>Surface-Dice loss function</method>"
2024,https://openalex.org/W4395055639,Biology,Foundation Models for Generalist Geospatial Artificial Intelligence,"Much of the progress in the development of highly adaptable and reusable artificial intelligence (AI) models is expected to have a profound impact on Earth science and remote sensing. Foundation models are pre-trained on large unlabeled datasets through self-supervision, and then fine-tuned for various downstream tasks with small labeled datasets. There is an increasing interest within the scientific community to investigate how to effectively build generalist AI models that exploit multi-sensor data in Earth observation applications. This paper introduces a first-of-its-kind framework for efficient pre-training and fine-tuning of foundational models on extensive geospatial data. We have utilized this framework to create Prithvi, a transformer-based geospatial foundational model pre-trained on more than 1 TB of multispectral satellite imagery from the Harmonized Landsat Sentinel-2 (HLS) dataset. Our study demonstrates the efficacy of our framework in successfully fine-tuning Prithvi to a range of Earth observation applications not previously analyzed by foundation models. Applications for which results are presented in this paper include multi-temporal cloud gap imputation, flood mapping, wildfire scar segmentation, and multi-temporal crop segmentation. We assessed the effect of Prithvi's pre-trained weights on downstream tasks and compared learning curves for (1) fine-tuning the entire model, (2) fine-tuning solely the decoder for the downstream task, and (3) training the model without utilizing Prithvi's pre-trained weights. Our experiments showed that the pre-trained model accelerates the fine-tuning process compared to leveraging randomly initialized weights. In addition, pre-trained Prithvi compared well against the state-of-the-art on downstream tasks; e.g., the model outperformed a conditional GAN model in multi-temporal cloud imputation by up to 5pp (or 5.7%) in the structural similarity index. Further, given the cost and time required for collecting labeled training data, we gradually reduced the quantity of available labeled data for refining the model to evaluate data efficiency. We found that labeled training data can be decreased substantially without affecting the model's accuracy. The pre-trained 100 million parameter model and corresponding fine-tuning workflows have been released publicly as open source contributions to the global Earth sciences community through Hugging Face","<method>foundation models</method>, <method>self-supervision</method>, <method>fine-tuning</method>, <method>transformer-based model</method>, <method>conditional GAN model</method>"
2024,https://openalex.org/W4404856766,Biology,Face mask identification with enhanced cuckoo optimization and deep learning-based faster regional neural network,"Abstract A mask identification and social distance monitoring system using Unmanned Aerial Vehicles (UAV) in the outdoors has been proposed for a health establishment. The above approach performed surveillance of the surrounding area using cameras installed in UAVs and internet of things technologies, and the captured images seem useful for tracking the entire environment. However, innate images from unmanned aerial vehicles show an adaptable visual effect in an uncontrolled environment, making face-mask detection and recognition harder. The UAV picture first had to be converted to grayscale, then its contrast was amplified. Image contrast was improved using Optimum Wavelet-Based Masking and the Enhanced Cuckoo Methodology (ECM). According to the contrast-enhanced image, Gabor-Transform (GT) and Stroke Width Transform (SWT) methods are used to derive attributes that help categorise mask-wearers and non-mask-wearers. Using the retrieved attributes, a Weighted Naive Bayes Classification (WNBC) detected masks in the images. Additionally, a deep neural network-based, the faster Region-Based Convolutional Neural Networks (R-CNN) algorithm combined with Adaptive Galactic Swarm Optimization (AGSO) is being used to identify appropriate and incorrect face mask wear in images, as well as to monitor social distancing among individuals in crowded areas. When the system recognises unmasked individuals, it sends their information to the doctor and the nearby police station. One unmanned aerial vehicle’s automated system alert people via speakers, ensuring social spacing. The problem involves a large percentage of appropriate and incorrect face mask wear using data from GitHub and Kaggle, including a training repository of 16,000 images and a testing data set of 12,751 images. To enhance the performance of the model’s learning, the methodology of 10-fold cross-validation will be used. Precision, recall, F1-score, and speed are then measured to determine the efficacy of the suggested approach.","<method>Optimum Wavelet-Based Masking</method>, <method>Enhanced Cuckoo Methodology (ECM)</method>, <method>Gabor-Transform (GT)</method>, <method>Stroke Width Transform (SWT)</method>, <method>Weighted Naive Bayes Classification (WNBC)</method>, <method>faster Region-Based Convolutional Neural Networks (R-CNN)</method>, <method>Adaptive Galactic Swarm Optimization (AGSO)</method>, <method>10-fold cross-validation</method>"
2024,https://openalex.org/W4399777548,Biology,Feature reduction for hepatocellular carcinoma prediction using machine learning algorithms,"Abstract Hepatocellular carcinoma (HCC) is a highly prevalent form of liver cancer that necessitates accurate prediction models for early diagnosis and effective treatment. Machine learning algorithms have demonstrated promising results in various medical domains, including cancer prediction. In this study, we propose a comprehensive approach for HCC prediction by comparing the performance of different machine learning algorithms before and after applying feature reduction methods. We employ popular feature reduction techniques, such as weighting features, hidden features correlation, feature selection, and optimized selection, to extract a reduced feature subset that captures the most relevant information related to HCC. Subsequently, we apply multiple algorithms, including Naive Bayes, support vector machines (SVM), Neural Networks, Decision Tree, and K nearest neighbors (KNN), to both the original high-dimensional dataset and the reduced feature set. By comparing the predictive accuracy, precision, F Score, recall, and execution time of each algorithm, we assess the effectiveness of feature reduction in enhancing the performance of HCC prediction models. Our experimental results, obtained using a comprehensive dataset comprising clinical features of HCC patients, demonstrate that feature reduction significantly improves the performance of all examined algorithms. Notably, the reduced feature set consistently outperforms the original high-dimensional dataset in terms of prediction accuracy and execution time. After applying feature reduction techniques, the employed algorithms, namely decision trees, Naive Bayes, KNN, neural networks, and SVM achieved accuracies of 96%, 97.33%, 94.67%, 96%, and 96.00%, respectively.","<method>Naive Bayes</method>, <method>support vector machines (SVM)</method>, <method>Neural Networks</method>, <method>Decision Tree</method>, <method>K nearest neighbors (KNN)</method>"
2024,https://openalex.org/W4392093325,Biology,Combining IC<sub>50</sub> or <i>K</i><sub><i>i</i></sub> Values from Different Sources Is a Source of Significant Noise,"As part of the ongoing quest to find or construct large data sets for use in validating new machine learning (ML) approaches for bioactivity prediction, it has become distressingly common for researchers to combine literature IC50 data generated using different assays into a single data set. It is well-known that there are many situations where this is a scientifically risky thing to do, even when the assays are against exactly the same target, but the risks of assays being incompatible are even higher when pulling data from large collections of literature data like ChEMBL. Here, we estimate the amount of noise present in combined data sets using cases where measurements for the same compound are reported in multiple assays against the same target. This approach shows that IC50 assays selected using minimal curation settings have poor agreement with each other: almost 65% of the points differ by more than 0.3 log units, 27% differ by more than one log unit, and the correlation between the assays, as measured by Kendall's τ, is only 0.51. Requiring that most of the assay metadata in ChEMBL matches (""maximal curation"") in order to combine two assays improves the situation (48% of the points differ by more than 0.3 log units, 13% by more than one log unit, and Kendall's τ is 0.71) at the expense of having smaller data sets. Surprisingly, our analysis shows similar amounts of noise when combining data from different literature Ki assays. We suggest that good scientific practice requires careful curation when combining data sets from different assays and hope that our maximal curation strategy will help to improve the quality of the data that are being used to build and validate ML models for bioactivity prediction. To help achieve this, the code and ChEMBL queries that we used for the maximal curation approach are available as open-source software in our GitHub repository, https://github.com/rinikerlab/overlapping_assays.",No methods found.
2024,https://openalex.org/W2801491042,Biology,Auslander–Reiten theory in extriangulated categories,"The notion of an extriangulated category gives a unification of existing theories in exact or abelian categories and in triangulated categories. In this article, we develop Auslander–Reiten theory for extriangulated categories. This unifies Auslander–Reiten theories developed in exact categories and triangulated categories independently. We give two different sets of sufficient conditions on the extriangulated category so that existence of almost split extensions becomes equivalent to that of an Auslander–Reiten–Serre duality. We also show that existence of almost split extensions is preserved under taking relative extriangulated categories, ideal quotients, and extension-closed subcategories. Moreover, we prove that the stable category <inline-formula content-type=""math/mathml""> <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" alttext=""script upper C underbar""> <mml:semantics> <mml:munder> <mml:mrow class=""MJX-TeXAtom-ORD""> <mml:mi mathvariant=""script"">C</mml:mi> </mml:mrow> <mml:mo>_<!-- _ --></mml:mo> </mml:munder> <mml:annotation encoding=""application/x-tex"">\underline {\mathscr {C}}</mml:annotation> </mml:semantics> </mml:math> </inline-formula> of an extriangulated category <inline-formula content-type=""math/mathml""> <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" alttext=""script upper C""> <mml:semantics> <mml:mrow class=""MJX-TeXAtom-ORD""> <mml:mi mathvariant=""script"">C</mml:mi> </mml:mrow> <mml:annotation encoding=""application/x-tex"">\mathscr {C}</mml:annotation> </mml:semantics> </mml:math> </inline-formula> is a <inline-formula content-type=""math/mathml""> <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" alttext=""tau""> <mml:semantics> <mml:mi>τ<!-- τ --></mml:mi> <mml:annotation encoding=""application/x-tex"">\tau</mml:annotation> </mml:semantics> </mml:math> </inline-formula>-category (see O. Iyama [Algebr. Represent. Theory 8 (2005), pp. 297–321]) if <inline-formula content-type=""math/mathml""> <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" alttext=""script upper C""> <mml:semantics> <mml:mrow class=""MJX-TeXAtom-ORD""> <mml:mi mathvariant=""script"">C</mml:mi> </mml:mrow> <mml:annotation encoding=""application/x-tex"">\mathscr {C}</mml:annotation> </mml:semantics> </mml:math> </inline-formula> has enough projectives, almost split extensions and source morphisms. This gives various consequences on <inline-formula content-type=""math/mathml""> <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" alttext=""script upper C underbar""> <mml:semantics> <mml:munder> <mml:mrow class=""MJX-TeXAtom-ORD""> <mml:mi mathvariant=""script"">C</mml:mi> </mml:mrow> <mml:mo>_<!-- _ --></mml:mo> </mml:munder> <mml:annotation encoding=""application/x-tex"">\underline {\mathscr {C}}</mml:annotation> </mml:semantics> </mml:math> </inline-formula>, including Igusa–Todorov’s Radical Layers Theorem (see K. Igusa and G. Todorov [J. Algebra 89 (1984), pp. 105–147]), Auslander–Reiten Combinatorics on dimensions of Hom-spaces, and Reconstruction Theorem of the associated completely graded category of <inline-formula content-type=""math/mathml""> <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" alttext=""script upper C underbar""> <mml:semantics> <mml:munder> <mml:mrow class=""MJX-TeXAtom-ORD""> <mml:mi mathvariant=""script"">C</mml:mi> </mml:mrow> <mml:mo>_<!-- _ --></mml:mo> </mml:munder> <mml:annotation encoding=""application/x-tex"">\underline {\mathscr {C}}</mml:annotation> </mml:semantics> </mml:math> </inline-formula> via the complete mesh category of the Auslander–Reiten species of <inline-formula content-type=""math/mathml""> <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" alttext=""script upper C underbar""> <mml:semantics> <mml:munder> <mml:mrow class=""MJX-TeXAtom-ORD""> <mml:mi mathvariant=""script"">C</mml:mi> </mml:mrow> <mml:mo>_<!-- _ --></mml:mo> </mml:munder> <mml:annotation encoding=""application/x-tex"">\underline {\mathscr {C}}</mml:annotation> </mml:semantics> </mml:math> </inline-formula>. Finally we prove that any locally finite symmetrizable <inline-formula content-type=""math/mathml""> <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" alttext=""tau""> <mml:semantics> <mml:mi>τ<!-- τ --></mml:mi> <mml:annotation encoding=""application/x-tex"">\tau</mml:annotation> </mml:semantics> </mml:math> </inline-formula>-quiver (=valued translation quiver) is an Auslander–Reiten quiver of some extriangulated category with sink morphisms and source morphisms.",No methods found.
2024,https://openalex.org/W4391572037,Biology,Machine Learning Applications in Healthcare: Current Trends and Future Prospects,"The integration of machine learning (ML) in healthcare has witnessed remarkable advancements, transforming the landscape of medical diagnosis, treatment, and overall patient care. This article provides a comprehensive review of the current trends and future prospects of machine learning applications in the healthcare domain.The current landscape is characterized by the utilization of ML algorithms for disease diagnosis and risk prediction, personalized treatment plans, and efficient healthcare resource management. Notable applications include image recognition for radiology and pathology, predictive analytics for disease prognosis, and the development of precision medicine tailored to individual patient profiles.This review explores the evolving role of ML in improving patient outcomes, enhancing clinical decision-making, and optimizing healthcare workflows. It delves into the challenges faced in integrating ML into existing healthcare systems, such as data privacy concerns, interpretability of complex models, and the need for robust validation processes.Additionally, the article discusses future prospects and emerging trends in ML healthcare applications, including the potential for predictive analytics to preemptively identify health issues, the integration of wearable devices and remote monitoring for continuous patient care, and the intersection of ML with genomics for personalized medicine.The overarching goal of this article is to provide healthcare professionals, researchers, and policymakers with insights into the current state of ML applications in healthcare, along with an outlook on the transformative potential that machine learning holds for the future of healthcare delivery and patient outcomes.","<method>machine learning (ML) algorithms</method>, <method>image recognition</method>, <method>predictive analytics</method>"
2024,https://openalex.org/W4399694730,Biology,RCSB protein Data Bank: exploring protein 3D similarities via comprehensive structural alignments,"Abstract Motivation Tools for pairwise alignments between 3D structures of proteins are of fundamental importance for structural biology and bioinformatics, enabling visual exploration of evolutionary and functional relationships. However, the absence of a user-friendly, browser-based tool for creating alignments and visualizing them at both 1D sequence and 3D structural levels makes this process unnecessarily cumbersome. Results We introduce a novel pairwise structure alignment tool (rcsb.org/alignment) that seamlessly integrates into the RCSB Protein Data Bank (RCSB PDB) research-focused RCSB.org web portal. Our tool and its underlying application programming interface (alignment.rcsb.org) empowers users to align several protein chains with a reference structure by providing access to established alignment algorithms (FATCAT, CE, TM-align, or Smith–Waterman 3D). The user-friendly interface simplifies parameter setup and input selection. Within seconds, our tool enables visualization of results in both sequence (1D) and structural (3D) perspectives through the RCSB PDB RCSB.org Sequence Annotations viewer and Mol* 3D viewer, respectively. Users can effortlessly compare structures deposited in the PDB archive alongside more than a million incorporated Computed Structure Models coming from the ModelArchive and AlphaFold DB. Moreover, this tool can be used to align custom structure data by providing a link/URL or uploading atomic coordinate files directly. Importantly, alignment results can be bookmarked and shared with collaborators. By bridging the gap between 1D sequence and 3D structures of proteins, our tool facilitates deeper understanding of complex evolutionary relationships among proteins through comprehensive sequence and structural analyses. Availability and implementation The alignment tool is part of the RCSB PDB research-focused RCSB.org web portal and available at rcsb.org/alignment. Programmatic access is available via alignment.rcsb.org. Frontend code has been published at github.com/rcsb/rcsb-pecos-app. Visualization is powered by the open-source Mol* viewer (github.com/molstar/molstar and github.com/molstar/rcsb-molstar) plus the Sequence Annotations in 3D Viewer (github.com/rcsb/rcsb-saguaro-3d).","<method>FATCAT</method>, <method>CE</method>, <method>TM-align</method>, <method>Smith–Waterman 3D</method>"
2024,https://openalex.org/W4392303127,Biology,Recent advancements and challenges of NLP-based sentiment analysis: A state-of-the-art review,"Sentiment analysis is a method within natural language processing that evaluates and identifies the emotional tone or mood conveyed in textual data. Scrutinizing words and phrases categorizes them into positive, negative, or neutral sentiments. The significance of sentiment analysis lies in its capacity to derive valuable insights from extensive textual data, empowering businesses to grasp customer sentiments, make informed choices, and enhance their offerings. For the further advancement of sentiment analysis, gaining a deep understanding of its algorithms, applications, current performance, and challenges is imperative. Therefore, in this extensive survey, we began exploring the vast array of application domains for sentiment analysis, scrutinizing them within the context of existing research. We then delved into prevalent pre-processing techniques, datasets, and evaluation metrics to enhance comprehension. We also explored Machine Learning, Deep Learning, Large Language Models and Pre-trained models in sentiment analysis, providing insights into their advantages and drawbacks. Subsequently, we precisely reviewed the experimental results and limitations of recent state-of-the-art articles. Finally, we discussed the diverse challenges encountered in sentiment analysis and proposed future research directions to mitigate these concerns. This extensive review provides a complete understanding of sentiment analysis, covering its models, application domains, results analysis, challenges, and research directions.","<method>Machine Learning</method>, <method>Deep Learning</method>, <method>Large Language Models</method>, <method>Pre-trained models</method>"
2024,https://openalex.org/W4393072087,Biology,FI-NPI: Exploring Optimal Control in Parallel Platform Systems,"Typically, the current and speed loop closure of servo motor of the parallel platform is accomplished with incremental PI regulation. The control method has strong robustness, but the parameter tuning process is cumbersome, and it is difficult to achieve the optimal control state. In order to further optimize the performance, this paper proposes a double-loop control structure based on fuzzy integral and neuron proportional integral (FI-NPI). The structure makes full use of the control advantages of the fuzzy controller and integrator to improve the performance of speed closed-loop control. And through the feedforward branch, the speed error is used as the teacher signal for neuron supervised learning, which improves the effect of current closed-loop control. Through comparative simulation experiments, this paper verifies that the FI-NPI controller has a faster dynamic response speed than the traditional PI controller. Finally, in this paper, the FI-NPI controller is implemented in C language in the servo-driven lower computer, and the speed closed-loop test of the BLDC motor is carried out. The experimental results show that the FI-NPI double-loop controller is better than the traditional double-PI controller in performance indicators such as convergence rate and RMSE, which confirms that the FI-NPI double-loop controller is more suitable for BLDC servo control.","<method>fuzzy integral</method>, <method>neuron proportional integral (FI-NPI)</method>, <method>neuron supervised learning</method>"
2024,https://openalex.org/W4393991132,Biology,Earl Grey: A Fully Automated User-Friendly Transposable Element Annotation and Analysis Pipeline,"Transposable elements (TEs) are major components of eukaryotic genomes and are implicated in a range of evolutionary processes. Yet, TE annotation and characterization remain challenging, particularly for nonspecialists, since existing pipelines are typically complicated to install, run, and extract data from. Current methods of automated TE annotation are also subject to issues that reduce overall quality, particularly (i) fragmented and overlapping TE annotations, leading to erroneous estimates of TE count and coverage, and (ii) repeat models represented by short sections of total TE length, with poor capture of 5' and 3' ends. To address these issues, we present Earl Grey, a fully automated TE annotation pipeline designed for user-friendly curation and annotation of TEs in eukaryotic genome assemblies. Using nine simulated genomes and an annotation of Drosophila melanogaster, we show that Earl Grey outperforms current widely used TE annotation methodologies in ameliorating the issues mentioned above while scoring highly in benchmarking for TE annotation and classification and being robust across genomic contexts. Earl Grey provides a comprehensive and fully automated TE annotation toolkit that provides researchers with paper-ready summary figures and outputs in standard formats compatible with other bioinformatics tools. Earl Grey has a modular format, with great scope for the inclusion of additional modules focused on further quality control and tailored analyses in future releases.",No methods found.
2024,https://openalex.org/W4391019749,Biology,CIFAKE: Image Classification and Explainable Identification of AI-Generated Synthetic Images,"Recent advances in synthetic data have enabled the generation of images with such high quality that human beings cannot tell the difference between real-life photographs and Artificial Intelligence (AI) generated images. Given the critical necessity of data reliability and authentication, this article proposes to enhance our ability to recognise AI-generated images through computer vision. Initially, a synthetic dataset is generated that mirrors the ten classes of the already available CIFAR-10 dataset with latent diffusion, providing a contrasting set of images for comparison to real photographs. The model is capable of generating complex visual attributes, such as photorealistic reflections in water. The two sets of data present as a binary classification problem with regard to whether the photograph is real or generated by AI. This study then proposes the use of a Convolutional Neural Network (CNN) to classify the images into two categories; Real or Fake. Following hyperparameter tuning and the training of 36 individual network topologies, the optimal approach could correctly classify the images with 92.98% accuracy. Finally, this study implements explainable AI via Gradient Class Activation Mapping to explore which features within the images are useful for classification. Interpretation reveals interesting concepts within the image, in particular, noting that the actual entity itself does not hold useful information for classification; instead, the model focuses on small visual imperfections in the background of the images. The complete dataset engineered for this study, referred to as the CIFAKE dataset, is made publicly available to the research community for future work.","<method>latent diffusion</method>, <method>Convolutional Neural Network (CNN)</method>, <method>hyperparameter tuning</method>, <method>Gradient Class Activation Mapping</method>"
2024,https://openalex.org/W4391288564,Biology,Innovations in hydrogel-based manufacturing: A comprehensive review of direct ink writing technique for biomedical applications,"Direct ink writing (DIW) stands as a pioneering additive manufacturing technique that holds transformative potential in the field of hydrogel fabrication. This innovative approach allows for the precise deposition of hydrogel inks layer by layer, creating complex three-dimensional structures with tailored shapes, sizes, and functionalities. By harnessing the versatility of hydrogels, DIW opens up possibilities for applications spanning from tissue engineering to soft robotics and wearable devices. This comprehensive review investigates DIW as applied to hydrogels and its multifaceted applications. The paper introduces a diverse range of printing techniques while providing a thorough exploration of DIW for hydrogel-based printing. The investigation aims to explain the progress made, challenges faced, and potential trajectories that lie ahead for DIW in hydrogel-based manufacturing. The fundamental principles underlying DIW are carefully examined, specifically focusing on rheological attributes and printing parameters, prompting a comprehensive survey of the wide variety of hydrogel materials. These encompass both natural and synthetic variations, all of which can be effectively harnessed for this purpose. Furthermore, the review explores the latest applications of DIW for hydrogels in biomedical areas, with a primary focus on tissue engineering, wound dressing, and drug delivery systems. The document not only consolidates the existing state of DIW within the context of hydrogel-based manufacturing but also charts potential avenues for further research and innovative breakthroughs.",No methods found.
2024,https://openalex.org/W4391750982,Biology,Elk herd optimizer: a novel nature-inspired metaheuristic algorithm,"Abstract This paper proposes a novel nature-inspired swarm-based optimization algorithm called elk herd optimizer (EHO). It is inspired by the breeding process of the elk herd. Elks have two main breeding seasons: rutting and calving. In the rutting season, the elk herd splits into different families of various sizes. This division is based on fighting for dominance between bulls, where the stronger bull can form a family with large numbers of harems. In the calving season, each family breeds new calves from its bull and harems. This inspiration is set in an optimization context where the optimization loop consists of three operators: rutting season, calving season, and selection season. During the selection season, all families are merged, including bulls, harems, and calves. The fittest elk herd will be selected for use in the upcoming rutting and calving seasons. In simple words, EHO divides the population into a set of groups, each with one leader and several followers in the rutting season. The number of followers is determined based on the fitness value of its leader group. Each group will generate new solutions based on its leader and followers in the calving season. The members of all groups including leaders, followers, and new solutions are combined and the fittest population is selected in the selection season. The performance of EHO is assessed using 29 benchmark optimization problems utilized in the CEC-2017 special sessions on real-parameter optimization and four traditional real-world engineering design problems. The comparative results were conducted against ten well-established metaheuristic algorithms and showed that the proposed EHO yielded the best results for almost all the benchmark functions used. Statistical testing using Friedman’s test post-hocked by Holm’s test function confirms the superiority of the proposed EHO when compared to other methods. In a nutshell, EHO is an efficient nature-inspired swarm-based optimization algorithm that can be used to tackle several optimization problems.","<method>elk herd optimizer (EHO)</method>, <method>metaheuristic algorithms</method>"
2024,https://openalex.org/W4399039179,Biology,Distributed Event-Triggered Output-Feedback Time-Varying Formation Fault-Tolerant Control for Nonlinear Multi-Agent Systems,"This paper studies the event-triggered time-varying formation control problem for nonlinear multi-agent systems with actuator faults. Based on the neural network approximation technique, a neural observer is constructed to estimate the unmeasured states of systems. Then, a distributed adaptive event-triggered time-varying formation control manner is proposed utilizing the intermittent estimated states information from the agent and its neighbors. To overcome the problem that estimated states triggering leads to virtual control laws is non-differentiable, a distributed continuous control scheme under regular output-feedback is designed firstly, upon which a distributed event-triggered controller is constructed by replacing estimated states with intermittent estimated ones. It is shown that the designed event-triggered output-feedback time-varying formation fault-tolerant controller can compensate for actuator faults, and all signals in closed-loop systems are semi-globally uniformly ultimately bounded. Finally, simulation results of a practical example are given to verify the effectiveness of the proposed control manner. <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Note to Practitioners</i> —Formation control has broad application prospects in modern military and civilian fields, such as combat aircraft flying formation, satellite formation, autonomous vehicle formation, etc. In formation control systems, when agents occur actuator faults, it may break the original formation and even cause collision between agents. As a result, the security of formation control systems is facing great challenges in practical engineering applications. On the other hand, communication bandwidth is limited in practical engineering systems, and how to make systems quickly form formation under the limited communication bandwidth has become a key topic. Inspired by the above discussions, a distributed state-triggered output-feedback time-varying formation fault-tolerant control scheme is designed in this paper, in which actuator faults are compensated by using adaptive technology. Meanwhile, to sufficiently save the usage of system communication resources, a dual-channel event-triggered mechanism is designed.","<method>neural network approximation technique</method>, <method>neural observer</method>, <method>distributed adaptive event-triggered time-varying formation control</method>, <method>distributed continuous control scheme under regular output-feedback</method>, <method>distributed event-triggered controller</method>, <method>distributed state-triggered output-feedback time-varying formation fault-tolerant control scheme</method>, <method>adaptive technology</method>, <method>dual-channel event-triggered mechanism</method>"
2024,https://openalex.org/W4395050972,Biology,Optimization of hepatological clinical guidelines interpretation by large language models: a retrieval augmented generation-based framework,"Abstract Large language models (LLMs) can potentially transform healthcare, particularly in providing the right information to the right provider at the right time in the hospital workflow. This study investigates the integration of LLMs into healthcare, specifically focusing on improving clinical decision support systems (CDSSs) through accurate interpretation of medical guidelines for chronic Hepatitis C Virus infection management. Utilizing OpenAI’s GPT-4 Turbo model, we developed a customized LLM framework that incorporates retrieval augmented generation (RAG) and prompt engineering. Our framework involved guideline conversion into the best-structured format that can be efficiently processed by LLMs to provide the most accurate output. An ablation study was conducted to evaluate the impact of different formatting and learning strategies on the LLM’s answer generation accuracy. The baseline GPT-4 Turbo model’s performance was compared against five experimental setups with increasing levels of complexity: inclusion of in-context guidelines, guideline reformatting, and implementation of few-shot learning. Our primary outcome was the qualitative assessment of accuracy based on expert review, while secondary outcomes included the quantitative measurement of similarity of LLM-generated responses to expert-provided answers using text-similarity scores. The results showed a significant improvement in accuracy from 43 to 99% ( p &lt; 0.001), when guidelines were provided as context in a coherent corpus of text and non-text sources were converted into text. In addition, few-shot learning did not seem to improve overall accuracy. The study highlights that structured guideline reformatting and advanced prompt engineering (data quality vs. data quantity) can enhance the efficacy of LLM integrations to CDSSs for guideline delivery.","<method>Large language models (LLMs)</method>, <method>OpenAI’s GPT-4 Turbo model</method>, <method>retrieval augmented generation (RAG)</method>, <method>prompt engineering</method>, <method>few-shot learning</method>"
2024,https://openalex.org/W4399885374,Biology,Survival Prediction Across Diverse Cancer Types Using Neural Networks,"Gastric cancer and Colon adenocarcinoma represent widespread and challenging malignancies with high mortality rates and complex treatment landscapes. In response to the critical need for accurate prognosis in cancer patients, the medical community has embraced the 5-year survival rate as a vital metric for estimating patient outcomes. This study introduces a pioneering approach to enhance survival prediction models for gastric and Colon adenocarcinoma patients. Leveraging advanced image analysis techniques, we sliced whole slide images (WSI) of these cancers, extracting comprehensive features to capture nuanced tumor characteristics. Subsequently, we constructed patient-level graphs, encapsulating intricate spatial relationships within tumor tissues. These graphs served as inputs for a sophisticated 4-layer graph convolutional neural network (GCN), designed to exploit the inherent connectivity of the data for comprehensive analysis and prediction. By integrating patients' total survival time and survival status, we computed C-index values for gastric cancer and Colon adenocarcinoma, yielding 0.57 and 0.64, respectively. Significantly surpassing previous convolutional neural network models, these results underscore the efficacy of our approach in accurately predicting patient survival outcomes. This research holds profound implications for both the medical and AI communities, offering insights into cancer biology and progression while advancing personalized treatment strategies. Ultimately, our study represents a significant stride in leveraging AI-driven methodologies to revolutionize cancer prognosis and improve patient outcomes on a global scale.","<method>graph convolutional neural network (GCN)</method>, <method>convolutional neural network</method>"
2024,https://openalex.org/W4391216149,Biology,"Benchmarking Micro-Action Recognition: Dataset, Methods, and Applications","Micro-action is an imperceptible non-verbal behaviour characterised by low-intensity movement. It offers insights into the feelings and intentions of individuals and is important for human-oriented applications such as emotion recognition and psychological assessment. However, the identification, differentiation, and understanding of micro-actions pose challenges due to the imperceptible and inaccessible nature of these subtle human behaviors in everyday life. In this study, we innovatively collect a new micro-action dataset designated as Micro-action-52 (MA-52), and propose a benchmark named micro-action network (MANet) for micro-action recognition (MAR) task. Uniquely, MA-52 provides the whole-body perspective including gestures, upper- and lower-limb movements, attempting to reveal comprehensive micro-action cues. In detail, MA-52 contains 52 micro-action categories along with seven body part labels, and encompasses a full array of realistic and natural micro-actions, accounting for 205 participants and 22,422 video instances collated from the psychological interviews. Based on the proposed dataset, we assess MANet and other nine prevalent action recognition methods. MANet incorporates squeeze-and-excitation (SE) and temporal shift module (TSM) into the ResNet architecture for modeling the spatiotemporal characteristics of micro-actions. Then a joint-embedding loss is designed for semantic matching between video and action labels; the loss is used to better distinguish between visually similar yet distinct micro-action categories. The extended application in emotion recognition has demonstrated one of the important values of our proposed dataset and method. In the future, further exploration of human behaviour, emotion, and psychological assessment will be conducted in depth. The dataset and source code are released at https://github.com/VUT-HFUT/Micro-Action.","<method>micro-action network (MANet)</method>, <method>squeeze-and-excitation (SE)</method>, <method>temporal shift module (TSM)</method>, <method>ResNet architecture</method>, <method>joint-embedding loss</method>, <method>prevalent action recognition methods</method>"
2024,https://openalex.org/W4391505626,Biology,Professionalizing Legal Translator Training: Prospects and Opportunities,"Legal transactions have permeated every aspect of our life. Much of this is accomplished through legal translators who, by their outputs, impact our personal and professional future. That said, this article seeks to tackle the challenges and opportunities in preparing legal translators for professional practice. The article is a quality review in its nature which adopts the descriptive approach. The interactionist perspective is adopted in this present article to examine the challenges faced by and the opportunities offered to legal translators under training. This examination is placed within the context of the rapidly evolving translation industry and its related interdisciplinary research, which covers the technology and legal translation, quality in legal translation, and training pathways for legal translators. The subjective perspective is acknowledged as the human experience is involved to explain the individual phenomena within broader context of legal translation profession. The article draws that there is a need to make changes in the legal translation status because we need to improve the translator’s perception of their role. Moreover, training models adopted to prepare legal translators have to be updated by revising the outdated practices of legal translation, and integrating the social role to face the new challenges as the translators are the intercultural mediators who facilitate the international legal communication.",No methods found.
2024,https://openalex.org/W4390652362,Biology,A comprehensive review of the development of land use regression approaches for modeling spatiotemporal variations of ambient air pollution: A perspective from 2011 to 2023,"Land use regression (LUR) models are widely used in epidemiological and environmental studies to estimate humans' exposure to air pollution within urban areas. However, the early models, developed using linear regressions and data from fixed monitoring stations and passive sampling, were primarily designed to model traditional and criteria air pollutants and had limitations in capturing high-resolution spatiotemporal variations of air pollution. Over the past decade, there has been a notable development of multi-source observations from low-cost monitors, mobile monitoring, and satellites, in conjunction with the integration of advanced statistical methods and spatially and temporally dynamic predictors, which have facilitated significant expansion and advancement of LUR approaches. This paper reviews and synthesizes the recent advances in LUR approaches from the perspectives of the changes in air quality data acquisition, novel predictor variables, advances in model-developing approaches, improvements in validation methods, model transferability, and modeling software as reported in 155 LUR studies published between 2011 and 2023. We demonstrate that these developments have enabled LUR models to be developed for larger study areas and encompass a wider range of criteria and unregulated air pollutants. LUR models in the conventional spatial structure have been complemented by more complex spatiotemporal structures. Compared with linear models, advanced statistical methods yield better predictions when handling data with complex relationships and interactions. Finally, this study explores new developments, identifies potential pathways for further breakthroughs in LUR methodologies, and proposes future research directions. In this context, LUR approaches have the potential to make a significant contribution to future efforts to model the patterns of long- and short-term exposure of urban populations to air pollution.","<method>linear regressions</method>, <method>advanced statistical methods</method>"
2024,https://openalex.org/W4400659510,Biology,Aspect-based drug review classification through a hybrid model with ant colony optimization using deep learning,"Abstract The task of aspect-level sentiment analysis is intricately designed to determine the sentiment polarity directed towards a specific target within a sentence. With the increasing availability of online reviews and the growing importance of healthcare decisions, analyzing drug reviews has become a critical task. Traditional sentiment analysis, which categorizes a whole review as positive, negative, or neutral, provides limited insights for consumers and healthcare professionals. Aspect-based sentiment analysis (ABSA) aims to overcome these limitations by identifying and evaluating the sentiment associated with specific aspects or attributes of drugs mentioned in the reviews. Various fields, including business, politics, and medicine, have been explored in the context of sentiment analysis. Automation of online user reviews allows pharmaceutical companies to assess large amounts of user feedback. This helps extract pharmacological efficacy and side effect insights. The data collected could improve pharmacovigilance. Reviewing user comments can provide valuable data that can be used to improve drug safety and efficacy monitoring procedures. This improves pharmacovigilance processes, improving pharmaceutical outcomes understanding and corporate decision-making. Therefore, we propose a pre-trained RoBERTa with a Bi-LSTM model to categorise drug reviews from online sources and pre-process the text data. Ant Colony Optimization can be used in feature selection for ABSA, helping to identify the most relevant aspects and sentiments. Further, RoBERTa is fine-tuned to perform ABSA on the dataset, enabling the system to categorize aspects and determine the associated sentiment. The outcomes reveal that the suggested framework has achieved higher accuracy (96.78%) and F1 score (98.29%) on druglib.com, and 95.02% on the drugs.com dataset, than several prior state-of-the-art methods.","<method>pre-trained RoBERTa</method>, <method>Bi-LSTM model</method>, <method>Ant Colony Optimization</method>, <method>fine-tuned RoBERTa</method>"
2024,https://openalex.org/W4390494339,Biology,"A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions","Automating the monitoring of industrial processes has the potential to enhance efficiency and optimize quality by promptly detecting abnormal events and thus facilitating timely interventions. Deep learning, with its capacity to discern non-trivial patterns within large datasets, plays a pivotal role in this process. Standard deep learning methods are suitable to solve a specific task given a specific type of data. During training, deep learning demands large volumes of labeled data. However, due to the dynamic nature of the industrial processes and environment, it is impractical to acquire large-scale labeled data for standard deep learning training for every slightly different case anew. Deep transfer learning offers a solution to this problem. By leveraging knowledge from related tasks and accounting for variations in data distributions, the transfer learning framework solves new tasks with little or even no additional labeled data. The approach bypasses the need to retrain a model from scratch for every new setup and dramatically reduces the labeled data requirement. This survey first provides an in-depth review of deep transfer learning, examining the problem settings of transfer learning and classifying the prevailing deep transfer learning methods. Moreover, we delve into applications of deep transfer learning in the context of a broad spectrum of time series anomaly detection tasks prevalent in primary industrial domains, e.g., manufacturing process monitoring, predictive maintenance, energy management, and infrastructure facility monitoring. We discuss the challenges and limitations of deep transfer learning in industrial contexts and conclude the survey with practical directions and actionable suggestions to address the need to leverage diverse time series data for anomaly detection in an increasingly dynamic production environment.","<method>deep learning</method>, <method>standard deep learning methods</method>, <method>deep transfer learning</method>, <method>transfer learning framework</method>"
2024,https://openalex.org/W4399326707,Biology,Enhancing precision agriculture: A comprehensive review of machine learning and AI vision applications in all-terrain vehicle for farm automation,"The automation of all-terrain vehicles (ATVs) through the integration of advanced technologies such as machine learning (ML) and artificial intelligence (AI) vision has significantly changed precision agriculture. This paper aims to analyse and develop trends to provide comprehensive knowledge of the current state of ATV-based precision agriculture and the future possibilities of ML and AI. A bibliometric analysis was conducted through network diagram with keywords taken from previous publications in the domain. This review comprehensively analyses the potential of machine learning and artificial intelligence in transforming farming operations through the automation of tasks and the deployment of all-terrain vehicles. The research extensively analyses how machine learning methods have influenced several aspects of agricultural activities, such as planting, harvesting, spraying, weeding, crop monitoring, and others. AI vision systems are being researched for their ability to enhance precise and prompt decision-making in ATV-driven agricultural automation. These technologies have been thoroughly tested to show how they can improve crop yield, reducing overall investment, and make farming more efficient. Examples include machine learning-based seeding accuracy, AI-enabled crop health monitoring, and the use of AI vision for accurate pesticide application. The assessment examines challenges such as data privacy problems and scalability constraints, along with potential advancements and future prospects in the field. This will assist researchers and practitioners in making well-informed judgments regarding farming practices that are efficient, sustainable, and technologically robust.","<method>machine learning</method>, <method>artificial intelligence vision</method>, <method>machine learning methods</method>, <method>AI vision systems</method>, <method>machine learning-based seeding accuracy</method>, <method>AI-enabled crop health monitoring</method>, <method>AI vision for accurate pesticide application</method>"
2024,https://openalex.org/W4391527655,Biology,"Micro(nano)plastics in the Human Body: Sources, Occurrences, Fates, and Health Risks","The increasing global attention on micro(nano)plastics (MNPs) is a result of their ubiquity in the water, air, soil, and biosphere, exposing humans to MNPs on a daily basis and threatening human health. However, crucial data on MNPs in the human body, including the sources, occurrences, behaviors, and health risks, are limited, which greatly impedes any systematic assessment of their impact on the human body. To further understand the effects of MNPs on the human body, we must identify existing knowledge gaps that need to be immediately addressed and provide potential solutions to these issues. Herein, we examined the current literature on the sources, occurrences, and behaviors of MNPs in the human body as well as their potential health risks. Furthermore, we identified key knowledge gaps that must be resolved to comprehensively assess the effects of MNPs on human health. Additionally, we addressed that the complexity of MNPs and the lack of efficient analytical methods are the main barriers impeding current investigations on MNPs in the human body, necessitating the development of a standard and unified analytical method. Finally, we highlighted the need for interdisciplinary studies from environmental, biological, medical, chemical, computer, and material scientists to fill these knowledge gaps and drive further research. Considering the inevitability and daily occurrence of human exposure to MNPs, more studies are urgently required to enhance our understanding of their potential negative effects on human health.",No methods found.
2024,https://openalex.org/W4394579747,Biology,An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study,"Background Large language models (LLMs) have shown remarkable capabilities in natural language processing (NLP), especially in domains where labeled data are scarce or expensive, such as the clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. Objective The objective of this study is to assess the effectiveness of various prompt engineering techniques, including 2 newly introduced types—heuristic and ensemble prompts, for zero-shot and few-shot clinical information extraction using pretrained language models. Methods This comprehensive experimental study evaluated different prompt types (simple prefix, simple cloze, chain of thought, anticipatory, heuristic, and ensemble) across 5 clinical NLP tasks: clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, and medication attribute extraction. The performance of these prompts was assessed using 3 state-of-the-art language models: GPT-3.5 (OpenAI), Gemini (Google), and LLaMA-2 (Meta). The study contrasted zero-shot with few-shot prompting and explored the effectiveness of ensemble approaches. Results The study revealed that task-specific prompt tailoring is vital for the high performance of LLMs for zero-shot clinical NLP. In clinical sense disambiguation, GPT-3.5 achieved an accuracy of 0.96 with heuristic prompts and 0.94 in biomedical evidence extraction. Heuristic prompts, alongside chain of thought prompts, were highly effective across tasks. Few-shot prompting improved performance in complex scenarios, and ensemble approaches capitalized on multiple prompt strengths. GPT-3.5 consistently outperformed Gemini and LLaMA-2 across tasks and prompt types. Conclusions This study provides a rigorous evaluation of prompt engineering methodologies and introduces innovative techniques for clinical information extraction, demonstrating the potential of in-context learning in the clinical domain. These findings offer clear guidelines for future prompt-based clinical NLP research, facilitating engagement by non-NLP experts in clinical NLP advancements. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative artificial intelligence, and we hope that it will inspire and inform future research in this area.","<method>in-context learning</method>, <method>prompt engineering</method>, <method>heuristic prompts</method>, <method>ensemble prompts</method>, <method>simple prefix prompts</method>, <method>simple cloze prompts</method>, <method>chain of thought prompts</method>, <method>anticipatory prompts</method>, <method>ensemble approaches</method>, <method>zero-shot prompting</method>, <method>few-shot prompting</method>"
2024,https://openalex.org/W4397001018,Biology,A systematic review of hyperspectral imaging in precision agriculture: Analysis of its current state and future prospects,"Hyperspectral sensor adaptability in precision agriculture to digital images is still at its nascent stage. Hyperspectral imaging (HSI) is data rich in solving agricultural problems like disease detection, weed detection, stress detection, crop monitoring, nutrient application, soil mineralogy, yield estimation, and sorting applications. With modern precision agriculture, the challenge now is to bring these applications to the field for real-time solutions, where machines are enabled to conduct analyses without expert supervision and communicate the results to users for better management of farmlands; a necessary step to gain complete autonomy in agricultural farmlands. Significant advancements in HSI technology for precision agriculture are required to fully realize its potential. As a wide-ranging collection of the status of HSI and analysis in precision agriculture is lacking, this review endeavors to provide a comprehensive overview of the recent advancements and trends of HSI in precision agriculture for real-time applications. In this study, a systematic review of 163 scientific articles published over the past twenty years (2003–2023) was conducted. Of these, 97 were selected for further analysis based on their relevance to the topic at hand. Topics include conventional data preprocessing techniques, hyperspectral data acquisition, data compression methods, and segmentation methods. The hardware implementation of field-programmable gate arrays (FPGAs) and graphics processing units (GPUs) for high-speed data processing and application of machine learning and deep learning technologies were explored. This review highlights the potential of HSI as a powerful tool for precision agriculture, particularly in real-time applications, discusses limitations, and provides insights into future research directions.","<method>machine learning</method>, <method>deep learning</method>"
2024,https://openalex.org/W4390597725,Biology,Critical review on water quality analysis using IoT and machine learning models,"Water quality and its management are the most precise concerns confronting humanity globally. This article evaluates the various sensors used for water quality monitoring and focuses on the water quality index considering the multiple physical, chemical, and biological parameters. A Review of Internet of Things (IoT) research for water quality monitoring and analysis, sensors used for water quality can help remote monitoring of the water quality parameters using various IoT-based sensors that convey the assembled estimations utilizing Low-Power Wide Area Network innovations. Overall, the IoT system was 95 % accurate in measuring pH, Turbidity, TDS, and Temperature, while the traditional method was only 85 % accurate. Also, this study reviewed the different A.I. techniques used to assess water quality, including conventional machine learning techniques, Support Vector Machines, Deep Neural Networks, and K-nearest neighbors. Compared to traditional methods, machine learning and deep learning can significantly increase the accuracy of measurements of groundwater quality. However, various variables, such as the caliber of the training data, the water quality metrics' complexity, and the monitoring frequency, will affect the accuracy. The geographical information system (GIS) is used for spatial data analysis and managing water resources. The quality of its data is also reviewed in the paper. Based on these analyses, the study has forecasted the future sensors, Geospatial Technology, and machine learning techniques for water quality monitoring and analysis.","<method>conventional machine learning techniques</method>, <method>Support Vector Machines</method>, <method>Deep Neural Networks</method>, <method>K-nearest neighbors</method>"
2024,https://openalex.org/W4390604402,Biology,A novel and dynamic land use/cover change research framework based on an improved PLUS model and a fuzzy multiobjective programming model,"Spatial reconstruction and scenario simulation of historical processes and future trends of land use/cover change (LUCC) can help to reveal the historical background of land conversion and the spatial distribution of future land. Moreover, there is a close relationship between the spatiotemporal dynamics of land use/cover and changes in different ecosystem services (ESs). Using this relationship to simulate future land use scenarios is important. In this study, an LUCC dynamic analysis framework (LSTM-PLUS-FMOP) was constructed based on a deep learning time series forecasting model (LSTM), a parallelized urban land use simulation (PLUS) model and a fuzzy multiobjective programming (FMOP) model. The PLUS model was used to analyze the driving mechanism of land expansion and explore the land conversion pattern. In addition, three land conversion scenarios were established: natural land expansion (NLE), economic development priority (EDP) and regional sustainable development (RSD). The FMOP model and the relationship between LUCC and ES were used to perform a spatial simulation of land conversion. The uncertainty parameters in the model were treated by intuitionistic fuzzy numbers (IFSs). This study applied the constructed framework to the Yellow River Basin of Shaanxi Province (YRB-SX). The results showed that (1) from 2000 to 2020, the cropland area of the YRB-SX continuously decreased by 12.67 × 104 ha, while the built-up area continuously increased by 28.25 × 104 ha. The net reduction in woodland and grassland area was 13.90 × 104 ha. (2) The relative error range of land prediction using the LSTM model was 0.0003– 0.0042. This model had better accuracy than the Markov chain prediction model. (3) The cropland area decreased by 0.26% (NLE), 0.85% (EDP) and 1.68% (RSD) under the three scenarios. The built-up area increased by 25.01%, 32.76% and 14.72%, respectively. The RSD scenario followed the principles of ecological protection and spatial constraints, which mitigated the degradation of the ecosystem to some extent. This coupled simulation framework will help to obtain land allocation schemes that meet the requirements of ecological protection and provide solutions for rational land management.","<method>LSTM</method>, <method>PLUS model</method>, <method>fuzzy multiobjective programming (FMOP) model</method>"
2024,https://openalex.org/W4391168980,Biology,Utilizing Hybrid Machine Learning and Soft Computing Techniques for Landslide Susceptibility Mapping in a Drainage Basin,"The hydrological system of thebasin of Lake Urmia is complex, deriving its supply from a network comprising 13 perennial rivers, along withnumerous small springs and direct precipitation onto the lake’s surface. Among these contributors, approximately half of the inflow is attributed to the Zarrineh River and the Simineh River. Remarkably, Lake Urmia lacks a natural outlet, with its water loss occurring solely through evaporation processes. This study employed a comprehensive methodology integrating ground surveys, remote sensing analyses, and meticulous documentation of historical landslides within the basin as primary information sources. Through this investigative approach, we preciselyidentified and geolocated a total of 512 historical landslide occurrences across the Urmia Lake drainage basin, leveraging GPS technology for precision. Thisarticle introduces a suite of hybrid machine learning predictive models, such as support-vector machine (SVM), random forest (RF), decision trees (DT), logistic regression (LR), fuzzy logic (FL), and the technique for order of preference by similarity to the ideal solution (TOPSIS). These models were strategically deployed to assess landslide susceptibility within the region. The outcomes of the landslide susceptibility assessment reveal that the main high susceptible zones for landslide occurrence are concentrated in the northwestern, northern, northeastern, and some southern and southeastern areas of the region. Moreover, when considering the implementation of predictions using different algorithms, it became evident that SVM exhibited superior performance regardingboth accuracy (0.89) and precision (0.89), followed by RF, with and accuracy of 0.83 and a precision of 0.83. However, it is noteworthy that TOPSIS yielded the lowest accuracy value among the algorithms assessed.","<method>support-vector machine (SVM)</method>, <method>random forest (RF)</method>, <method>decision trees (DT)</method>, <method>logistic regression (LR)</method>, <method>fuzzy logic (FL)</method>, <method>technique for order of preference by similarity to the ideal solution (TOPSIS)</method>"
2024,https://openalex.org/W4391831565,Biology,Comparison of Random Forest and XGBoost Classifiers Using Integrated Optical and SAR Features for Mapping Urban Impervious Surface,"The integration of optical and SAR datasets through ensemble machine learning models shows promising results in urban remote sensing applications. The integration of multi-sensor datasets enhances the accuracy of information extraction. This research presents a comparison of two ensemble machine learning classifiers (random forest and extreme gradient boost (XGBoost)) classifiers using an integration of optical and SAR features and simple layer stacking (SLS) techniques. Therefore, Sentinel-1 (SAR) and Landsat 8 (optical) datasets were used with SAR textures and enhanced modified indices to extract features for the year 2023. The classification process utilized two machine learning algorithms, random forest and XGBoost, for urban impervious surface extraction. The study focused on three significant East Asian cities with diverse urban dynamics: Jakarta, Manila, and Seoul. This research proposed a novel index called the Normalized Blue Water Index (NBWI), which distinguishes water from other features and was utilized as an optical feature. Results showed an overall accuracy of 81% for UIS classification using XGBoost and 77% with RF while classifying land use land cover into four major classes (water, vegetation, bare soil, and urban impervious). However, the proposed framework with the XGBoost classifier outperformed the RF algorithm and Dynamic World (DW) data product and comparatively showed higher classification accuracy. Still, all three results show poor separability with bare soil class compared to ground truth data. XGBoost outperformed random forest and Dynamic World in classification accuracy, highlighting its potential use in urban remote sensing applications.","<method>ensemble machine learning models</method>, <method>random forest</method>, <method>extreme gradient boost (XGBoost)</method>, <method>simple layer stacking (SLS)</method>"
2024,https://openalex.org/W4399009670,Biology,Exploring factors influencing the acceptance of ChatGPT in higher education: A smart education perspective,"AI-powered chatbots hold great promise for enhancing learning experiences and outcomes in today's rapidly evolving education system. However, despite the increasing demand for such technologies, there remains a significant research gap regarding the factors influencing users' acceptance and adoption of AI-powered chatbots in educational contexts. This study aims to address this gap by investigating the factors that shape users' attitudes, intentions, and behaviors towards adopting ChatGPT for smart education systems. This research employed a quantitative research approach, data were collected from 458 of participants through a structured questionnaire designed to measure various constructs related to technology acceptance, including perceived ease of use, perceived usefulness, feedback quality, assessment quality, subject norms, attitude towards use, and behavioral intention to use ChatGPT. Structural model analysis (SEM) Statistical techniques were then utilized to examine the relationships between these constructs. The findings of the study revealed that Perceived ease of use and perceived usefulness emerged as significant predictors of users' attitudes towards ChatGPT for smart education. Additionally, feedback quality, assessment quality, and subject norms were found to positively influence users' behavioral intentions to use ChatGPT for smart educational purposes. Moreover, users' attitudes towards use and behavioral intentions were significantly proved for the actual adoption of ChatGPT. However, a few hypotheses, such as the relationship between trust in ChatGPT and perceived usefulness, were not supported by the data. This study contributes to the existing body information systems applications for the determining factor of technology acceptance in smart education context.",No methods found.
2024,https://openalex.org/W4390669592,Biology,Enhancing plasticity in optoelectronic artificial synapses: A pathway to efficient neuromorphic computing,"The continuous growth in artificial intelligence and high-performance computing has necessitated the development of efficient optoelectronic artificial synapses crucial for neuromorphic computing (NC). Ga2O3 is an emerging wide-bandgap semiconductor with high deep ultraviolet absorption, tunable persistent photoconductivity, and excellent stability toward electric fields, making it a promising component for optoelectronic artificial synapses. Currently reported Ga2O3 optoelectronic artificial synapses often suffer from complex fabrication processes and potential room for improvement due to plasticity. To address the issue of low device plasticity and practical application scenarios, we present an amorphous Ga2O3 (α-GaOx) flexible optoelectronic artificial synapse. This synapse modulates light stimulus signals using electron/oxygen vacancies and optical stimulation and operates as a visual storage device for information processing. We investigate the improvement of the optoelectronic synapses' plasticity by controlling the number of oxygen vacancies via a plasma treatment method and demonstrate its effective application in a three-layer backpropagation neural network for handwritten digit classification. Under the same stimulus conditions, the synaptic weight of samples treated with Ar plasma exhibits a higher rate of change, with the current levels increasing by 2–3 orders of magnitude, achieving greater plasticity. The improved optoelectronic synapses achieved an accuracy of 93.34%/94%, demonstrating their potential as efficient computing solutions and insights for future applications in NC chips.",<method>backpropagation neural network</method>
2024,https://openalex.org/W4392763392,Biology,ANYmal parkour: Learning agile navigation for quadrupedal robots,"Performing agile navigation with four-legged robots is a challenging task because of the highly dynamic motions, contacts with various parts of the robot, and the limited field of view of the perception sensors. Here, we propose a fully learned approach to training such robots and conquer scenarios that are reminiscent of parkour challenges. The method involves training advanced locomotion skills for several types of obstacles, such as walking, jumping, climbing, and crouching, and then using a high-level policy to select and control those skills across the terrain. Thanks to our hierarchical formulation, the navigation policy is aware of the capabilities of each skill, and it will adapt its behavior depending on the scenario at hand. In addition, a perception module was trained to reconstruct obstacles from highly occluded and noisy sensory data and endows the pipeline with scene understanding. Compared with previous attempts, our method can plan a path for challenging scenarios without expert demonstration, offline computation, a priori knowledge of the environment, or taking contacts explicitly into account. Although these modules were trained from simulated data only, our real-world experiments demonstrate successful transfer on hardware, where the robot navigated and crossed consecutive challenging obstacles with speeds of up to 2 meters per second.","<method>fully learned approach</method>, <method>training advanced locomotion skills</method>, <method>high-level policy to select and control skills</method>, <method>hierarchical formulation</method>, <method>perception module trained to reconstruct obstacles from sensory data</method>"
2024,https://openalex.org/W4393201659,Biology,Garlic Origin Traceability and Identification Based on Fusion of Multi-Source Heterogeneous Spectral Information,"The chemical composition and nutritional content of garlic are greatly impacted by its production location, leading to distinct flavor profiles and functional properties among garlic varieties from diverse origins. Consequently, these variations determine the preference and acceptance among diverse consumer groups. In this study, purple-skinned garlic samples were collected from five regions in China: Yunnan, Shandong, Henan, Anhui, and Jiangsu Provinces. Mid-infrared spectroscopy and ultraviolet spectroscopy were utilized to analyze the components of garlic cells. Three preprocessing methods, including Multiple Scattering Correction (MSC), Savitzky–Golay Smoothing (SG Smoothing), and Standard Normalized Variate (SNV), were applied to reduce the background noise of spectroscopy data. Following variable feature extraction by Genetic Algorithm (GA), a variety of machine learning algorithms, including XGboost, Support Vector Classification (SVC), Random Forest (RF), and Artificial Neural Network (ANN), were used according to the fusion of spectral data to obtain the best processing results. The results showed that the best-performing model for ultraviolet spectroscopy data was SNV-GA-ANN, with an accuracy of 99.73%. The best-performing model for mid-infrared spectroscopy data was SNV-GA-RF, with an accuracy of 97.34%. After the fusion of ultraviolet and mid-infrared spectroscopy data, the SNV-GA-SVC, SNV-GA-RF, SNV-GA-ANN, and SNV-GA-XGboost models achieved 100% accuracy in both training and test sets. Although there were some differences in the accuracy of the four models under different preprocessing methods, the fusion of ultraviolet and mid-infrared spectroscopy data yielded the best outcomes, with an accuracy of 100%. Overall, the combination of ultraviolet and mid-infrared spectroscopy data fusion and chemometrics established in this study provides a theoretical foundation for identifying the origin of garlic, as well as that of other agricultural products.","<method>Genetic Algorithm (GA)</method>, <method>XGboost</method>, <method>Support Vector Classification (SVC)</method>, <method>Random Forest (RF)</method>, <method>Artificial Neural Network (ANN)</method>"
2024,https://openalex.org/W4391037822,Biology,Estimating compressive strength of concrete containing rice husk ash using interpretable machine learning-based models,"The construction sector is a major contributor to global greenhouse gas emissions. Using recycled and waste materials in concrete is a practical solution to address environmental challenges. Currently, agricultural waste is widely used as a substitute for cement in the production of eco-friendly concrete. However, traditional methods for assessing the strength of such materials are both expensive and time-consuming. Therefore, this study uses machine learning techniques to develop prediction models for the compressive strength (CS) of rice husk ash (RHA) concrete. The ML techniques used in the present study include random forest (RF), light gradient boosting machine (LightGBM), ridge regression, and extreme gradient boosting (XGBoost). A total of 348 values of CS were collected from the experimental studies, and five characteristics of RHA concrete were taken as input variables. For the performance assessment of the models, multiple statistical metrics were used. During the training phase, the correlation coefficients (R) obtained for ridge regression, RF, XGBoost, and LightGBM were 0.943, 0.981, 0.985, and 0.996, respectively. In the testing set, these values demonstrated even higher performance, with correlation coefficients of 0.971, 0.993, 0.992, and 0.998 for ridge regression, RF, XGBoost, and LightGBM, respectively. The statistical analysis revealed that the LightGBM model outperformed other models, whereas the ridge regression model exhibited comparatively lower accuracy. SHapley Additive exPlanation (SHAP) method was employed for the interpretability of the developed model. The SHAP analysis revealed that water-to-cement is a controlling parameter in estimating the CS of RHA concrete. In conclusion, this study provides valuable guidance for builders and researchers to estimate the CS of RHA concrete. However, it is suggested that more input variables be incorporated and hybrid models utilized to further enhance the reliability and precision of the models.","<method>random forest (RF)</method>, <method>light gradient boosting machine (LightGBM)</method>, <method>ridge regression</method>, <method>extreme gradient boosting (XGBoost)</method>, <method>SHapley Additive exPlanation (SHAP)</method>"
2024,https://openalex.org/W4393012885,Biology,Improving Thyroid Disorder Diagnosis via Ensemble Stacking and Bidirectional Feature Selection,"Thyroid disorders represent a significant global health challenge with hypothyroidism and hyperthyroidism as two common conditions arising from dysfunction in the thyroid gland.Accurate and timely diagnosis of these disorders is crucial for effective treatment and patient care.This research introduces a comprehensive approach to improve the accuracy of thyroid disorder diagnosis through the integration of ensemble stacking and advanced feature selection techniques.Sequential forward feature selection, sequential backward feature elimination, and bidirectional feature elimination are investigated in this study.In ensemble learning, random forest, adaptive boosting, and bagging classifiers are employed.The effectiveness of these techniques is evaluated using two different datasets obtained from the University of California Irvine-Machine Learning Repository, both of which undergo preprocessing steps, including outlier removal, addressing missing data, data cleansing, and feature reduction.Extensive experimentation demonstrates the remarkable success of proposed ensemble stacking and bidirectional feature elimination achieving 100% and 99.86% accuracy in identifying hyperthyroidism and hypothyroidism, respectively.Beyond enhancing detection accuracy, the ensemble stacking model also demonstrated a streamlined computational complexity which is pivotal for practical medical applications.It significantly outperformed existing studies with similar objectives underscoring the viability and effectiveness of the proposed scheme.This research offers an innovative perspective and sets the platform for improved thyroid disorder diagnosis with broader implications for healthcare and patient well-being.","<method>ensemble stacking</method>, <method>sequential forward feature selection</method>, <method>sequential backward feature elimination</method>, <method>bidirectional feature elimination</method>, <method>random forest</method>, <method>adaptive boosting</method>, <method>bagging classifiers</method>"
2024,https://openalex.org/W4396877909,Biology,The Capacity and Robustness Trade-off: Revisiting the Channel Independent Strategy for Multivariate Time Series Forecasting,"Multivariate time series data comprises various channels of variables. The multivariate forecasting models need to capture the relationship between the channels to accurately predict future values. However, recently, there has been an emergence of methods that employ the Channel Independent (CI) strategy. These methods view multivariate time series data as separate univariate time series and disregard the correlation between channels. Surprisingly, our empirical results have shown that models trained with the CI strategy outperform those trained with the Channel Dependent (CD) strategy, usually by a significant margin. Nevertheless, the reasons behind this phenomenon have not yet been thoroughly explored in the literature. This paper provides comprehensive empirical and theoretical analyses of the characteristics of multivariate time series datasets and the CI/CD strategy. Our results conclude that the CD approach has higher capacity but often lacks robustness to accurately predict distributionally drifted time series. In contrast, the CI approach trades capacity for robust prediction. Practical measures inspired by these analyses are proposed to address the capacity and robustness dilemma, including a modified CD method called Predict Residuals with Regularization (PRReg) that can surpass the CI strategy. We hope our findings can raise awareness among researchers about the characteristics of multivariate time series and inspire the construction of better forecasting models.","<method>Channel Independent (CI) strategy</method>, <method>Channel Dependent (CD) strategy</method>, <method>Predict Residuals with Regularization (PRReg)</method>"
2024,https://openalex.org/W4391558635,Biology,Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning,"Code comment generation aims at generating natural language descriptions for a code snippet to facilitate developers' program comprehension activities. Despite being studied for a long time, a bottleneck for existing approaches is that given a code snippet, they can only generate one comment while developers usually need to know information from diverse perspectives such as what is the functionality of this code snippet and how to use it. To tackle this limitation, this study empirically investigates the feasibility of utilizing large language models (LLMs) to generate comments that can fulfill developers' diverse intents. Our intuition is based on the facts that (1) the code and its pairwise comment are used during the pre-training process of LLMs to build the semantic connection between the natural language and programming language, and (2) comments in the real-world projects, which are collected for the pre-training, usually contain different developers' intents. We thus postulate that the LLMs can already understand the code from different perspectives after the pre-training. Indeed, experiments on two large-scale datasets demonstrate the rationale of our insights: by adopting the in-context learning paradigm and giving adequate prompts to the LLM (e.g., providing it with ten or more examples), the LLM can significantly outperform a state-of-the-art supervised learning approach on generating comments with multiple intents. Results also show that customized strategies for constructing the prompts and post-processing strategies for reranking the results can both boost the LLM's performances, which shed light on future research directions for using LLMs to achieve comment generation.","<method>large language models (LLMs)</method>, <method>in-context learning paradigm</method>, <method>supervised learning approach</method>"
2024,https://openalex.org/W4393344670,Biology,"Unveiling the Glucose Oxidase‐Like and Catalase‐Like Activities of Highly Conjugated 3,4,9,10‐Perylenetetracarboxylic Dianhydride for Boosting Biofuel Cells","Abstract Carbon‐based nanozymes have received considerable attention due to their superior biosafety, enhanced tolerance to extreme conditions, and ease of chemical modification. However, due to limited material diversity and unconfirmed molecular structure, carbon‐based nanozymes face challenges such as relatively low enzyme activity and unclear catalytic mechanisms. The development of materials with well‐defined structures and controllable properties is crucial for promoting the rapid progress of nanozymes. Herein, 3,4,9,10‐perylenetetracarboxylic dianhydride (PD) exhibits both glucose oxidase (GOx)‐like and catalase (CAT)‐like activities, which may be due to the fact that PD possesses the features of highly conjugated structure and high electron mobility. In addition, it is demonstrated that the enzymatic activity is related to the degree of PD aggregation via the characterization of its morphology and size. Based on the excellent GOx‐like and CAT‐like activities of PD, a self‐cascade catalytic system is constructed for application in biofuel cells (BFCs). It is worth mentioning that such BFC still maintains high stability after working for 30 days. Therefore, this study expands the enzyme‐like systems and discovers that nanomaterials with highly conjugated structures and high electron mobility can mimic enzymes. Additionally, the multi‐enzyme activities are utilized to construct self‐cascade systems, which can effectively improve the performance of BFCs.",No methods found.
2024,https://openalex.org/W4396798020,Biology,"The triple exposure nexus of microplastic particles, plastic-associated chemicals, and environmental pollutants from a human health perspective","The presence of microplastics (MPs) is increasing at a dramatic rate globally, posing risks for exposure and subsequent potential adverse effects on human health. Apart from being physical objects, MP particles contain thousands of plastic-associated chemicals (i.e., monomers, chemical additives, and non-intentionally added substances) captured within the polymer matrix. These chemicals are often migrating from MPs and can be found in various environmental matrices and human food chains; increasing the risks for exposure and health effects. In addition to the physical and chemical attributes of MPs, plastic surfaces effectively bind exogenous chemicals, including environmental pollutants (e.g., heavy metals, persistent organic pollutants). Therefore, MPs can act as vectors of environmental pollution across air, drinking water, and food, further amplifying health risks posed by MP exposure. Critically, fragmentation of plastics in the environment increases the risk for interactions with cells, increases the presence of available surfaces to leach plastic-associated chemicals, and adsorb and transfer environmental pollutants. This review proposes the so-called triple exposure nexus approach to comprehensively map existing knowledge on interconnected health effects of MP particles, plastic-associated chemicals, and environmental pollutants. Based on the available data, there is a large knowledge gap in regard to the interactions and cumulative health effects of the triple exposure nexus. Each component of the triple nexus is known to induce genotoxicity, inflammation, and endocrine disruption, but knowledge about long-term and inter-individual health effects is lacking. Furthermore, MPs are not readily excreted from organisms after ingestion and they have been found accumulated in human blood, cardiac tissue, placenta, etc. Even though the number of studies on MPs-associated health impacts is increasing rapidly, this review underscores that there is a pressing necessity to achieve an integrated assessment of MPs' effects on human health in order to address existing and future knowledge gaps.",No methods found.
2024,https://openalex.org/W4390738897,Biology,Enhancing crop recommendation systems with explainable artificial intelligence: a study on agricultural decision-making,"Abstract Crop Recommendation Systems are invaluable tools for farmers, assisting them in making informed decisions about crop selection to optimize yields. These systems leverage a wealth of data, including soil characteristics, historical crop performance, and prevailing weather patterns, to provide personalized recommendations. In response to the growing demand for transparency and interpretability in agricultural decision-making, this study introduces XAI-CROP an innovative algorithm that harnesses eXplainable artificial intelligence (XAI) principles. The fundamental objective of XAI-CROP is to empower farmers with comprehensible insights into the recommendation process, surpassing the opaque nature of conventional machine learning models. The study rigorously compares XAI-CROP with prominent machine learning models, including Gradient Boosting (GB), Decision Tree (DT), Random Forest (RF), Gaussian Naïve Bayes (GNB), and Multimodal Naïve Bayes (MNB). Performance evaluation employs three essential metrics: Mean Squared Error (MSE), Mean Absolute Error (MAE), and R-squared (R2). The empirical results unequivocally establish the superior performance of XAI-CROP. It achieves an impressively low MSE of 0.9412, indicating highly accurate crop yield predictions. Moreover, with an MAE of 0.9874, XAI-CROP consistently maintains errors below the critical threshold of 1, reinforcing its reliability. The robust R 2 value of 0.94152 underscores XAI-CROP's ability to explain 94.15% of the data's variability, highlighting its interpretability and explanatory power.","<method>XAI-CROP</method>, <method>Gradient Boosting (GB)</method>, <method>Decision Tree (DT)</method>, <method>Random Forest (RF)</method>, <method>Gaussian Naïve Bayes (GNB)</method>, <method>Multimodal Naïve Bayes (MNB)</method>"
2024,https://openalex.org/W4391177783,Biology,"Making food systems more resilient to food safety risks by including artificial intelligence, big data, and internet of things into food safety early warning and emerging risk identification tools","Abstract To enhance the resilience of food systems to food safety risks, it is vitally important for national authorities and international organizations to be able to identify emerging food safety risks and to provide early warning signals in a timely manner. This review provides an overview of existing and experimental applications of artificial intelligence (AI), big data, and internet of things as part of early warning and emerging risk identification tools and methods in the food safety domain. There is an ongoing rapid development of systems fed by numerous, real‐time, and diverse data with the aim of early warning and identification of emerging food safety risks. The suitability of big data and AI to support such systems is illustrated by two cases in which climate change drives the emergence of risks, namely, harmful algal blooms affecting seafood and fungal growth and mycotoxin formation in crops. Automation and machine learning are crucial for the development of future real‐time food safety risk early warning systems. Although these developments increase the feasibility and effectiveness of prospective early warning and emerging risk identification tools, their implementation may prove challenging, particularly for low‐ and middle‐income countries due to low connectivity and data availability. It is advocated to overcome these challenges by improving the capability and capacity of national authorities, as well as by enhancing their collaboration with the private sector and international organizations.","<method>artificial intelligence (AI)</method>, <method>machine learning</method>"
2024,https://openalex.org/W4391783116,Biology,Assessing water quality of an ecologically critical urban canal incorporating machine learning approaches,"This study assessed water quality (WQ) in Tongi Canal, an ecologically critical and economically important urban canal in Bangladesh. The researchers employed the Root Mean Square Water Quality Index (RMS-WQI) model, utilizing seven WQ indicators, including temperature, dissolve oxygen, electrical conductivity, lead, cadmium, and iron to calculate the water quality index (WQI) score. The results showed that most of the water sampling locations showed poor WQ, with many indicators violating Bangladesh's environmental conservation regulations. This study employed eight machine learning algorithms, where the Gaussian process regression (GPR) model demonstrated superior performance (training RMSE = 1.77, testing RMSE = 0.0006) in predicting WQI scores. To validate the GPR model's performance, several performance measures, including the coefficient of determination (R2), the Nash-Sutcliffe efficiency (NSE), the model efficiency factor (MEF), Z statistics, and Taylor diagram analysis, were employed. The GPR model exhibited higher sensitivity (R2 = 1.0) and efficiency (NSE = 1.0, MEF = 0.0) in predicting WQ. The analysis of model uncertainty (standard uncertainty = 7.08 ± 0.9025; expanded uncertainty = 7.08 ± 1.846) indicates that the RMS-WQI model holds potential for assessing the WQ of inland waterbodies. These findings indicate that the RMS-WQI model could be an effective approach for assessing inland waters across Bangladesh. The study's results showed that most of the WQ indicators did not meet the recommended guidelines, indicating that the water in the Tongi Canal is unsafe and unsuitable for various purposes. The study's implications extend beyond the Tongi Canal and could contribute to WQ management initiatives across Bangladesh.",<method>Gaussian process regression (GPR)</method>
2024,https://openalex.org/W4400748541,Biology,Comparing YOLOv8 and Mask R-CNN for instance segmentation in complex orchard environments,"Instance segmentation, an important image processing operation for automation in agriculture, is used to precisely delineate individual objects of interest within images, which provides foundational information for various automated or robotic tasks such as selective harvesting and precision pruning. This study compares the one-stage YOLOv8 and the two-stage Mask R-CNN machine learning models for instance segmentation under varying orchard conditions across two datasets. Dataset 1, collected in dormant season, includes images of dormant apple trees, which were used to train multi-object segmentation models delineating tree branches and trunks. Dataset 2, collected in the early growing season, includes images of apple tree canopies with green foliage and immature (green) apples (also called fruitlet), which were used to train single-object segmentation models delineating only immature green apples. The results showed that YOLOv8 performed better than Mask R-CNN, achieving good precision and near-perfect recall across both datasets at a confidence threshold of 0.5. Specifically, for Dataset 1, YOLOv8 achieved a precision of 0.90 and a recall of 0.95 for all classes. In comparison, Mask R-CNN demonstrated a precision of 0.81 and a recall of 0.81 for the same dataset. With Dataset 2, YOLOv8 achieved a precision of 0.93 and a recall of 0.97. Mask R-CNN, in this single-class scenario, achieved a precision of 0.85 and a recall of 0.88. Additionally, the inference times for YOLOv8 were 10.9 ms for multi-class segmentation (Dataset 1) and 7.8 ms for single-class segmentation (Dataset 2), compared to 15.6 ms and 12.8 ms achieved by Mask R-CNN's, respectively. These findings show YOLOv8's superior accuracy and efficiency in machine learning applications compared to two-stage models, specifically Mask-R-CNN, which suggests its suitability in developing smart and automated orchard operations, particularly when real-time applications are necessary in such cases as robotic harvesting and robotic immature green fruit thinning.","<method>YOLOv8</method>, <method>Mask R-CNN</method>"
2024,https://openalex.org/W4390739507,Biology,AI in Decision Making: Transforming Business Strategies,"This paper delves into the transformative impact of Artificial Intelligence (AI) on strategic business decision-making, offering a nuanced perspective on how AI is reshaping the corporate world. The primary purpose of this study is to explore the emergence and evolution of AI within the realm of business strategy, examining its role in disrupting traditional decision models and enhancing business agility. This study systematically analyzes academic and industry sources through a meticulous literature review, providing a comprehensive understanding of AI’s multifaceted role in business. The methodology adopted is a systematic literature review, which serves as a robust framework for evaluating source credibility and synthesizing insights. This approach enables a thorough examination of AI’s integration into business management, its influence on corporate performance metrics, and its potential in fostering inclusive business practices. The study also addresses the unique challenges and opportunities presented by AI in the business context. Key findings reveal that AI is not merely a technological tool but a strategic asset that significantly redefines business decision-making. The integration of AI into business strategies demonstrates substantial potential in enhancing corporate performance and promoting sustainable business practices. The study concludes that AI is a cornerstone in business evolution, offering unparalleled opportunities for innovation and efficiency. Recommendations advocate for a balanced approach to AI integration, emphasizing the need for businesses to align AI with their core values and strategic objectives. As AI continues to evolve, its role in business decision-making is expected to shape the corporate landscape significantly.",No methods found.
2024,https://openalex.org/W4401691402,Biology,Optimal truss design with MOHO: A multi-objective optimization perspective,"This research article presents the Multi-Objective Hippopotamus Optimizer (MOHO), a unique approach that excels in tackling complex structural optimization problems. The Hippopotamus Optimizer (HO) is a novel approach in meta-heuristic methodology that draws inspiration from the natural behaviour of hippos. The HO is built upon a trinary-phase model that incorporates mathematical representations of crucial aspects of Hippo's behaviour, including their movements in aquatic environments, defense mechanisms against predators, and avoidance strategies. This conceptual framework forms the basis for developing the multi-objective (MO) variant MOHO, which was applied to optimize five well-known truss structures. Balancing safety precautions and size constraints concerning stresses on individual sections and constituent parts, these problems also involved competing objectives, such as reducing the weight of the structure and the maximum nodal displacement. The findings of six popular optimization methods were used to compare the results. Four industry-standard performance measures were used for this comparison and qualitative examination of the finest Pareto-front plots generated by each algorithm. The average values obtained by the Friedman rank test and comparison analysis unequivocally showed that MOHO outperformed other methods in resolving significant structure optimization problems quickly. In addition to finding and preserving more Pareto-optimal sets, the recommended algorithm produced excellent convergence and variance in the objective and decision fields. MOHO demonstrated its potential for navigating competing objectives through diversity analysis. Additionally, the swarm plots effectively visualize MOHO's solution distribution of MOHO across iterations, highlighting its superior convergence behaviour. Consequently, MOHO exhibits promise as a valuable method for tackling complex multi-objective structure optimization issues.","<method>Multi-Objective Hippopotamus Optimizer (MOHO)</method>, <method>Hippopotamus Optimizer (HO)</method>"
2024,https://openalex.org/W4390659289,Biology,Cognition-Driven Structural Prior for Instance-Dependent Label Transition Matrix Estimation,"The label transition matrix has emerged as a widely accepted method for mitigating label noise in machine learning. In recent years, numerous studies have centered on leveraging deep neural networks to estimate the label transition matrix for individual instances within the context of instance-dependent noise. However, these methods suffer from low search efficiency due to the large space of feasible solutions. Behind this drawback, we have explored that the real murderer lies in the invalid class transitions, that is, the actual transition probability between certain classes is zero but is estimated to have a certain value. To mask the <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">invalid class transitions</i> , we introduced a human-cognition-assisted method with structural information from human cognition. Specifically, we introduce a structured transition matrix network ( <bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">STMN</b> ) designed with an adversarial learning process to balance instance features and prior information from human cognition. The proposed method offers two advantages: 1) better estimation effectiveness is obtained by sparing the transition matrix and 2) better estimation accuracy is obtained with the assistance of human cognition. By exploiting these two advantages, our method parametrically estimates a sparse label transition matrix, effectively converting noisy labels into true labels. The efficiency and superiority of our proposed method are substantiated through comprehensive comparisons with state-of-the-art methods on three synthetic datasets and a real-world dataset. Our code will be available at https://github.com/WheatCao/STMN-Pytorch.","<method>label transition matrix</method>, <method>deep neural networks</method>, <method>structured transition matrix network (STMN)</method>, <method>adversarial learning process</method>"
2024,https://openalex.org/W4390880765,Biology,Opportunities and Challenges of Generative AI in Construction Industry: Focusing on Adoption of Text-Based Models,"In the last decade, despite rapid advancements in artificial intelligence (AI) transforming many industry practices, construction largely lags in adoption. Recently, the emergence and rapid adoption of advanced large language models (LLMs) like OpenAI’s GPT, Google’s PaLM, and Meta’s Llama have shown great potential and sparked considerable global interest. However, the current surge lacks a study investigating the opportunities and challenges of implementing Generative AI (GenAI) in the construction sector, creating a critical knowledge gap for researchers and practitioners. This underlines the necessity to explore the prospects and complexities of GenAI integration. Bridging this gap is fundamental to optimizing GenAI’s early stage adoption within the construction sector. Given GenAI’s unprecedented capabilities to generate human-like content based on learning from existing content, we reflect on two guiding questions: What will the future bring for GenAI in the construction industry? What are the potential opportunities and challenges in implementing GenAI in the construction industry? This study delves into reflected perception in literature, analyzes the industry perception using programming-based word cloud and frequency analysis, and integrates authors’ opinions to answer these questions. This paper recommends a conceptual GenAI implementation framework, provides practical recommendations, summarizes future research questions, and builds foundational literature to foster subsequent research expansion in GenAI within the construction and its allied architecture and engineering domains.",No methods found.
2024,https://openalex.org/W4391479301,Biology,Comparative Assessment of Two Global Sensitivity Approaches Considering Model and Parameter Uncertainty,"Abstract Global Sensitivity Analysis (GSA) is key to assisting appraisal of the behavior of hydrological systems through model diagnosis considering multiple sources of uncertainty. Uncertainty sources typically comprise incomplete knowledge in (a) conceptual and mathematical formulation of models and (b) parameters embedded in the models. In this context, there is the need for detailed investigations aimed at a robust quantification of the importance of model and parameter uncertainties in a rigorous multi‐model context. This study aims at evaluating and comparing two modern multi‐model GSA methodologies. These are the first GSA approaches embedding both model and parameter uncertainty sources and encompass the variance‐based framework based on Sobol indices (as derived by Dai &amp; Ye, 2015, https://doi.org/10.1016/j.jhydrol.2015.06.034 ) and the moment‐based approach upon which the formulation of the multi‐model AMA indices (as derived by Dell'Oca et al., 2020, https://doi.org/10.1029/2019wr025754 ) is based. We provide an assessment of various aspects of sensitivity upon considering a joint analysis of these two approaches in a multi‐model context. Our work relies on well‐established scenarios that comprise (a) a synthetic setting related to reactive transport across a groundwater system and (b) an experimentally‐based study considering heavy metal sorption onto a soil. Our study documents that the joint use of these GSA approaches can provide different while complementary information to assess mutual consistency of approaches and to enrich the information content provided by GSA under model and parameter uncertainty. While being related to groundwater settings, our results can be considered as reference for future GSA studies coping with model and parameter uncertainty.",No methods found.
2024,https://openalex.org/W4391963340,Biology,Ultrahigh-Spatial-Resolution Photon-counting Detector CT Angiography of Coronary Artery Disease for Stenosis Assessment,"Background Coronary CT angiography is a first-line test in coronary artery disease but is limited by severe calcifications. Photon-counting-detector (PCD) CT improves spatial resolution. Purpose To investigate the effect of improved spatial resolution on coronary stenosis assessment and reclassification. Materials and Methods Coronary stenoses were evaluated prospectively in a vessel phantom (in vitro) containing two stenoses (25%, 50%), and retrospectively in patients (in vivo) who underwent ultrahigh-spatial-resolution cardiac PCD CT (from July 2022 to April 2023). Images were reconstructed at standard resolution (section thickness, 0.6 mm; increment, 0.4 mm; Bv44 kernel), high spatial resolution (section thickness, 0.4 mm; increment, 0.2 mm; Bv44 kernel), and ultrahigh spatial resolution (section thickness, 0.2; increment, 0.1 mm; Bv64 kernel). Percentages of diameter stenosis (DS) were compared between reconstructions. In vitro values were compared with the manufacturer specifications of the phantom and patient results were assessed regarding effects on Coronary Artery Disease Reporting and Data System (CAD-RADS) reclassification. Results The in vivo sample included 114 patients (mean age, 68 years ± 9 [SD]; 71 male patients). In vitro percentage DS measurements were more accurate with increasing spatial resolution for both 25% and 50% stenoses (mean bias for standard resolution, high spatial resolution, and ultrahigh spatial resolution, respectively: 10.1%, 8.0%, and 2.3%;",No methods found.
2024,https://openalex.org/W4392385353,Biology,Virtual Worlds for Learning in Metaverse: A Narrative Review,"As digital technologies continue to evolve, they offer unprecedented opportunities to transform traditional educational paradigms. Virtual worlds offer a dynamic and immersive platform for fostering sustainability education, bridging the gap between theoretical knowledge and practical application. In these interactive environments, students can engage with complex ecological systems and sustainability challenges in a risk-free setting, allowing for experimentation and exploration that would be impractical or impossible in the real world. This study aims to investigate the application of various types of virtual worlds in educational settings, examine their characteristics and potential, and explore how they foster critical 21st-century skills like critical thinking, creativity, communication, and collaboration. This paper comprehensively explores various types of virtual worlds—Adventure World, Simulation World, Creative World, Role-Playing World, and Collaborative World—assessing their impact on educational processes and outcomes. Adventure Worlds, with narrative-driven quests, engage students in exploratory learning within a story context. Simulation Worlds replicate real-world environments, allowing students to practice and hone practical skills in a risk-free setting. Creative Worlds provide open-ended, sandbox-like environments where innovation and imagination are paramount. Role-Playing Worlds facilitate empathy and perspective-taking through character-driven scenarios, while Collaborative Worlds emphasize teamwork and problem-solving in group projects. The narrative review methodology was adopted for the comprehensive analysis and synthesis of the literature to assess the impact and integration of virtual worlds in education, focusing on identifying trends, challenges, and opportunities within this domain. The evaluation methodology used in this study incorporates a mix of the Theory of Inventive Problem Solving (TRIZ), Concept-Knowledge (C-K) theory, Structure-behavior-function (SBF) modeling, the Framework for 21st Century Learning (P21), and Universal Design for Learning (UDL) to evaluate the characteristics and educational potential of different virtual world types. Findings indicate that virtual worlds effectively support critical thinking, creativity, communication, and collaboration skills, presenting a comprehensive analysis of how these environments can support, supplement, or transform traditional educational models. The main outcome of the study is the comprehensive exploration of various types of virtual worlds—Adventure World, Simulation World, Creative World, Role-Playing World, and Collaborative World—in education, demonstrating their significant potential to enhance learning experiences and outcomes through immersive, interactive environments that foster critical thinking, creativity, communication, and collaboration skills.",No methods found.
2024,https://openalex.org/W4392547070,Biology,Challenges in High-Throughput Inorganic Materials Prediction and Autonomous Synthesis,"Materials discovery lays the foundation for many technological advancements. The prediction and discovery of new materials are not simple tasks. Here, we outline some basic principles of solid-state chemistry, which might help to advance both, and discuss pitfalls and challenges in materials discovery. Using the recent work of Szymanski [Nature 624, 86 (2023)], which reported the autonomous discovery of 43 novel materials, as an example, we discuss problems that can arise in unsupervised materials discovery and hope that by addressing these, autonomous materials discovery can be brought closer to reality. We discuss all 43 synthetic products and point out four common shortfalls in the analysis. These errors unfortunately lead to the conclusion that no new materials have been discovered in that work. We conclude that there are two important points of improvement that require future work from the community, as follows. (i) Automated Rietveld analysis of powder x-ray diffraction data is not yet reliable. Future improvement of such, and the development of a reliable artificial-intelligence-based tool for Rietveld fitting, would be very helpful, not only for autonomous materials discovery but also for the community in general. (ii) We find that disorder in materials is often neglected in predictions. The predicted compounds investigated herein have all their elemental components located on distinct crystallographic positions but in reality, elements can share crystallographic sites, resulting in higher-symmetry space groups and—very often—known alloys or solid solutions. This error might be related to the difficulty of modeling disorder in a computationally economical way and needs to be addressed both by computational and experimental material scientists. We find that two thirds of the claimed successful materials in Szymanski are likely to be known compositionally disordered versions of the predicted ordered compounds. We highlight important issues in materials discovery, computational chemistry, and autonomous interpretation of x-ray diffraction. We discuss concepts of materials discovery from an experimentalist point of view, which we hope will be helpful for the community to further advance this important new aspect of our field. Published by the American Physical Society 2024",No methods found.
2024,https://openalex.org/W4392611134,Biology,"Employing advanced control, energy storage, and renewable technologies to enhance power system stability","As the world witnesses a surge in the adoption of renewable energy sources to meet the surging global power demands, the dynamic and intermittent nature of these sources emerges as a significant hurdle. This article extensively explores the potential of advanced control systems, energy storage technologies, and renewable resources to fortify stability within power systems. Advanced control methodologies are strategically amalgamated with energy storage deployment and the utilization of renewable energy, to advance the reliability, predictability, and sustainability of power systems. The stability analysis, with a dedicated focus on Input-to-input-to-state stability (ISS), is conducted meticulously by applying the Lyapunov function and the Reciprocally Convex Approach, resulting in an impressive stability rate of 23.6%. Additionally, Positive Realness is substantiated by extracting Linear Matrix Inequalities (LMI) in the context of Enhancing Grid Stability with Wind Power. The study places particular emphasis on evaluating ISS and Passivity in both delayed and non-delayed systems, with a specific focus on neutral time-delay systems. This evaluation involves the selection of an appropriate Lyapunov-Krasovskii Functional (LKF) and its derivation, coupled with the integration of reciprocally convex methods, descriptive approach, and Jensen inequality. The outcomes of these analyses shed light on the causes of excess energy and its effective storage, along with highlighting the synergistic impact of integrating renewable sources and controlling grid frequency, voltage, and power in real time. The study also accentuates the robustness achieved through Enhanced Stability and the mitigation of Reduced Fluctuations, especially in the context of renewable energy sources.",No methods found.
2024,https://openalex.org/W4392980686,Biology,Data-driven evolution of water quality models: An in-depth investigation of innovative outlier detection approaches-A case study of Irish Water Quality Index (IEWQI) model,"Recently, there has been a significant advancement in the water quality index (WQI) models utilizing data-driven approaches, especially those integrating machine learning and artificial intelligence (ML/AI) technology. Although, several recent studies have revealed that the data-driven model has produced inconsistent results due to the data outliers, which significantly impact model reliability and accuracy. The present study was carried out to assess the impact of data outliers on a recently developed Irish Water Quality Index (IEWQI) model, which relies on data-driven techniques. To the author's best knowledge, there has been no systematic framework for evaluating the influence of data outliers on such models. For the purposes of assessing the outlier impact of the data outliers on the water quality (WQ) model, this was the first initiative in research to introduce a comprehensive approach that combines machine learning with advanced statistical techniques. The proposed framework was implemented in Cork Harbour, Ireland, to evaluate the IEWQI model's sensitivity to outliers in input indicators to assess the water quality. In order to detect the data outlier, the study utilized two widely used ML techniques, including Isolation Forest (IF) and Kernel Density Estimation (KDE) within the dataset, for predicting WQ with and without these outliers. For validating the ML results, the study used five commonly used statistical measures. The performance metric (R2) indicates that the model performance improved slightly (R2 increased from 0.92 to 0.95) in predicting WQ after removing the data outlier from the input. But the IEWQI scores revealed that there were no statistically significant differences among the actual values, predictions with outliers, and predictions without outliers, with a 95% confidence interval at p < 0.05. The results of model uncertainty also revealed that the model contributed <1% uncertainty to the final assessment results for using both datasets (with and without outliers). In addition, all statistical measures indicated that the ML techniques provided reliable results that can be utilized for detecting outliers and their impacts on the IEWQI model. The findings of the research reveal that although the data outliers had no significant impact on the IEWQI model architecture, they had moderate impacts on the rating schemes' of the model. This finding indicated that detecting the data outliers could improve the accuracy of the IEWQI model in rating WQ as well as be helpful in mitigating the model eclipsing problem. In addition, the results of the research provide evidence of how the data outliers influenced the data-driven model in predicting WQ and reliability, particularly since the study confirmed that the IEWQI model's could be effective for accurately rating WQ despite the presence of the data outliers in the input. It could occur due to the spatio-temporal variability inherent in WQ indicators. However, the research assesses the influence of data input outliers on the IEWQI model and underscores important areas for future investigation. These areas include expanding temporal analysis using multi-year data, examining spatial outlier patterns, and evaluating detection methods. Moreover, it is essential to explore the real-world impacts of revised rating categories, involve stakeholders in outlier management, and fine-tune model parameters. Analysing model performance across varying temporal and spatial resolutions and incorporating additional environmental data can significantly enhance the accuracy of WQ assessment. Consequently, this study offers valuable insights to strengthen the IEWQI model's robustness and provides avenues for enhancing its utility in broader WQ assessment applications. Moreover, the study successfully adopted the framework for evaluating how data input outliers affect the data-driven model, such as the IEWQI model. The current study has been carried out in Cork Harbour for only a single year of WQ data. The framework should be tested across various domains for evaluating the response of the IEWQI model's in terms of the spatio-temporal resolution of the domain. Nevertheless, the study recommended that future research should be conducted to adjust or revise the IEWQI model's rating schemes and investigate the practical effects of data outliers on updated rating categories. However, the study provides potential recommendations for enhancing the IEWQI model's adaptability and reveals its effectiveness in expanding its applicability in more general WQ assessment scenarios.","<method>Isolation Forest (IF)</method>, <method>Kernel Density Estimation (KDE)</method>"
2024,https://openalex.org/W4396707011,Biology,DeepAVP-TPPred: identification of antiviral peptides using transformed image-based localized descriptors and binary tree growth algorithm,"Abstract Motivation Despite the extensive manufacturing of antiviral drugs and vaccination, viral infections continue to be a major human ailment. Antiviral peptides (AVPs) have emerged as potential candidates in the pursuit of novel antiviral drugs. These peptides show vigorous antiviral activity against a diverse range of viruses by targeting different phases of the viral life cycle. Therefore, the accurate prediction of AVPs is an essential yet challenging task. Lately, many machine learning-based approaches have developed for this purpose; however, their limited capabilities in terms of feature engineering, accuracy, and generalization make these methods restricted. Results In the present study, we aim to develop an efficient machine learning-based approach for the identification of AVPs, referred to as DeepAVP-TPPred, to address the aforementioned problems. First, we extract two new transformed feature sets using our designed image-based feature extraction algorithms and integrate them with an evolutionary information-based feature. Next, these feature sets were optimized using a novel feature selection approach called binary tree growth Algorithm. Finally, the optimal feature space from the training dataset was fed to the deep neural network to build the final classification model. The proposed model DeepAVP-TPPred was tested using stringent 5-fold cross-validation and two independent dataset testing methods, which achieved the maximum performance and showed enhanced efficiency over existing predictors in terms of both accuracy and generalization capabilities. Availability and implementation https://github.com/MateeullahKhan/DeepAVP-TPPred.","<method>machine learning-based approaches</method>, <method>image-based feature extraction algorithms</method>, <method>binary tree growth Algorithm</method>, <method>deep neural network</method>"
2024,https://openalex.org/W4390825883,Biology,Transforming Object Design and Creation: Biomaterials and Contemporary Manufacturing Leading the Way,"In the field of three-dimensional object design and fabrication, this paper explores the transformative potential at the intersection of biomaterials, biopolymers, and additive manufacturing. Drawing inspiration from the intricate designs found in the natural world, this study contributes to the evolving landscape of manufacturing and design paradigms. Biomimicry, rooted in emulating nature's sophisticated solutions, serves as the foundational framework for developing materials endowed with remarkable characteristics, including adaptability, responsiveness, and self-transformation. These advanced engineered biomimetic materials, featuring attributes such as shape memory and self-healing properties, undergo rigorous synthesis and characterization procedures, with the overarching goal of seamless integration into the field of additive manufacturing. The resulting synergy between advanced manufacturing techniques and nature-inspired materials promises to revolutionize the production of objects capable of dynamic responses to environmental stimuli. Extending beyond the confines of laboratory experimentation, these self-transforming objects hold significant potential across diverse industries, showcasing innovative applications with profound implications for object design and fabrication. Through the reduction of waste generation, minimization of energy consumption, and the reduction of environmental footprint, the integration of biomaterials, biopolymers, and additive manufacturing signifies a pivotal step towards fostering ecologically conscious design and manufacturing practices. Within this context, inanimate three-dimensional objects will possess the ability to transcend their static nature and emerge as dynamic entities capable of evolution, self-repair, and adaptive responses in harmony with their surroundings. The confluence of biomimicry and additive manufacturing techniques establishes a seminal precedent for a profound reconfiguration of contemporary approaches to design, manufacturing, and ecological stewardship, thereby decisively shaping a more resilient and innovative global milieu.",No methods found.
2024,https://openalex.org/W4391057570,Biology,Scalable deposition techniques for large-area perovskite photovoltaic technology: A multi-perspective review,"Perovskite photovoltaic technology holds great promise for the solar energy industry due to its high efficiency and fascinating potentialities for low-cost production. However, overcoming stability and scalability challenges by addressing environmental concerns is crucial for their successful deployment in the market. While perovskite solar cells have shown promising power conversion efficiency and long-term lifetime once produced by using small-scale production facilities, the scaling up of the device production without losing in performance and stability, still remains a not-trivial task. As matter of the fact, research and development efforts are currently ongoing to bring the perovskite solar technology closer to the market. From this point of view, the choice and the optimization of a specific deposition method depends on several factors like scalability, cost-effectiveness, and peculiar requirements required by the featured application. In this review, a comprehensive description of deposition methods for large area substrate is provided followed by an exhaustive collection of the main published progresses on large area perovskite solar devices. In particular, the large-area compatible deposition techniques will be categorized in the two main families based on solution and physical deposition processes, while pros and cons of both families are investigated in view of their potentialities for low-cost and high-throughput industrial production. Not least, hybrid deposition methods combining specific features of both solution and vacuum deposition techniques will be suggested as a way to leverage both their advantages. In addition, the main characterization techniques for large area devices will be described as a powerful way to select the proper deposition technique for supporting the development of commercial product based on perovskite technology. The opened challenges and the new routes for moving perovskite from lab to fab are finally suggested and discussed.",No methods found.
2024,https://openalex.org/W4391235397,Biology,Real-Time Plant Disease Dataset Development and Detection of Plant Disease Using Deep Learning,"Agriculture plays a significant role in meeting food needs and providing food security for the increasingly growing global population, which has increased by 0.88% since 2022. Plant diseases can reduce food production and affect food security. Worldwide crop loss due to plant disease is estimated to be around 14.1%. The lack of proper identification of plant disease at the early stages of infection can result in inappropriate disease control measures. Therefore, the automatic identification and diagnosis of plant diseases are highly recommended. Lack of availability of large amounts of data that are not processed to a large extent is one of the main challenges in plant disease diagnosis. In the current manuscript, we developed datasets for food grains specifically for rice, wheat, and maize to address the identified challenges. The developed datasets consider the common diseases (two bacterial diseases and two fungal diseases of rice, four fungal diseases of maize, and four fungal diseases of wheat) that affect crop yields and cause damage to the whole plant. The datasets developed were applied to eight fine-tuned deep learning models with the same training hyperparameters. The experimental results based on eight fine-tuned deep learning models show that, while recognizing maize leaf diseases, the models Xception and MobileNet performed best with a testing accuracy of 0.9580 and 0.9464 respectively. Similarly, while recognizing the wheat leaf diseases, the models MobileNetV2 and MobileNet performed best with a testing accuracy of 0.9632 and 0.9628 respectively. The Xception and Inception V3 models performed best, with a testing accuracy of 0.9728 and 0.9620, respectively, for recognizing rice leaf diseases. The research also proposes a new convolutional neural network (CNN) model trained from scratch on all three food grain datasets developed. The proposed model performs well and shows a testing accuracy of 0.9704, 0.9706, and 0.9808 respectively on the maize, rice, and wheat datasets.","<method>fine-tuned deep learning models</method>, <method>Xception</method>, <method>MobileNet</method>, <method>MobileNetV2</method>, <method>Inception V3</method>, <method>convolutional neural network (CNN) model trained from scratch</method>"
2024,https://openalex.org/W4393070891,Biology,Economic and environmental benefits of digital agricultural technologies in crop production: A review,"This comprehensive review delved into the economic and environmental benefits of Digital Agricultural Technologies (DATs) in crop production, synthesising data from 136 peer-reviewed papers and 28 documents with empirical data from relevant EU projects. This analysis highlighted the substantial contribution of DATs across five key categories: Recording and Mapping Technologies (RMT), Guidance and Controlled Traffic Farming (CTF) Technologies, Variable Rate Technologies (VRT), Robotic Systems or Smart Machines (RSSM), and Farm Management Information Systems (FMIS). Specifically, it provided an overview of the various benefits that these technologies can deliver with the most significant ones revealing reductions of up to 80% in fertiliser usage with RMT and CTF applications, while VRT demonstrated a 60% decrease in fertiliser usage and up to 80% reduction in pesticide use. VRT also showed an increase in yield by 62%. RSSM was able to reduce labour by 97% and diesel consumption by 50%. FMIS improved yield by 10% to 15%, facilitating simultaneous reductions in labour and input costs, illustrating the critical role of integrated digital solutions in enhancing agricultural efficiency and sustainability. From an environmental point of view, VRT has emerged as a major factor in environmental sustainability, demonstrating water savings of 20% to 50% in vineyards and pear orchards and a significant reduction in greenhouse gas emissions. These findings highlighted the significant benefits of DATs on enhancing productivity and promoting environmental sustainability. They provided a compelling case for further investment and research in DATs through quantifiable benefits in crop production.",No methods found.
2024,https://openalex.org/W4396494945,Biology,Vision–language foundation model for echocardiogram interpretation,"Abstract The development of robust artificial intelligence models for echocardiography has been limited by the availability of annotated clinical data. Here, to address this challenge and improve the performance of cardiac imaging models, we developed EchoCLIP, a vision–language foundation model for echocardiography, that learns the relationship between cardiac ultrasound images and the interpretations of expert cardiologists across a wide range of patients and indications for imaging. After training on 1,032,975 cardiac ultrasound videos and corresponding expert text, EchoCLIP performs well on a diverse range of benchmarks for cardiac image interpretation, despite not having been explicitly trained for individual interpretation tasks. EchoCLIP can assess cardiac function (mean absolute error of 7.1% when predicting left ventricular ejection fraction in an external validation dataset) and identify implanted intracardiac devices (area under the curve (AUC) of 0.84, 0.92 and 0.97 for pacemakers, percutaneous mitral valve repair and artificial aortic valves, respectively). We also developed a long-context variant (EchoCLIP-R) using a custom tokenizer based on common echocardiography concepts. EchoCLIP-R accurately identified unique patients across multiple videos (AUC of 0.86), identified clinical transitions such as heart transplants (AUC of 0.79) and cardiac surgery (AUC 0.77) and enabled robust image-to-text search (mean cross-modal retrieval rank in the top 1% of candidate text reports). These capabilities represent a substantial step toward understanding and applying foundation models in cardiovascular imaging for preliminary interpretation of echocardiographic findings.","<method>vision–language foundation model</method>, <method>EchoCLIP</method>, <method>EchoCLIP-R</method>, <method>custom tokenizer</method>"
2024,https://openalex.org/W4398780590,Biology,Greater cane rat algorithm (GCRA): A nature-inspired metaheuristic for optimization problems,"This paper introduces a new metaheuristic technique known as the Greater Cane Rat Algorithm (GCRA) for addressing optimization problems. The optimization process of GCRA is inspired by the intelligent foraging behaviors of greater cane rats during and off mating season. Being highly nocturnal, they are intelligible enough to leave trails as they forage through reeds and grass. Such trails would subsequently lead to food and water sources and shelter. The exploration phase is achieved when they leave the different shelters scattered around their territory to forage and leave trails. It is presumed that the alpha male maintains knowledge about these routes, and as a result, other rats modify their location according to this information. Also, the males are aware of the breeding season and separate themselves from the group. The assumption is that once the group is separated during this season, the foraging activities are concentrated within areas of abundant food sources, which aids the exploitation. Hence, the smart foraging paths and behaviors during the mating season are mathematically represented to realize the design of the GCR algorithm and carry out the optimization tasks. The performance of GCRA is tested using twenty-two classical benchmark functions, ten CEC 2020 complex functions, and the CEC 2011 real-world continuous benchmark problems. To further test the performance of the proposed algorithm, six classic problems in the engineering domain were used. Furthermore, a thorough analysis of computational and convergence results is presented to shed light on the efficacy and stability levels of GCRA. The statistical significance of the results is compared with ten state-of-the-art algorithms using Friedman's and Wilcoxon's signed rank tests. These findings show that GCRA produced optimal or nearly optimal solutions and evaded the trap of local minima, distinguishing it from the rival optimization algorithms employed to tackle similar problems. The GCRA optimizer source code is publicly available at: https://www.mathworks.com/matlabcentral/fileexchange/165241-greater-cane-rat-algorithm-gcra",<method>Greater Cane Rat Algorithm (GCRA)</method>
2024,https://openalex.org/W4391089359,Biology,Groundwater level prediction using an improved SVR model integrated with hybrid particle swarm optimization and firefly algorithm,"The demand for water resources has increased due to rapid increase of metropolitan areas brought on by growth in population and industrialisation. In addition, the groundwater recharge is being afftected by shifting land use pattern caused by urban development. Using precise and trustworthy estimates of groundwater level is vital for the sustainable groundwater resources management in the face of changing climatic circumstances. In this context, machine learning (ML) methods offer a new and promising approach for accurately forecasting long-term changes in the groundwater level (GWL) without computational effort of developing a comprehensive flow model. In order to simulate GWL, five data-driven (DD) models, including the hybridization of support vector regression (SVR) with two optimisation algorithms i.e., firefly algorithm and particle swarm optimisation (FFAPSO), SVR-FFA, SVR-PSO, SVR and Multilayer perception (MLP), have been examined in the present study. Spatial clustering was utilised to choose four observation wells within Cuttack district in order to study and assess the water levels. Six scenarios were created by incorporating numerous variables, such as GWL in the previous months, evapotranspiration, temperature, precipitation, and river discharge. The goal was to identify the variables that were most efficient in predicting GWL. The SVR-FFAPSO model performs best in GWL forecasting for Khuntuni station, according to the quantitative analysis with correlation coefficient (R) = 0.9978, Nash–Sutcliffe efficiency (NSE) = 0.9933, mean absolute error (MAE) = 0.00025 (m), root mean squared error (RMSE) = 0.00775 (m) during the training phase. It is advised that groundwater monitoring network and data collecting system are strengthen in India for ensuring effective modelling of long-term management of groundwater resources.","<method>support vector regression (SVR)</method>, <method>firefly algorithm</method>, <method>particle swarm optimisation (PSO)</method>, <method>SVR-FFA</method>, <method>SVR-PSO</method>, <method>Multilayer perception (MLP)</method>"
2024,https://openalex.org/W4391145008,Biology,Assessing ChatGPT’s Mastery of Bloom’s Taxonomy Using Psychosomatic Medicine Exam Questions: Mixed-Methods Study,"Background Large language models such as GPT-4 (Generative Pre-trained Transformer 4) are being increasingly used in medicine and medical education. However, these models are prone to “hallucinations” (ie, outputs that seem convincing while being factually incorrect). It is currently unknown how these errors by large language models relate to the different cognitive levels defined in Bloom’s taxonomy. Objective This study aims to explore how GPT-4 performs in terms of Bloom’s taxonomy using psychosomatic medicine exam questions. Methods We used a large data set of psychosomatic medicine multiple-choice questions (N=307) with real-world results derived from medical school exams. GPT-4 answered the multiple-choice questions using 2 distinct prompt versions: detailed and short. The answers were analyzed using a quantitative approach and a qualitative approach. Focusing on incorrectly answered questions, we categorized reasoning errors according to the hierarchical framework of Bloom’s taxonomy. Results GPT-4’s performance in answering exam questions yielded a high success rate: 93% (284/307) for the detailed prompt and 91% (278/307) for the short prompt. Questions answered correctly by GPT-4 had a statistically significant higher difficulty than questions answered incorrectly (P=.002 for the detailed prompt and P&lt;.001 for the short prompt). Independent of the prompt, GPT-4’s lowest exam performance was 78.9% (15/19), thereby always surpassing the “pass” threshold. Our qualitative analysis of incorrect answers, based on Bloom’s taxonomy, showed that errors were primarily in the “remember” (29/68) and “understand” (23/68) cognitive levels; specific issues arose in recalling details, understanding conceptual relationships, and adhering to standardized guidelines. Conclusions GPT-4 demonstrated a remarkable success rate when confronted with psychosomatic medicine multiple-choice exam questions, aligning with previous findings. When evaluated through Bloom’s taxonomy, our data revealed that GPT-4 occasionally ignored specific facts (remember), provided illogical reasoning (understand), or failed to apply concepts to a new situation (apply). These errors, which were confidently presented, could be attributed to inherent model biases and the tendency to generate outputs that maximize likelihood.","<method>large language models</method>, <method>GPT-4 (Generative Pre-trained Transformer 4)</method>"
2024,https://openalex.org/W4391750864,Biology,Efficacy of virtual reality-based training programs and games on the improvement of cognitive disorders in patients: a systematic review and meta-analysis,"Abstract Introduction Cognitive impairments present challenges for patients, impacting memory, attention, and problem-solving abilities. Virtual reality (VR) offers innovative ways to enhance cognitive function and well-being. This study explores the effects of VR-based training programs and games on improving cognitive disorders. Methods PubMed, Scopus, and Web of Science were systematically searched until May 20, 2023. Two researchers selected and extracted data based on inclusion and exclusion criteria, resolving disagreements through consultation with two other authors. Inclusion criteria required studies of individuals with any cognitive disorder engaged in at least one VR-based training session, reporting cognitive impairment data via scales like the MMSE. Only English-published RCTs were considered, while exclusion criteria included materials not primarily focused on the intersection of VR and cognitive disorders. The risk of bias in the included studies was assessed using the MMAT tool. Publication bias was assessed using funnel plots and Egger’s test. The collected data were utilized to calculate the standardized mean differences (Hedges’s g) between the treatment and control groups. The heterogeneity variance was estimated using the Q test and I2 statistic. The analysis was conducted using Stata version 17.0. Results Ten studies were included in the analysis out of a total of 3,157 retrieved articles. VR had a statistically significant improvement in cognitive impairments among patients (Hedges’s g = 0.42, 95% CI: 0.15, 0.68; p _value = 0.05). games (Hedges’s g = 0.61, 95% CI: 0.30, 0.39; p _value = 0.20) had a more significant impact on cognitive impairment improvement compared to cognitive training programs (Hedges’s g = 0.29, 95% CI: -0.11, 0.69; p _value = 0.24). The type of VR intervention was a significant moderator of the heterogeneity between studies. Conclusion VR-based interventions have demonstrated promise in enhancing cognitive function and addressing cognitive impairment, highlighting their potential as valuable tools in improving care for individuals with cognitive disorders. The findings underscore the relevance of incorporating virtual reality into therapeutic approaches for cognitive disorders.",No methods found.
2024,https://openalex.org/W4391855187,Biology,Machine learning-assisted in-situ adaptive strategies for the control of defects and anomalies in metal additive manufacturing,"In metal additive manufacturing (AM), the material microstructure and part geometry are formed incrementally. Consequently, the resulting part could be defect- and anomaly-free if sufficient care is taken to deposit each layer under optimal process conditions. Conventional closed-loop control (CLC) engineering solutions which sought to achieve this were deterministic and rule-based, thus resulting in limited success in the stochastic environment experienced in the highly dynamic AM process. On the other hand, emerging machine learning (ML) based strategies are better suited to providing the robustness, scope, flexibility, and scalability required for process control in an uncertain environment. Offline ML models that help optimise AM process parameters before a build begins and online ML models that efficiently processed in-situ sensory data to detect and diagnose flaws in real-time (or near-real-time) have been developed. However, ML models that enable a process to take evasive or corrective actions in relation to flaws via on the fly decision-making are only emerging. These models must possess prognostic capabilities to provide context-sensitive recommendations for in-situ process control based on real-time diagnostics. In this article, we pinpoint the shortcomings in traditional CLC strategies, and provide a framework for defect and anomaly control through ML-assisted CLC in AM. We discuss flaws in terms of their causes, in-situ detectability, and controllability, and examine their management under three scenarios: avoidance, mitigation, and repair. Then, we summarise the research into ML models developed for offline optimisation and in-situ diagnosis before initiating a detailed conversation on the implementation of ML-assisted in-situ process control. We found that researchers favoured reinforcement learning approaches or inverse ML models for making rapid, situation-aware control decisions. We also observed that, to-date, the defects addressed were those that may be quantified relatively easily autonomously, and that mitigation (rather than avoidance or repair) was the aim of ML-assisted in-situ control strategies. Additionally, we highlight the various technologies that must seamlessly combine to advance the field of autonomous in-situ control so that it becomes a reality in industrial settings. Finally, we raise awareness of seldom discussed, yet highly pertinent, topics relevant to adaptive control. Our work closes a significant gap in the current AM literature by broaching wide-ranging discussions on matters relevant to in-situ adaptive control in AM.","<method>machine learning (ML) based strategies</method>, <method>offline ML models</method>, <method>online ML models</method>, <method>reinforcement learning approaches</method>, <method>inverse ML models</method>"
2024,https://openalex.org/W4392640075,Biology,Performance assessment of machine learning algorithms for mapping of land use/land cover using remote sensing data,"The rapid increase in population accelerates the rate of change of Land use/Land cover (LULC) in various parts of the world. This phenomenon caused a huge strain for natural resources. Hence, continues monitoring of LULC changes gained a significant importance for management of natural resources and assessing the climate change impacts. Recently, application of machine learning algorithms on RS (remote sensing) data for rapid and accurate mapping of LULC gained significant importance due to growing need of LULC estimation for ecosystem services, natural resource management and environmental management. Hence, it is crucial to access and compare the performance of different machine learning classifiers for accurate mapping of LULC. The primary objective of this study was to compare the performance of CART (Classification and Regression Tree), RF (Random Forest) and SVM (Support Vector Machine) for LULC estimation by processing RS data on Google Earth Engine (GEE). In total four classes of LULC (Water Bodies, Vegetation Cover, Urban Land and Barren Land) for city of Lahore were extracted using satellite images from Landsat-7, Landsat-8 and Landsat-9 for years 2008, 2015 and 2022, respectively. According to results, RF is the best performing classifier with maximum overall accuracy of 95.2% and highest Kappa coefficient value of 0.87, SVM achieved maximum accuracy of 89.8% with highest Kappa of 0.84 and CART showed maximum overall accuracy of 89.7% with Kappa value of 0.79. Results from this study can give assistance for decision makers, planners and RS experts to choose a suitable machine learning algorithm for LULC classification in an unplanned urbanized city like Lahore.","<method>Classification and Regression Tree (CART)</method>, <method>Random Forest (RF)</method>, <method>Support Vector Machine (SVM)</method>"
2024,https://openalex.org/W4390480832,Biology,Joint Optimization Risk Factor and Energy Consumption in IoT Networks With TinyML-Enabled Internet of UAVs,"The high mobility of Internet of Unmanned Aerial Vehicles (IUAVs) has attracted attention in the field of data collection. With the rapid development of the Internet of Things (IoT), more and more data are generated by IoT networks. IUAV-aided IoT networks can efficiently collect data in specific areas, which is of great significance in disaster relief. In the data collection task, it is necessary to plan the flight trajectory for the data collector—IUAV, so that the IUAV can collect data efficiently. However, existing research basically only considers the efficiency of data collection by IUAVs, but rarely considers the safety of IUAVs during flight. Therefore, this paper proposes an IUAV trajectory planning algorithm that integrates energy efficiency and safety using local search to address the issues mentioned above. At the same time, a Tiny Machine Learning (TinyML) algorithm is designed to assist the IUAV in making real-time decisions during flight. First, we build a general mathematical model that describes the risk in a particular region. Then consider guiding the IUAV to a safer trajectory by introducing virtual nodes in the flight trajectory. Furthermore, we designed a local search algorithm for the three tasks of IUAV access sequence, IoT Networks cluster heads selection and virtual nodes selection, and solved them through iterative optimization. We also consider the unreachable situation of the virtual nodes and use TinyML technology to help the IUAV adjust the position of the virtual nodes in real time in case of an emergency.In the end, an IUAV trajectory is obtained that can efficiently collect IoT networks' data and fly safely. We have conducted a large number of simulation experiments to demonstrate the efficiency of the proposed algorithm compared to the baseline algorithm.","<method>Tiny Machine Learning (TinyML)</method>, <method>local search algorithm</method>"
2024,https://openalex.org/W4390754233,Biology,Groundwater Quality Assessment and Irrigation Water Quality Index Prediction Using Machine Learning Algorithms,"The evaluation of groundwater quality is crucial for irrigation purposes; however, due to financial constraints in developing countries, such evaluations suffer from insufficient sampling frequency, hindering comprehensive assessments. Therefore, associated with machine learning approaches and the irrigation water quality index (IWQI), this research aims to evaluate the groundwater quality in Naama, a region in southwest Algeria. Hydrochemical parameters (cations, anions, pH, and EC), qualitative indices (SAR,RSC,Na%,MH,and PI), as well as geospatial representations were used to determine the groundwater’s suitability for irrigation in the study area. In addition, efficient machine learning approaches for forecasting IWQI utilizing Extreme Gradient Boosting (XGBoost), Support vector regression (SVR), and K-Nearest Neighbours (KNN) models were implemented. In this research, 166 groundwater samples were used to calculate the irrigation index. The results showed that 42.18% of them were of excellent quality, 34.34% were of very good quality, 6.63% were good quality, 9.64% were satisfactory, and 4.21% were considered unsuitable for irrigation. On the other hand, results indicate that XGBoost excels in accuracy and stability, with a low RMSE (of 2.8272 and a high R of 0.9834. SVR with only four inputs (Ca2+, Mg2+, Na+, and K) demonstrates a notable predictive capability with a low RMSE of 2.6925 and a high R of 0.98738, while KNN showcases robust performance. The distinctions between these models have important implications for making informed decisions in agricultural water management and resource allocation within the region.","<method>Extreme Gradient Boosting (XGBoost)</method>, <method>Support Vector Regression (SVR)</method>, <method>K-Nearest Neighbours (KNN)</method>"
2024,https://openalex.org/W4391347933,Biology,CCL-DTI: contributing the contrastive loss in drug–target interaction prediction,"Abstract Background The Drug–Target Interaction (DTI) prediction uses a drug molecule and a protein sequence as inputs to predict the binding affinity value. In recent years, deep learning-based models have gotten more attention. These methods have two modules: the feature extraction module and the task prediction module. In most deep learning-based approaches, a simple task prediction loss (i.e., categorical cross entropy for the classification task and mean squared error for the regression task) is used to learn the model. In machine learning, contrastive-based loss functions are developed to learn more discriminative feature space. In a deep learning-based model, extracting more discriminative feature space leads to performance improvement for the task prediction module. Results In this paper, we have used multimodal knowledge as input and proposed an attention-based fusion technique to combine this knowledge. Also, we investigate how utilizing contrastive loss function along the task prediction loss could help the approach to learn a more powerful model. Four contrastive loss functions are considered: (1) max-margin contrastive loss function, (2) triplet loss function, (3) Multi-class N-pair Loss Objective, and (4) NT-Xent loss function. The proposed model is evaluated using four well-known datasets: Wang et al. dataset, Luo's dataset, Davis, and KIBA datasets. Conclusions Accordingly, after reviewing the state-of-the-art methods, we developed a multimodal feature extraction network by combining protein sequences and drug molecules, along with protein–protein interaction networks and drug–drug interaction networks. The results show it performs significantly better than the comparable state-of-the-art approaches.","<method>deep learning-based models</method>, <method>attention-based fusion technique</method>, <method>contrastive loss function</method>, <method>max-margin contrastive loss function</method>, <method>triplet loss function</method>, <method>Multi-class N-pair Loss Objective</method>, <method>NT-Xent loss function</method>, <method>multimodal feature extraction network</method>"
2024,https://openalex.org/W4391248672,Biology,Pedestrian 3D Shape Understanding for Person Re-Identification via Multi-View Learning,"Recent development in computing power has resulted in performance improvements on holistic(none-occluded) person Re-Identification (ReID) tasks. Nevertheless, the precision of the recent research will diminish when a pedestrian is obstructed by obstacles. Within the realm of 2D space, the loss of information from obstructed objects continues to pose significant challenges in the context of person ReID. Person is a 3D non-grid object, and thus semantic representation learning in only 2D space limits the understanding of occluded person. In the present work, we propose a network based on 3D multi-view learning, allowing it to acquire geometric and shape details of an occluded pedestrian from 3D space. Simultaneously, it capitalizes on advancements in 2D-based networks to extract semantic representations from 3D multi-views. Specifically, the surface random selection strategy is proposed to convert images of 2D RGB into 3D multi-views. Using this strategy, we build four extensive 3D multi-view data collections for person ReID. After that, Pedestrian 3D Shape Understanding for Person Re-Identification via Multi-View Learning(MV-3DSReID), is proposed for identifying the person by learning person geometry and structure representation from the groups of multi-view images. In comparison to alternative data formats (e.g., 2D RGB, 3D point cloud), multi-view images complement each other's detailed features of the 3D object by adjusting rendering viewpoints, thus facilitating a more comprehensive understanding of the person for both holistic and occluded ReID situations. Experiments on occluded and holistic ReID tasks demonstrate performance levels comparable to state-of-the-art methods, validating the effectiveness of our proposed approach in tackling challenges related to occlusion. The code is available at https://github.com/hangjiaqi1/MV-TransReID.","<method>3D multi-view learning</method>, <method>2D-based networks</method>, <method>surface random selection strategy</method>, <method>Pedestrian 3D Shape Understanding for Person Re-Identification via Multi-View Learning (MV-3DSReID)</method>"
2024,https://openalex.org/W4391291174,Biology,Unraveling metagenomics through long-read sequencing: a comprehensive review,"Abstract The study of microbial communities has undergone significant advancements, starting from the initial use of 16S rRNA sequencing to the adoption of shotgun metagenomics. However, a new era has emerged with the advent of long-read sequencing (LRS), which offers substantial improvements over its predecessor, short-read sequencing (SRS). LRS produces reads that are several kilobases long, enabling researchers to obtain more complete and contiguous genomic information, characterize structural variations, and study epigenetic modifications. The current leaders in LRS technologies are Pacific Biotechnologies (PacBio) and Oxford Nanopore Technologies (ONT), each offering a distinct set of advantages. This review covers the workflow of long-read metagenomics sequencing, including sample preparation (sample collection, sample extraction, and library preparation), sequencing, processing (quality control, assembly, and binning), and analysis (taxonomic annotation and functional annotation). Each section provides a concise outline of the key concept of the methodology, presenting the original concept as well as how it is challenged or modified in the context of LRS. Additionally, the section introduces a range of tools that are compatible with LRS and can be utilized to execute the LRS process. This review aims to present the workflow of metagenomics, highlight the transformative impact of LRS, and provide researchers with a selection of tools suitable for this task.",No methods found.
2024,https://openalex.org/W4391814337,Biology,Normalizing Large Scale Sensor-Based MWD Data: An Automated Method toward A Unified Database,"In the context of geo-infrastructures and specifically tunneling projects, analyzing the large-scale sensor-based measurement-while-drilling (MWD) data plays a pivotal role in assessing rock engineering conditions. However, handling the big MWD data due to multiform stacking is a time-consuming and challenging task. Extracting valuable insights and improving the accuracy of geoengineering interpretations from MWD data necessitates a combination of domain expertise and data science skills in an iterative process. To address these challenges and efficiently normalize and filter out noisy data, an automated processing approach integrating the stepwise technique, mode, and percentile gate bands for both single and peer group-based holes was developed. Subsequently, the mathematical concept of a novel normalizing index for classifying such big datasets was also presented. The visualized results from different geo-infrastructure datasets in Sweden indicated that outliers and noisy data can more efficiently be eliminated using single hole-based normalizing. Additionally, a relational unified PostgreSQL database was created to store and automatically transfer the processed and raw MWD as well as real time grouting data that offers a cost effective and efficient data extraction tool. The generated database is expected to facilitate in-depth investigations and enable application of the artificial intelligence (AI) techniques to predict rock quality conditions and design appropriate support systems based on MWD data.",No methods found.
2024,https://openalex.org/W4394685122,Biology,Sequence Training and Data Shuffling to Enhance the Accuracy of Recurrent Neural Network Based Battery Voltage Models,"&lt;div class=""section abstract""&gt;&lt;div class=""htmlview paragraph""&gt;Battery terminal voltage modelling is crucial for various applications, including electric vehicles, renewable energy systems, and portable electronics. Terminal voltage models are used to determine how a battery will respond under load and can be used to calculate run-time, power capability, and heat generation and as a component of state estimation approaches, such as for state of charge. Previous studies have shown better voltage modelling accuracy for long short-term memory (LSTM) recurrent neural networks than other traditional methods (e.g., equivalent circuit and electrochemical models). This study presents two new approaches – sequence training and data shuffling – to improve LSTM battery voltage models further, making them an even better candidate for the high-accuracy modelling of lithium-ion batteries. Because the LSTM memory captures information from past time steps, it must typically be trained using one series of continuous data. Instead, the proposed sequence training approach feeds a fixed window of prior data (e.g., 100 seconds) into the LSTM at each time step to initialize the memory states properly and then only uses the output at the current time step. With this method, the LSTM just requires the prior data window to be continuous, thereby allowing the handling of discontinuities. This also means that during the training process, the data can be shuffled randomly, enabling mini-batches to speed up the training significantly. When these approaches were applied, LSTM voltage estimation error was reduced by 22%, from 28.5 mV to 22.3 mV RMS error over four drive cycles and temperatures from -20 to 25°C.&lt;/div&gt;&lt;/div&gt;","<method>long short-term memory (LSTM) recurrent neural networks</method>, <method>sequence training</method>, <method>data shuffling</method>"
2024,https://openalex.org/W4396729544,Biology,Reweighting UK Biobank corrects for pervasive selection bias due to volunteering,"Abstract Background Biobanks typically rely on volunteer-based sampling. This results in large samples (power) at the cost of representativeness (bias). The problem of volunteer bias is debated. Here, we (i) show that volunteering biases associations in UK Biobank (UKB) and (ii) estimate inverse probability (IP) weights that correct for volunteer bias in UKB. Methods Drawing on UK Census data, we constructed a subsample representative of UKB’s target population, which consists of all individuals invited to participate. Based on demographic variables shared between the UK Census and UKB, we estimated IP weights (IPWs) for each UKB participant. We compared 21 weighted and unweighted bivariate associations between these demographic variables to assess volunteer bias. Results Volunteer bias in all associations, as naively estimated in UKB, was substantial—in some cases so severe that unweighted estimates had the opposite sign of the association in the target population. For example, older individuals in UKB reported being in better health, in contrast to evidence from the UK Census. Using IPWs in weighted regressions reduced 87% of volunteer bias on average. Volunteer-based sampling reduced the effective sample size of UKB substantially, to 32% of its original size. Conclusions Estimates from large-scale biobanks may be misleading due to volunteer bias. We recommend IP weighting to correct for such bias. To aid in the construction of the next generation of biobanks, we provide suggestions on how to best ensure representativeness in a volunteer-based design. For UKB, IPWs have been made available.",No methods found.
2024,https://openalex.org/W4401844424,Biology,AlphaFold predictions of fold-switched conformations are driven by structure memorization,"Abstract Recent work suggests that AlphaFold (AF)–a deep learning-based model that can accurately infer protein structure from sequence–may discern important features of folded protein energy landscapes, defined by the diversity and frequency of different conformations in the folded state. Here, we test the limits of its predictive power on fold-switching proteins, which assume two structures with regions of distinct secondary and/or tertiary structure. We find that (1) AF is a weak predictor of fold switching and (2) some of its successes result from memorization of training-set structures rather than learned protein energetics. Combining &gt;280,000 models from several implementations of AF2 and AF3, a 35% success rate was achieved for fold switchers likely in AF’s training sets. AF2’s confidence metrics selected against models consistent with experimentally determined fold-switching structures and failed to discriminate between low and high energy conformations. Further, AF captured only one out of seven experimentally confirmed fold switchers outside of its training sets despite extensive sampling of an additional ~280,000 models. Several observations indicate that AF2 has memorized structural information during training, and AF3 misassigns coevolutionary restraints. These limitations constrain the scope of successful predictions, highlighting the need for physically based methods that readily predict multiple protein conformations.","<method>AlphaFold (AF)</method>, <method>AF2</method>, <method>AF3</method>"
2024,https://openalex.org/W4390533386,Biology,"A Systematic Literature Review of Digital Twin Research for Healthcare Systems: Research Trends, Gaps, and Realization Challenges","Using the PRISMA approach, we present the first systematic literature review of digital twin (DT) research in healthcare systems (HSs). This endeavor stems from the pressing need for a thorough analysis of this emerging yet fragmented research area, with the goal of consolidating knowledge to catalyze its growth. Our findings are structured around three research questions aimed at identifying: (i) current research trends, (ii) gaps, and (iii) realization challenges. Current trends indicate global interest and interdisciplinary collaborations to address complex HS challenges. However, existing research predominantly focuses on conceptualization; research on integration, verification, and implementation is nascent. Additionally, we document that a substantial body of papers mislabel their work, often disregarding modeling and twinning methods that are necessary elements of a DT. Furthermore, we provide a non-exhaustive classification of the literature based on two axes: <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">the object</i> (i.e., product or process) and <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">the context</i> (i.e., patient's body, medical procedures, healthcare facilities, and public health). While this is a testament to the diversity of the field, it implies a specific pattern that could be reimagined. We also identify two gaps: (i) considering the human-in-the-loop nature of HSs with a focus on provider decision-making and (ii) implementation research. Lastly, we discuss two challenges for broad-scale implementation of DTs in HSs: improving virtual-to-physical connectivity and data-related issues. In conclusion, this study suggests that DT research could potentially help alleviate the acute shortcomings of HSs that are often manifested in the inability to concurrently improve the quality of care, provider wellbeing, and cost efficiency.",No methods found.
2024,https://openalex.org/W4391482136,Biology,A comprehensive analysis of the emerging modern trends in research on photovoltaic systems and desalination in the era of artificial intelligence and machine learning,"Integration of photovoltaic (PV) systems, desalination technologies, and Artificial Intelligence (AI) combined with Machine Learning (ML) has introduced a new era of remarkable research and innovation. This review article thoroughly examines the recent advancements in the field, focusing on the interplay between PV systems and water desalination within the framework of AI and ML applications, along with it analyses current research to identify significant patterns, obstacles, and prospects in this interdisciplinary field. Furthermore, review examines the incorporation of AI and ML methods in improving the performance of PV systems. This includes raising their efficiency, implementing predictive maintenance strategies, and enabling real-time monitoring. It also explores the transformative influence of intelligent algorithms on desalination techniques, specifically addressing concerns pertaining to energy usage, scalability, and environmental sustainability. This article provides a thorough analysis of the current literature, identifying areas where research is lacking and suggesting potential future avenues for investigation. These advancements have resulted in increased efficiency, decreased expenses, and improved sustainability of PV system. By utilizing artificial intelligence technologies, freshwater productivity can increase by 10 % and efficiency. This review offers significant and informative perspectives for researchers, engineers, and policymakers involved in renewable energy and water technology. It sheds light on the latest advancements in photovoltaic systems and desalination, which are facilitated by AI and ML. The review aims to guide towards a more sustainable and technologically advanced future.","<method>Artificial Intelligence (AI)</method>, <method>Machine Learning (ML)</method>, <method>predictive maintenance strategies</method>, <method>intelligent algorithms</method>"
2024,https://openalex.org/W4392077419,Biology,Selection of sustainable food suppliers using the Pythagorean fuzzy CRITIC-MARCOS method,"Sustainable food supplier selection (SFSS) can be handled as an uncertain decision-making issue. The Pythagorean fuzzy set (PFS), a type of non-standard fuzzy set, offers an expanded description space for articulating fuzzy and uncertain data. Accordingly, this paper proposes a Pythagorean fuzzy synthetic decision method-based selection framework for solving the SFSS problem within a subjective context. Then, the weighted distance measures for the PFS are introduced to derive the importance degrees of the experts, which can provide a more objective decision result. Then, an information fusion method with a PFS-weighted power average (WPA) operator is introduced to form a group decision matrix competent to accommodate the deviation effect. Next, an extended PF-measurement of alternatives and ranking according to compromise solution (MARCOS) method integrating PF-criteria importance through inter-criteria correlation (CRITIC) is presented to calculate the priority of each supplier, which can capture the inter-correlations between criteria. Finally, a numerical example of SFSS is implemented to show the application of the proposed synthetic decision approach. Subsequently, the sensitivity analysis of distance parameters and comparison analysis among different SFSS approaches were conducted to test the rationality and advantages of the proposed framework for resolving the SFSS problem. The results show that the reported method can provide a practical way to resolve the SFSS problems with uncertain data.","<method>Pythagorean fuzzy synthetic decision method</method>, <method>weighted distance measures for the Pythagorean fuzzy set (PFS)</method>, <method>information fusion method with a PFS-weighted power average (WPA) operator</method>, <method>extended PF-measurement of alternatives and ranking according to compromise solution (MARCOS) method integrating PF-criteria importance through inter-criteria correlation (CRITIC)</method>"
2024,https://openalex.org/W4390885355,Biology,Evaluating spatially variable gene detection methods for spatial transcriptomics data,"Abstract Background The identification of genes that vary across spatial domains in tissues and cells is an essential step for spatial transcriptomics data analysis. Given the critical role it serves for downstream data interpretations, various methods for detecting spatially variable genes (SVGs) have been proposed. However, the lack of benchmarking complicates the selection of a suitable method. Results Here we systematically evaluate a panel of popular SVG detection methods on a large collection of spatial transcriptomics datasets, covering various tissue types, biotechnologies, and spatial resolutions. We address questions including whether different methods select a similar set of SVGs, how reliable is the reported statistical significance from each method, how accurate and robust is each method in terms of SVG detection, and how well the selected SVGs perform in downstream applications such as clustering of spatial domains. Besides these, practical considerations such as computational time and memory usage are also crucial for deciding which method to use. Conclusions Our study evaluates the performance of each method from multiple aspects and highlights the discrepancy among different methods when calling statistically significant SVGs across diverse datasets. Overall, our work provides useful considerations for choosing methods for identifying SVGs and serves as a key reference for the future development of related methods.",No methods found.
2024,https://openalex.org/W4391265035,Biology,"An integrated framework for sustainable and efficient building maintenance operations aligning with climate change, SDGs, and emerging technology","Improving the operation and maintenance of buildings can significantly reduce carbon emissions, energy consumption, and other environmental challenges while promoting sustainability. While existing literature offers various frameworks, they primarily focus on traditional building maintenance procedures and overlook the importance of integrating sustainability, climate change, environmental factors, and emerging technologies. To address this gap, this research has developed a comprehensive framework that caters to current needs, challenges, and future priorities. The integrated framework for building maintenance operations aligns with the Sustainable Development Goals (SDGs), climate change mitigation and adaptation, the adoption of emerging technology, energy conservation, as well as safety, resilience, and effectiveness. The development of the framework encompassed four phases: pre-development phases 1 and 2, development phase 3, and validation phase 4. During this process, current issues and challenges were identified, impacts were assessed, and strategies were developed. The framework serves as a roadmap to address these challenges and requirements in future building maintenance operations, making significant contributions to all three dimensions of sustainability: environmental, social, and economic. In summary, this study offers a comprehensive and in-depth analysis of the current issues, challenges, and potential improvements and benefits in building maintenance operations, providing a practical guide for industry stakeholders and making a significant contribution to the existing body of knowledge.",No methods found.
2024,https://openalex.org/W4391737478,Biology,"Ethical considerations in implementing generative AI for healthcare supply chain optimization: A cross-country analysis across India, the United Kingdom, and the United States of America","This review paper critically examines the ethical considerations involved in implementing generative Artificial Intelligence (AI) in healthcare supply chain optimization across three distinct regions: India, the United Kingdom, and the United States of America. The study synthesizes findings from various case studies and academic research to highlight both common and unique ethical challenges faced in these countries. Key themes such as data privacy, algorithmic transparency, and equitable access to AI-driven healthcare solutions are explored, alongside the unique socio-cultural, legal, and regulatory challenges specific to each region. The paper proposes a set of best practices for incorporating ethical considerations into the deployment of generative AI in healthcare. These include the development of inclusive ethical frameworks, regular ethical audits, comprehensive training and education programs, public engagement initiatives, and interdisciplinary collaboration. The paper also delves into future research directions and policy development, emphasizing the need to address healthcare disparities, adapt legal and regulatory frameworks, enhance generative AI explainability, and evaluate long-term outcomes.The study concludes by underscoring the importance of ethical design and deployment of generative AI systems in healthcare, advocating for a balanced approach that aligns technological advancements with ethical standards and global healthcare needs. This comprehensive review aims to contribute to the discourse on ethical generative AI implementation, offering insights and recommendations for policymakers, healthcare professionals, and generative AI developers to foster responsible and beneficial use of generative AI in healthcare globally.",No methods found.
2024,https://openalex.org/W4391796054,Biology,Optical remote sensing of crop biophysical and biochemical parameters: An overview of advances in sensor technologies and machine learning algorithms for precision agriculture,"This paper provides an overview of the recent developments in remote sensing technology and machine learning algorithms for estimating important biophysical and biochemical parameters for precision farming. The objectives are (i) to provide an overview of recent advances in remotely sensed retrieval of biophysical and biochemical parameters brought by the developments in sensor technologies and robust machine learning algorithms and (ii) to identify the sources of uncertainty in retrieving biophysical and biochemical parameters and implications for precision agriculture. The review revealed that developments in crop biophysical and biochemical parameters retrieval techniques were mainly driven by announcements and the availability of new sensors. Two ground-breaking events can be identified, i.e., the availability of Sentinel-2 and the SuperDove constellation. The two provide high temporal-high spatial resolution data relevant for site-specific management and super-spectral configuration, enabling retrieval of crop growth and health parameters. The free availability of Sentinel-2 triggered the testing of its spectral configurations and upscaling of retrieval approaches using simulated data from field spectrometers and airborne hyperspectral sensors. SuperDoves will likely reduce the cost of very high-resolution data while providing unprecedented capabilities for detailed, accurate and frequent characterisation of field variability. Studies showed that the red-edge bands and hybrid models coupling Radiative Transfer Model (RTM) and machine learning regression algorithms (MLRA) are promising for operational and accurate monitoring of stress-related crop parameters to aid time-sensitive agronomic decisions. However, such models were tested in Mediterranean climates and performed poorly in African semi-arid areas and China's temperate continental semi-humid monsoon climates. Therefore, locally-calibrated RTM models incorporating crop-type maps and other spatio-temporal constraints may reduce uncertainties when adapted to data-scarce regions. Generally, permanent experimental sites and a lack of systematic calibration data on various crops are some limiting factors to using remote sensing technologies for PA in Sub-Saharan Africa. Other complexities arise from farm configurations, such as small field sizes and mixed cropping practices. Therefore, future studies should develop generic, scalable and transferable models, especially within under-studied areas.",<method>machine learning regression algorithms (MLRA)</method>
2024,https://openalex.org/W4394822945,Biology,A Critical Review of Artificial Intelligence Based Approaches in Intrusion Detection: A Comprehensive Analysis,"Intrusion detection (ID) is critical in securing computer networks against various malicious attacks. Recent advancements in machine learning (ML), deep learning (DL), federated learning (FL), and explainable artificial intelligence (XAI) have drawn significant attention as potential approaches for ID. DL-based approaches have shown impressive performance in ID by automatically learning relevant features from data but require significant labelled data and computational resources to train complex models. ML-based approaches require fewer computational resources and labelled data, but their ability to generalize to unseen data is limited. FL is a relatively new approach that enables multiple entities to train a model collectively without exchanging their data, providing privacy and security benefits, making it an attractive option for ID. However, FL-based approaches require more communication resources and additional computation to aggregate models from different entities. XAI is critical for understanding how AI models make decisions, improving interpretability and transparency. While existing literature has explored the strengths and weaknesses of DL, ML, FL, and XAI-based approaches for ID, a significant gap exists in providing a comprehensive analysis of the specific use cases and scenarios where each approach is most suitable. This paper seeks to fill this void by delivering an in-depth review that not only highlights strengths and weaknesses but also offers guidance for selecting the appropriate approach based on the unique ID context and available resources. The selection of an appropriate approach depends on the specific use case, and this work provides insights into which method is best suited for various network sizes, data availability, privacy, and security concerns, thus aiding practitioners in making informed decisions for their ID needs.","<method>machine learning (ML)</method>, <method>deep learning (DL)</method>, <method>federated learning (FL)</method>, <method>explainable artificial intelligence (XAI)</method>"
2024,https://openalex.org/W4399052867,Biology,Generative artificial intelligence in manufacturing: opportunities for actualizing Industry 5.0 sustainability goals,"Purpose This study offers practical insights into how generative artificial intelligence (AI) can enhance responsible manufacturing within the context of Industry 5.0. It explores how manufacturers can strategically maximize the potential benefits of generative AI through a synergistic approach. Design/methodology/approach The study developed a strategic roadmap by employing a mixed qualitative-quantitative research method involving case studies, interviews and interpretive structural modeling (ISM). This roadmap visualizes and elucidates the mechanisms through which generative AI can contribute to advancing the sustainability goals of Industry 5.0. Findings Generative AI has demonstrated the capability to promote various sustainability objectives within Industry 5.0 through ten distinct functions. These multifaceted functions address multiple facets of manufacturing, ranging from providing data-driven production insights to enhancing the resilience of manufacturing operations. Practical implications While each identified generative AI function independently contributes to responsible manufacturing under Industry 5.0, leveraging them individually is a viable strategy. However, they synergistically enhance each other when systematically employed in a specific order. Manufacturers are advised to strategically leverage these functions, drawing on their complementarities to maximize their benefits. Originality/value This study pioneers by providing early practical insights into how generative AI enhances the sustainability performance of manufacturers within the Industry 5.0 framework. The proposed strategic roadmap suggests prioritization orders, guiding manufacturers in decision-making processes regarding where and for what purpose to integrate generative AI.",<method>generative artificial intelligence (generative AI)</method>
2024,https://openalex.org/W4399802093,Biology,Fractional order PID controller for load frequency control in a deregulated hybrid power system using Aquila Optimization,"This paper presents an innovative approach for automatic generation control for power system under a deregulated setting. The main objective of this work is to optimally tune the parameters of the fractional-order controller using the newly developed Aquila Optimizer (AO) to enhance system performance. A test system comprising a thermal power plant, a hydroelectric system, a gas turbine-based power plant, and wind energy sources is examined under deregulated environment. The study emphasizes the minimization of frequency variations, tie line deviations, and area control errors during diverse operational shifts. The proposed control strategy explores the response of generators in a hybrid deregulated power system, emphasizing the critical role of properly tuned Fractional Order Proportional-Integral-Derivative (FOPID) controllers in ensuring system stability. The potential and effectiveness of the proposed algorithm are compared with particle swarm optimization (PSO) and whale optimization algorithm (WOA) based controller performance for the same test system. The objective function for optimization is set as the minimization of the integral time and absolute error (ITAE) performance index. Furthermore, the efficacy of the proposed technique is compared with the Unified Power Flow Controller (UPFC) and its superiority is validated. Performance evaluation of the hybrid power system is conducted under Poolco agreement, bilateral agreement, and varying operating conditions. Comparative assessments reveal the superiority of the AO-driven FOPID over other techniques, demonstrating improved system metrics, including frequencies across different areas, tie-line power variations, and generator outputs.","<method>Aquila Optimizer (AO)</method>, <method>Fractional Order Proportional-Integral-Derivative (FOPID) controllers</method>, <method>particle swarm optimization (PSO)</method>, <method>whale optimization algorithm (WOA)</method>"
2024,https://openalex.org/W4399857583,Biology,Integrating artificial intelligence to assess emotions in learning environments: a systematic literature review,"Introduction Artificial Intelligence (AI) is transforming multiple sectors within our society, including education. In this context, emotions play a fundamental role in the teaching-learning process given that they influence academic performance, motivation, information retention, and student well-being. Thus, the integration of AI in emotional assessment within educational environments offers several advantages that can transform how we understand and address the socio-emotional development of students. However, there remains a lack of comprehensive approach that systematizes advancements, challenges, and opportunities in this field. Aim This systematic literature review aims to explore how artificial intelligence (AI) is used to evaluate emotions within educational settings. We provide a comprehensive overview of the current state of research, focusing on advancements, challenges, and opportunities in the domain of AI-driven emotional assessment within educational settings. Method The review involved a search across the following academic databases: Pubmed, Web of Science, PsycINFO and Scopus. Forty-one articles were selected that meet the established inclusion criteria. These articles were analyzed to extract key insights related to the integration of AI and emotional assessment within educational environments. Results The findings reveal a variety of AI-driven approaches that were developed to capture and analyze students’ emotional states during learning activities. The findings are summarized in four fundamental topics: (1) emotion recognition in education, (2) technology integration and learning outcomes, (3) special education and assistive technology, (4) affective computing. Among the key AI techniques employed are machine learning and facial recognition, which are used to assess emotions. These approaches demonstrate promising potential in enhancing pedagogical strategies and creating adaptive learning environments that cater to individual emotional needs. The review identified emerging factors that, while important, require further investigation to understand their relationships and implications fully. These elements could significantly enhance the use of AI in assessing emotions within educational settings. Specifically, we are referring to: (1) federated learning, (2) convolutional neural network (CNN), (3) recurrent neural network (RNN), (4) facial expression databases, and (5) ethics in the development of intelligent systems. Conclusion This systematic literature review showcases the significance of AI in revolutionizing educational practices through emotion assessment. While advancements are evident, challenges related to accuracy, privacy, and cross-cultural validity were also identified. The synthesis of existing research highlights the need for further research into refining AI models for emotion recognition and emphasizes the importance of ethical considerations in implementing AI technologies within educational contexts.","<method>machine learning</method>, <method>facial recognition</method>, <method>federated learning</method>, <method>convolutional neural network (CNN)</method>, <method>recurrent neural network (RNN)</method>"
2024,https://openalex.org/W4401163187,Biology,Monthly climate prediction using deep convolutional neural network and long short-term memory,"Climate change affects plant growth, food production, ecosystems, sustainable socio-economic development, and human health. The different artificial intelligence models are proposed to simulate climate parameters of Jinan city in China, include artificial neural network (ANN), recurrent NN (RNN), long short-term memory neural network (LSTM), deep convolutional NN (CNN), and CNN-LSTM. These models are used to forecast six climatic factors on a monthly ahead. The climate data for 72 years (1 January 1951–31 December 2022) used in this study include monthly average atmospheric temperature, extreme minimum atmospheric temperature, extreme maximum atmospheric temperature, precipitation, average relative humidity, and sunlight hours. The time series of 12 month delayed data are used as input signals to the models. The efficiency of the proposed models are examined utilizing diverse evaluation criteria namely mean absolute error, root mean square error (RMSE), and correlation coefficient (R). The modeling result inherits that the proposed hybrid CNN-LSTM model achieves a greater accuracy than other compared models. The hybrid CNN-LSTM model significantly reduces the forecasting error compared to the models for the one month time step ahead. For instance, the RMSE values of the ANN, RNN, LSTM, CNN, and CNN-LSTM models for monthly average atmospheric temperature in the forecasting stage are 2.0669, 1.4416, 1.3482, 0.8015 and 0.6292 °C, respectively. The findings of climate simulations shows the potential of CNN-LSTM models to improve climate forecasting. Climate prediction will contribute to meteorological disaster prevention and reduction, as well as flood control and drought resistance.","<method>artificial neural network (ANN)</method>, <method>recurrent NN (RNN)</method>, <method>long short-term memory neural network (LSTM)</method>, <method>deep convolutional NN (CNN)</method>, <method>CNN-LSTM</method>"
2024,https://openalex.org/W4390590719,Biology,Terms of debate: Consensus definitions to guide the scientific discourse on visual distraction,"Hypothesis-driven research rests on clearly articulated scientific theories. The building blocks for communicating these theories are scientific terms. Obviously, communication - and thus, scientific progress - is hampered if the meaning of these terms varies idiosyncratically across (sub)fields and even across individual researchers within the same subfield. We have formed an international group of experts representing various theoretical stances with the goal to homogenize the use of the terms that are most relevant to fundamental research on visual distraction in visual search. Our discussions revealed striking heterogeneity and we had to invest much time and effort to increase our mutual understanding of each other's use of central terms, which turned out to be strongly related to our respective theoretical positions. We present the outcomes of these discussions in a glossary and provide some context in several essays. Specifically, we explicate how central terms are used in the distraction literature and consensually sharpen their definitions in order to enable communication across theoretical standpoints. Where applicable, we also explain how the respective constructs can be measured. We believe that this novel type of adversarial collaboration can serve as a model for other fields of psychological research that strive to build a solid groundwork for theorizing and communicating by establishing a common language. For the field of visual distraction, the present paper should facilitate communication across theoretical standpoints and may serve as an introduction and reference text for newcomers.",No methods found.
2024,https://openalex.org/W4390940873,Biology,Pattern recognition in the nucleation kinetics of non-equilibrium self-assembly,"Abstract Inspired by biology’s most sophisticated computer, the brain, neural networks constitute a profound reformulation of computational principles 1–3 . Analogous high-dimensional, highly interconnected computational architectures also arise within information-processing molecular systems inside living cells, such as signal transduction cascades and genetic regulatory networks 4–7 . Might collective modes analogous to neural computation be found more broadly in other physical and chemical processes, even those that ostensibly play non-information-processing roles? Here we examine nucleation during self-assembly of multicomponent structures, showing that high-dimensional patterns of concentrations can be discriminated and classified in a manner similar to neural network computation. Specifically, we design a set of 917 DNA tiles that can self-assemble in three alternative ways such that competitive nucleation depends sensitively on the extent of colocalization of high-concentration tiles within the three structures. The system was trained in silico to classify a set of 18 grayscale 30 × 30 pixel images into three categories. Experimentally, fluorescence and atomic force microscopy measurements during and after a 150 hour anneal established that all trained images were correctly classified, whereas a test set of image variations probed the robustness of the results. Although slow compared to previous biochemical neural networks, our approach is compact, robust and scalable. Our findings suggest that ubiquitous physical phenomena, such as nucleation, may hold powerful information-processing capabilities when they occur within high-dimensional multicomponent systems.",<method>neural networks</method>
2024,https://openalex.org/W4391042040,Biology,The Promise of Inferring the Past Using the Ancestral Recombination Graph,"Abstract The ancestral recombination graph (ARG) is a structure that represents the history of coalescent and recombination events connecting a set of sequences (Hudson RR. In: Futuyma D, Antonovics J, editors. Gene genealogies and the coalescent process. In: Oxford Surveys in Evolutionary Biology; 1991. p. 1 to 44.). The full ARG can be represented as a set of genealogical trees at every locus in the genome, annotated with recombination events that change the topology of the trees between adjacent loci and the mutations that occurred along the branches of those trees (Griffiths RC, Marjoram P. An ancestral recombination graph. In: Donnelly P, Tavare S, editors. Progress in population genetics and human evolution. Springer; 1997. p. 257 to 270.). Valuable insights can be gained into past evolutionary processes, such as demographic events or the influence of natural selection, by studying the ARG. It is regarded as the “holy grail” of population genetics (Hubisz M, Siepel A. Inference of ancestral recombination graphs using ARGweaver. In: Dutheil JY, editors. Statistical population genomics. New York, NY: Springer US; 2020. p. 231–266.) since it encodes the processes that generate all patterns of allelic and haplotypic variation from which all commonly used summary statistics in population genetic research (e.g. heterozygosity and linkage disequilibrium) can be derived. Many previous evolutionary inferences relied on summary statistics extracted from the genotype matrix. Evolutionary inferences using the ARG represent a significant advancement as the ARG is a representation of the evolutionary history of a sample that shows the past history of recombination, coalescence, and mutation events across a particular sequence. This representation in theory contains as much information, if not more, than the combination of all independent summary statistics that could be derived from the genotype matrix. Consistent with this idea, some of the first ARG-based analyses have proven to be more powerful than summary statistic-based analyses (Speidel L, Forest M, Shi S, Myers SR. A method for genome-wide genealogy estimation for thousands of samples. Nat Genet. 2019:51(9):1321 to 1329.; Stern AJ, Wilton PR, Nielsen R. An approximate full-likelihood method for inferring selection and allele frequency trajectories from DNA sequence data. PLoS Genet. 2019:15(9):e1008384.; Hubisz MJ, Williams AL, Siepel A. Mapping gene flow between ancient hominins through demography-aware inference of the ancestral recombination graph. PLoS Genet. 2020:16(8):e1008895.; Fan C, Mancuso N, Chiang CWK. A genealogical estimate of genetic relationships. Am J Hum Genet. 2022:109(5):812–824.; Fan C, Cahoon JL, Dinh BL, Ortega-Del Vecchyo D, Huber C, Edge MD, Mancuso N, Chiang CWK. A likelihood-based framework for demographic inference from genealogical trees. bioRxiv. 2023.10.10.561787. 2023.; Hejase HA, Mo Z, Campagna L, Siepel A. A deep-learning approach for inference of selective sweeps from the ancestral recombination graph. Mol Biol Evol. 2022:39(1):msab332.; Link V, Schraiber JG, Fan C, Dinh B, Mancuso N, Chiang CWK, Edge MD. Tree-based QTL mapping with expected local genetic relatedness matrices. bioRxiv. 2023.04.07.536093. 2023.; Zhang BC, Biddanda A, Gunnarsson ÁF, Cooper F, Palamara PF. Biobank-scale inference of ancestral recombination graphs enables genealogical analysis of complex traits. Nat Genet. 2023:55(5):768–776.). As such, there has been significant interest in the field to investigate 2 main problems related to the ARG: (i) How can we estimate the ARG based on genomic data, and (ii) how can we extract information of past evolutionary processes from the ARG? In this perspective, we highlight 3 topics that pertain to these main issues: The development of computational innovations that enable the estimation of the ARG; remaining challenges in estimating the ARG; and methodological advances for deducing evolutionary forces and mechanisms using the ARG. This perspective serves to introduce the readers to the types of questions that can be explored using the ARG and to highlight some of the most pressing issues that must be addressed in order to make ARG-based inference an indispensable tool for evolutionary research.",No methods found.
2024,https://openalex.org/W4391341367,Biology,Automated Tool Support for Glaucoma Identification With Explainability Using Fundus Images,"Glaucoma is a progressive eye condition that causes irreversible vision loss due to damage to the optic nerve. Recent developments in deep learning and the accessibility of computing resources have provided tool support for automated glaucoma diagnosis. Despite deep learning's advances in disease diagnosis using medical images, generic convolutional neural networks are still not widely used in medical practices due to the limited trustworthiness of these models. Although deep learning-based glaucoma classification has gained popularity in recent years, only a few of them have addressed the explainability and interpretability of the models, which increases confidence in using such applications. This study presents state-of-the-art deep learning techniques to segment and classify fundus images to predict glaucoma conditions and applies visualization techniques to explain the results to ease understandability. Our predictions are based on U-Net with attention mechanisms with ResNet50 for the segmentation process and a modified Inception V3 architecture for the classification. Attention U-Net with modified ResNet50 backbone obtained 99.58% and 98.05% accuracies for optic disc segmentation and optic cup segmentation, respectively for the RIM-ONE dataset. Additionally, we generate heatmaps that highlight the regions that impacted the glaucoma diagnosis using both Gradient-weighted Class Activation Mapping (Grad-CAM) and Grad-CAM++. Our model that classifies the segmented images achieves accuracy, sensitivity, and specificity values of 98.97%, 99.42%, and 95.59%, respectively, with the RIM-ONE dataset. This model can be used as a support tool for automated glaucoma identification using fundus images.","<method>U-Net with attention mechanisms</method>, <method>ResNet50</method>, <method>modified Inception V3 architecture</method>, <method>Attention U-Net with modified ResNet50 backbone</method>, <method>Gradient-weighted Class Activation Mapping (Grad-CAM)</method>, <method>Grad-CAM++</method>"
2024,https://openalex.org/W4391610180,Biology,"Generative artificial intelligence in drug discovery: basic framework, recent advances, challenges, and opportunities","There are two main ways to discover or design small drug molecules. The first involves fine-tuning existing molecules or commercially successful drugs through quantitative structure-activity relationships and virtual screening. The second approach involves generating new molecules through de novo drug design or inverse quantitative structure-activity relationship. Both methods aim to get a drug molecule with the best pharmacokinetic and pharmacodynamic profiles. However, bringing a new drug to market is an expensive and time-consuming endeavor, with the average cost being estimated at around $2.5 billion. One of the biggest challenges is screening the vast number of potential drug candidates to find one that is both safe and effective. The development of artificial intelligence in recent years has been phenomenal, ushering in a revolution in many fields. The field of pharmaceutical sciences has also significantly benefited from multiple applications of artificial intelligence, especially drug discovery projects. Artificial intelligence models are finding use in molecular property prediction, molecule generation, virtual screening, synthesis planning, repurposing, among others. Lately, generative artificial intelligence has gained popularity across domains for its ability to generate entirely new data, such as images, sentences, audios, videos, novel chemical molecules, etc. Generative artificial intelligence has also delivered promising results in drug discovery and development. This review article delves into the fundamentals and framework of various generative artificial intelligence models in the context of drug discovery via de novo drug design approach. Various basic and advanced models have been discussed, along with their recent applications. The review also explores recent examples and advances in the generative artificial intelligence approach, as well as the challenges and ongoing efforts to fully harness the potential of generative artificial intelligence in generating novel drug molecules in a faster and more affordable manner. Some clinical-level assets generated form generative artificial intelligence have also been discussed in this review to show the ever-increasing application of artificial intelligence in drug discovery through commercial partnerships.","<method>quantitative structure-activity relationships</method>, <method>virtual screening</method>, <method>de novo drug design</method>, <method>inverse quantitative structure-activity relationship</method>, <method>artificial intelligence models</method>, <method>molecular property prediction</method>, <method>molecule generation</method>, <method>virtual screening</method>, <method>synthesis planning</method>, <method>repurposing</method>, <method>generative artificial intelligence</method>"
2024,https://openalex.org/W4391923223,Biology,Improved random forest algorithms for increasing the accuracy of forest aboveground biomass estimation using Sentinel-2 imagery,"A simpler, unbiased, and comprehensive random forest (RF) model is needed to improve the accuracy of aboveground biomass (AGB) estimation. In this study, data were obtained from 128 sample plots of Pinus yunnanensis forest located in Chuxiong prefecture, Yunnan province, China. Sentinel-2 imagery data were applied to extract the important predictors of forest AGB, which were screened using the Boruta algorithm. We compared the fitting performance of two modified random forest models − regularized random forest (RRF) and quantile random forest (QRF) − with the random forest model. Moreover, we combined the smallest mean error of each quantile model as the best QRF (QRFb). The result showed: (1) Window sizes of 3 × 3 pixels and 5 × 5 pixels demonstrated greater sensitivity and suitability for estimating AGB than the 7 × 7 pixels window size. Enhanced vegetation indices derived from Red Edge 1 (B5) and Near-Infrared bands (B8A) were strongly correlated with AGB, indicating the heightened sensitivity of B5 and B8A bands to biomass and their potential in AGB estimation. (2) The RRF model outperformed both the standard RF and QRF in fitting performance, with an R2 of 0.56 and RMSE 57.14 Mg/ha. (3) The QRFb model exhibited the highest R2 of 0.88 and lowest RMSE of 29.56 Mg/ha, significantly reducing overestimation and underestimation issues. The modified RF regression supplies new insights into improving forest AGB estimation, which will be helpful for future research addressing carbon cycling.","<method>random forest (RF)</method>, <method>Boruta algorithm</method>, <method>regularized random forest (RRF)</method>, <method>quantile random forest (QRF)</method>"
2024,https://openalex.org/W4392301491,Biology,Graduate instructors navigating the AI frontier: The role of ChatGPT in higher education,"This research study explores the use of artificial intelligence (AI) in undergraduate assessments, specifically focusing on the ability of graduate teaching assistants (GTAs) to identify AI-generated assessments and the performance of ChatGPT, an AI model, in producing high-quality work. The study examines four guiding research questions and hypotheses related to the accuracy of GTA identification, the achievement of AI-generated work compared to student marks, the impact of GTA characteristics on identification accuracy, and the variation in identification and assessment across different subject areas. The study incorporates ten AI-generated assessments across seven classes taught by five GTAs. The findings reveal that ChatGPT consistently excelled the average student in all classes receiving 10 scores of A or higher out of 11 and receiving the top mark in 8 of the ten classes. GTAs accurately identified 50% of the AI-generated assessments, with results suggesting a potential connection between class size and GTA accuracy in identifying AI-generated work. GTAs with prior experience and familiarity with ChatGPT demonstrated higher accuracy in identifying AI-generated assessments. However, further research is needed to explore this comprehensively. This study also reviews the effectiveness of TurnItin's new AI detector, highlighting an accuracy of 92% across the ten assessments. The study highlights the adaptability of ChatGPT across different subject areas and assessment types, producing assessments that align with diverse educational contexts. In conclusion, this research study contributes to understanding the effectiveness and adaptability of AI in undergraduate assessments. It underscores the need to further explore and develop AI technologies in education.",No methods found.
2024,https://openalex.org/W4401384485,Biology,GAN based augmentation using a hybrid loss function for dermoscopy images,"Dermatology is the most appropriate field to utilize pattern recognition-based automated techniques for objective, accurate, and rapid diagnosis because diagnosis mainly relies on visual examinations of skin lesions. Recent approaches utilizing deep learning techniques have shown remarkable results in this field. However, they necessitate a substantial quantity of images and the availability of dermoscopy images is often limited. Also, even if enough images are available, their labeling requires expert knowledge and is time-consuming. To overcome these issues, an efficient augmentation approach is needed to expand training datasets from input images. Therefore, in this work, a generative adversarial network has been developed using a new hybrid loss function constructed with traditional loss functions to enhance the generation power of the architecture. Also, the effect of the proposed approach and different generative network-based augmentations, which have been used with dermoscopy images in the literature, on the classification of skin lesions has been investigated. Therefore, the main contributions of this work are: (i) introducing a new generative model for the augmentation of dermoscopy images; (ii) presenting the effect of the proposed model on the classification of the images; (iii) comparative evaluations of the effectiveness of different generative network-based augmentations in the classification of seven forms of skin lesions. The classification accuracy when the proposed augmentation is used is 93.12%, which is higher than its counterparts. Experimental results indicate the significance of augmentation techniques in the classification of skin lesions and the efficiency of the proposed structure in improving the classification accuracy.","<method>deep learning techniques</method>, <method>generative adversarial network</method>, <method>generative network-based augmentations</method>"
2024,https://openalex.org/W4391097085,Biology,Lightweight Context-Aware Network Using Partial-Channel Transformation for Real-Time Semantic Segmentation,"Optimizing the computational efficiency of the artificial neural networks is crucial for resource-constrained platforms like autonomous driving systems. To address this challenge, we proposed a Lightweight Context-aware Network (LCNet) that accelerates semantic segmentation while maintaining a favorable trade-off between inference speed and segmentation accuracy in this paper. The proposed LCNet introduces a partial-channel transformation (PCT) strategy to minimize computing latency and hardware requirements of the basic unit. Within the PCT block, a three-branch context aggregation (TCA) module expands the feature receptive fields, capturing multiscale contextual information. Additionally, a dual-attention-guided decoder (DD) recovers spatial details and enhances pixel prediction accuracy. Extensive experiments on three benchmarks demonstrate the effectiveness and efficiency of the proposed LCNet model. Remarkably, a smaller model LCNet <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""> <tex-math notation=""LaTeX"">$_{3\_7}$</tex-math> </inline-formula> achieves 73.8% mIoU with only 0.51 million parameters, with an impressive inference speed of <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""> <tex-math notation=""LaTeX"">$\sim$</tex-math> </inline-formula> 142.5 fps and <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""> <tex-math notation=""LaTeX"">$\sim$</tex-math> </inline-formula> 9 fps using a single RTX 3090 GPU and Jetson Xavier NX, respectively, on the Cityscapes test set at <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""> <tex-math notation=""LaTeX"">$1024\times 1024$</tex-math> </inline-formula> resolution. A more accurate version of the LCNet <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""> <tex-math notation=""LaTeX"">$_{3\_11}$</tex-math> </inline-formula> can achieve 75.8% mIoU with 0.74 million parameters at <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""> <tex-math notation=""LaTeX"">$\sim$</tex-math> </inline-formula> 117 fps inference speed on Cityscapes at the same resolution. Much faster inference speed can be achieved at smaller image resolutions. LCNet strikes a great balance between computational efficiency and prediction capability for mobile application scenarios. The code is available at https://github.com/lztjy/LCNet.","<method>Lightweight Context-aware Network (LCNet)</method>, <method>partial-channel transformation (PCT) strategy</method>, <method>three-branch context aggregation (TCA) module</method>, <method>dual-attention-guided decoder (DD)</method>"
2024,https://openalex.org/W4391317367,Biology,Automated localization of mandibular landmarks in the construction of mandibular median sagittal plane,"Abstract Objective To use deep learning to segment the mandible and identify three-dimensional (3D) anatomical landmarks from cone-beam computed tomography (CBCT) images, the planes constructed from the mandibular midline landmarks were compared and analyzed to find the best mandibular midsagittal plane (MMSP). Methods A total of 400 participants were randomly divided into a training group ( n = 360) and a validation group ( n = 40). Normal individuals were used as the test group ( n = 50). The PointRend deep learning mechanism segmented the mandible from CBCT images and accurately identified 27 anatomic landmarks via PoseNet. 3D coordinates of 5 central landmarks and 2 pairs of side landmarks were obtained for the test group. Every 35 combinations of 3 midline landmarks were screened using the template mapping technique. The asymmetry index (AI) was calculated for each of the 35 mirror planes. The template mapping technique plane was used as the reference plane; the top four planes with the smallest AIs were compared through distance, volume difference, and similarity index to find the plane with the fewest errors. Results The mandible was segmented automatically in 10 ± 1.5 s with a 0.98 Dice similarity coefficient. The mean landmark localization error for the 27 landmarks was 1.04 ± 0.28 mm. MMSP should use the plane made by B (supramentale), Gn (gnathion), and F (mandibular foramen). The average AI grade was 1.6 (min–max: 0.59–3.61). There was no significant difference in distance or volume ( P &gt; 0.05); however, the similarity index was significantly different ( P &lt; 0.01). Conclusion Deep learning can automatically segment the mandible, identify anatomic landmarks, and address medicinal demands in people without mandibular deformities. The most accurate MMSP was the B-Gn-F plane.","<method>PointRend deep learning mechanism</method>, <method>PoseNet</method>"
2024,https://openalex.org/W4391515422,Biology,Predicting transient wind loads on tall buildings in three-dimensional spatial coordinates using machine learning,"Machine learning (ML) as a subset of artificial intelligence (AI), has gained significant attention in wind engineering applications over the past decade. Wind load predictions for tall buildings using ML studies presented in literature have always been limited to static pressure measurements or time history measurements without considering the spatial coordinates system. To design wind-sensitive tall buildings, ML models must be capable of estimating transient wind flow quantities along with its spatial distribution. Thus, in this study, for the first time, the authors used ML to model the transient wind pressure on a tall building using a three-dimensional (3D) spatial coordinates system. A series of Boundary Layer Wind Tunnel tests were performed to obtain the transient pressure readings on building surfaces, which were used to validate the Computational Fluid Dynamics (CFD) models. Turbulence was modelled using large eddy simulations and the data obtained through CFD simulations were utilised to generate the ML models. The popular Extreme Gradient Boosting (XGBoost) model was selected as the ML model due to its capability of efficient data handling. The trained XGBoost model accurately predicted the transient wind pressure throughout the flow time. The XGBoost model has captured the extreme values well, closely following the flow patterns. In addition, special flow features like flow separation, reattachment, and steep pressure gradients have been well captured over the corresponding surfaces. Therefore, this study showcases the ability to use ML to predict pressures on tall buildings, capturing all key flow features time-efficiently.",<method>Extreme Gradient Boosting (XGBoost)</method>
2024,https://openalex.org/W4391693196,Biology,Predicting DNA structure using a deep learning method,"Understanding the mechanisms of protein-DNA binding is critical in comprehending gene regulation. Three-dimensional DNA structure, also described as DNA shape, plays a key role in these mechanisms. In this study, we present a deep learning-based method, Deep DNAshape, that fundamentally changes the current k-mer based high-throughput prediction of DNA shape features by accurately accounting for the influence of extended flanking regions, without the need for extensive molecular simulations or structural biology experiments. By using the Deep DNAshape method, DNA structural features can be predicted for any length and number of DNA sequences in a high-throughput manner, providing an understanding of the effects of flanking regions on DNA structure in a target region of a sequence. The Deep DNAshape method provides access to the influence of distant flanking regions on a region of interest. Our findings reveal that DNA shape readout mechanisms of a core target are quantitatively affected by flanking regions, including extended flanking regions, providing valuable insights into the detailed structural readout mechanisms of protein-DNA binding. Furthermore, when incorporated in machine learning models, the features generated by Deep DNAshape improve the model prediction accuracy. Collectively, Deep DNAshape can serve as versatile and powerful tool for diverse DNA structure-related studies.","<method>deep learning-based method</method>, <method>machine learning models</method>"
2024,https://openalex.org/W4391943312,Biology,"Machine Learning and Deep Learning in Synthetic Biology: Key Architectures, Applications, and Challenges","Machine learning (ML), particularly deep learning (DL), has made rapid and substantial progress in synthetic biology in recent years. Biotechnological applications of biosystems, including pathways, enzymes, and whole cells, are being probed frequently with time. The intricacy and interconnectedness of biosystems make it challenging to design them with the desired properties. ML and DL have a synergy with synthetic biology. Synthetic biology can be employed to produce large data sets for training models (for instance, by utilizing DNA synthesis), and ML/DL models can be employed to inform design (for example, by generating new parts or advising unrivaled experiments to perform). This potential has recently been brought to light by research at the intersection of engineering biology and ML/DL through achievements like the design of novel biological components, best experimental design, automated analysis of microscopy data, protein structure prediction, and biomolecular implementations of ANNs (Artificial Neural Networks). I have divided this review into three sections. In the first section, I describe predictive potential and basics of ML along with myriad applications in synthetic biology, especially in engineering cells, activity of proteins, and metabolic pathways. In the second section, I describe fundamental DL architectures and their applications in synthetic biology. Finally, I describe different challenges causing hurdles in the progress of ML/DL and synthetic biology along with their solutions.","<method>machine learning (ML)</method>, <method>deep learning (DL)</method>, <method>Artificial Neural Networks (ANNs)</method>"
2024,https://openalex.org/W4391997375,Biology,A machine learning approach to predict the efficiency of corrosion inhibition by natural product-based organic inhibitors,"Abstract This paper presents a quantitative structure–property relationship (QSPR)-based machine learning (ML) framework designed for predicting corrosion inhibition efficiency (CIE) values in natural organic inhibitor compounds. The modeling dataset comprises 50 natural organic compounds, with 11 quantum chemical properties (QCP) serving as input features, and the target variable being the corrosion inhibition efficiency (CIE) value. To enhance the predictive accuracy of the ML model, the kernel density estimation (KDE) function is employed to generate virtual samples during the training process, with the overarching goal of refining the precision of the ML model. Three distinct models, namely random forest (RF), gradient boosting (GB), and k-nearest neighbor (KNN), are tested in the study. The results demonstrate a noteworthy enhancement in the prediction performance of the models, attributable to the incorporation of virtual samples that effectively improve the correlation between input features and target values. Consequently, the accuracy of the predicted CIE values is significantly augmented, aligning more closely with the actual CIE values. Performance improvements were evident across all models after the incorporation of virtual samples. The GB, RF, and KNN models exhibited increments in R 2 values from 0.557 to 0.996, 0.522 to 0.999, and 0.415 to 0.994, respectively, concomitant with the introduction of 500 virtual samples. Additionally, each model demonstrated a notable reduction in RMSE values, transitioning from 1.41 to 0.19, 1.27 to 0.10, and 1.22 to 0.16, respectively. While the GB model initially outperformed others before the addition of virtual samples, the performance of the model exhibited fluctuation as the number of virtual samples varied. This behavior suggests that the KDE function provides a certain level of resilience against model variations. The proposed approach contributes to the effective design and exploration of corrosion inhibitor candidates, offering a reliable and accurate predictive tool that bridges the gap between theoretical studies and experimental synthesis.","<method>random forest (RF)</method>, <method>gradient boosting (GB)</method>, <method>k-nearest neighbor (KNN)</method>, <method>kernel density estimation (KDE)</method>"
2024,https://openalex.org/W4393905353,Biology,Theranostics and artificial intelligence: new frontiers in personalized medicine,"The field of theranostics is rapidly advancing, driven by the goals of enhancing patient care. Recent breakthroughs in artificial intelligence (AI) and its innovative theranostic applications have marked a critical step forward in nuclear medicine, leading to a significant paradigm shift in precision oncology. For instance, AI-assisted tumor characterization, including automated image interpretation, tumor segmentation, feature identification, and prediction of high-risk lesions, improves diagnostic processes, offering a precise and detailed evaluation. With a comprehensive assessment tailored to an individual's unique clinical profile, AI algorithms promise to enhance patient risk classification, thereby benefiting the alignment of patient needs with the most appropriate treatment plans. By uncovering potential factors unseeable to the human eye, such as intrinsic variations in tumor radiosensitivity or molecular profile, AI software has the potential to revolutionize the prediction of response heterogeneity. For accurate and efficient dosimetry calculations, AI technology offers significant advantages by providing customized phantoms and streamlining complex mathematical algorithms, making personalized dosimetry feasible and accessible in busy clinical settings. AI tools have the potential to be leveraged to predict and mitigate treatment-related adverse events, allowing early interventions. Additionally, generative AI can be utilized to find new targets for developing novel radiopharmaceuticals and facilitate drug discovery. However, while there is immense potential and notable interest in the role of AI in theranostics, these technologies do not lack limitations and challenges. There remains still much to be explored and understood. In this study, we investigate the current applications of AI in theranostics and seek to broaden the horizons for future research and innovation.","<method>AI-assisted tumor characterization</method>, <method>automated image interpretation</method>, <method>tumor segmentation</method>, <method>feature identification</method>, <method>prediction of high-risk lesions</method>, <method>AI algorithms for patient risk classification</method>, <method>AI software for prediction of response heterogeneity</method>, <method>AI technology for dosimetry calculations</method>, <method>AI tools to predict and mitigate treatment-related adverse events</method>, <method>generative AI for novel radiopharmaceutical target discovery and drug development</method>"
2024,https://openalex.org/W4395037579,Biology,Assessing ChatGPT 4.0’s test performance and clinical diagnostic accuracy on USMLE STEP 2 CK and clinical case reports,"Abstract While there is data assessing the test performance of artificial intelligence (AI) chatbots, including the Generative Pre-trained Transformer 4.0 (GPT 4) chatbot (ChatGPT 4.0), there is scarce data on its diagnostic accuracy of clinical cases. We assessed the large language model (LLM), ChatGPT 4.0, on its ability to answer questions from the United States Medical Licensing Exam (USMLE) Step 2, as well as its ability to generate a differential diagnosis based on corresponding clinical vignettes from published case reports. A total of 109 Step 2 Clinical Knowledge (CK) practice questions were inputted into both ChatGPT 3.5 and ChatGPT 4.0, asking ChatGPT to pick the correct answer. Compared to its previous version, ChatGPT 3.5, we found improved accuracy of ChatGPT 4.0 when answering these questions, from 47.7 to 87.2% ( p = 0.035) respectively. Utilizing the topics tested on Step 2 CK questions, we additionally found 63 corresponding published case report vignettes and asked ChatGPT 4.0 to come up with its top three differential diagnosis. ChatGPT 4.0 accurately created a shortlist of differential diagnoses in 74.6% of the 63 case reports (74.6%). We analyzed ChatGPT 4.0’s confidence in its diagnosis by asking it to rank its top three differentials from most to least likely. Out of the 47 correct diagnoses, 33 were the first (70.2%) on the differential diagnosis list, 11 were second (23.4%), and three were third (6.4%). Our study shows the continued iterative improvement in ChatGPT’s ability to answer standardized USMLE questions accurately and provides insights into ChatGPT’s clinical diagnostic accuracy.","<method>Generative Pre-trained Transformer 4.0 (GPT 4) chatbot (ChatGPT 4.0)</method>, <method>ChatGPT 3.5</method>, <method>large language model (LLM)</method>"
2024,https://openalex.org/W4399173789,Biology,Utilizing Deep Learning and the Internet of Things to Monitor the Health of Aquatic Ecosystems to Conserve Biodiversity,"The decline in water conditions contributes to the crisis in clean water biodiversity. The interactions between water conditions indicators and the correlations among these variables and taxonomic groupings are intricate in their impact on biodiversity. However, since there are just a few kinds of Internet of Things (IoT) that are accessible to purchase, many chemical and biological measurements still need laboratory studies. The newest progress in Deep Learning and the IoT allows for the use of this method in the real-time surveillance of water quality, therefore contributing to preserving biodiversity. This paper presents a thorough examination of the scientific literature about the water quality factors that have a significant influence on the variety of freshwater ecosystems. It selected the ten most crucial water quality criteria. The connections between the quantifiable and valuable aspects of the IoT are assessed using a Generalized Regression-based Neural Networks (G-RNN) framework and a multi-variational polynomial regression framework. These models depend on historical data from the monitoring of water quality. The projected findings in an urbanized river were validated using a combination of traditional field water testing, in-lab studies, and the created IoT-depend water condition management system. The G-RNN effectively differentiates abnormal increases in variables from typical scenarios. The assessment coefficients for the system for degree 8 are as follows: 0.87, 0.73, 0.89, and 0.79 for N-O3-N, BO-D5, P-O4, and N-H3-N. The suggested methods and prototypes were verified against laboratory findings to assess their efficacy and effectiveness. The general efficacy was deemed suitable, with most forecasting mistakes smaller than 0.3 mg/L. This validation offers valuable insights into IoT methods' usage in pollutants released observation and additional water quality regulating usage, specifically for freshwater biodiversity preservation.","<method>Generalized Regression-based Neural Networks (G-RNN)</method>, <method>multi-variational polynomial regression framework</method>"
2024,https://openalex.org/W4401593044,Biology,Overcoming the Limits of Cross-Sensitivity: Pattern Recognition Methods for Chemiresistive Gas Sensor Array,"Abstract As information acquisition terminals for artificial olfaction, chemiresistive gas sensors are often troubled by their cross-sensitivity, and reducing their cross-response to ambient gases has always been a difficult and important point in the gas sensing area. Pattern recognition based on sensor array is the most conspicuous way to overcome the cross-sensitivity of gas sensors. It is crucial to choose an appropriate pattern recognition method for enhancing data analysis, reducing errors and improving system reliability, obtaining better classification or gas concentration prediction results. In this review, we analyze the sensing mechanism of cross-sensitivity for chemiresistive gas sensors. We further examine the types, working principles, characteristics, and applicable gas detection range of pattern recognition algorithms utilized in gas-sensing arrays. Additionally, we report, summarize, and evaluate the outstanding and novel advancements in pattern recognition methods for gas identification. At the same time, this work showcases the recent advancements in utilizing these methods for gas identification, particularly within three crucial domains: ensuring food safety, monitoring the environment, and aiding in medical diagnosis. In conclusion, this study anticipates future research prospects by considering the existing landscape and challenges. It is hoped that this work will make a positive contribution towards mitigating cross-sensitivity in gas-sensitive devices and offer valuable insights for algorithm selection in gas recognition applications.",<method>pattern recognition</method>
2024,https://openalex.org/W4390492164,Biology,Efficient Camouflaged Object Detection Network Based on Global Localization Perception and Local Guidance Refinement,"Camouflaged Object Detection (COD) is a challenging visual task due to its complex contour, diverse scales, and high similarity to the background. Existing COD methods encounter two predicaments: One is that they are prone to falling into local perception, resulting in inaccurate object localization; Another issue is the difficulty in achieving precise object segmentation due to a lack of detailed information. In addition, most COD methods typically require larger parameter amounts and higher computational complexity in pursuit of better performance. To this end, we propose a global localization perception and local guidance refinement network (PRNet), that simultaneously addresses performance and computational costs. Through effective aggregation and use of semantic and details information, the PRNet can achieve accurate localization and refined segmentation of camouflaged objects. Specifically, with the help of a Cascaded Attention Perceptron (CAP) designed, we can effectively integrate and perceive multi-scale information to localize camouflaged objects. We also design a Guided Refinement Decoder (GRD) in a top-down manner to extract context information and aggregate details to further refine camouflaged prediction results. Extensive experimental results demonstrate that our PRNet outperforms 12 state-of-the-art models on 4 challenging datasets. Meanwhile, the PRNet has a smaller number of parameters (12.74M), lower computational complexity (10.24G), and real-time inference speed (105FPS). Source codes are available at https://github.com/hu-xh/PRNet.","<method>global localization perception and local guidance refinement network (PRNet)</method>, <method>Cascaded Attention Perceptron (CAP)</method>, <method>Guided Refinement Decoder (GRD)</method>"
2024,https://openalex.org/W4390501772,Biology,Remote sensing based forest cover classification using machine learning,"Abstract Pakistan falls significantly below the recommended forest coverage level of 20 to 30 percent of total area, with less than 6 percent of its land under forest cover. This deficiency is primarily attributed to illicit deforestation for wood and charcoal, coupled with a failure to embrace advanced techniques for forest estimation, monitoring, and supervision. Remote sensing techniques leveraging Sentinel-2 satellite images were employed. Both single-layer stacked images and temporal layer stacked images from various dates were utilized for forest classification. The application of an artificial neural network (ANN) supervised classification algorithm yielded notable results. Using a single-layer stacked image from Sentinel-2, an impressive 91.37% training overall accuracy and 0.865 kappa coefficient were achieved, along with 93.77% testing overall accuracy and a 0.902 kappa coefficient. Furthermore, the temporal layer stacked image approach demonstrated even better results. This method yielded 98.07% overall training accuracy, 97.75% overall testing accuracy, and kappa coefficients of 0.970 and 0.965, respectively. The random forest (RF) algorithm, when applied, achieved 99.12% overall training accuracy, 92.90% testing accuracy, and kappa coefficients of 0.986 and 0.882. Notably, with the temporal layer stacked image of the Sentinel-2 satellite, the RF algorithm reached exceptional performance with 99.79% training accuracy, 96.98% validation accuracy, and kappa coefficients of 0.996 and 0.954. In terms of forest cover estimation, the ANN algorithm identified 31.07% total forest coverage in the District Abbottabad region. In comparison, the RF algorithm recorded a slightly higher 31.17% of the total forested area. This research highlights the potential of advanced remote sensing techniques and machine learning algorithms in improving forest cover assessment and monitoring strategies.","<method>artificial neural network (ANN) supervised classification algorithm</method>, <method>random forest (RF) algorithm</method>"
2024,https://openalex.org/W4390588870,Biology,Machine learning-driven optimization of enterprise resource planning (ERP) systems: a comprehensive review,"Abstract In the dynamic and changing realm of technology and business operations, staying abreast of recent trends is paramount. This review evaluates the progress in the development of the integration of machine learning (ML) with enterprise resource planning (ERP) systems, revealing the impact of these trends on the ERP optimization. In recent years, there has been a significant advancement in the integration of ML technology within ERP environments. ML algorithms characterized by their ability to extract intricate patterns from vast datasets are being harnessed to enable ERP systems to make more accurate predictions and data-driven decisions. Therefore, ML enables ERP systems to adapt dynamically based on real-time insights, resulting in enhanced efficiency and adaptability. Furthermore, organizations are increasingly looking for artificial intelligence (AI) solutions as they actually try to make ML models within ERP clear and comprehensible for stakeholders. These solutions enable ERP systems to process and act on data as it flows in, due to the utilization of ML models, which enables enterprises to react effectively to changing circumstances. The rapid insights and useful intelligence offered by this trend have had a significant impact across industries. IoT (Internet of Things) and ML integration with ERP are continuously gaining significance. These algorithms allow for the creation of adaptable strategies supported by ongoing learning and data-driven optimization, which has a number of benefits for ERP system optimization. In addition, the Industrial Internet of Things (IIoT) was investigated in this review to provide the state-of-the-art and emerging challenges due to ML integration. This review provides a comprehensive analysis of the integration of machine learning algorithms across several ERP applications by conducting an extensive literature assessment of recent publications. By synthesizing the latest research findings, this comprehensive review provides an in-depth analysis of the cutting-edge techniques and recent advancements in the context of machine learning (ML)-driven optimization of enterprise resource planning (ERP) systems. It not only provides an insight into the methodology and impact of the state-of-the-art but also offers valuable insights into where the future of ML in ERP may lead, propelling ERP systems into a new era of intelligence, efficiency, and innovation.",<method>machine learning (ML) algorithms</method>
2024,https://openalex.org/W4390660406,Biology,Improving River Routing Using a Differentiable Muskingum‐Cunge Model and Physics‐Informed Machine Learning,"Abstract Recently, rainfall‐runoff simulations in small headwater basins have been improved by methodological advances such as deep neural networks (NNs) and hybrid physics‐NN models—particularly, a genre called differentiable modeling that intermingles NNs with physics to learn relationships between variables. However, hydrologic routing simulations, necessary for simulating floods in stem rivers downstream of large heterogeneous basins, had not yet benefited from these advances and it was unclear if the routing process could be improved via coupled NNs. We present a novel differentiable routing method ( δ MC‐Juniata‐hydroDL2) that mimics the classical Muskingum‐Cunge routing model over a river network but embeds an NN to infer parameterizations for Manning's roughness ( n ) and channel geometries from raw reach‐scale attributes like catchment areas and sinuosity. The NN was trained solely on downstream hydrographs. Synthetic experiments show that while the channel geometry parameter was unidentifiable, n can be identified with moderate precision. With real‐world data, the trained differentiable routing model produced more accurate long‐term routing results for both the training gage and untrained inner gages for larger subbasins (&gt;2,000 km 2 ) than either a machine learning model assuming homogeneity, or simply using the sum of runoff from subbasins. The n parameterization trained on short periods gave high performance in other periods, despite significant errors in runoff inputs. The learned n pattern was consistent with literature expectations, demonstrating the framework's potential for knowledge discovery, but the absolute values can vary depending on training periods. The trained n parameterization can be coupled with traditional models to improve national‐scale hydrologic flood simulations.","<method>deep neural networks (NNs)</method>, <method>hybrid physics‐NN models</method>, <method>differentiable modeling</method>, <method>differentiable routing method (δ MC‐Juniata‐hydroDL2)</method>, <method>machine learning model assuming homogeneity</method>"
2024,https://openalex.org/W4390691678,Biology,Neural heterogeneity controls computations in spiking neural networks,"The brain is composed of complex networks of interacting neurons that express considerable heterogeneity in their physiology and spiking characteristics. How does this neural heterogeneity influence macroscopic neural dynamics, and how might it contribute to neural computation? In this work, we use a mean-field model to investigate computation in heterogeneous neural networks, by studying how the heterogeneity of cell spiking thresholds affects three key computational functions of a neural population: the gating, encoding, and decoding of neural signals. Our results suggest that heterogeneity serves different computational functions in different cell types. In inhibitory interneurons, varying the degree of spike threshold heterogeneity allows them to gate the propagation of neural signals in a reciprocally coupled excitatory population. Whereas homogeneous interneurons impose synchronized dynamics that narrow the dynamic repertoire of the excitatory neurons, heterogeneous interneurons act as an inhibitory offset while preserving excitatory neuron function. Spike threshold heterogeneity also controls the entrainment properties of neural networks to periodic input, thus affecting the temporal gating of synaptic inputs. Among excitatory neurons, heterogeneity increases the dimensionality of neural dynamics, improving the network’s capacity to perform decoding tasks. Conversely, homogeneous networks suffer in their capacity for function generation, but excel at encoding signals via multistable dynamic regimes. Drawing from these findings, we propose intra-cell-type heterogeneity as a mechanism for sculpting the computational properties of local circuits of excitatory and inhibitory spiking neurons, permitting the same canonical microcircuit to be tuned for diverse computational tasks.",No methods found.
2024,https://openalex.org/W4390821180,Biology,Can ChatGPT effectively complement teacher assessment of undergraduate students’ academic writing?,"The integration of ChatGPT as a supplementary tool for writing instruction has gained traction. However, uncertainties persist regarding how ChatGPT complements teacher assessment and the overall effectiveness of this combined approach. To address this, we conducted a mixed-methods investigation involving 46 undergraduate students from a research university in southern China, engaging them in a Chinese academic writing task. The intraclass correlation coefficient results revealed ChatGPT's efficiency in scoring students' writing, showing moderate to good consistency with teacher evaluations. A paired sample t-test unveiled significant differences in feedback quantity and types between ChatGPT and teacher assessments. Drawing from both interview data and quantitative findings, the study uncovers three ways in which ChatGPT complements teacher assessment, benefiting students with various writing proficiency levels: (1) fostering deeper comprehension of teacher assessments among students, (2) encouraging students to make judgments regarding feedback, and (3) promoting independent thinking about revisions. This study contributes to a more comprehensive understanding of the role of ChatGPT within the context of a combined assessment approach. It underscores that certain inherent weaknesses in ChatGPT's functioning can paradoxically lead to favorable outcomes. By shedding light on the synergy between ChatGPT and teacher assessments, this research seeks to inform and enhance writing instruction in higher education.",No methods found.
2024,https://openalex.org/W4390954471,Biology,Traffic Sign Detection and Recognition Using YOLO Object Detection Algorithm: A Systematic Review,"Context: YOLO (You Look Only Once) is an algorithm based on deep neural networks with real-time object detection capabilities. This state-of-the-art technology is widely available, mainly due to its speed and precision. Since its conception, YOLO has been applied to detect and recognize traffic signs, pedestrians, traffic lights, vehicles, and so on. Objective: The goal of this research is to systematically analyze the YOLO object detection algorithm, applied to traffic sign detection and recognition systems, from five relevant aspects of this technology: applications, datasets, metrics, hardware, and challenges. Method: This study performs a systematic literature review (SLR) of studies on traffic sign detection and recognition using YOLO published in the years 2016–2022. Results: The search found 115 primary studies relevant to the goal of this research. After analyzing these investigations, the following relevant results were obtained. The most common applications of YOLO in this field are vehicular security and intelligent and autonomous vehicles. The majority of the sign datasets used to train, test, and validate YOLO-based systems are publicly available, with an emphasis on datasets from Germany and China. It has also been discovered that most works present sophisticated detection, classification, and processing speed metrics for traffic sign detection and recognition systems by using the different versions of YOLO. In addition, the most popular desktop data processing hardwares are Nvidia RTX 2080 and Titan Tesla V100 and, in the case of embedded or mobile GPU platforms, Jetson Xavier NX. Finally, seven relevant challenges that these systems face when operating in real road conditions have been identified. With this in mind, research has been reclassified to address these challenges in each case. Conclusions: This SLR is the most relevant and current work in the field of technology development applied to the detection and recognition of traffic signs using YOLO. In addition, insights are provided about future work that could be conducted to improve the field.",<method>YOLO (You Only Look Once)</method>
2024,https://openalex.org/W4392015292,Biology,Avoiding fusion plasma tearing instability with deep reinforcement learning,"Abstract For stable and efficient fusion energy production using a tokamak reactor, it is essential to maintain a high-pressure hydrogenic plasma without plasma disruption. Therefore, it is necessary to actively control the tokamak based on the observed plasma state, to manoeuvre high-pressure plasma while avoiding tearing instability, the leading cause of disruptions. This presents an obstacle-avoidance problem for which artificial intelligence based on reinforcement learning has recently shown remarkable performance 1–4 . However, the obstacle here, the tearing instability, is difficult to forecast and is highly prone to terminating plasma operations, especially in the ITER baseline scenario. Previously, we developed a multimodal dynamic model that estimates the likelihood of future tearing instability based on signals from multiple diagnostics and actuators 5 . Here we harness this dynamic model as a training environment for reinforcement-learning artificial intelligence, facilitating automated instability prevention. We demonstrate artificial intelligence control to lower the possibility of disruptive tearing instabilities in DIII-D 6 , the largest magnetic fusion facility in the United States. The controller maintained the tearing likelihood under a given threshold, even under relatively unfavourable conditions of low safety factor and low torque. In particular, it allowed the plasma to actively track the stable path within the time-varying operational space while maintaining H-mode performance, which was challenging with traditional preprogrammed control. This controller paves the path to developing stable high-performance operational scenarios for future use in ITER.","<method>reinforcement learning</method>, <method>multimodal dynamic model</method>"
2024,https://openalex.org/W4393094733,Biology,Advancing entity recognition in biomedicine via instruction tuning of large language models,"Abstract Motivation Large Language Models (LLMs) have the potential to revolutionize the field of Natural Language Processing, excelling not only in text generation and reasoning tasks but also in their ability for zero/few-shot learning, swiftly adapting to new tasks with minimal fine-tuning. LLMs have also demonstrated great promise in biomedical and healthcare applications. However, when it comes to Named Entity Recognition (NER), particularly within the biomedical domain, LLMs fall short of the effectiveness exhibited by fine-tuned domain-specific models. One key reason is that NER is typically conceptualized as a sequence labeling task, whereas LLMs are optimized for text generation and reasoning tasks. Results We developed an instruction-based learning paradigm that transforms biomedical NER from a sequence labeling task into a generation task. This paradigm is end-to-end and streamlines the training and evaluation process by automatically repurposing pre-existing biomedical NER datasets. We further developed BioNER-LLaMA using the proposed paradigm with LLaMA-7B as the foundational LLM. We conducted extensive testing on BioNER-LLaMA across three widely recognized biomedical NER datasets, consisting of entities related to diseases, chemicals, and genes. The results revealed that BioNER-LLaMA consistently achieved higher F1-scores ranging from 5% to 30% compared to the few-shot learning capabilities of GPT-4 on datasets with different biomedical entities. We show that a general-domain LLM can match the performance of rigorously fine-tuned PubMedBERT models and PMC-LLaMA, biomedical-specific language model. Our findings underscore the potential of our proposed paradigm in developing general-domain LLMs that can rival SOTA performances in multi-task, multi-domain scenarios in biomedical and health applications. Availability and implementation Datasets and other resources are available at https://github.com/BIDS-Xu-Lab/BioNER-LLaMA.","<method>instruction-based learning paradigm</method>, <method>few-shot learning</method>, <method>fine-tuned domain-specific models</method>, <method>LLaMA-7B</method>, <method>BioNER-LLaMA</method>, <method>GPT-4 few-shot learning</method>, <method>PubMedBERT fine-tuning</method>, <method>PMC-LLaMA biomedical-specific language model</method>"
2024,https://openalex.org/W4393306481,Biology,Reliable water quality prediction and parametric analysis using explainable AI models,"Abstract The consumption of water constitutes the physical health of most of the living species and hence management of its purity and quality is extremely essential as contaminated water has to potential to create adverse health and environmental consequences. This creates the dire necessity to measure, control and monitor the quality of water. The primary contaminant present in water is Total Dissolved Solids (TDS), which is hard to filter out. There are various substances apart from mere solids such as potassium, sodium, chlorides, lead, nitrate, cadmium, arsenic and other pollutants. The proposed work aims to provide the automation of water quality estimation through Artificial Intelligence and uses Explainable Artificial Intelligence (XAI) for the explanation of the most significant parameters contributing towards the potability of water and the estimation of the impurities. XAI has the transparency and justifiability as a white-box model since the Machine Learning (ML) model is black-box and unable to describe the reasoning behind the ML classification. The proposed work uses various ML models such as Logistic Regression, Support Vector Machine (SVM), Gaussian Naive Bayes, Decision Tree (DT) and Random Forest (RF) to classify whether the water is drinkable. The various representations of XAI such as force plot, test patch, summary plot, dependency plot and decision plot generated in SHAPELY explainer explain the significant features, prediction score, feature importance and justification behind the water quality estimation. The RF classifier is selected for the explanation and yields optimum Accuracy and F1-Score of 0.9999, with Precision and Re-call of 0.9997 and 0.998 respectively. Thus, the work is an exploratory analysis of the estimation and management of water quality with indicators associated with their significance. This work is an emerging research at present with a vision of addressing the water quality for the future as well.","<method>Artificial Intelligence</method>, <method>Explainable Artificial Intelligence (XAI)</method>, <method>Logistic Regression</method>, <method>Support Vector Machine (SVM)</method>, <method>Gaussian Naive Bayes</method>, <method>Decision Tree (DT)</method>, <method>Random Forest (RF)</method>"
2024,https://openalex.org/W4399319394,Biology,Multi-task aquatic toxicity prediction model based on multi-level features fusion,"With the escalating menace of organic compounds in environmental pollution imperiling the survival of aquatic organisms, the investigation of organic compound toxicity across diverse aquatic species assumes paramount significance for environmental protection. Understanding how different species respond to these compounds helps assess the potential ecological impact of pollution on aquatic ecosystems as a whole. Compared with traditional experimental methods, deep learning methods have higher accuracy in predicting aquatic toxicity, faster data processing speed and better generalization ability. This article presents ATFPGT-multi, an advanced multi-task deep neural network prediction model for organic toxicity. The model integrates molecular fingerprints and molecule graphs to characterize molecules, enabling the simultaneous prediction of acute toxicity for the same organic compound across four distinct fish species. Furthermore, to validate the advantages of multi-task learning, we independently construct prediction models, named ATFPGT-single, for each fish species. We employ cross-validation in our experiments to assess the performance and generalization ability of ATFPGT-multi. The experimental results indicate, first, that ATFPGT-multi outperforms ATFPGT-single on four fish datasets with AUC improvements of 9.8%, 4%, 4.8%, and 8.2%, respectively, demonstrating the superiority of multi-task learning over single-task learning. Furthermore, in comparison with previous algorithms, ATFPGT-multi outperforms comparative methods, emphasizing that our approach exhibits higher accuracy and reliability in predicting aquatic toxicity. Moreover, ATFPGT-multi utilizes attention scores to identify molecular fragments associated with fish toxicity in organic molecules, as demonstrated by two organic molecule examples in the main text, demonstrating the interpretability of ATFPGT-multi. In summary, ATFPGT-multi provides important support and reference for the further development of aquatic toxicity assessment. All of codes and datasets are freely available online at https://github.com/zhaoqi106/ATFPGT-multi.","<method>deep learning methods</method>, <method>multi-task deep neural network prediction model</method>, <method>multi-task learning</method>, <method>single-task learning</method>, <method>cross-validation</method>"
2024,https://openalex.org/W4390984444,Biology,"Trends and challenges of fruit by-products utilization: insights into safety, sensory, and benefits of the use for the development of innovative healthy food: a review","Abstract A significant portion of the human diet is comprised of fruits, which are consumed globally either raw or after being processed. A huge amount of waste and by-products such as skins, seeds, cores, rags, rinds, pomace, etc. are being generated in our homes and agro-processing industries every day. According to previous statistics, nearly half of the fruits are lost or discarded during the entire processing chain. The concern arises when those wastes and by-products damage the environment and simultaneously cause economic losses. There is a lot of potential in these by-products for reuse in a variety of applications, including the isolation of valuable bioactive ingredients and their application in developing healthy and functional foods. The development of novel techniques for the transformation of these materials into marketable commodities may offer a workable solution to this waste issue while also promoting sustainable economic growth from the bio-economic viewpoint. This approach can manage waste as well as add value to enterprises. The goal of this study is twofold based on this scenario. The first is to present a brief overview of the most significant bioactive substances found in those by-products. The second is to review the current status of their valorization including the trends and techniques, safety assessments, sensory attributes, and challenges. Moreover, specific attention is drawn to the future perspective, and some solutions are discussed in this report.",No methods found.
2024,https://openalex.org/W4391321561,Biology,A survey on training challenges in generative adversarial networks for biomedical image analysis,"Abstract In biomedical image analysis, the applicability of deep learning methods is directly impacted by the quantity of image data available. This is due to deep learning models requiring large image datasets to provide high-level performance. Generative Adversarial Networks (GANs) have been widely utilized to address data limitations through the generation of synthetic biomedical images. GANs consist of two models. The generator, a model that learns how to produce synthetic images based on the feedback it receives. The discriminator, a model that classifies an image as synthetic or real and provides feedback to the generator. Throughout the training process, a GAN can experience several technical challenges that impede the generation of suitable synthetic imagery. First, the mode collapse problem whereby the generator either produces an identical image or produces a uniform image from distinct input features. Second, the non-convergence problem whereby the gradient descent optimizer fails to reach a Nash equilibrium. Thirdly, the vanishing gradient problem whereby unstable training behavior occurs due to the discriminator achieving optimal classification performance resulting in no meaningful feedback being provided to the generator. These problems result in the production of synthetic imagery that is blurry, unrealistic, and less diverse. To date, there has been no survey article outlining the impact of these technical challenges in the context of the biomedical imagery domain. This work presents a review and taxonomy based on solutions to the training problems of GANs in the biomedical imaging domain. This survey highlights important challenges and outlines future research directions about the training of GANs in the domain of biomedical imagery.","<method>deep learning</method>, <method>Generative Adversarial Networks (GANs)</method>, <method>generator</method>, <method>discriminator</method>, <method>gradient descent optimizer</method>"
2024,https://openalex.org/W4391137578,Psychology,A multinational study on the factors influencing university students’ attitudes and usage of ChatGPT,"Abstract Artificial intelligence models, like ChatGPT, have the potential to revolutionize higher education when implemented properly. This study aimed to investigate the factors influencing university students’ attitudes and usage of ChatGPT in Arab countries. The survey instrument “TAME-ChatGPT” was administered to 2240 participants from Iraq, Kuwait, Egypt, Lebanon, and Jordan. Of those, 46.8% heard of ChatGPT, and 52.6% used it before the study. The results indicated that a positive attitude and usage of ChatGPT were determined by factors like ease of use, positive attitude towards technology, social influence, perceived usefulness, behavioral/cognitive influences, low perceived risks, and low anxiety. Confirmatory factor analysis indicated the adequacy of the “TAME-ChatGPT” constructs. Multivariate analysis demonstrated that the attitude towards ChatGPT usage was significantly influenced by country of residence, age, university type, and recent academic performance. This study validated “TAME-ChatGPT” as a useful tool for assessing ChatGPT adoption among university students. The successful integration of ChatGPT in higher education relies on the perceived ease of use, perceived usefulness, positive attitude towards technology, social influence, behavioral/cognitive elements, low anxiety, and minimal perceived risks. Policies for ChatGPT adoption in higher education should be tailored to individual contexts, considering the variations in student attitudes observed in this study.",No methods found.
2024,https://openalex.org/W4390609372,Psychology,Investigation of the moderation effect of gender and study level on the acceptance and use of generative <scp>AI</scp> by higher education students: Comparative evidence from Poland and Egypt,"Abstract This study delves into the implications of incorporating AI tools, specifically ChatGPT, in higher education contexts. With a primary focus on understanding the acceptance and utilization of ChatGPT among university students, the research utilizes the Unified Theory of Acceptance and Use of Technology (UTAUT) as the guiding framework. The investigation probes into four crucial constructs of UTAUT—performance expectancy, effort expectancy, social influence and facilitating conditions—to understand their impact on the intent and actual use behaviour of students. The study relies on data collected from six universities in two countries and assessed through descriptive statistics and structural equation modelling techniques, and also takes into account participants' gender and study level. The key findings show that performance expectancy, effort expectancy, and social influence significantly influence behavioural intention. Furthermore, behavioural intention, when considered alongside facilitating conditions, influences actual use behaviour. This research also explores the moderating impact of gender and study level on the relationships among these variables. The results not only augment our comprehension of technology acceptance in the context of AI tools but also provide valuable input for formulating strategies that promote effective incorporation of ChatGPT in higher education. The study underscores the need for effective awareness initiatives, bespoke training programmes, and intuitive tool designs to bolster students' perceptions and foster the wider adoption of AI tools in education. Practitioner notes What is already known about this topic ChatGPT is a tool that is quickly gaining worldwide recognition. ChatGPT helps with writing essays and solving assignments. ChatGPT raises ethical concerns about authorship, plagiarism and ethics. What this paper adds This study explores students' acceptance of ChatGPT as an aid in their education, which has not been studied previously. We used the extended Unified Technology Acceptance and Use of Technology theory to test what factors mostly influence the use of ChatGPT by students. We conducted a multiple study in Poland and Egypt based on sampling strategy from six universities. Implications for practice and/or policy ChatGPT is a global game changer and should be incorporated into study programmes. The limitations of ChatGPT should be well explained and known since it is prone to making mistakes. Higher education teachers should be aware of ChatGPT's capabilities.",<method>structural equation modelling</method>
2024,https://openalex.org/W4399450035,Psychology,Power Hungry Processing: Watts Driving the Cost of AI Deployment?,"Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of ""generality"" comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and 'general-purpose' models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.","<method>finetuned models</method>, <method>multi-purpose generative architectures</method>"
2024,https://openalex.org/W4391776447,Psychology,Theories of motivation: A comprehensive analysis of human behavior drivers,"This paper explores theories of motivation, including instinct theory, arousal theory, incentive theory, intrinsic theory, extrinsic theory, the ARCS model, self-determination theory, expectancy-value theory, and goal-orientation theory. Each theory is described in detail, along with its key concepts, assumptions, and implications for behavior. Intrinsic theory suggests that individuals are motivated by internal factors like enjoyment and satisfaction, while extrinsic theory suggests that external factors like rewards and social pressure drive behavior. Arousal theory says that to feel motivated, people try to keep an optimal level of activation or excitement. Incentive theory suggests that behavior is driven by the promise of rewards or the threat of punishment. The ARCS model, designed to motivate learners, incorporates elements of attention, relevance, confidence, and satisfaction. Self-determination theory proposes that individuals are motivated by their needs for autonomy, competence, and relatedness. The expectation-value theory suggests that behavior is influenced by individuals' beliefs about their ability to succeed and the value they place on the task. The goal-orientation theory suggests that individuals have different goals for engaging in a behavior. By understanding these different theories of motivation, educators, coaches, managers, and individuals may analyze what drives behavior and how to harness it to achieve their goals. In essence, a nuanced comprehension of these diverse motivation theories equips individuals across varied domains with a strategic toolkit to navigate the complex landscape of human behavior, fostering a more profound understanding of what propels actions and how to channel these insights toward the attainment of overarching goals.",No methods found.
2024,https://openalex.org/W4392791588,Psychology,Can large language models replace humans in systematic reviews? Evaluating <scp>GPT</scp>‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages,"Systematic reviews are vital for guiding practice, research and policy, although they are often slow and labour-intensive. Large language models (LLMs) could speed up and automate systematic reviews, but their performance in such tasks has yet to be comprehensively evaluated against humans, and no study has tested Generative Pre-Trained Transformer (GPT)-4, the biggest LLM so far. This pre-registered study uses a ""human-out-of-the-loop"" approach to evaluate GPT-4's capability in title/abstract screening, full-text review and data extraction across various literature types and languages. Although GPT-4 had accuracy on par with human performance in some tasks, results were skewed by chance agreement and dataset imbalance. Adjusting for these caused performance scores to drop across all stages: for data extraction, performance was moderate, and for screening, it ranged from none in highly balanced literature datasets (~1:1) to moderate in those datasets where the ratio of inclusion to exclusion in studies was imbalanced (~1:3). When screening full-text literature using highly reliable prompts, GPT-4's performance was more robust, reaching ""human-like"" levels. Although our findings indicate that, currently, substantial caution should be exercised if LLMs are being used to conduct systematic reviews, they also offer preliminary evidence that, for certain review tasks delivered under specific conditions, LLMs can rival human performance.","<method>Large language models (LLMs)</method>, <method>Generative Pre-Trained Transformer (GPT)-4</method>"
2024,https://openalex.org/W4392343959,Psychology,Investigating factors of students' behavioral intentions to adopt chatbot technologies in higher education: Perspective from expanded diffusion theory of innovation,"With the emergence of emerging 4IR technologies, AI application tools (chatbots) are becoming more and more popular and widespread in various fields, including education. This study investigates the factors that influence undergraduate students' inclination to utilize AI application tools, specifically chatbots, for educational purposes. We applied an expanded diffusion theory of innovation framework to examine the relationships between relative advantages, compatibility, trialability, trust, perceived usefulness, perceived ease of use, and behavioral intention. Data from 842 undergraduate students were collected through a questionnaire using a 7-point scale, and the findings were analyzed using SmartPLS 4.0.9.2 software with a covariance-based structural equation model. The results confirm the hypotheses regarding the relative advantages, compatibility, trialability, perceived usefulness, and trust of chatbots. Students who perceive the benefits of chatbots express a strong intention to use them for academic purposes. The perception of compatibility between students and chatbots positively influences their adoption intention, and those who have the opportunity to try out chatbots are more likely to use them, indicating the importance of trialability. Surprisingly, the study did not find direct relationships between perceived usefulness, perceived ease of use, and behavioral intention, suggesting the presence of other influencing factors or dynamics in the adoption of chatbots for educational purposes. The findings offer practical insights for students and contribute to the theoretical understanding of the diffusion theory of innovation. Future research can further explore these findings to gain deeper insights into the complexities of chatbot adoption and enhance the adoption of AI tools in educational settings.",No methods found.
2024,https://openalex.org/W4394884079,Psychology,TRANSFORMING FINTECH FRAUD DETECTION WITH ADVANCED ARTIFICIAL INTELLIGENCE ALGORITHMS,"The rapid evolution of financial technology (fintech) platforms has exponentially increased the volume and sophistication of financial transactions, concurrently elevating the risk and complexity of fraudulent activities. This necessitates a paradigm shift in fraud detection methodologies towards more agile, accurate, and predictive solutions. This paper presents a comprehensive study on the transformative potential of advanced Artificial Intelligence (AI) algorithms in enhancing fintech fraud detection mechanisms. By leveraging cutting-edge AI techniques including deep learning, machine learning, and natural language processing, this research aims to develop a robust fraud detection framework capable of identifying, analyzing, and preventing fraudulent transactions in real-time.&#x0D; Our methodology encompasses the deployment of several AI algorithms on extensive datasets comprising genuine and fraudulent financial transactions. Through a comparative analysis, we identify the most effective algorithms in terms of accuracy, efficiency, and scalability. Key findings reveal that deep learning models, particularly those employing neural networks, outperform traditional machine learning models in detecting complex and nuanced fraudulent activities. Furthermore, the integration of natural language processing enables the extraction and analysis of unstructured data, significantly enhancing the detection capabilities.&#x0D; Conclusively, this paper underscores the critical role of advanced AI algorithms in revolutionizing fintech fraud detection. It highlights the superior performance of AI-based models over conventional methods, offering fintech platforms a more dynamic and predictive approach to fraud prevention. This research not only contributes to the academic discourse on financial security but also provides practical insights for fintech companies striving to safeguard their operations against fraud.&#x0D; Keywords: Artificial Intelligence, Fintech, Fraud Detection, Ethical Ai, Regulatory Compliance, Data Privacy, Algorithmic Bias, Predictive Analytics, Blockchain Technology, Quantum Computing, Interdisciplinary Collaboration, Innovation, Transparency, Accountability, Continuous Learning, Ethical Principles, Real-Time Processing, Financial Sector.","<method>deep learning</method>, <method>machine learning</method>, <method>natural language processing</method>, <method>neural networks</method>"
2024,https://openalex.org/W4392820427,Psychology,Understanding the diffusion of AI-generative (ChatGPT) in higher education: Does students' integrity matter?,"ChatGPT, an AI-powered language model, is revolutionising the academic world. Scholars, researchers, and students use its advanced capabilities to achieve their educational objectives, including generating innovative ideas, delivering assignments, and conducting extensive research projects. Nevertheless, the use of ChatGPT among students is contentious, giving rise to significant apprehensions regarding integrity and AI-facilitated deceit. At the same time, scholarly communities currently need more well-defined standards for adopting such academia-oriented technology. This study aims to determine students' use of ChatGPT using the Unified Theory of Acceptance and Use of Technology (UTAUT) and Social Cognitive Theory (SCT), notably the role of students' integrity in determining adoption behaviour. The analysis of 921 responses demonstrated that the utilisation of ChatGPT is influenced positively by performance expectancy, social influence, educational self-efficacy, technology self-efficacy, and personal anxiety. Conversely, student integrity was found to negatively impact usage. Remarkably, student integrity has a positive moderating effect between effort expectancy and ChatGPT usage. At the same time, it has a negative moderating effect on the link between performance expectancy and technology self-efficacy with ChatGPT usage. Hence, we propose that the academic community, AI language model developers, publishers, and relevant stakeholders collaborate to establish explicit rules for the utilisation of AI chatbots in an ethical manner for educational purposes.",No methods found.
2024,https://openalex.org/W4391822043,Psychology,Role of information processing and digital supply chain in supply chain resilience through supply chain risk management,"Purpose Supply chain (SC) management is more challenging than ever. Significantly, the pandemic has provoked global and economic destruction that appeared in the manufacturing industry as a “black swan.” Therefore, the purpose of this study was to examine the role of information processing and digital supply chain in supply chain resilience through supply chain risk management. Design/methodology/approach This study examines SC risk management and resilience from an information processing theory perspective. The authors used data collected from 251 SC professionals in the manufacturing industry, and the authors used a quantitative method to analyze the data. The data was analyzed using partial least squares-structural equation modeling. To confirm the higher-order measurement model, the authors used SmartPLS version 4 software. Findings This study found that information processing capability (disruptive orientation and visibility in high-order) and digital SC significantly and positively affect SC risk management and resilience. Similarly, SC risk management positively mediates the relationship between information processing capability and digital SC. However, information processing capability was found to have a more substantial effect on SC risk management than the digital SC. Research limitations/implications This study has both academic and practical contributions. It contributed to existing information processing theory, and manufacturing firms can improve their performance by proactively responding to SC disruptions by recognizing the pivotal role of study variables in risk management for a resilient SC. Originality/value The conceptual model of this study is based on information processing theory, which asserts that synchronizing information processing capabilities and digital SCs allows a firm to deal with unplanned events. SC disruption orientation and visibility are considered risk controllers as they allow the firms to be more proactive. An integrated model of conceptualizing the disruption orientation, visibility (higher-order) and digital SC with information processing theory makes this research novel.",No methods found.
2024,https://openalex.org/W4392202731,Psychology,Applying large language models and chain-of-thought for automatic scoring,"This study investigates the application of large language models (LLMs), specifically GPT-3.5 and GPT-4, with Chain-of-Though (CoT) in the automatic scoring of student-written responses to science assessments. We focused on overcoming the challenges of accessibility, technical complexity, and lack of explainability that have previously limited the use of artificial intelligence-based automatic scoring tools among researchers and educators. With a testing dataset comprising six assessment tasks (three binomial and three trinomial) with 1650 student responses, we employed six prompt engineering strategies to automatically score student responses. The six strategies combined zero-shot or few-shot learning with CoT, either alone or alongside item stem and scoring rubrics, developed based on a novel approach, WRVRT (prompt writing, reviewing, validating, revising, and testing). Results indicated that few-shot (acc = 0.67) outperformed zero-shot learning (acc = 0.60), with 12.6% increase. CoT, when used without item stem and scoring rubrics, did not significantly affect scoring accuracy (acc = 0.60). However, CoT prompting paired with contextual item stems and rubrics proved to be a significant contributor to scoring accuracy (13.44% increase for zero-shot; 3.7% increase for few-shot). We found a more balanced accuracy across different proficiency categories when CoT was used with a scoring rubric, highlighting the importance of domain-specific reasoning in enhancing the effectiveness of LLMs in scoring tasks. We also found that GPT-4 demonstrated superior performance over GPT -3.5 in various scoring tasks when combined with the single-call greedy sampling or ensemble voting nucleus sampling strategy, showing 8.64% difference. Particularly, the single-call greedy sampling strategy with GPT-4 outperformed other approaches. This study also demonstrates the potential of LLMs in facilitating explainable and interpretable automatic scoring, emphasizing that CoT enhances accuracy and transparency, particularly when used with item stem and scoring rubrics.","<method>large language models (LLMs)</method>, <method>GPT-3.5</method>, <method>GPT-4</method>, <method>Chain-of-Thought (CoT)</method>, <method>zero-shot learning</method>, <method>few-shot learning</method>, <method>prompt engineering strategies</method>, <method>WRVRT (prompt writing, reviewing, validating, revising, and testing)</method>, <method>single-call greedy sampling strategy</method>, <method>ensemble voting nucleus sampling strategy</method>"
2024,https://openalex.org/W4391103530,Psychology,Transformative Breast Cancer Diagnosis using CNNs with Optimized ReduceLROnPlateau and Early Stopping Enhancements,"Abstract Breast cancer stands as a paramount public health concern worldwide, underscoring an imperative necessity within the research sphere for precision-driven and efficacious methodologies facilitating accurate detection. The existing diagnostic approaches in breast cancer often suffer from limitations in accuracy and efficiency, leading to delayed detection and subsequent challenges in personalized treatment planning. The primary focus of this research is to overcome these shortcomings by harnessing the power of advanced deep learning techniques, thereby revolutionizing the precision and reliability of breast cancer classification. This research addresses the critical need for improved breast cancer diagnostics by introducing a novel Convolutional Neural Network (CNN) model integrated with an Early Stopping callback and ReduceLROnPlateau callback. By enhancing the precision and reliability of breast cancer classification, the study aims to overcome the limitations of existing diagnostic methods, ultimately leading to better patient outcomes and reduced mortality rates. The comprehensive methodology includes diverse datasets, meticulous image preprocessing, robust model training, and validation strategies, emphasizing the model's adaptability and reliability in varied clinical contexts. The findings showcase the CNN model's exceptional performance, achieving a 95.2% accuracy rate in distinguishing cancerous and non-cancerous breast tissue in the integrated dataset, thereby demonstrating its potential for enhancing clinical decision-making and fostering the development of AI-driven diagnostic solutions.","<method>Convolutional Neural Network (CNN)</method>, <method>Early Stopping callback</method>, <method>ReduceLROnPlateau callback</method>"
2024,https://openalex.org/W4391216329,Psychology,"Student psychological well-being in higher education: The role of internal team environment, institutional, friends and family support and academic engagement","Psychological well-being of students is an area of concern in higher education institutes across the world. Although several studies have explored the factors associated with students’ psychological well-being, limited research has focused on the relation between the overall support for students and psychological well-being. Students of higher education may get formal support, in the form of team environment and institutional support; and informal support, in the form of family and friends’ support. The purpose of this study is to examine the relation of these four kinds of support with psychological well-being of management students. We also examine the intervening role of academic engagement in this relationship. Analysis using structural equation modeling and hierarchical regression on data collected from 309 management students from Indian universities, shows that positive internal team environment, and institutional and family support positively relate to students’ psychological well-being. Academic engagement partially mediates the relation between positive internal team environment and psychological well-being, and family support and psychological well-being. Also, academic engagement fully mediates the relation between institutional support and psychological well-being. The study highlights the significance of internal team environment and institutional support for students’ academic engagement and psychological well-being, and the role of academic engagement in determining well-being. Based on these findings, we suggest interventions that can be undertaken by educational institutions to enhance psychological well-being of students. Theoretical implications and research avenues are discussed.",No methods found.
2024,https://openalex.org/W4392166859,Psychology,"Towards a New Conceptual Model of AI-Enhanced Learning for College Students: The Roles of Artificial Intelligence Capabilities, General Self-Efficacy, Learning Motivation, and Critical Thinking Awareness","In the aftermath of the COVID-19 pandemic, college students have faced various challenges that could negatively impact their critical thinking abilities due to disruptions to education, increased stress and anxiety, less social interaction, and the advancement of distance learning relying more heavily on digital tools. With the increasing integration of AI technology across sectors, higher education institutions have deployed various AI capabilities for intelligent campuses and modernized teaching. However, how to fully utilize AI capabilities to promote students’ thinking awareness on learning effectiveness is still not clear, as critical thinking is an essential skill set holding significant implications for college students’ development. This research adopts the resource-based theory (RBT) to conceptualize the university as a unified entity of artificial intelligence (AI) resources. It aims to investigate whether AI capabilities can foster critical thinking awareness among students by enhancing general self-efficacy and learning motivation. In particular, it examines the causal relationships between AI capabilities, general self-efficacy, motivation and critical thinking awareness. Primary data was collected through a questionnaire administered to 637 college students. Structural equation modeling was employed to test hypotheses pertaining to causality. The results showed that AI capabilities could indirectly enhance students’ critical thinking awareness by strengthening general self-efficacy and learning motivation, but the effect on critical thinking awareness was not significant. Meanwhile, general self-efficacy significantly affected the formation of learning motivation and critical thinking awareness. This indicates that AI capabilities are able to reshape the cognitive learning process, but its direct influence on thinking awareness needs to be viewed with caution. This study explored the role of AI capabilities in education from the perspective of organizational capabilities. It not only proves how AI facilitates cognition, but also discovered the important mediating role of general self-efficacy and motivation in this process. This finding explains the inherent connections between the mechanism links. Furthermore, the study expands research on AI capabilities research from the technical level to the educational field. It provides a comprehensive and in-depth theoretical explanation theoretically, guiding the practice and application of AI in education. The study is of positive significance for understanding the need for the future development of the cultivation of critical thinking awareness talents needed for future development through AI capabilities in education.",<method>Structural equation modeling</method>
2024,https://openalex.org/W4392816775,Psychology,Virtual reality and augmented reality in medical education: an umbrella review,"Objective This umbrella review aims to ascertain the extent to which immersive Virtual Reality (VR) and Augmented Reality (AR) technologies improve specific competencies in healthcare professionals within medical education and training, in contrast to traditional educational methods or no intervention. Methods Adhering to PRISMA guidelines and the PICOS approach, a systematic literature search was conducted across major databases to identify studies examining the use of VR and AR in medical education. Eligible studies were screened and categorized based on the PICOS criteria. Descriptive statistics and chi-square tests were employed to analyze the data, supplemented by the Fisher test for small sample sizes or specific conditions. Analysis The analysis involved cross-tabulating the stages of work (Development and Testing, Results, Evaluated) and variables of interest (Performance, Engagement, Performance and Engagement, Effectiveness, no evaluated) against the types of technologies used. Chi-square tests assessed the associations between these categorical variables. Results A total of 28 studies were included, with the majority reporting increased or positive effects from the use of immersive technologies. VR was the most frequently studied technology, particularly in the “Performance” and “Results” stages. The chi-square analysis, with a Pearson value close to significance ( p = 0.052), suggested a non-significant trend toward the association of VR with improved outcomes. Conclusions The results indicate that VR is a prevalent tool in the research landscape of medical education technologies, with a positive trend toward enhancing educational outcomes. However, the statistical analysis did not reveal a significant association, suggesting the need for further research with larger sample sizes. This review underscores the potential of immersive technologies to enhance medical training yet calls for more rigorous studies to establish definitive evidence of their efficacy.",No methods found.
2024,https://openalex.org/W4394681533,Psychology,REVIEWING THE IMPACT OF HEALTH INFORMATION TECHNOLOGY ON HEALTHCARE MANAGEMENT EFFICIENCY,"This research paper explores the intricate relationship between Health Information Technology (HIT) and healthcare management efficiency, investigating current trends, emerging technologies, and their potential implications. The study encompasses a thorough literature review, highlighting the impact of HIT on operational and clinical aspects of healthcare delivery. Key findings reveal the transformative role of technology in streamlining administrative processes, improving communication, and enhancing overall patient care. Ethical considerations, patient privacy, and regulation compliance are crucial factors in successfully implementing HIT. Looking towards the future, the paper anticipates the integration of emerging technologies such as Artificial Intelligence, Blockchain, and the Internet of Things, signalling a paradigm shift in healthcare management. While acknowledging the potential benefits, the research also underscores the importance of ethical frameworks, transparency, and user-centred design in adopting these technologies. The study concludes with reflections on the limitations of the research, suggesting avenues for future exploration. Recommendations emphasize the need for ongoing research, longitudinal studies, and a global perspective to ensure healthcare organizations effectively leverage technology while maintaining ethical standards. The findings of this research carry implications for healthcare practitioners, policymakers, and technology innovators, encouraging a strategic and ethical approach to the ever-evolving landscape of health information technology.&#x0D; Keywords: Health Information Technology, Healthcare Management Efficiency, Emerging Technologies, Ethical Considerations, Patient Privacy.",<method>Artificial Intelligence</method>
2024,https://openalex.org/W4402780379,Psychology,Investigating Spatial Effects through Machine Learning and Leveraging Explainable AI for Child Malnutrition in Pakistan,"While socioeconomic gradients in regional health inequalities are firmly established, the synergistic interactions between socioeconomic deprivation and climate vulnerability within convenient proximity and neighbourhood locations with health disparities remain poorly explored and thus require deep understanding within a regional context. Furthermore, disregarding the importance of spatial spillover effects and nonlinear effects of covariates on childhood stunting are inevitable in dealing with an enduring issue of regional health inequalities. The present study aims to investigate the spatial inequalities in childhood stunting at the district level in Pakistan and validate the importance of spatial lag in predicting childhood stunting. Furthermore, it examines the presence of any nonlinear relationships among the selected independent features with childhood stunting. The study utilized data related to socioeconomic features from MICS 2017–2018 and climatic data from Integrated Contextual Analysis. A multi-model approach was employed to address the research questions, which included Ordinary Least Squares Regression (OLS), various Spatial Models, Machine Learning Algorithms and Explainable Artificial Intelligence methods. Firstly, OLS was used to analyse and test the linear relationships among selected variables. Secondly, Spatial Durbin Error Model (SDEM) was used to detect and capture the impact of spatial spillover on childhood stunting. Third, XGBoost and Random Forest machine learning algorithms were employed to examine and validate the importance of the spatial lag component. Finally, EXAI methods such as SHapley were utilized to identify potential nonlinear relationships. The study found a clear pattern of spatial clustering and geographical disparities in childhood stunting, with multidimensional poverty, high climate vulnerability and early marriage worsening childhood stunting. In contrast, low climate vulnerability, high exposure to mass media and high women’s literacy were found to reduce childhood stunting. The use of machine learning algorithms, specifically XGBoost and Random Forest, highlighted the significant role played by the average value in the neighbourhood in predicting childhood stunting in nearby districts, confirming that the spatial spillover effect is not bounded by geographical boundaries. Furthermore, EXAI methods such as partial dependency plot reveal the existence of a nonlinear relationship between multidimensional poverty and childhood stunting. The study’s findings provide valuable insights into the spatial distribution of childhood stunting in Pakistan, emphasizing the importance of considering spatial effects in predicting childhood stunting. Individual and household-level factors such as exposure to mass media and women’s literacy have shown positive implications for childhood stunting. It further provides a justification for the usage of EXAI methods to draw better insights and propose customised intervention policies accordingly.","<method>Ordinary Least Squares Regression (OLS)</method>, <method>Spatial Durbin Error Model (SDEM)</method>, <method>XGBoost</method>, <method>Random Forest</method>, <method>Explainable Artificial Intelligence (EXAI) methods</method>, <method>SHapley</method>, <method>partial dependency plot</method>"
2024,https://openalex.org/W4390659289,Psychology,Cognition-Driven Structural Prior for Instance-Dependent Label Transition Matrix Estimation,"The label transition matrix has emerged as a widely accepted method for mitigating label noise in machine learning. In recent years, numerous studies have centered on leveraging deep neural networks to estimate the label transition matrix for individual instances within the context of instance-dependent noise. However, these methods suffer from low search efficiency due to the large space of feasible solutions. Behind this drawback, we have explored that the real murderer lies in the invalid class transitions, that is, the actual transition probability between certain classes is zero but is estimated to have a certain value. To mask the <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">invalid class transitions</i> , we introduced a human-cognition-assisted method with structural information from human cognition. Specifically, we introduce a structured transition matrix network ( <bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">STMN</b> ) designed with an adversarial learning process to balance instance features and prior information from human cognition. The proposed method offers two advantages: 1) better estimation effectiveness is obtained by sparing the transition matrix and 2) better estimation accuracy is obtained with the assistance of human cognition. By exploiting these two advantages, our method parametrically estimates a sparse label transition matrix, effectively converting noisy labels into true labels. The efficiency and superiority of our proposed method are substantiated through comprehensive comparisons with state-of-the-art methods on three synthetic datasets and a real-world dataset. Our code will be available at https://github.com/WheatCao/STMN-Pytorch.","<method>label transition matrix</method>, <method>deep neural networks</method>, <method>human-cognition-assisted method</method>, <method>structured transition matrix network (STMN)</method>, <method>adversarial learning process</method>"
2024,https://openalex.org/W4391107516,Psychology,Multiple Classification of Brain MRI Autism Spectrum Disorder by Age and Gender Using Deep Learning,"Abstract The fact that the rapid and definitive diagnosis of autism cannot be made today and that autism cannot be treated provides an impetus to look into novel technological solutions. To contribute to the resolution of this problem through multiple classifications by considering age and gender factors, in this study, two quadruple and one octal classifications were performed using a deep learning (DL) approach. Gender in one of the four classifications and age groups in the other were considered. In the octal classification, classes were created considering gender and age groups. In addition to the diagnosis of ASD (Autism Spectrum Disorders), another goal of this study is to find out the contribution of gender and age factors to the diagnosis of ASD by making multiple classifications based on age and gender for the first time. Brain structural MRI (sMRI) scans of participators with ASD and TD (Typical Development) were pre-processed in the system originally designed for this purpose. Using the Canny Edge Detection (CED) algorithm, the sMRI image data was cropped in the data pre-processing stage, and the data set was enlarged five times with the data augmentation (DA) techniques. The most optimal convolutional neural network (CNN) models were developed using the grid search optimization (GSO) algorism. The proposed DL prediction system was tested with the five-fold cross-validation technique. Three CNN models were designed to be used in the system. The first of these models is the quadruple classification model created by taking gender into account (model 1), the second is the quadruple classification model created by taking into account age (model 2), and the third is the eightfold classification model created by taking into account both gender and age (model 3). ). The accuracy rates obtained for all three designed models are 80.94, 85.42 and 67.94, respectively. These obtained accuracy rates were compared with pre-trained models by using the transfer learning approach. As a result, it was revealed that age and gender factors were effective in the diagnosis of ASD with the system developed for ASD multiple classifications, and higher accuracy rates were achieved compared to pre-trained models.","<method>deep learning (DL) approach</method>, <method>Canny Edge Detection (CED) algorithm</method>, <method>data augmentation (DA) techniques</method>, <method>convolutional neural network (CNN) models</method>, <method>grid search optimization (GSO) algorithm</method>, <method>five-fold cross-validation technique</method>, <method>transfer learning approach</method>"
2024,https://openalex.org/W4400461591,Psychology,Counterfactual Explanations and Algorithmic Recourses for Machine Learning: A Review,"Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine learning based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this article, we seek to review and categorize research on counterfactual explanations , a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.","<method>counterfactual explanations</method>, <method>counterfactual explanation algorithms</method>"
2024,https://openalex.org/W4390629445,Psychology,Investigating pre-service teachers’ artificial intelligence perception from the perspective of planned behavior theory,"There is a need for teachers who are prepared to teach Artificial Intelligence (AI) across the K-12 learning contexts. Owing to the dearth of teacher education programmes on AI, it is helpful to explore factors to be considered in designing an effective AI programme for future teachers. We posit that understanding how to encourage pre-service teachers to learn AI is thus critical for practitioners and policymakers while designing effective instructional AI teacher education programmes. This exploratory study examined the perceptions of pre-service teachers and their behavioral intention to learn AI, by identifying factors that might affect learning and promoting AI in teacher preparation programmes. This study proposed a research model supported by the theory of planned behavior and expanded with other constructs. The factors that were examined include basic knowledge of AI, subjective norm, AI for social good, perceived self-efficacy, self-transcendent goals, personal relevance, AI anxiety, behavioral intention to learn AI, and actual learning of AI. Using a duly validated questionnaire, we surveyed 796 pre-service teachers in Nigerian Universities. Through structural equation modeling approach analyses, our proposed model explains about 79% of the variance in pre-service teachers' intention to learn AI. Basic knowledge and subjective norm were found to be the most important determinant in pre-service teachers' intention to learn AI. All our hypotheses were supported except for self-efficacy and personal relevance, personal relevance and social good, and behavioral intention and actual learning behavior. The findings provide practitioners, researchers, and policymakers with valuable information to consider in designing effective AI teacher education programmes.",<method>structural equation modeling</method>
2024,https://openalex.org/W4391531220,Psychology,An Explainable AI Paradigm for Alzheimer’s Diagnosis Using Deep Transfer Learning,"Alzheimer’s disease (AD) is a progressive neurodegenerative disorder that affects millions of individuals worldwide, causing severe cognitive decline and memory impairment. The early and accurate diagnosis of AD is crucial for effective intervention and disease management. In recent years, deep learning techniques have shown promising results in medical image analysis, including AD diagnosis from neuroimaging data. However, the lack of interpretability in deep learning models hinders their adoption in clinical settings, where explainability is essential for gaining trust and acceptance from healthcare professionals. In this study, we propose an explainable AI (XAI)-based approach for the diagnosis of Alzheimer’s disease, leveraging the power of deep transfer learning and ensemble modeling. The proposed framework aims to enhance the interpretability of deep learning models by incorporating XAI techniques, allowing clinicians to understand the decision-making process and providing valuable insights into disease diagnosis. By leveraging popular pre-trained convolutional neural networks (CNNs) such as VGG16, VGG19, DenseNet169, and DenseNet201, we conducted extensive experiments to evaluate their individual performances on a comprehensive dataset. The proposed ensembles, Ensemble-1 (VGG16 and VGG19) and Ensemble-2 (DenseNet169 and DenseNet201), demonstrated superior accuracy, precision, recall, and F1 scores compared to individual models, reaching up to 95%. In order to enhance interpretability and transparency in Alzheimer’s diagnosis, we introduced a novel model achieving an impressive accuracy of 96%. This model incorporates explainable AI techniques, including saliency maps and grad-CAM (gradient-weighted class activation mapping). The integration of these techniques not only contributes to the model’s exceptional accuracy but also provides clinicians and researchers with visual insights into the neural regions influencing the diagnosis. Our findings showcase the potential of combining deep transfer learning with explainable AI in the realm of Alzheimer’s disease diagnosis, paving the way for more interpretable and clinically relevant AI models in healthcare.","<method>deep learning</method>, <method>explainable AI (XAI)</method>, <method>deep transfer learning</method>, <method>ensemble modeling</method>, <method>pre-trained convolutional neural networks (CNNs)</method>, <method>VGG16</method>, <method>VGG19</method>, <method>DenseNet169</method>, <method>DenseNet201</method>, <method>Ensemble-1 (VGG16 and VGG19)</method>, <method>Ensemble-2 (DenseNet169 and DenseNet201)</method>, <method>saliency maps</method>, <method>grad-CAM (gradient-weighted class activation mapping)</method>"
2024,https://openalex.org/W4394785902,Psychology,"Evidence-based potential of generative artificial intelligence large language models in orthodontics: a comparative study of ChatGPT, Google Bard, and Microsoft Bing","Summary Background The increasing utilization of large language models (LLMs) in Generative Artificial Intelligence across various medical and dental fields, and specifically orthodontics, raises questions about their accuracy. Objective This study aimed to assess and compare the answers offered by four LLMs: Google’s Bard, OpenAI’s ChatGPT-3.5, and ChatGPT-4, and Microsoft’s Bing, in response to clinically relevant questions within the field of orthodontics. Materials and methods Ten open-type clinical orthodontics-related questions were posed to the LLMs. The responses provided by the LLMs were assessed on a scale ranging from 0 (minimum) to 10 (maximum) points, benchmarked against robust scientific evidence, including consensus statements and systematic reviews, using a predefined rubric. After a 4-week interval from the initial evaluation, the answers were reevaluated to gauge intra-evaluator reliability. Statistical comparisons were conducted on the scores using Friedman’s and Wilcoxon’s tests to identify the model providing the answers with the most comprehensiveness, scientific accuracy, clarity, and relevance. Results Overall, no statistically significant differences between the scores given by the two evaluators, on both scoring occasions, were detected, so an average score for every LLM was computed. The LLM answers scoring the highest, were those of Microsoft Bing Chat (average score = 7.1), followed by ChatGPT 4 (average score = 4.7), Google Bard (average score = 4.6), and finally ChatGPT 3.5 (average score 3.8). While Microsoft Bing Chat statistically outperformed ChatGPT-3.5 (P-value = 0.017) and Google Bard (P-value = 0.029), as well, and Chat GPT-4 outperformed Chat GPT-3.5 (P-value = 0.011), all models occasionally produced answers with a lack of comprehensiveness, scientific accuracy, clarity, and relevance. Limitations The questions asked were indicative and did not cover the entire field of orthodontics. Conclusions Language models (LLMs) show great potential in supporting evidence-based orthodontics. However, their current limitations pose a potential risk of making incorrect healthcare decisions if utilized without careful consideration. Consequently, these tools cannot serve as a substitute for the orthodontist’s essential critical thinking and comprehensive subject knowledge. For effective integration into practice, further research, clinical validation, and enhancements to the models are essential. Clinicians must be mindful of the limitations of LLMs, as their imprudent utilization could have adverse effects on patient care.",<method>large language models (LLMs)</method>
2024,https://openalex.org/W4402827393,Psychology,Larger and more instructable language models become less reliable,"Abstract The prevailing methods to make large language models more powerful and amenable have been based on continuous scaling up (that is, increasing their size, data volume and computational resources 1 ) and bespoke shaping up (including post-filtering 2,3 , fine tuning or use of human feedback 4,5 ). However, larger and more instructable large language models may have become less reliable. By studying the relationship between difficulty concordance, task avoidance and prompting stability of several language model families, here we show that easy instances for human participants are also easy for the models, but scaled-up, shaped-up models do not secure areas of low difficulty in which either the model does not err or human supervision can spot the errors. We also find that early models often avoid user questions but scaled-up, shaped-up models tend to give an apparently sensible yet wrong answer much more often, including errors on difficult questions that human supervisors frequently overlook. Moreover, we observe that stability to different natural phrasings of the same question is improved by scaling-up and shaping-up interventions, but pockets of variability persist across difficulty levels. These findings highlight the need for a fundamental shift in the design and development of general-purpose artificial intelligence, particularly in high-stakes areas for which a predictable distribution of errors is paramount.","<method>continuous scaling up</method>, <method>post-filtering</method>, <method>fine tuning</method>, <method>use of human feedback</method>"
2024,https://openalex.org/W624313879,Psychology,Economic Structure and Maturity: Collected Papers in Input-output Modelling and Applications,"Introduction - economic structure and maturity. Part 1 Some theoretical aspects of economic stability: some conditions of macroeconomic stability of multiregional input-output model comparative stability analysis of multiregional input-output models - column, row and Lontief-Strout gravity coefficient models supply-side multiregional input-output models qualitative input-output analysis. Part 2 Empirical investigations into economic stability and the emerging problem of economic maturity: comparative stability analysis of demand-side and supply-side input-output models comparative stability analysis of demand-side and supply-side input-output models in the UK comparative stability analysis of demand-side and supply-side input-output models - the case of Jap, 1960-90 comparative stability analysis of demand-side and supply-side input-output models - toward an index of economic maturity coparative stability analysis of demand-side and supply-side input-output models - the case of Turkey, 1973-90. Part 3 Empirical studies of construction activity and economic maturity: the role of construction in the nationl economy - a comparison of the fundamental strucutre of the US and Japanese input-output tables since World War II direct and indirect resource utilization by the construction sector - the case of the US since World War II historical comparison of construction sectors in the US, Japan, Italy and Finland using input-output tables what do we mean by building technology? the future of international construction - secular patterns of growth and decline new construction versus maintenance and repair construction technology in the US since World War II an input-output analysis of the Italian construction sector, 1959-88 some new evidence of old trends - Japanese construction 1960-90 the Italian residential construction sector - an input-output analysis, 1959-92 an input-output analysis of the Turkish construction sector, 1973-90.",No methods found.
2024,https://openalex.org/W4391126287,Psychology,Evaluating the ChatGPT family of models for biomedical reasoning and classification,"Abstract Objective Large language models (LLMs) have shown impressive ability in biomedical question-answering, but have not been adequately investigated for more specific biomedical applications. This study investigates ChatGPT family of models (GPT-3.5, GPT-4) in biomedical tasks beyond question-answering. Materials and Methods We evaluated model performance with 11 122 samples for two fundamental tasks in the biomedical domain—classification (n = 8676) and reasoning (n = 2446). The first task involves classifying health advice in scientific literature, while the second task is detecting causal relations in biomedical literature. We used 20% of the dataset for prompt development, including zero- and few-shot settings with and without chain-of-thought (CoT). We then evaluated the best prompts from each setting on the remaining dataset, comparing them to models using simple features (BoW with logistic regression) and fine-tuned BioBERT models. Results Fine-tuning BioBERT produced the best classification (F1: 0.800-0.902) and reasoning (F1: 0.851) results. Among LLM approaches, few-shot CoT achieved the best classification (F1: 0.671-0.770) and reasoning (F1: 0.682) results, comparable to the BoW model (F1: 0.602-0.753 and 0.675 for classification and reasoning, respectively). It took 78 h to obtain the best LLM results, compared to 0.078 and 0.008 h for the top-performing BioBERT and BoW models, respectively. Discussion The simple BoW model performed similarly to the most complex LLM prompting. Prompt engineering required significant investment. Conclusion Despite the excitement around viral ChatGPT, fine-tuning for two fundamental biomedical natural language processing tasks remained the best strategy.","<method>ChatGPT family of models (GPT-3.5, GPT-4)</method>, <method>zero-shot prompting</method>, <method>few-shot prompting</method>, <method>chain-of-thought (CoT) prompting</method>, <method>Bag of Words (BoW) with logistic regression</method>, <method>fine-tuned BioBERT models</method>"
2024,https://openalex.org/W4391609543,Psychology,Does AI-Driven Technostress Promote or Hinder Employees’ Artificial Intelligence Adoption Intention? A Moderated Mediation Model of Affective Reactions and Technical Self-Efficacy,"Purpose: The increasing integration of Artificial Intelligence (AI) within enterprises is generates significant technostress among employees, potentially influencing their intention to adopt AI. However, existing research on the psychological effects of this phenomenon remains inconclusive. Drawing on the Affective Events Theory (AET) and the Challenge–Hindrance Stressor Framework (CHSF), the current study aims to explore the “black box” between challenge and hindrance technology stressors and employees’ intention to adopt AI, as well as the boundary conditions of this mediation relationship. Methods: The study employs a quantitative approach and utilizes three-wave data. Data were collected through the snowball sampling technique and a structured questionnaire survey. The sample comprises employees from 11 distinct organizations located in Guangdong Province, China. We received 301 valid questionnaires, representing an overall response rate of 75%. The theoretical model was tested through confirmatory factor analysis and regression analyses using Mplus and the Process macro for SPSS. Results: The results indicate that positive affect mediates the positive relationship between challenge technology stressors and AI adoption intention, whereas AI anxiety mediates the negative relationship between hindrance technology stressors and AI adoption intention. Furthermore, the results reveal that technical self-efficacy moderates the effects of challenge and hindrance technology stressors on affective reactions and the indirect effects of challenge and hindrance technology stressors on AI adoption intention through positive affect and AI anxiety, respectively. Conclusion: Overall, our study suggests that AI-driven challenge technology stressors positively impact AI adoption intention through the cultivation of positive affect, while hindrance technology stressors impede AI adoption intention by triggering AI anxiety. Additionally, technical self-efficacy emerges as a crucial moderator in shaping these relationships. This research has the potential to make a meaningful contribution to the literature on AI adoption intention, deepening our holistic understanding of the influential mechanisms involved. Furthermore, the study affirms the applicability and relevance of Affective Events Theory (AET) and the Challenge-Hindrance Stressor Framework (CHSF). In practical terms, the research provides actionable insights for organizations to effectively manage employees’ AI adoption intention. Keywords: challenge and hindrance technology stressors, AI adoption intention, positive affect, AI anxiety, technical self-efficacy",No methods found.
2024,https://openalex.org/W4392285688,Psychology,Machine Learning and Digital Biomarkers Can Detect Early Stages of Neurodegenerative Diseases,"Neurodegenerative diseases (NDs) such as Alzheimer’s Disease (AD) and Parkinson’s Disease (PD) are devastating conditions that can develop without noticeable symptoms, causing irreversible damage to neurons before any signs become clinically evident. NDs are a major cause of disability and mortality worldwide. Currently, there are no cures or treatments to halt their progression. Therefore, the development of early detection methods is urgently needed to delay neuronal loss as soon as possible. Despite advancements in Medtech, the early diagnosis of NDs remains a challenge at the intersection of medical, IT, and regulatory fields. Thus, this review explores “digital biomarkers” (tools designed for remote neurocognitive data collection and AI analysis) as a potential solution. The review summarizes that recent studies combining AI with digital biomarkers suggest the possibility of identifying pre-symptomatic indicators of NDs. For instance, research utilizing convolutional neural networks for eye tracking has achieved significant diagnostic accuracies. ROC-AUC scores reached up to 0.88, indicating high model performance in differentiating between PD patients and healthy controls. Similarly, advancements in facial expression analysis through tools have demonstrated significant potential in detecting emotional changes in ND patients, with some models reaching an accuracy of 0.89 and a precision of 0.85. This review follows a structured approach to article selection, starting with a comprehensive database search and culminating in a rigorous quality assessment and meaning for NDs of the different methods. The process is visualized in 10 tables with 54 parameters describing different approaches and their consequences for understanding various mechanisms in ND changes. However, these methods also face challenges related to data accuracy and privacy concerns. To address these issues, this review proposes strategies that emphasize the need for rigorous validation and rapid integration into clinical practice. Such integration could transform ND diagnostics, making early detection tools more cost-effective and globally accessible. In conclusion, this review underscores the urgent need to incorporate validated digital health tools into mainstream medical practice. This integration could indicate a new era in the early diagnosis of neurodegenerative diseases, potentially altering the trajectory of these conditions for millions worldwide. Thus, by highlighting specific and statistically significant findings, this review demonstrates the current progress in this field and the potential impact of these advancements on the global management of NDs.",<method>convolutional neural networks</method>
2024,https://openalex.org/W4392855331,Psychology,The Making of the “Good Bad” Job: How Algorithmic Management Manufactures Consent Through Constant and Confined Choices,"This research explores how a new relation of production—the shift from human managers to algorithmic managers on digital platforms—manufactures workplace consent. While most research has argued that the task standardization and surveillance that accompany algorithmic management will give rise to the quintessential “bad job” (Kalleberg, Reskin, and Hudson, 2000; Kalleberg, 2011), I find that, surprisingly, many workers report liking and finding choice while working under algorithmic management. Drawing on a seven-year qualitative study of the largest sector in the gig economy, the ride-hailing industry, I describe how workers navigate being managed by an algorithm. I begin by showing how algorithms segment the work at multiple sites of human–algorithm interactions and how this configuration of the work process allows for more-frequent and narrow choice. I find that workers use two sets of tactics. In engagement tactics, individuals generally follow the algorithmic nudges and do not try to get around the system; in deviance tactics, individuals manipulate their input into the algorithmic management system. While the behaviors associated with these tactics are practical opposites, they both elicit consent, or active, enthusiastic participation by workers to align their efforts with managerial interests, and both contribute to workers seeing themselves as skillful agents. However, this choice-based consent can mask the more-structurally problematic elements of the work, contributing to the growing popularity of what I call the “good bad” job.",No methods found.
2024,https://openalex.org/W4390569131,Psychology,Rapport with a chatbot? The underlying role of anthropomorphism in socio-cognitive perceptions of rapport and e-word of mouth,"This study examines the impact of rapport with chatbots on electronic word of mouth (e-WOM), in the first phase, by considering several antecedents including anthropomorphism. In the second phase, deeper insights are provided into the moderated mediation role of rapport and the moderated moderation effect of value creation and hedonic motivation on e-WOM engagement. With tourism services as the research context, a survey was conducted among 257 visitors from three countries (China, India and New Zealand), selected due to their diverse cultural backgrounds and higher number of inbound visitors to Australia. The partial least squares method was used for data analysis along with multi-group analysis. Findings report the positive role of anthropomorphism in developing rapport with chatbots in digital interactions. Interestingly, rapport had the highest moderated mediation impact in the data from China followed by the data from India. The moderated moderation impact of hedonic motivation was only significant in the data from China, whereas value creation was a significant moderator in the data from both China and New Zealand. The study extends social exchange theory in a human–chatbot or artificial intelligence (AI) interaction context with cultural implications. The findings are useful for organizations relying on customer rapport with AI-based chatbots to ensure long-term customer service through digital interactions.","<method>partial least squares method</method>, <method>multi-group analysis</method>"
2024,https://openalex.org/W4390951664,Psychology,Economics students’ behavioural intention and usage of ChatGPT in higher education: a hybrid structural equation modelling-artificial neural network approach,"The Chat Generative Pre-Trained Transformer, popularly referred to as ChatGPT, is an AI-based technology with the potential to revolutionise conventional teaching and learning in higher education institutions (HEIs). However, it remains unclear which factors influence the behavioural intentions and the actual usage of ChatGPT among economics students in Ghanaian HEIs. In pursuit of this goal, we employed the extended Unified Theory of Acceptance and Use of Technology (UTAUT2) to gain a better understanding of the antecedents influencing the behavioural intentions and actual usage of ChatGPT among economics students. The study surveyed 306 Ghanaian students enrolled in economics at a public university. These students were aware of the existence of ChatGPT applications. We applied a hybrid analytical approach, combining structural equation modelling and artificial neural network (SEM-ANN), to elucidate the causal relationships between variables believed to impact perceived trust, intentions, and actual usage. The results showed that design and interactivity have a significant impact on perceived trust. Similarly, perceived trust, social influence, performance expectancy, hedonic motivation, and habits drive behavioural intentions. Among the various factors influencing behavioural intentions, hedonic motivation emerged as the most dominant. Moreover, behavioural intentions and facilitating conditions significantly drive students' actual use of the ChatGPT. Nevertheless, ethics is not a significant factor in perceived trust, and effort expectancy does not affect behavioral intention. These findings, however, offer theoretical and practical contributions that can serve as guide for a thoughtful and responsible integration of AI-based tools as a future strategy to enhance education accessibility and inclusivity opportunities","<method>structural equation modelling</method>, <method>artificial neural network (SEM-ANN)</method>"
2024,https://openalex.org/W4390917192,Psychology,Exploring the influence of ChatGPT on tourism behavior using the technology acceptance model,"Purpose The present study’s aims are twofold: 1) to contribute to theory development by accounting for both personality and trust in the conceptualization of technology acceptance using the technology acceptance model (TAM) as the theoretical framework; and 2) to explore the influence of ChatGPT-integrated chatbots on tourism behavior. Design/methodology/approach The target population for this study was travelers who previously used technology (website/ app) to plan their holiday abroad. An online survey questionnaire created with Google Forms was distributed via a panel company (iPanel). A screening question was included to filter out respondents who have not previously used technological means to plan their holiday abroad. A panel company (iPanel) was hired to collect data from a convenience sample of 305 Israeli tourists who met the above criterion between August 22 and 27, 2023, and were at least 18. Findings A significant and positive relationship was observed between trust in ChaptGPT and perceived usefulness. Furthermore, a significant and positive association was observed between perceived ease of use and intentions to use ChatGPT-integrated chatbots to plan future holidays. Post hoc analyses suggest that perceived ease of use mediates the relationship between extraversion and trust, trust mediates the relationship between perceived ease of use and perceived usefulness and age moderates the relationship between perceived ease of use and behavioral intentions. Research limitations/implications Data was collected from a convenience sample of Israeli travelers. Hence, generalizations to other countries, nationalities and cultures should be treated carefully; the study is cross-sectional and thus represents respondents’ beliefs and behavioral intentions at a particular time; and the study is based on one of several theoretical frameworks that can be used to conceptualize behaviors associated with using AI by tourists. Practical implications The findings of the present study point to the importance of accounting for tourists’ personal factors, such as personality and age, in developing AI products in the tourism industry. chief executive officers and relevant shareholders would benefit from conducting market research to obtain insights into the factors that may enhance or hamper tourists’ adoption of AI-based technology for planning their holidays abroad. Originality/value Previous work falls short of accounting for personality traits and trust in a single model using the TAM framework. To the best of the authors’ knowledge, this is the first study empirically investigating tourism behavior related to ChatGPT based chatbots as a tool to plan future holidays abroad. Furthermore, the possible role of age as a moderating variable was overlooked in past research.",No methods found.
2024,https://openalex.org/W4390987311,Psychology,Reliability of ChatGPT for performing triage task in the emergency department using the Korean Triage and Acuity Scale,"Background Artificial intelligence (AI) technology can enable more efficient decision-making in healthcare settings. There is a growing interest in improving the speed and accuracy of AI systems in providing responses for given tasks in healthcare settings. Objective This study aimed to assess the reliability of ChatGPT in determining emergency department (ED) triage accuracy using the Korean Triage and Acuity Scale (KTAS). Methods Two hundred and two virtual patient cases were built. The gold standard triage classification for each case was established by an experienced ED physician. Three other human raters (ED paramedics) were involved and rated the virtual cases individually. The virtual cases were also rated by two different versions of the chat generative pre-trained transformer (ChatGPT, 3.5 and 4.0). Inter-rater reliability was examined using Fleiss’ kappa and intra-class correlation coefficient (ICC). Results The kappa values for the agreement between the four human raters and ChatGPTs were .523 (version 4.0) and .320 (version 3.5). Of the five levels, the performance was poor when rating patients at levels 1 and 5, as well as case scenarios with additional text descriptions. There were differences in the accuracy of the different versions of GPTs. The ICC between version 3.5 and the gold standard was .520, and that between version 4.0 and the gold standard was .802. Conclusions A substantial level of inter-rater reliability was revealed when GPTs were used as KTAS raters. The current study showed the potential of using GPT in emergency healthcare settings. Considering the shortage of experienced manpower, this AI method may help improve triaging accuracy.","<method>chat generative pre-trained transformer (ChatGPT, 3.5 and 4.0)</method>"
2024,https://openalex.org/W4391573023,Psychology,Deep Reinforcement Learning Unleashing the Power of AI in Decision-Making,"Deep Reinforcement Learning (DRL) has emerged as a transformative paradigm in the field of artificial intelligence (AI), offering unprecedented capabilities in decision-making across diverse domains. This article explores the profound impact of DRL on enhancing the decision-making capabilities of AI systems, elucidating its underlying principles, applications, and implications.DRL represents a fusion of deep learning and reinforcement learning, enabling machines to learn complex behaviors and make decisions by interacting with their environment. The utilization of neural networks allows DRL algorithms to handle high-dimensional input spaces, making it well-suited for tasks that involve intricate decision-making processes.One of the key strengths of DRL lies in its ability to address problems with sparse and delayed rewards, common challenges in traditional reinforcement learning. Through a process of trial and error, DRL algorithms can learn optimal decision strategies by navigating through a vast decision space, adapting to dynamic environments, and maximizing cumulative rewards over time.The applications of DRL span various domains, including robotics, finance, healthcare, gaming, and autonomous systems. In robotics, DRL facilitates the development of intelligent agents capable of autonomously navigating complex environments, performing intricate tasks, and adapting to unforeseen circumstances. In finance, DRL is leveraged for portfolio optimization, algorithmic trading, and risk management, demonstrating its potential to revolutionize traditional financial strategies.","<method>Deep Reinforcement Learning (DRL)</method>, <method>deep learning</method>, <method>reinforcement learning</method>"
2024,https://openalex.org/W4396712983,Psychology,3WC-GBNRS++: A novel three-way classifier with granular-ball neighborhood rough sets based on uncertainty,"Three-way decision with neighborhood rough sets (3WDNRS) is adept at addressing uncertain problems involving continuous data by configuring the neighborhood radius. However, on one hand, the inputs of 3WDNRS are individual neighborhood granules, which reduce the decision efficiency and generality; on other hand, the thresholds of 3WDNRS require prior knowledge to be approximately set in advance, making it difficult to apply in cases where such knowledge is unavailable. To address these issues, we introduce granular-ball computing (GBC) into 3WDNRS from the perspective of uncertainty. Firstly, we propose an enhanced granular-ball generation method based on DBSCAN called DBGBC. Subsequently, we present an improved granular-ball neighborhood rough sets model (GBNRS++) by combining DBGBC with a quality index. Furthermore, we construct a three-way classifier with granular-ball neighborhood rough sets (3WC-GBNRS++) based on the principle of minimum fuzziness loss. This approach provides an objective and efficient way to determine the thresholds. To further enhance classification accuracy, we design an adaptive granular-ball neighborhood within the subsequent classification process of 3WC-GBNRS++. Finally, experimental results demonstrate that, 3WC-GBNRS++ almost outperformed other comparison methods in terms of effectiveness and robustness, including 4 state-of-the-art granular-balls-based classifiers and 5 classical machine learning classifiers on 12 public benchmark datasets. Moreover, we discuss the limitations of our work and the outlook for future research.","<method>three-way decision with neighborhood rough sets (3WDNRS)</method>, <method>granular-ball computing (GBC)</method>, <method>DBSCAN</method>, <method>granular-ball neighborhood rough sets model (GBNRS++)</method>, <method>three-way classifier with granular-ball neighborhood rough sets (3WC-GBNRS++)</method>, <method>adaptive granular-ball neighborhood</method>"
2024,https://openalex.org/W4396723505,Psychology,MentaLLaMA: Interpretable Mental Health Analysis on Social Media with Large Language Models,"As an integral part of people's daily lives, social media is becoming a rich source for automatic mental health analysis.As traditional discriminative methods bear poor generalization ability and low interpretability, the recent large language models (LLMs) have been explored for interpretable mental health analysis on social media, which aims to provide detailed explanations along with predictions in zero-shot or few-shot settings.The results show that LLMs still achieve unsatisfactory classification performance in a zero-shot/few-shot manner, which further significantly affects the quality of the generated explanations.Domain-specific finetuning is an effective solution, but faces two critical challenges: 1) lack of high-quality training data.2) no open-source foundation LLMs.To alleviate these problems, we formally model interpretable mental health analysis as a text generation task, and build the first multi-task and multi-source interpretable mental health instruction (IMHI) dataset with 105K data samples to support LLM instruction tuning and evaluation.The raw social media data are collected from 10 existing sources covering 8 mental health analysis tasks.We prompt ChatGPT with expert-designed few-shot prompts to obtain explanations.To ensure the reliability of the explanations, we perform strict automatic and human evaluations on the correctness, consistency, and quality of generated data.Based on the IMHI dataset and LLaMA2 foundation models, we train MentaLLaMA, the first open-source instruction-following LLM series for interpretable mental health analysis on social media.We evaluate Men-taLLaMA and other advanced methods on the IMHI benchmark, the first holistic evaluation benchmark for interpretable mental health analysis.The results show that MentaLLaMA approaches state-of-the-art discriminative methods in correctness and generates human-level explanations.MentaLLaMA models also show strong generalizability to unseen tasks.The project is available at https://github.com/SteveKGYang/MentaLLaMA.","<method>large language models (LLMs)</method>, <method>zero-shot learning</method>, <method>few-shot learning</method>, <method>domain-specific finetuning</method>, <method>instruction tuning</method>, <method>instruction-following large language models</method>"
2024,https://openalex.org/W4400015499,Psychology,Building entrepreneurial resilience during crisis using generative AI: An empirical study on SMEs,"Recently, Gen AI has garnered significant attention across various sectors of society, particularly capturing the interest of small business due to its capacity to allow them to reassess their business models with minimal investment. To understand how small and medium-sized firms have utilised Gen AI-based tools to cope with the market's high level of turbulence caused by the COVID-19 pandemic, geopolitical crises, and economic slowdown, researchers have conducted an empirical study. Although Gen AI is receiving more attention, there remains a dearth of empirical studies that investigate how it influences the entrepreneurial orientation of firms and their ability to cultivate entrepreneurial resilience amidst market turbulence. Most of the literature offers anecdotal evidence. To address this research gap, the authors have grounded their theoretical model and research hypotheses in the contingent view of dynamic capability. They tested the research hypotheses using cross-sectional data from a pre-tested survey instrument, which yielded 87 useable responses from small and medium enterprises in France. The authors used variance-based structural equation modelling with the commercial WarpPLS 7.0 software to test the theoretical model. The study's findings suggest that Gen AI and EO have a significant influence on building entrepreneurial resilience as higher-order and lower-order dynamic capabilities. However, market turbulence has a negative moderating effect on the path that joins entrepreneurial orientation and entrepreneurial resilience. The results suggest that the assumption that high market turbulence will have positive effects on dynamic capabilities and competitive advantage is not always true, and the linear assumption does not hold, which is consistent with some scholars' assumptions. The study's results offer significant contributions to the contingent view of dynamic capabilities and open new research avenues that require further investigation into the non-linear relationship of market turbulence.",<method>variance-based structural equation modelling</method>
2024,https://openalex.org/W4397026358,Psychology,Automated Classification of Cognitive Visual Objects Using Multivariate Swarm Sparse Decomposition From Multichannel EEG-MEG Signals,"In visual object decoding, magnetoencephalogram (MEG) and electroencephalogram (EEG) activation patterns demonstrate the utmost discriminative cognitive analysis due to their multivariate oscillatory nature. However, high noise in the recorded EEG-MEG signals and subject-specific variability make it extremely difficult to classify subject's cognitive responses to different visual stimuli. The proposed method is a multivariate extension of the swarm sparse decomposition method (MSSDM) for multivariate pattern analysis of EEG-MEG-based visual activation signals. In comparison, it is an advanced technique for decomposing nonstationary multicomponent signals into a finite number of channel-aligned oscillatory components that significantly enhance visual activation-related sub-bands. The MSSDM method adopts multivariate swarm filtering and sparse spectrum to automatically deliver optimal frequency bands in channel-specific sparse spectrums, resulting in improved filter banks. By combining the advantages of the multivariate SSDM and Riemann's correlation-assisted fusion feature (RCFF), the MSSDM-RCFF algorithm is investigated to improve the visual object recognition ability of EEG-MEG signals. We have also proposed time–frequency representation based on MSSDM to analyze discriminative cognitive patterns of different visual object classes from multichannel EEG-MEG signals. A proposed MSSDM is evaluated on multivariate synthetic signals and multivariate EEG-MEG signals using five classifiers. The proposed fusion feature and linear discriminant analysis classifier-based framework outperformed all existing state-of-the-art methods used for visual object detection and achieved the highest accuracy of 86.42% using tenfold cross-validation on EEG-MEG multichannel signals.","<method>swarm sparse decomposition method (MSSDM)</method>, <method>multivariate swarm filtering</method>, <method>sparse spectrum</method>, <method>Riemann's correlation-assisted fusion feature (RCFF)</method>, <method>MSSDM-RCFF algorithm</method>, <method>time–frequency representation based on MSSDM</method>, <method>linear discriminant analysis classifier</method>"
2024,https://openalex.org/W4391345489,Psychology,CLARUS: An interactive explainable AI platform for manual counterfactuals in graph neural networks,"Lack of trust in artificial intelligence (AI) models in medicine is still the key blockage for the use of AI in clinical decision support systems (CDSS). Although AI models are already performing excellently in systems medicine, their black-box nature entails that patient-specific decisions are incomprehensible for the physician. Explainable AI (XAI) algorithms aim to ""explain"" to a human domain expert, which input features influenced a specific recommendation. However, in the clinical domain, these explanations must lead to some degree of causal understanding by a clinician. We developed the CLARUS platform, aiming to promote human understanding of graph neural network (GNN) predictions. CLARUS enables the visualisation of patient-specific networks, as well as, relevance values for genes and interactions, computed by XAI methods, such as GNNExplainer. This enables domain experts to gain deeper insights into the network and more importantly, the expert can interactively alter the patient-specific network based on the acquired understanding and initiate re-prediction or retraining. This interactivity allows us to ask manual counterfactual questions and analyse the effects on the GNN prediction. We present the first interactive XAI platform prototype, CLARUS, that allows not only the evaluation of specific human counterfactual questions based on user-defined alterations of patient networks and a re-prediction of the clinical outcome but also a retraining of the entire GNN after changing the underlying graph structures. The platform is currently hosted by the GWDG on https://rshiny.gwdg.de/apps/clarus/.","<method>graph neural network (GNN)</method>, <method>Explainable AI (XAI) algorithms</method>, <method>GNNExplainer</method>"
2024,https://openalex.org/W4394580101,Psychology,A systematic review and multivariate meta-analysis of the physical and mental health benefits of touch interventions,"Receiving touch is of critical importance, as many studies have shown that touch promotes mental and physical well-being. We conducted a pre-registered (PROSPERO: CRD42022304281) systematic review and multilevel meta-analysis encompassing 137 studies in the meta-analysis and 75 additional studies in the systematic review (n = 12,966 individuals, search via Google Scholar, PubMed and Web of Science until 1 October 2022) to identify critical factors moderating touch intervention efficacy. Included studies always featured a touch versus no touch control intervention with diverse health outcomes as dependent variables. Risk of bias was assessed via small study, randomization, sequencing, performance and attrition bias. Touch interventions were especially effective in regulating cortisol levels (Hedges' g = 0.78, 95% confidence interval (CI) 0.24 to 1.31) and increasing weight (0.65, 95% CI 0.37 to 0.94) in newborns as well as in reducing pain (0.69, 95% CI 0.48 to 0.89), feelings of depression (0.59, 95% CI 0.40 to 0.78) and state (0.64, 95% CI 0.44 to 0.84) or trait anxiety (0.59, 95% CI 0.40 to 0.77) for adults. Comparing touch interventions involving objects or robots resulted in similar physical (0.56, 95% CI 0.24 to 0.88 versus 0.51, 95% CI 0.38 to 0.64) but lower mental health benefits (0.34, 95% CI 0.19 to 0.49 versus 0.58, 95% CI 0.43 to 0.73). Adult clinical cohorts profited more strongly in mental health domains compared with healthy individuals (0.63, 95% CI 0.46 to 0.80 versus 0.37, 95% CI 0.20 to 0.55). We found no difference in health benefits in adults when comparing touch applied by a familiar person or a health care professional (0.51, 95% CI 0.29 to 0.73 versus 0.50, 95% CI 0.38 to 0.61), but parental touch was more beneficial in newborns (0.69, 95% CI 0.50 to 0.88 versus 0.39, 95% CI 0.18 to 0.61). Small but significant small study bias and the impossibility to blind experimental conditions need to be considered. Leveraging factors that influence touch intervention efficacy will help maximize the benefits of future interventions and focus research in this field.",No methods found.
2024,https://openalex.org/W4400134137,Psychology,Computational intelligence-based classification system for the diagnosis of memory impairment in psychoactive substance users,"Abstract Computational intelligence techniques have emerged as a promising approach for diagnosing various medical conditions, including memory impairment. Increased abuse of psychoactive drugs poses a global public health burden, as repeated exposure to these substances can cause neurodegeneration, premature aging, and negatively affect memory impairment. Many studies in the literature relied on statistical studies, but they remained inaccurate. Some studies relied on physical data because the time factor was not considered, until Artificial Intelligence (AI) techniques came along that proved their worth in this diagnosis. The variable deep neural network method was used to adapt to the intermediate results and re-process the intermediate in case the result is undesirable. Computational intelligence was used in this study to classify a brain image from MRI or CT scans and to show the effectiveness of the dose ratio on health with treatment time, and to diagnose memory impairment in users of psychoactive substances. Understanding the neurotoxic profiles of psychoactive substances and the underlying pathways is hypothesized to be of great importance in improving the risk assessment and treatment of substance use disorders. The results proved the worth of the proposed method in terms of the accuracy of recognition rate as well as the possibility of diagnosis. It can be concluded that the diagnostic efficiency is increased by increasing the number of hidden layers in the neural network and controlling the weights and variables that control the deep learning algorithm. Thus, we conclude that good classification in this field may save human life or early detection of memory impairment.","<method>variable deep neural network method</method>, <method>computational intelligence</method>, <method>deep learning algorithm</method>"
2024,https://openalex.org/W4400981456,Psychology,Multimodal data integration for oncology in the era of deep neural networks: a review,"Cancer research encompasses data across various scales, modalities, and resolutions, from screening and diagnostic imaging to digitized histopathology slides to various types of molecular data and clinical records. The integration of these diverse data types for personalized cancer care and predictive modeling holds the promise of enhancing the accuracy and reliability of cancer screening, diagnosis, and treatment. Traditional analytical methods, which often focus on isolated or unimodal information, fall short of capturing the complex and heterogeneous nature of cancer data. The advent of deep neural networks has spurred the development of sophisticated multimodal data fusion techniques capable of extracting and synthesizing information from disparate sources. Among these, Graph Neural Networks (GNNs) and Transformers have emerged as powerful tools for multimodal learning, demonstrating significant success. This review presents the foundational principles of multimodal learning including oncology data modalities, taxonomy of multimodal learning, and fusion strategies. We delve into the recent advancements in GNNs and Transformers for the fusion of multimodal data in oncology, spotlighting key studies and their pivotal findings. We discuss the unique challenges of multimodal learning, such as data heterogeneity and integration complexities, alongside the opportunities it presents for a more nuanced and comprehensive understanding of cancer. Finally, we present some of the latest comprehensive multimodal pan-cancer data sources. By surveying the landscape of multimodal data integration in oncology, our goal is to underline the transformative potential of multimodal GNNs and Transformers. Through technological advancements and the methodological innovations presented in this review, we aim to chart a course for future research in this promising field. This review may be the first that highlights the current state of multimodal modeling applications in cancer using GNNs and transformers, presents comprehensive multimodal oncology data sources, and sets the stage for multimodal evolution, encouraging further exploration and development in personalized cancer care.","<method>deep neural networks</method>, <method>Graph Neural Networks (GNNs)</method>, <method>Transformers</method>"
2024,https://openalex.org/W4390721566,Psychology,Thousands of AI Authors on the Future of AI,"In the largest survey of its kind, 2,778 researchers who had published in top-tier artificial intelligence (AI) venues gave predictions on the pace of AI progress and the nature and impacts of advanced AI systems The aggregate forecasts give at least a 50% chance of AI systems achieving several milestones by 2028, including autonomously constructing a payment processing site from scratch, creating a song indistinguishable from a new song by a popular musician, and autonomously downloading and fine-tuning a large language model. If science continues undisrupted, the chance of unaided machines outperforming humans in every possible task was estimated at 10% by 2027, and 50% by 2047. The latter estimate is 13 years earlier than that reached in a similar survey we conducted only one year earlier [Grace et al., 2022]. However, the chance of all human occupations becoming fully automatable was forecast to reach 10% by 2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey). Most respondents expressed substantial uncertainty about the long-term value of AI progress: While 68.3% thought good outcomes from superhuman AI are more likely than bad, of these net optimists 48% gave at least a 5% chance of extremely bad outcomes such as human extinction, and 59% of net pessimists gave 5% or more to extremely good outcomes. Between 38% and 51% of respondents gave at least a 10% chance to advanced AI leading to outcomes as bad as human extinction. More than half suggested that ""substantial"" or ""extreme"" concern is warranted about six different AI-related scenarios, including misinformation, authoritarian control, and inequality. There was disagreement about whether faster or slower AI progress would be better for the future of humanity. However, there was broad agreement that research aimed at minimizing potential risks from AI systems ought to be prioritized more.",No methods found.
2024,https://openalex.org/W4391774550,Psychology,Role of machine learning and deep learning techniques in EEG-based BCI emotion recognition system: a review,"Abstract Emotion is a subjective psychophysiological reaction coming from external stimuli which impacts every aspect of our daily lives. Due to the continuing development of non-invasive and portable sensor technologies, such as brain-computer interfaces (BCI), intellectuals from several fields have been interested in emotion recognition techniques. Human emotions can be recognised using a variety of behavioural cues, including gestures and body language, voice, and physiological markers. The first three, however, might be ineffective because people sometimes conceal their genuine emotions either intentionally or unknowingly. More precise and objective emotion recognition can be accomplished using physiological signals. Among other physiological signals, Electroencephalogram (EEG) is more responsive and sensitive to variation in affective states. Various EEG-based emotion recognition methods have recently been introduced. This study reviews EEG-based BCIs for emotion identification and gives an outline of the progress made in this field. A summary of the datasets and techniques utilised to evoke human emotions and various emotion models is also given. We discuss several EEG feature extractions, feature selection/reduction, machine learning, and deep learning algorithms in accordance with standard emotional identification process. We provide an overview of the human brain's EEG rhythms, which are closely related to emotional states. We also go over a number of EEG-based emotion identification research and compare numerous machine learning and deep learning techniques. In conclusion, this study highlights the applications, challenges and potential areas for future research in identification and classification of human emotional states.","<method>feature extraction</method>, <method>feature selection/reduction</method>, <method>machine learning algorithms</method>, <method>deep learning algorithms</method>"
2024,https://openalex.org/W4392241969,Psychology,All models are wrong and yours are useless: making clinical prediction models impactful for patients,"All models are wrong and yours are useless: making clinical prediction models impactful for patients Florian MarkowetzCheck for updates Most published clinical prediction models are never used in clinical practice and there is a huge gap between academic research and clinical implementation.Here, I propose ways for academic researchers to be proactive partners in improving clinical practice and to design models in ways that ultimately benefit patients.""All models are wrong, but some are useful"" is an aphorism attributed to the statistician George Box.There is humility in claiming your model is wrong, but there is also bravado in implying your model might be useful.And, honestly, I don't think it is.I think your model is useless.How would I know?I don't even know who you are.Well, it is a bet.A bet I am willing to take because the odds are ridiculously in my favour.I will explain what I mean in the context of clinical prediction models.My points apply to a wide range of preclinical models, both computational and biological, but my own core expertise is with clinical prediction tools.These are computational models from statistics, machine learning or AI that try to predict clinically relevant variables and ultimately aim to help doctors to treat patients better.The papers describing them make claims like ""this model can be used in the clinic""; generally softened with words like ""might"", ""could"", ""potential"", ""promise"", or other techniques to reduce accountability.The Box quote offers a yardstick to measure the success of these models; not by how correctly they describe reality but by how useful they are in helping patients.And in general, almost none of these tools ever help anyone.There is a wealth of systematic reviews in different fields to show how many models have been proposed and how few have even been validated, let alone been adopted in the clinic.For example, 408(!) models for chronic obstructive pulmonary disease were systematically reviewed 1 and as a summary the authors bleakly note ""several methodological pitfalls in their development and a low rate of external validation"".And whatever biomedical area you work in, your experiences will mirror this resultmany novel prediction models, little help for patients.I believe that a model designed to be used for patients is useless unless it is actually used for patients.",No methods found.
2024,https://openalex.org/W4392672404,Psychology,Green intellectual capital and environmental performance: identifying the pivotal role of green ambidexterity innovation and top management environmental awareness,"Purpose This research focuses on analyzing the impact of Green Intellectual Capital (GIC) on the Environmental Performance (EP) of Spanish wineries, as well as the mediating effect of Green Ambidexterity Innovation (GAI) on the main relationship posed (GIC–EP), and the moderating effect of Top Management Environmental Awareness (TMEA) on the GAI–EP link. In addition, age, size and size and membership in a Protected Designation of Origin (PDO) to increase the accuracy of the cause–effect relationships examined. Design/methodology/approach The study proposes a conceptual model based on previous studies, which is tested using structural equations (PLS-SEM) with data collected from 196 Spanish wineries between September 2022 and January 2023. Findings The results of the research reveal the existence of a positive and significant relationship between the development of GIC and EP of Spanish wineries, as well as the partial mediation of GAI in this link and the positive moderation of TMEA in the GAI–EP relationship. Originality/value The originality of the study is explained by several factors. First, the study pioneered the exploration of TMEA as a moderator of the relationship between GAI and EP, allowing such analysis to improve understanding of the dynamic interaction between innovation and environmental management. Second, to the best of the authors' knowledge, there are no preceding studies that have previously proposed the theoretical model presented in this research, thus providing new scientific knowledge on the intellectual capital field. Third, the contextualization of the study in the wine sector, traditionally perceived as little knowledge-intensive, contributes significantly to the existing body of scientific knowledge on the environmental management of wineries, given that it allows the identification of the catalysing variables of EP in the Spanish wine context.",No methods found.
2024,https://openalex.org/W4395010883,Psychology,Understanding the Role of Teacher-Student Relationships in Students’ Online Learning Engagement: Mediating Role of Academic Motivation,"Strengthening online learning outcomes requires the establishment of strong student-teacher relationships to engage students actively in learning activities. Teacher-student relationships are also pivotal factors for enhancing academic motivation for online learning engagement. Generally, however, research on online teaching remains underdeveloped. We aimed, in this study, to investigate the complex interplay in higher education in Pakistan between teacher-student relationships, academic motivation, and online learning engagement. We used Self-Determination Theory to frame an exploration of the impact of positive teacher-student relationships as mediated by intrinsic or extrinsic academic motivation on students' engagement in online learning activities. We administered a student self-report questionnaire to 437 participants from diverse universities in Sindh province. Using Structural Equation Modeling, we confirmed a model fit in which there were positive correlations between teacher-student relationships and students' online learning engagement; and between students' intrinsic and extrinsic academic motivations and their on line learning engagement. Our findings emphasized the need for communication, personalized support, and a sense of belonging in virtual education. Moreover, our findings revealed the mediating role of students' intrinsic and extrinsic academic motivation in teacher-student relationships, highlighting the nuanced dynamics of academic motivation in the virtual learning environment, with intrinsic motivation having the greatest mediating impact in the relationship between teacher-student relationships and on line learning engagement. Our study's practical implications include a need for professional educators to foster positive teacher-student relationships and integrate student motivational elements into online course design.",No methods found.
2024,https://openalex.org/W4401434014,Psychology,Crafting personalized learning paths with AI for lifelong learning: a systematic literature review,"The rapid evolution of knowledge requires constantly acquiring and updating skills, making lifelong learning crucial. Despite decades of artificial intelligence, recent advances promote new solutions to personalize learning in this context. The purpose of this article is to explore the current state of research on the development of artificial intelligence-mediated solutions for the design of personalized learning paths. To achieve this, a systematic literature review (SRL) of 78 articles published between 2019 and 2024 from the Scopus and Web or Science databases was conducted, answering seven questions grouped into three themes: characteristics of the published research, context of the research, and type of solution analyzed. This study identified that: (a) the greatest production of scientific research on the topic is developed in China, India and the United States, (b) the focus is mainly directed towards the educational context at the higher education level with areas of opportunity for application in the work context, and (c) the development of adaptive learning technologies predominates; however, there is a growing interest in the application of generative language models. This article contributes to the growing interest and literature related to personalized learning under artificial intelligence mediated solutions that will serve as a basis for academic institutions and organizations to design programs under this model.","<method>adaptive learning technologies</method>, <method>generative language models</method>"
2024,https://openalex.org/W4390777660,Psychology,Unlocking the Potential of XAI for Improved Alzheimer’s Disease Detection and Classification Using a ViT-GRU Model,"Alzheimer's Disease (AD) is a significant cause of dementia worldwide, and its progression from mild to severe affects an individual's ability to perform daily activities independently. The accurate and early diagnosis of AD is crucial for effective clinical intervention. However, interpreting AD from medical images can be challenging, even for experienced radiologists. Therefore, there is a need for an automatic diagnosis of AD, and researchers have investigated the potential of utilizing Artificial Intelligence (AI) techniques, particularly deep learning models, to address this challenge. This study proposes a framework that combines a Vision Transformer (ViT) and a Gated Recurrent Unit (GRU) to detect AD characteristics from Magnetic Resonance Imaging (MRI) images accurately and reliably. The ViT identifies crucial features from the input image, and the GRU establishes clear correlations between these features. The proposed model overcomes the class imbalance issue in the MRI image dataset and achieves superior accuracy and performance compared to existing methods. The model was trained on the Alzheimer's MRI Preprocessed Dataset obtained from Kaggle, achieving notable accuracies of 99.53% for 4-class and 99.69% for binary classification. It also demonstrated a high accuracy of 99.26% for 3-class on the AD Neuroimaging Initiative (ADNI) Baseline Database. These results were validated through a thorough 10-fold cross-validation process. Furthermore, Explainable AI (XAI) techniques were incorporated to make the model interpretable and explainable. This allows clinicians to understand the model's decision-making process and gain insights into the underlying factors driving the AD diagnosis.","<method>Vision Transformer (ViT)</method>, <method>Gated Recurrent Unit (GRU)</method>, <method>Explainable AI (XAI) techniques</method>"
2024,https://openalex.org/W4391325398,Psychology,A fast and robust method for detecting trend turning points in InSAR displacement time series,"Ground deformation monitoring is a crucial task in geohazard management to ensure the safety of lives and infrastructure. Persistent scatterer interferometric synthetic aperture radar (PS-InSAR) is an advanced technique for measuring small displacements on the Earth's surface. Estimated PS-InSAR time series acquired by Sentinel-1 satellites provide a great opportunity for effective monitoring of ground deformation in recent years. However, challenges arise when processing these time series due to their non-uniform sampling, noise from atmosphere and preprocessing issues including phase unwrapping and others. Therefore, estimating the location and direction of trend turning in such time series, as an indicator of ground deformation, is not an easy task. In this work, a sequential turning point detection method (STPD) is proposed and compared with other change point detection methods. Using a large set of simulated time series with various noise types, it is shown that STPD outperforms other methods in terms of overall accuracy and root mean square error for location and direction of trend turnings. As a case study, STPD is applied to detect turning points within PS-InSAR time series for the province of Frosinone in Italy and classified using topography and land cover/use. In addition, an area susceptible to landslides is selected to estimate the starting dates of potential slow-moving landslides. It is also shown that the turning points in the local precipitation time series have a high correlation with the ones in the PS-InSAR time series, indicating that precipitation is a major triggering factor of the displacements in the area. The STPD can rapidly and effectively detect locations and directions of trend turnings and is freely available online in both MATLAB and python.","<method>sequential turning point detection method (STPD)</method>, <method>change point detection methods</method>"
2024,https://openalex.org/W4391810207,Psychology,Machine Learning–Based Prediction of Suicidality in Adolescents With Allergic Rhinitis: Derivation and Validation in 2 Independent Nationwide Cohorts,"Background Given the additional risk of suicide-related behaviors in adolescents with allergic rhinitis (AR), it is important to use the growing field of machine learning (ML) to evaluate this risk. Objective This study aims to evaluate the validity and usefulness of an ML model for predicting suicide risk in patients with AR. Methods We used data from 2 independent survey studies, Korea Youth Risk Behavior Web-based Survey (KYRBS; n=299,468) for the original data set and Korea National Health and Nutrition Examination Survey (KNHANES; n=833) for the external validation data set, to predict suicide risks of AR in adolescents aged 13 to 18 years, with 3.45% (10,341/299,468) and 1.4% (12/833) of the patients attempting suicide in the KYRBS and KNHANES studies, respectively. The outcome of interest was the suicide attempt risks. We selected various ML-based models with hyperparameter tuning in the discovery and performed an area under the receiver operating characteristic curve (AUROC) analysis in the train, test, and external validation data. Results The study data set included 299,468 (KYRBS; original data set) and 833 (KNHANES; external validation data set) patients with AR recruited between 2005 and 2022. The best-performing ML model was the random forest model with a mean AUROC of 84.12% (95% CI 83.98%-84.27%) in the original data set. Applying this result to the external validation data set revealed the best performance among the models, with an AUROC of 89.87% (sensitivity 83.33%, specificity 82.58%, accuracy 82.59%, and balanced accuracy 82.96%). While looking at feature importance, the 5 most important features in predicting suicide attempts in adolescent patients with AR are depression, stress status, academic achievement, age, and alcohol consumption. Conclusions This study emphasizes the potential of ML models in predicting suicide risks in patients with AR, encouraging further application of these models in other conditions to enhance adolescent health and decrease suicide rates.",<method>random forest</method>
2024,https://openalex.org/W4390509863,Psychology,Unveiling the Dark Side of ChatGPT: Exploring Cyberattacks and Enhancing User Awareness,"The Chat Generative Pre-training Transformer (GPT), also known as ChatGPT, is a powerful generative AI model that can simulate human-like dialogues across a variety of domains. However, this popularity has attracted the attention of malicious actors who exploit ChatGPT to launch cyberattacks. This paper examines the tactics that adversaries use to leverage ChatGPT in a variety of cyberattacks. Attackers pose as regular users and manipulate ChatGPT’s vulnerability to malicious interactions, particularly in the context of cyber assault. The paper presents illustrative examples of cyberattacks that are possible with ChatGPT and discusses the realm of ChatGPT-fueled cybersecurity threats. The paper also investigates the extent of user awareness of the relationship between ChatGPT and cyberattacks. A survey of 253 participants was conducted, and their responses were measured on a three-point Likert scale. The results provide a comprehensive understanding of how ChatGPT can be used to improve business processes and identify areas for improvement. Over 80% of the participants agreed that cyber criminals use ChatGPT for malicious purposes. This finding underscores the importance of improving the security of this novel model. Organizations must take steps to protect their computational infrastructure. This analysis also highlights opportunities for streamlining processes, improving service quality, and increasing efficiency. Finally, the paper provides recommendations for using ChatGPT in a secure manner, outlining ways to mitigate potential cyberattacks and strengthen defenses against adversaries.",No methods found.
2024,https://openalex.org/W4390604331,Psychology,Generative Artificial Intelligence (AI) Technology Adoption Model for Entrepreneurs: Case of ChatGPT,"This article presents an extensive Generative AI Technology Adoption Model intended to elucidate the complex process that entrepreneurs and other innovation ecosystem actors, for instance, libraries, go through for its adoption. The model suggests that the adoption process happens in three stages: Pre-Perception & Perception, Assessment, and Outcome. During the Pre-Perception & Perception Phase, entrepreneurs initiate their technology exploration by navigating social factors, domain experience, technological familiarity, system quality, training and support, interaction convenience, and anthropomorphism; with utilitarian value and hedonic values playing an important role. As they transition to the Assessment Stage, perceived usefulness, ease of use, and a novel addition, perceived enjoyment, shape their evaluations, leading to generations of emotions toward it, with utilitarian value overweighting hedonic values. The model finishes with the Outcome Stage, where emotions developed in the Assessment Stage become tangible intentions to switch (use technology or switch to human services). The adoption model highlights the adoption factors (also called latent variables) and their relationships grounded on researcher's professional experiences and need to be further empirically validated. Entrepreneurial implications highlight the strategic insights of the model, providing a decision-making roadmap and highlighting the interaction between utilitarian and hedonistic values. Entrepreneurs can create well-informed technological integrations that are in line with business objectives by using the incremental decision-making process. The model's focus on comparative evaluations gives entrepreneurs the ability to strategically map the usability of technology for the best possible commercial results. The Generative AI Technology Adoption Model offers a nuanced understanding of entrepreneurs' technology adoption processes, which is also applicable to other actors in the innovation ecosystem.",No methods found.
2024,https://openalex.org/W4390918182,Psychology,How to engage and attract virtual influencers’ followers: a new non-human approach in the age of influencer marketing,"Purpose The purpose of this study is to identify the process of virtual influencer stickiness in the age of influencer marketing, which has received little attention in the literature. This is essential because the research creates a theoretical model of follower loyalty/stickiness to virtual influencer techniques from the standpoint of influencer marketing, which has a substantial effect on the evolution of the global marketing world. Design/methodology/approach In 2022, 302 people who currently follow an Instafamous virtual influencer took part in an Instagram self-administered online survey. Findings The findings show that both expertise and trustworthiness have a positive and significant influence on parasocial interaction, which in turn has a significant influence on virtual engagement and stickiness. Originality/value This research will specifically assist international readers in understanding how to harness and increase the efficiency and efficacy of interactive marketing strategies and methods to engage and retain followers of Instafamous virtual influencer. Moreover, the findings will be beneficial to opinion leaders, brand managers, company investors, entrepreneurs and service designers. Highlights The study pioneers a holistic virtual follower stickiness mechanism that comprises the role of source credibility, parasocial interaction, informational influence and virtual follower’s engagement and their interrelationship to each other. This study is based on parasocial interaction theory and source credibility theory to understand the relationship between virtual followers and influencers stickiness process at social media platforms. In addition, the study examined the subsequent effects of sources of credibility components on parasocial interaction; as well as, on virtual follower engagement and stickiness. This study also categorized and examined the moderating effects exerted by the genres of informative influence of virtual influencer.",No methods found.
2024,https://openalex.org/W4392780082,Psychology,Navigating the double bind: Strategies for women leaders in overcoming stereotypes and leadership biases,"Women leaders often face a ""double bind,"" a phenomenon where they are expected to exhibit both stereotypically feminine traits (e.g., warmth, empathy) and stereotypically masculine traits (e.g., assertiveness, ambition) to be perceived as effective leaders. This abstract explores strategies for women leaders to navigate this double bind, challenging stereotypes and biases to achieve leadership success. The abstract begins by acknowledging the pervasive nature of gender stereotypes and biases in leadership, highlighting their impact on women's advancement in leadership roles. Research suggests that women who conform too closely to feminine stereotypes may be perceived as lacking in leadership qualities, while those who adopt more masculine traits may be viewed as lacking in warmth and likability. This double bind poses a significant challenge for women leaders, requiring them to navigate a narrow path to leadership success. To overcome the double bind, women leaders can employ a range of strategies. One approach is to adopt an ""androgynous"" leadership style, incorporating both stereotypically feminine and masculine traits as appropriate. By demonstrating a balance of warmth and assertiveness, women can challenge traditional gender stereotypes and expand perceptions of effective leadership. Additionally, women leaders can leverage their unique strengths and experiences to differentiate themselves in leadership roles. Emphasizing qualities such as empathy, collaboration, and emotional intelligence can help women leaders build trust and rapport with their teams, enhancing their effectiveness as leaders. Furthermore, women leaders can benefit from mentorship and networking opportunities to navigate the challenges of leadership. Building a strong support network of mentors, sponsors, and peers can provide women with valuable guidance, feedback, and advocacy in their leadership journey. In conclusion, navigating the double bind requires women leaders to challenge stereotypes and biases, adopt a balanced leadership approach, and leverage their unique strengths and experiences. By employing these strategies, women can overcome barriers to leadership success and contribute to creating more inclusive and diverse leadership cultures.",No methods found.
2024,https://openalex.org/W4393222196,Psychology,Finding the Right XAI Method—A Guide for the Evaluation and Ranking of Explainable AI Methods in Climate Science,"Abstract Explainable artificial intelligence (XAI) methods shed light on the predictions of machine learning algorithms. Several different approaches exist and have already been applied in climate science. However, usually missing ground truth explanations complicate their evaluation and comparison, subsequently impeding the choice of the XAI method. Therefore, in this work, we introduce XAI evaluation in the climate context and discuss different desired explanation properties, namely, robustness, faithfulness, randomization, complexity, and localization. To this end, we chose previous work as a case study where the decade of annual-mean temperature maps is predicted. After training both a multilayer perceptron (MLP) and a convolutional neural network (CNN), multiple XAI methods are applied and their skill scores in reference to a random uniform explanation are calculated for each property. Independent of the network, we find that XAI methods such as Integrated Gradients, layerwise relevance propagation, and input times gradients exhibit considerable robustness, faithfulness, and complexity while sacrificing randomization performance. Sensitivity methods, gradient, SmoothGrad, NoiseGrad, and FusionGrad, match the robustness skill but sacrifice faithfulness and complexity for the randomization skill. We find architecture-dependent performance differences regarding robustness, complexity, and localization skills of different XAI methods, highlighting the necessity for research task-specific evaluation. Overall, our work offers an overview of different evaluation properties in the climate science context and shows how to compare and benchmark different explanation methods, assessing their suitability based on strengths and weaknesses, for the specific research problem at hand. By that, we aim to support climate researchers in the selection of a suitable XAI method. Significance Statement Explainable artificial intelligence (XAI) helps to understand the reasoning behind the prediction of a neural network. XAI methods have been applied in climate science to validate networks and provide new insight into physical processes. However, the increasing number of XAI methods can overwhelm practitioners, making it difficult to choose an explanation method. Since XAI methods’ results can vary, uninformed choices might cause misleading conclusions about the network decision. In this work, we introduce XAI evaluation to compare and assess the performance of explanation methods based on five desirable properties. We demonstrate that XAI evaluation reveals the strengths and weaknesses of different XAI methods. Thus, our work provides climate researchers with the tools to compare, analyze, and subsequently choose explanation methods.","<method>multilayer perceptron (MLP)</method>, <method>convolutional neural network (CNN)</method>, <method>Integrated Gradients</method>, <method>layerwise relevance propagation</method>, <method>input times gradients</method>, <method>gradient</method>, <method>SmoothGrad</method>, <method>NoiseGrad</method>, <method>FusionGrad</method>"
2024,https://openalex.org/W4394869902,Psychology,2023: Weather and Climate Extremes Hitting the Globe with Emerging Features,"Globally, 2023 was the warmest observed year on record since at least 1850 and, according to proxy evidence, possibly of the past 100 000 years. As in recent years, the record warmth has again been accompanied with yet more extreme weather and climate events throughout the world. Here, we provide an overview of those of 2023, with details and key background causes to help build upon our understanding of the roles of internal climate variability and anthropogenic climate change. We also highlight emerging features associated with some of these extreme events. Hot extremes are occurring earlier in the year, and increasingly simultaneously in differing parts of the world (e.g., the concurrent hot extremes in the Northern Hemisphere in July 2023). Intense cyclones are exacerbating precipitation extremes (e.g., the North China flooding in July and the Libya flooding in September). Droughts in some regions (e.g., California and the Horn of Africa) have transitioned into flood conditions. Climate extremes also show increasing interactions with ecosystems via wildfires (e.g., those in Hawaii in August and in Canada from spring to autumn 2023) and sandstorms (e.g., those in Mongolia in April 2023). Finally, we also consider the challenges to research that these emerging characteristics present for the strategy and practice of adaptation.",No methods found.
2024,https://openalex.org/W4401203858,Psychology,"Generative AI as a transformative force for innovation: a review of opportunities, applications and challenges","Purpose This study examines the existing literature on generative artificial intelligence (Gen AI) and its impact across many sectors. This analysis explores the potential, applications, and challenges of Gen AI in driving innovation and creativity and generating ideas. Design/methodology/approach The study adopts a comprehensive literature review approach, carefully assessing current scientific articles on Gen AI published from 2022 to 2024. The analysis examines trends and insights derived from research. Findings The review indicates that Gen AI has significant potential to augment human creativity and innovation processes as a collaborative partner. However, it is imperative to prioritize responsible development and ethical frameworks in order to effectively tackle biases, privacy concerns, and other challenges. Gen AI is significantly transforming business models, processes, and value propositions in several industries, but with varying degrees of effect. Findings indicate also that despite the theory-driven approach to investigating Gen AI's creative and innovative potential, cutting-edge applications research prioritizes examining the possibilities of Gen AI models. Research limitations/implications Although this review offers a picture of great possibilities, it concurrently underlines the necessity for a deep knowledge of Gen AI nuances to fully harness its capabilities. The findings indicate that continuous research and exploration efforts are required to address the challenges of Gen AI and assure its responsible and ethical implementation. Therefore, more study is needed on enhancing human-AI collaboration and defining ethical norms for varied circumstances. Originality/value This study presents a relevant analysis of Gen AI's transformational potential as an innovation catalyst. It emphasizes major potential, applications across industries, and ethical issues for responsible integration.",No methods found.
2024,https://openalex.org/W4403839497,Psychology,When combinations of humans and AI are useful: A systematic review and meta-analysis,"Abstract Inspired by the increasing use of artificial intelligence (AI) to augment humans, researchers have studied human–AI systems involving different tasks, systems and populations. Despite such a large body of work, we lack a broad conceptual understanding of when combinations of humans and AI are better than either alone. Here we addressed this question by conducting a preregistered systematic review and meta-analysis of 106 experimental studies reporting 370 effect sizes. We searched an interdisciplinary set of databases (the Association for Computing Machinery Digital Library, the Web of Science and the Association for Information Systems eLibrary) for studies published between 1 January 2020 and 30 June 2023. Each study was required to include an original human-participants experiment that evaluated the performance of humans alone, AI alone and human–AI combinations. First, we found that, on average, human–AI combinations performed significantly worse than the best of humans or AI alone (Hedges’ g = −0.23; 95% confidence interval, −0.39 to −0.07). Second, we found performance losses in tasks that involved making decisions and significantly greater gains in tasks that involved creating content. Finally, when humans outperformed AI alone, we found performance gains in the combination, but when AI outperformed humans alone, we found losses. Limitations of the evidence assessed here include possible publication bias and variations in the study designs analysed. Overall, these findings highlight the heterogeneity of the effects of human–AI collaboration and point to promising avenues for improving human–AI systems.",No methods found.
2024,https://openalex.org/W4390616591,Psychology,"To Earmark or to Nonearmark? The Role of Control, Transparency, and Warm-Glow","Problem definition: Charities face tension when deciding whether to earmark donations, that is, allow donors to restrict the use of their donations for a specific purpose. Research shows that earmarking decreases operational performance because it limits charities’ flexibility to use donations. However, there is also a common belief that earmarking increases donations. Earmarking is assumed to increase donations through three mechanisms: by (i) giving donors control over their donations, (ii) increasing operational transparency of donations, and (iii) changing donors’ levels of altruism and warm-glow. To resolve this tension, we study how, when, and why earmarking affects donors’ decisions. We consider three important decisions donors make that impact the fundraising outcome: (i) preference between earmarking and nonearmarking, (ii) decision to donate or not (i.e., donor activation), and (iii) the donation amount. Methodology/results: We design three online experiments that allow us to quantify the effect of earmarking on donors’ decisions and to investigate the role of the three aforementioned mechanisms in fundraising. Our results reveal, for example, that earmarking activates more donors but it does not always increase donation amounts. In addition, we determine the conditions under which the three mechanisms affect the outcome of fundraising campaigns. Managerial implications: Our findings provide actionable insights for how charities can design fundraising campaigns more effectively and suggest when to leverage earmarking and the three mechanisms depending on the charities’ fundraising goals. Funding: The authors gratefully acknowledge financial support provided by the Leeds School of Business at the University of Colorado Boulder. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0096 .",No methods found.
2024,https://openalex.org/W4390660035,Psychology,An Empirical Study on Correlations Between Deep Neural Network Fairness and Neuron Coverage Criteria,"Recently, with the widespread use of deep neural networks (DNNs) in high-stakes decision-making systems (such as fraud detection and prison sentencing), concerns have arisen about the fairness of DNNs in terms of the potential negative impact they may have on individuals and society. Therefore, fairness testing has become an important research topic in DNN testing. At the same time, the neural network coverage criteria (such as criteria based on neuronal activation) is considered as an adequacy test for DNN white-box testing. It is implicitly assumed that improving the coverage can enhance the quality of test suites. Nevertheless, the correlation between DNN fairness (a test property) and coverage criteria (a test method) has not been adequately explored. To address this issue, we conducted a systematic empirical study on seven coverage criteria, six fairness metrics, three fairness testing techniques, and five bias mitigation methods on five DNN models and nine fairness datasets to assess the correlation between coverage criteria and DNN fairness. Our study achieved the following findings: 1) with the increase in the size of the test suite, some of the coverage and fairness metrics changed significantly, as the size of the test suite increased; 2) the statistical correlation between coverage criteria and DNN fairness is limited; and 3) after bias mitigation for improving the fairness of DNN, the change pattern in coverage criteria is different; 4) Models debiased by different bias mitigation methods have a lower correlation between coverage and fairness compared to the original models. Our findings cast doubt on the validity of coverage criteria concerning DNN fairness (i.e., increasing the coverage may even have a negative impact on the fairness of DNNs). Therefore, we warn DNN testers against blindly pursuing higher coverage of coverage criteria at the cost of test properties of DNNs (such as fairness).","<method>deep neural networks (DNNs)</method>, <method>neural network coverage criteria</method>, <method>fairness testing techniques</method>, <method>bias mitigation methods</method>"
2024,https://openalex.org/W4390742710,Psychology,Machine Learning as a Tool for Hypothesis Generation,"Abstract While hypothesis testing is a highly formalized activity, hypothesis generation remains largely informal. We propose a systematic procedure to generate novel hypotheses about human behavior, which uses the capacity of machine learning algorithms to notice patterns people might not. We illustrate the procedure with a concrete application: judge decisions about whom to jail. We begin with a striking fact: the defendant’s face alone matters greatly for the judge’s jailing decision. In fact, an algorithm given only the pixels in the defendant’s mug shot accounts for up to half of the predictable variation. We develop a procedure that allows human subjects to interact with this black-box algorithm to produce hypotheses about what in the face influences judge decisions. The procedure generates hypotheses that are both interpretable and novel: they are not explained by demographics (e.g., race) or existing psychology research, nor are they already known (even if tacitly) to people or experts. Though these results are specific, our procedure is general. It provides a way to produce novel, interpretable hypotheses from any high-dimensional data set (e.g., cell phones, satellites, online behavior, news headlines, corporate filings, and high-frequency time series). A central tenet of our article is that hypothesis generation is a valuable activity, and we hope this encourages future work in this largely “prescientific” stage of science.",<method>machine learning algorithms</method>
2024,https://openalex.org/W4391174596,Psychology,Generative Large Language Models for Detection of Speech Recognition Errors in Radiology Reports,"This study evaluated the ability of generative large language models (LLMs) to detect speech recognition errors in radiology reports. A dataset of 3233 CT and MRI reports was assessed by radiologists for speech recognition errors. Errors were categorized as clinically significant or not clinically significant. Performances of five generative LLMs—GPT-3.5-turbo, GPT-4, text-davinci-003, Llama-v2–70B-chat, and Bard—were compared in detecting these errors, using manual error detection as the reference standard. Prompt engineering was used to optimize model performance. GPT-4 demonstrated high accuracy in detecting clinically significant errors (precision, 76.9%; recall, 100%; F1 score, 86.9%) and not clinically significant errors (precision, 93.9%; recall, 94.7%; F1 score, 94.3%). Text-davinci-003 achieved F1 scores of 72% and 46.6% for clinically significant and not clinically significant errors, respectively. GPT-3.5-turbo obtained 59.1% and 32.2% F1 scores, while Llama-v2–70B-chat scored 72.8% and 47.7%. Bard showed the lowest accuracy, with F1 scores of 47.5% and 20.9%. GPT-4 effectively identified challenging errors of nonsense phrases and internally inconsistent statements. Longer reports, resident dictation, and overnight shifts were associated with higher error rates. In conclusion, advanced generative LLMs show potential for automatic detection of speech recognition errors in radiology reports. Keywords: CT, Large Language Model, Machine Learning, MRI, Natural Language Processing, Radiology Reports, Speech, Unsupervised Learning Supplemental material is available for this article. © RSNA, 2024","<method>generative large language models (LLMs)</method>, <method>GPT-3.5-turbo</method>, <method>GPT-4</method>, <method>text-davinci-003</method>, <method>Llama-v2–70B-chat</method>, <method>Bard</method>, <method>prompt engineering</method>"
2024,https://openalex.org/W4391641063,Psychology,Predictors for estimating subcortical EEG responses to continuous speech,"Perception of sounds and speech involves structures in the auditory brainstem that rapidly process ongoing auditory stimuli. The role of these structures in speech processing can be investigated by measuring their electrical activity using scalp-mounted electrodes. However, typical analysis methods involve averaging neural responses to many short repetitive stimuli that bear little relevance to daily listening environments. Recently, subcortical responses to more ecologically relevant continuous speech were detected using linear encoding models. These methods estimate the temporal response function (TRF), which is a regression model that minimises the error between the measured neural signal and a predictor derived from the stimulus. Using predictors that model the highly non-linear peripheral auditory system may improve linear TRF estimation accuracy and peak detection. Here, we compare predictors from both simple and complex peripheral auditory models for estimating brainstem TRFs on electroencephalography (EEG) data from 24 participants listening to continuous speech. We also investigate the data length required for estimating subcortical TRFs, and find that around 12 minutes of data is sufficient for clear wave V peaks (&gt;3 dB SNR) to be seen in nearly all participants. Interestingly, predictors derived from simple filterbank-based models of the peripheral auditory system yield TRF wave V peak SNRs that are not significantly different from those estimated using a complex model of the auditory nerve, provided that the nonlinear effects of adaptation in the auditory system are appropriately modelled. Crucially, computing predictors from these simpler models is more than 50 times faster compared to the complex model. This work paves the way for efficient modelling and detection of subcortical processing of continuous speech, which may lead to improved diagnosis metrics for hearing impairment and assistive hearing technology.","<method>linear encoding models</method>, <method>temporal response function (TRF)</method>, <method>regression model</method>"
2024,https://openalex.org/W4399568894,Psychology,Artificial intelligence capability and organizational performance: unraveling the mediating mechanisms of decision-making processes,"Purpose This study investigates the profound impact of artificial intelligence (AI) capabilities on decision-making processes and organizational performance, addressing a crucial gap in the literature by exploring the mediating role of decision-making speed and quality. Design/methodology/approach Drawing upon resource-based theory and prior research, this study constructs a comprehensive model and hypotheses to illuminate the influence of AI capabilities within organizations on decision-making speed, decision quality, and, ultimately, organizational performance. A dataset comprising 230 responses from diverse organizations forms the basis of the analysis, with the study employing a partial least squares structural equation model (PLS-SEM) for robust data examination. Findings The results demonstrate the pivotal role of AI capabilities in shaping organizational decision-making processes and performance. AI capability significantly and positively affects decision-making speed, decision quality, and overall organizational performance. Notably, decision-making speed is a critical factor contributing significantly to enhanced organizational performance. The study further uncovered partial mediation effects, suggesting that decision-making processes partially mediate the relationship between AI capabilities and organizational performance through decision-making speed. Originality/value This study contributes to the existing body of literature by providing empirical evidence of the multifaceted impact of AI capabilities on organizational decision-making and performance. Elucidating the mediating role of decision-making processes advances our understanding of the complex mechanisms through which AI capabilities drive organizational success.",<method>partial least squares structural equation model (PLS-SEM)</method>
2024,https://openalex.org/W4400770903,Psychology,"A Survey of Imitation Learning: Algorithms, Recent Developments, and Challenges","In recent years, the development of robotics and artificial intelligence (AI) systems has been nothing short of remarkable. As these systems continue to evolve, they are being utilized in increasingly complex and unstructured environments, such as autonomous driving, aerial robotics, and natural language processing. As a consequence, programming their behaviors manually or defining their behavior through the reward functions as done in reinforcement learning (RL) has become exceedingly difficult. This is because such environments require a high degree of flexibility and adaptability, making it challenging to specify an optimal set of rules or reward signals that can account for all the possible situations. In such environments, learning from an expert's behavior through imitation is often more appealing. This is where imitation learning (IL) comes into play -a process where desired behavior is learned by imitating an expert's behavior, which is provided through demonstrations.This article aims to provide an introduction to IL and an overview of its underlying assumptions and approaches. It also offers a detailed description of recent advances and emerging areas of research in the field. Additionally, this article discusses how researchers have addressed common challenges associated with IL and provides potential directions for future research. Overall, the goal of this article is to provide a comprehensive guide to the growing field of IL in robotics and AI.","<method>reinforcement learning (RL)</method>, <method>imitation learning (IL)</method>"
2024,https://openalex.org/W4390716276,Psychology,The role of organizational learning and innovative organizational culture for ambidextrous innovation,"Purpose In the era of hyper-competitiveness, firms, especially project-based management structures, have to focus on ideas for both new and existing sets of products and services, i.e. ambidextrous innovation. The ambidextrous innovation can be helpful, but achieving such a level is a problem to be solved. This study aims to yield ambidextrous innovation by using innovative culture and knowledge that has been gained from learning. Design/methodology/approach The present research collected data from Saudi Arabian public-sector firms. The data collected is analyzed using the partial least squares structural equation modeling (PLS-SEM). Findings The findings of the study suggest that a range of factors can be operationalized in project-based firms to establish organizational learning and innovation culture. These factors include agile-based project management, leveraging existing innovative capabilities and growth mindset in case of innovative organizational culture and additional factors of agile-based knowledge management along with others in case of organizational learning. The PLS-SEM further concluded that both organizational learning and innovative organizational culture, in turn, help project-based Saudi Arabian public-sector firms to develop their ambidextrous innovation capability. Originality/value The PLS-SEM further concluded that both the organizational learning and innovative organizational culture, in turn, help project-based Saudi Arabian public-sector firms to develop their ambidextrous innovation capability.",No methods found.
2024,https://openalex.org/W4390847963,Psychology,Crowdfunding adoption in emerging economies: insights for entrepreneurs and policymakers,"Purpose Crowdfunding has emerged as an alternative financing tool and recently gained attention to foster entrepreneurial dynamism and innovation. The current research has identified the determinants impacting the behavioral intentions of entrepreneurs to use crowdfunding for financing their small and medium-sized enterprises (SMEs). Design/methodology/approach The current article is based on a cross-sectional research design. This research collected the data of 422 owners and managers of SMEs through self-administered questionnaires in the Indian National Capital Region (NCR). The responses were collected from July 17 to October 27, 2022. This article used “partial least squares structural equation modeling” (PLS-SEM) for data analysis. Findings This article offered a robust model with a high explanatory value of 66% of behavioral intention and 62.1% variance in crowdfunding use behavior. The finding also highlighted that performance expectancy, social influence, facilitating conditions, trialability and perceived value significantly impact behavioral intention. However, effort expectancy and perceived risk insignificantly influence behavioral intention. Notably, facilitating conditions, trialability and behavioral intention positively impact use behavior. Practical implications The results of this study will bridge the gap in empirical research on crowdfunding adoption, shedding light on why entrepreneurs hesitate to adopt crowdfunding for financing. Moreover, these results will offer strategic insights for crowdfunding managers and policymakers, aiding them in making informed decisions. Originality/value To the best of the authors' knowledge, this pioneering study built the theoretical framework using three credible technology determinant models. The authors examined crowdfunding-specific contextual factors to improve understanding of the positive effect of technological orientation. This addition assists in strategically arranging entrepreneurs' fundraising conversations more efficiently.",<method>partial least squares structural equation modeling (PLS-SEM)</method>
2024,https://openalex.org/W4391475057,Psychology,Leadership and Environmental Sustainability: An Integrative Conceptual Model of Multilevel Antecedents and Consequences of Leader Green Behavior,"Environmental sustainability is a strategic and ethical imperative for organizations, and numerous studies have investigated associations between leadership and employee pro-environmental or “green” behavior. However, these studies have typically focused on leadership styles that conflate leader behavior with its assumed antecedents or consequences. Moreover, the literature on relations between leadership and environmental sustainability constructs is fragmented and in need of systematic integration to effectively guide future research and practice. Accordingly, we pursue three goals in this conceptual paper. First, after a brief review of key insights from extant theoretical and empirical research, we define leadership in the context of environmental sustainability and leader green behavior based on established theoretical frameworks. Second, based on a systematic integration and extension of the literature, we present an integrative conceptual model of multilevel antecedents and consequences of leader green behavior. We further develop eight propositions on multiple known and novel pathways toward leader and follower green behaviors, as well as multiple known and novel pathways toward consequences related to environmental sustainability at the leader, follower, and organizational levels. Finally, based on our integrative conceptual model and propositions, we outline several recommendations for future research on leadership and environmental sustainability, including theoretical and methodological considerations.",No methods found.
2024,https://openalex.org/W4392499245,Psychology,Exploring the role of skin temperature in thermal sensation and thermal comfort: A comprehensive review,"The role of skin temperature as a determinant of human thermal sensation and comfort has gained increasing recognition, prompting a need for a systematic review. This review examines the relationship between skin temperature and thermal sensation, synthesizing insights from 172 studies published since 2000. It uniquely focuses on the indispensable roles of local and mean skin temperatures, a perspective not comprehensively explored in previous literature. The review reveals that the most common measurement points for skin temperature are the face and hands, attributed to their higher thermal sensitivity and the practical ease of measurement. It establishes a clear linear relationship between mean skin temperature and user thermal sensation, though affected by the choice of measurement locations and number of points. A notable finding is the varying impact of local skin temperature on overall thermal sensation in changing environments, with local heating less influential than cooling. The review also uncovers significant demographic variations in thermal sensation, strongly influenced by differing skin temperatures across age groups, genders, and climatic regions. For example, elderly populations exhibit a decreased temperature sensitivity, especially towards warmth. Gender differences are also significant, with females experiencing higher skin temperatures in warmer environments and lower in colder ones. Machine learning (ML)-based methods, especially classification tree-based and support vector machine (SVM) techniques, dominate in predicting thermal sensation and comfort, leveraging skin temperature data. While ML methods are prevalent, statistical regression-based approaches offer valuable empirical insights. Thermo-physiological model-based methods provide reliable results by incorporating detailed skin temperature dynamics. The review identifies a gap in understanding how gender, age, and regional differences influence thermal comfort in diverse environments. The study recommends conducting more nuanced experiments to dissect the impact of these factors and proposes the integration of individual demographic variables into ML models to personalize thermal comfort predictions.","<method>classification tree-based</method>, <method>support vector machine (SVM)</method>, <method>statistical regression-based approaches</method>, <method>thermo-physiological model-based methods</method>"
2024,https://openalex.org/W4392516399,Psychology,Artificial intelligence in dermatology: advancements and challenges in skin of color,"Abstract Artificial intelligence (AI) uses algorithms and large language models in computers to simulate human‐like problem‐solving and decision‐making. AI programs have recently acquired widespread popularity in the field of dermatology through the application of online tools in the assessment, diagnosis, and treatment of skin conditions. A literature review was conducted using PubMed and Google Scholar analyzing recent literature (from the last 10 years through October 2023) to evaluate current AI programs in use for dermatologic purposes, identifying challenges in this technology when applied to skin of color (SOC), and proposing future steps to enhance the role of AI in dermatologic practice. Challenges surrounding AI and its application to SOC stem from the underrepresentation of SOC in datasets and issues with image quality and standardization. With these existing issues, current AI programs inevitably do worse at identifying lesions in SOC. Additionally, only 30% of the programs identified in this review had data reported on their use in dermatology, specifically in SOC. Significant development of these applications is required for the accurate depiction of darker skin tone images in datasets. More research is warranted in the future to better understand the efficacy of AI in aiding diagnosis and treatment options for SOC patients.","<method>algorithms</method>, <method>large language models</method>"
2024,https://openalex.org/W4392890128,Psychology,"Motivation, work experience, and teacher performance: A comparative study","This research study investigates the effect of intrinsic and extrinsic motivation on employee performance, with a specific focus on the moderating role of employees' work experience. This investigation utilizes a proposed framework, focusing on higher educational institutions in West Bengal, India. It contributes to the human resource management field by comparing teacher performance in private and government academic institutions based on their motivation levels. The study employs a quantitative approach, collecting data from 250 teachers in West Bengal, India, using a structured questionnaire. The dataset underwent analysis employing Partial Least Squares Structural Equation Modeling (PLS-SEM) due to its inherent capacity to accommodate smaller sample sizes while delivering precise and insightful outcomes. The results indicate a strong positive relationship between intrinsic and extrinsic motivation and teacher performance in both types of institutions. Work experience moderates the connection between intrinsic motivation and performance in both sectors but has no significant impact on the relationship between extrinsic motivation and performance in private academic institutions. This study links a gap in the literature by empirically exploring the impact of teacher motivation on their performance and provides valuable insights into the complex interplay among motivation, work experience, and performance. Practically, it emphasizes the importance of employee motivation and accumulated work experience in enhancing performance. This study attempts to underscore the role of work experience as a moderating variable, thereby contributing to the novel discourse in the educational landscape of the post-pandemic era. The findings demand to identification of diverse organizational developmental drivers as work experience does not exhibit a strong mediation effect. However, limitations such as potential response bias should be considered in future research in this area.",<method>Partial Least Squares Structural Equation Modeling (PLS-SEM)</method>
2024,https://openalex.org/W4399154552,Psychology,Seeking in Ride-on-Demand Service: A Reinforcement Learning Model With Dynamic Price Prediction,"Recent years witness the increasing popularity of ride-on-demand (RoD) services such as Uber and Didi. Compared with traditional taxi, RoD service is more ""data-driven"" and adopts dynamic pricing to manipulate the supply and demand in real time. Dynamic price could be viewed as an accurate and quantitative indicator of the supply and demand, and could provide clues to drivers, passengers, and the service providers, possibly reshaping the ways in which some problems are solved. In this paper, we focus on the seeking route recommendation problem that aims at increasing driver revenue by recommending highly profitable seeking routes to drivers of vacant cars with the help of dynamic prices. We first justify our motivation by showing the importance of route recommendation and answering why it is necessary to consider dynamic prices, based on the analysis of real service data. We then design a dynamic price prediction model to generate the dynamic prices at any given time and location based on multi-source urban data. After that, a reinforcement learning model is adopted to perform seeking route recommendation based on predicted dynamic prices. We conduct extensive experiments in different spatio-temporal combinations and make comparisons with multiple baselines. Results first show that our dynamic price prediction model achieves an accuracy ranging from 83.82% to 90.67% under different settings. It also proves that considering the real-time predicted dynamic prices significantly increases driver revenue by, for example, 12% and 47.5% during weekday evening rush hours, than merely using the average prices or completely ignoring dynamic prices.","<method>dynamic price prediction model</method>, <method>reinforcement learning model</method>"
2024,https://openalex.org/W4400077472,Psychology,Exploring the factors that influence academic performance in Jordanian higher education institutions,"Since the coronavirus 2019 (COVID-19) pandemic hit the world, many universities have used digital asynchronous learning tools such as Digital Learning Management Systems (DLMS) to continue the educational process. Despite its global usage, only a few studies have investigated its quality in the Jordanian context during the COVID-19 period from a quantitative and qualitative approach perspective. Thus, the current study aims to explore the factors that influence academic performance in Jordanian higher education institutions during the COVID-19 pandemic. A mixed methods research approach was employed to evaluate the quality of the teaching-learning process for Jordanian students in higher education institutions. The triangulated data focused on three core pillars namely if students saw a difference in their grades prior to, during, and after the pandemic, the challenges faced and improvement suggestions. Accordingly, the quantitative approach with an online questionnaire and the qualitative approach with structured interviews were applied to collect the required data from Jordanian students in higher education institutions. The results of the current study revealed that the evaluation of the teaching-learning process quality during the pandemic period affected students' academic performance in different proportions based on their specialization area. In addition, the study results also identified the most important challenges that faced the students during this period and suggested procedures to overcome them and improve the distance learning process. The current study offers empirical evidence on critical success factors underlying digital learning management systems in the COVID-19 era, which can help policymakers in Jordanian universities and the ministry of higher education and scientific research to improve the quality of the teaching-learning process in the Jordanian context.",No methods found.
2024,https://openalex.org/W4390688548,Psychology,Understanding the acceptance of business intelligence from healthcare professionals’ perspective: an empirical study of healthcare organizations,"Purpose Due to its ability to support well-informed decision-making, business intelligence (BI) has grown in popularity among executives across a range of industries. However, given the volume of data collected in health-care organizations, there is a lack of exploration concerning its implementation. Consequently, this research paper aims to investigate the key factors affecting the acceptance and use of BI in healthcare organizations. Design/methodology/approach Leveraging the theoretical lens of the “unified theory of acceptance and use of technology” (UTAUT), a study framework was proposed and integrated with three context-related factors, including “rational decision-making culture” (RDC), “perceived threat to professional autonomy” (PTA) and “medical–legal risk” (MLR). The variables in the study framework were categorized as follows: information systems (IS) perspective; organizational perspective; and user perspective. In Jordan, 434 healthcare professionals participated in a cross-sectional online survey that was used to collect data. Findings The findings of the “structural equation modeling” revealed that professionals’ behavioral intentions toward using BI systems were significantly affected by performance expectancy, social influence, facilitating conditions, MLR, RDC and PTA. Also, an insignificant effect of PTA on PE was found based on the results of statistical analysis. These variables explained 68% of the variance ( R 2 ) in the individuals’ intentions to use BI-based health-care systems. Practical implications To promote the acceptance and use of BI technology in health-care settings, developers, designers, service providers and decision-makers will find this study to have a number of practical implications. Additionally, it will support the development of effective strategies and BI-based health-care systems based on these study results, attracting the interest of many users. Originality/value To the best of the author’s knowledge, this is one of the first studies that integrates the UTAUT model with three contextual factors (RDC, PTA and MLR) in addition to examining the suggested framework in a developing nation (Jordan). This study is one of the few in which the users’ acceptance behavior of BI systems was investigated in a health-care setting. More specifically, to the best of the author’s knowledge, this is the first study that reveals the critical antecedents of individuals’ intention to accept BI for health-care purposes in the Jordanian context.",No methods found.
2024,https://openalex.org/W4391655574,Psychology,The stability of cognitive abilities: A meta-analytic review of longitudinal studies.,"Cognitive abilities, including general intelligence and domain-specific abilities such as fluid reasoning, comprehension knowledge, working memory capacity, and processing speed, are regarded as some of the most stable psychological traits, yet there exist no large-scale systematic efforts to document the specific patterns by which their rank-order stability changes over age and time interval, or how their stability differs across abilities, tests, and populations. Determining the conditions under which cognitive abilities exhibit high or low degrees of stability is critical not just to theory development but to applied contexts in which cognitive assessments guide decisions regarding treatment and intervention decisions with lasting consequences for individuals. In order to supplement this important area of research, we present a meta-analysis of longitudinal studies investigating the stability of cognitive abilities. The meta-analysis relied on data from 205 longitudinal studies that involved a total of 87,408 participants, resulting in 1,288 test-retest correlation coefficients among manifest variables. For an age of 20 years and a test-retest interval of 5 years, we found a mean rank-order stability of ρ = .76. The effect of mean sample age on stability was best described by a negative exponential function, with low stability in preschool children, rapid increases in stability in childhood, and consistently high stability from late adolescence to late adulthood. This same functional form continued to best describe age trends in stability after adjusting for test reliability. Stability declined with increasing test-retest interval. This decrease flattened out from an interval of approximately 5 years onward. According to the age and interval moderation models, minimum stability sufficient for individual-level diagnostic decisions (",No methods found.
2024,https://openalex.org/W4391715895,Psychology,Deciphering Digital Social Dynamics: A Comparative Study of Logistic Regression and Random Forest in Predicting E-Commerce Customer Behavior,"This study compares Logistic Regression and Random Forest in predicting e-commerce customer churn. Utilizing the E-commerce Customer dataset, it navigates the complexities of customer interactions and behaviors, offering a rich context for analysis. The methodology focuses on meticulous data preprocessing to ensure data integrity, setting the stage for applying and evaluating Logistic Regression and Random Forest. Both models were assessed using accuracy, precision, recall, F1-Score, and AUC-ROC. Logistic Regression showed an accuracy of 90%, precision of 91% for class 0 and 82% for class 1, recall of 98% for class 0 and 50% for class 1, F1-Score of 94% for class 0 and 62% for class 1, and AUC-ROC of 0.88. Random Forest, with its ability to handle complex patterns, demonstrated higher overall performance with an accuracy of 95%, precision of 95% for class 0 and 93% for class 1, recall of 99% for class 0 and 74% for class 1, F1-Score of 97% for class 0 and 82% for class 1, and an AUC-ROC of 0.97. This comparative analysis offers insights into each model's strengths and suitability for predicting customer churn. The findings contribute to a deeper understanding of machine learning applications in e-commerce, guiding stakeholders in enhancing customer retention strategies. This research provides a foundation for further exploration into the digital social dynamics that shape customer behavior in the evolving digital marketplace.","<method>Logistic Regression</method>, <method>Random Forest</method>"
2024,https://openalex.org/W4396622079,Psychology,The power of Deep Learning techniques for predicting student performance in Virtual Learning Environments: A systematic literature review,"With the advances in Artificial Intelligence (AI) and the increasing volume of online educational data, Deep Learning techniques have played a critical role in predicting student performance. Recent developments have assisted instructors in determining the strengths and weaknesses of student achievement. This understanding will benefit from adopting the necessary interventions to assist students in improving their performance, helping at-risk of failure students, and preventing dropout rates. The review analyzed 46 studies between 2019 and 2023 that apply one or more Deep Learning (DL) techniques, either single or in combination with Machine Learning (ML) or Ensemble Learning techniques. Moreover, the review utilized datasets from public Massive Open Online Courses (MOOCs), private Learning Management Systems (LMSs), and other platforms. Four categories were used to group the features: demographic, previous academic performance, current academic performance, and learning behavior/activity features. The analysis revealed that the DNNs and CNN-LSTM models were the most common techniques. Moreover, the studies that used DL techniques, such as CNNs, DNNs, and LSTMs, performed well by achieving high prediction accuracy above 90%; other studies achieved accuracy ranging (60 to 90)%. For datasets used within the reviewed studies, even though 44% of the studies used LMSs datasets, Open University Learning Analytics Dataset (OULAD) was the most used dataset from MOOCs. The analysis of grouped features shows that among the various categories examined, learning behavior and activity features stand out as the most significant predictors, suggesting that students engagement with their learning environment through their overall participation offers crucial insights into their success. The educational prediction findings hopefully serve as a strong foundation for administrators and instructors to observe student performance and provide a suitable educational adaptation that can meet their needs to protect them from failure and prevent their dropout.","<method>Deep Learning (DL) techniques</method>, <method>Machine Learning (ML) techniques</method>, <method>Ensemble Learning techniques</method>, <method>DNNs</method>, <method>CNN-LSTM models</method>, <method>CNNs</method>, <method>DNNs</method>, <method>LSTMs</method>"
2024,https://openalex.org/W4399442306,Psychology,Integrative analysis of AI-driven optimization in HIV treatment regimens,"The integration of artificial intelligence (AI) into HIV treatment regimens has revolutionized the approach to personalized care and optimization strategies. This study presents an in-depth analysis of the role of AI in transforming HIV treatment, focusing on its ability to tailor therapy to individual patient needs and enhance treatment outcomes. AI-driven optimization in HIV treatment involves the utilization of advanced algorithms and computational techniques to analyze vast amounts of patient data, including genetic information, viral load measurements, and treatment history. By harnessing the power of machine learning and predictive analytics, AI algorithms can identify patterns and trends in patient data that may not be readily apparent to human clinicians. One of the key benefits of AI-driven optimization is its ability to personalize treatment regimens based on individual patient characteristics and disease progression. By considering factors such as drug resistance profiles, comorbidities, and lifestyle factors, AI algorithms can recommend the most effective and well-tolerated treatment options for each patient, leading to improved adherence and clinical outcomes. Furthermore, AI enables continuous monitoring and adjustment of treatment regimens in real time, allowing healthcare providers to respond rapidly to changes in patient status and evolving viral dynamics. This proactive approach to HIV management can help prevent treatment failure and the development of drug resistance, ultimately leading to better long-term outcomes for patients. Despite its transformative potential, AI-driven optimization in HIV treatment is not without challenges. Ethical considerations, data privacy concerns, and the need for robust validation and regulatory oversight are all important factors that must be addressed to ensure the safe and effective implementation of AI algorithms in clinical practice. In conclusion, the integrative analysis presented in this study underscores the significant impact of AI-driven optimization on the personalization and optimization of HIV treatment regimens. By leveraging AI technologies, healthcare providers can tailor treatment approaches to individual patient needs, leading to improved outcomes and quality of life for people living with HIV. Keywords: Integrative Analysis, AI- Driven, Optimization, HIV Treatment, Regimens.","<method>machine learning</method>, <method>predictive analytics</method>"
2024,https://openalex.org/W4390506438,Psychology,Artificial intelligence for oral squamous cell carcinoma detection based on oral photographs: A comprehensive literature review,"Abstract Introduction Oral squamous cell carcinoma (OSCC) presents a significant global health challenge. The integration of artificial intelligence (AI) and computer vision holds promise for the early detection of OSCC through the analysis of digitized oral photographs. This literature review explores the landscape of AI‐driven OSCC automatic detection, assessing both the performance and limitations of the current state of the art. Materials and Methods An electronic search using several data base was conducted, and a systematic review performed in accordance with PRISMA guidelines (CRD42023441416). Results Several studies have demonstrated remarkable results for this task, consistently achieving sensitivity rates exceeding 85% and accuracy rates surpassing 90%, often encompassing around 1000 images. The review scrutinizes these studies, shedding light on their methodologies, including the use of recent machine learning and pattern recognition approaches coupled with different supervision strategies. However, comparing the results from different papers is challenging due to variations in the datasets used. Discussion Considering these findings, this review underscores the urgent need for more robust and reliable datasets in the field of OSCC detection. Furthermore, it highlights the potential of advanced techniques such as multi‐task learning, attention mechanisms, and ensemble learning as crucial tools in enhancing the accuracy and sensitivity of OSCC detection through oral photographs. Conclusion These insights collectively emphasize the transformative impact of AI‐driven approaches on early OSCC diagnosis, with the potential to significantly improve patient outcomes and healthcare practices.","<method>machine learning</method>, <method>pattern recognition</method>, <method>multi-task learning</method>, <method>attention mechanisms</method>, <method>ensemble learning</method>"
2024,https://openalex.org/W4390572205,Psychology,Predicting m-health acceptance from the perspective of unified theory of acceptance and use of technology,"Abstract Addressing the growing popularity of mobile health (m-Health) technology in the health industry, the current study examined consumers’ intention and behaviour related to the usage of digital applications based on the unified theory of acceptance and use of technology (UTAUT). In particular, this study quantitatively assessed the moderating role of perceived product value and mediating role of intention to use m-Health application among Indonesians. This study adopted a cross-sectional design and collected quantitative data from conveniently selected respondents through an online survey, which involved 2068 Telegram users in Indonesia. All data were subjected to the analysis of partial least square- structural equation modeling (PLS-SEM). The obtained results demonstrated the moderating effect of perceived product value on the relationship between intention to use m-Health application (m-health app) and actual usage of m-Health app and the mediating effects of intention to use m-Health app on the relationships of perceived critical mass, perceived usefulness, perceived convenience, perceived technology accuracy, and perceived privacy protection on actual usage of m-Health app. However, the intention to use m-Health app did not mediate the influence of health consciousness and health motivation on the actual usage of m-Health app. Overall, this study’s findings on the significance of intention to use m-Health app and perceived product value based on the UTAUT framework serve as insightful guideline to expand the usage of m-Health app among consumers.",No methods found.
2024,https://openalex.org/W4390665001,Psychology,A deep learning model for brain age prediction using minimally preprocessed T1w images as input,"Introduction In the last few years, several models trying to calculate the biological brain age have been proposed based on structural magnetic resonance imaging scans (T1-weighted MRIs, T1w) using multivariate methods and machine learning. We developed and validated a convolutional neural network (CNN)-based biological brain age prediction model that uses one T1w MRI preprocessing step when applying the model to external datasets to simplify implementation and increase accessibility in research settings. Our model only requires rigid image registration to the MNI space, which is an advantage compared to previous methods that require more preprocessing steps, such as feature extraction. Methods We used a multicohort dataset of cognitively healthy individuals (age range = 32.0–95.7 years) comprising 17,296 MRIs for training and evaluation. We compared our model using hold-out (CNN1) and cross-validation (CNN2–4) approaches. To verify generalisability, we used two external datasets with different populations and MRI scan characteristics to evaluate the model. To demonstrate its usability, we included the external dataset’s images in the cross-validation training (CNN3). To ensure that our model used only the brain signal on the image, we also predicted brain age using skull-stripped images (CNN4). Results: The trained models achieved a mean absolute error of 2.99, 2.67, 2.67, and 3.08 years for CNN1–4, respectively. The model’s performance in the external dataset was in the typical range of mean absolute error (MAE) found in the literature for testing sets. Adding the external dataset to the training set (CNN3), overall, MAE is unaffected, but individual cohort MAE improves (5.63–2.25 years). Salience maps of predictions reveal that periventricular, temporal, and insular regions are the most important for age prediction. Discussion We provide indicators for using biological (predicted) brain age as a metric for age correction in neuroimaging studies as an alternative to the traditional chronological age. In conclusion, using different approaches, our CNN-based model showed good performance using one T1w brain MRI preprocessing step. The proposed CNN model is made publicly available for the research community to be easily implemented and used to study ageing and age-related disorders.","<method>multivariate methods</method>, <method>machine learning</method>, <method>convolutional neural network (CNN)</method>, <method>hold-out approach</method>, <method>cross-validation approach</method>"
2024,https://openalex.org/W4390952671,Psychology,How service robots’ human-like appearance impacts consumer trust: a study across diverse cultures and service settings,"Purpose This study aims to compares the effects of different human-like appearances (low vs. medium vs. high) of service robots (SRs) on consumer trust in service robots (CTSR), examines the mediating role of perceived warmth (WA) and perceived competence (CO) and demonstrates the moderating role of culture and service setting. Design/methodology/approach The research design includes three scenario-based experiments (Chinese hotel setting, American hotel setting, Chinese hospital setting). Findings Study 1 found SR’s human-like appearance can arouse perceived anthropomorphism (PA), which positively affects CTSR through parallel mediators (WA and CO). Study 2 revealed consumers from Chinese (vs. American) culture had higher CTSR. Study 3 showed consumers had higher WA and CO for SRs in the credence (vs. experience) service setting. The authors also had an exploratory analysis of the uncanny valley phenomenon. Practical implications The findings have practical implications for promoting the diffusion of SRs in the hospitality industry. Managers can increase CTSR by augmenting the anthropomorphic design of SRs; however, they must consider the differences in this effect across all service recipients (consumers from different cultures) and service settings. Originality/value The authors introduce WA and CO as mediators between PA and CTSR and set the culture and service setting as moderators.",No methods found.
2024,https://openalex.org/W4391035240,Psychology,Detection of epileptic seizure in EEG signals using machine learning and deep learning techniques,"Abstract Around 50 million individuals worldwide suffer from epilepsy, a chronic, non-communicable brain disorder. Several screening methods, including electroencephalography, have been proposed to identify epileptic episodes. EEG data, which are frequently utilised to enhance epilepsy analysis, offer essential information on the electrical processes of the brain. Prior to the emergence of deep learning (DL), feature extraction was accomplished by standard machine learning techniques. As a result, they were only as good as the people who made the features by hand. But with DL, both feature extraction and classification are fully automated. These methods have significantly advanced several fields of medicine, including the diagnosis of epilepsy. In this paper, the works focused on automated epileptic seizure detection using ML and DL techniques are presented as well as their comparative analysis is done. The UCI-Epileptic Seizure Recognition dataset is used for training and validation. Some of the conventional ML and DL algorithms are used with a proposed model which uses long short-term memory (LSTM) to find the best approach. Post that comparative analysis is performed on these algorithms to find the best approach for epileptic seizure detection. As a result, the proposed model LSTM gives a validation accuracy of 97% giving the most appropriate and precise result as compared to other mentioned algorithms used in this study.","<method>machine learning</method>, <method>deep learning</method>, <method>long short-term memory (LSTM)</method>"
2024,https://openalex.org/W4391164242,Psychology,Socially Aware Synthetic Data Generation for Suicidal Ideation Detection Using Large Language Models,"Suicidal ideation detection is a vital research area that holds great potential for improving mental health support systems. However, the sensitivity surrounding suicide-related data poses challenges in accessing large-scale, annotated datasets necessary for training effective machine learning models. To address this limitation, we introduce an innovative strategy that leverages the capabilities of generative AI models, such as ChatGPT, Flan-T5, and Llama, to create synthetic data for suicidal ideation detection. Our data generation approach is grounded in social factors extracted from psychology literature and aims to ensure coverage of essential information related to suicidal ideation. In our study, we benchmarked against state-of-the-art NLP classification models, specifically, those centered around the BERT family structures. When trained on the real-world dataset, UMD, these conventional models tend to yield F1-scores ranging from 0.75 to 0.87. Our synthetic data-driven method, informed by social factors, offers consistent F1-scores of 0.82 for both models, suggesting that the richness of topics in synthetic data can bridge the performance gap across different model complexities. Most impressively, when we combined a mere 30% of the UMD dataset with our synthetic data, we witnessed a substantial increase in performance, achieving an F1-score of 0.88 on the UMD test set. Such results underscore the cost-effectiveness and potential of our approach in confronting major challenges in the field, such as data scarcity and the quest for diversity in data representation.","<method>generative AI models</method>, <method>ChatGPT</method>, <method>Flan-T5</method>, <method>Llama</method>, <method>NLP classification models</method>, <method>BERT family structures</method>"
2024,https://openalex.org/W4392138877,Psychology,Prevalence and risk factors analysis of postpartum depression at early stage using hybrid deep learning model,"Postpartum Depression Disorder (PPDD) is a prevalent mental health condition and results in severe depression and suicide attempts in the social community. Prompt actions are crucial in tackling PPDD, which requires a quick recognition and accurate analysis of the probability factors associated with this condition. This concern requires attention. The primary aim of our research is to investigate the feasibility of anticipating an individual's mental state by categorizing individuals with depression from those without depression using a dataset consisting of text along with audio recordings from patients diagnosed with PPDD. This research proposes a hybrid PPDD framework that combines Improved Bi-directional Long Short-Term Memory (IBi-LSTM) with Transfer Learning (TL) based on two Convolutional Neural Network (CNN) architectures, respectively CNN-text and CNN audio. In the proposed model, the CNN section efficiently utilizes TL to obtain crucial knowledge from text and audio characteristics, whereas the improved Bi-LSTM module combines written material and sound data to obtain intricate chronological interpersonal relationships. The proposed model incorporates an attention technique to augment the effectiveness of the Bi-LSTM scheme. An experimental analysis is conducted on the PPDD online textual and speech audio dataset collected from UCI. It includes textual features such as age, women's health tracks, medical histories, demographic information, daily life metrics, psychological evaluations, and 'speech records' of PPDD patients. Data pre-processing is applied to maintain the data integrity and achieve reliable model performance. The proposed model demonstrates a great performance in better precision, recall, accuracy, and F1-score over existing deep learning models, including VGG-16, Base-CNN, and CNN-LSTM. These metrics indicate the model's ability to differentiate among women at risk of PPDD vs. non-PPDD. In addition, the feature importance analysis demonstrates that specific risk factors substantially impact the prediction of PPDD. The findings of this research establish a basis for improved precision and promptness in assessing the risk of PPDD, which may ultimately result in earlier implementation of interventions and the establishment of support networks for women who are susceptible to PPDD.","<method>Improved Bi-directional Long Short-Term Memory (IBi-LSTM)</method>, <method>Transfer Learning (TL)</method>, <method>Convolutional Neural Network (CNN)</method>, <method>Attention technique</method>, <method>VGG-16</method>, <method>Base-CNN</method>, <method>CNN-LSTM</method>"
2024,https://openalex.org/W4399244247,Psychology,Investigating influencing factors of learning satisfaction in AI ChatGPT for research: University students perspective,"This study investigates the determinants of ChatGPT adoption among university students and its impact on learning satisfaction. Utilizing the Technology Acceptance Model (TAM) and incorporating insights from interaction learning, collaborative learning, and information quality, a structural equation modeling approach was employed. This research collected valuable responses from 262 students at King Faisal University in Saudi Arabia through the use of self-report questionnaires. The data's reliability and validity were assessed using confirmation factor analysis, followed by path analysis to explore the hypotheses in the proposed model. The results indicate the pivotal roles of interaction learning and collaborative learning in fostering ChatGPT adoption. Social interaction played a significant role, as researchers engaging in conversations and knowledge-sharing expressed increased comfort with ChatGPT. Information quality was found to substantially influence researchers' decisions to continue using ChatGPT, emphasizing the need for ongoing improvement in the accuracy and relevance of content provided. Perceived ease of use and perceived usefulness played intermediary roles in linking ChatGPT engagement to learning satisfaction. User-friendly interfaces and perceived utility were identified as crucial factors affecting overall satisfaction levels. Notably, ChatGPT positively impacted learning motivation, indicating its potential to enhance student engagement and interest in learning. The study's findings have implications for educational practitioners seeking to improve the implementation of AI technologies in university students, emphasizing user-friendly design, collaborative learning, and factors influencing satisfaction. The study concludes with insights into the complex interplay between AI-powered tools, learning objectives, and motivation, highlighting the need for continued research to comprehensively understand these dynamics. This study investigates the determinants of ChatGPT adoption among university students and its impact on learning satisfaction. Utilizing the Technology Acceptance Model (TAM) and incorporating insights from interaction learning, collaborative learning, and information quality, a structural equation modeling approach was employed. This research collected valuable responses from 262 students at King Faisal University in Saudi Arabia through the use of self-report questionnaires. The data's reliability and validity were assessed using confirmation factor analysis, followed by path analysis to explore the hypotheses in the proposed model. The results indicate the pivotal roles of interaction learning and collaborative learning in fostering ChatGPT adoption. Social interaction played a significant role, as researchers engaging in conversations and knowledge-sharing expressed increased comfort with ChatGPT. Information quality was found to substantially influence researchers' decisions to continue using ChatGPT, emphasizing the need for ongoing improvement in the accuracy and relevance of content provided. Perceived ease of use and perceived usefulness played intermediary roles in linking ChatGPT engagement to learning satisfaction. User-friendly interfaces and perceived utility were identified as crucial factors affecting overall satisfaction levels. Notably, ChatGPT positively impacted learning motivation, indicating its potential to enhance student engagement and interest in learning. The study's findings have implications for educational practitioners seeking to improve the implementation of AI technologies in university students, emphasizing user-friendly design, collaborative learning, and factors influencing satisfaction. The study concludes with insights into the complex interplay between AI-powered tools, learning objectives, and motivation, highlighting the need for continued research to comprehensively understand these dynamics.","<method>Technology Acceptance Model (TAM)</method>, <method>structural equation modeling</method>, <method>confirmation factor analysis</method>, <method>path analysis</method>"
2024,https://openalex.org/W4399363436,Psychology,Collective Constitutional AI: Aligning a Language Model with Public Input,"There is growing consensus that language model (LM) developers should not be the sole deciders of LM behavior, creating a need for methods that enable the broader public to collectively shape the behavior of LM systems that affect them. To address this need, we present Collective Constitutional AI (CCAI): a multi-stage process for sourcing and integrating public input into LMs—from identifying a target population to sourcing principles to training and evaluating a model. We demonstrate the real-world practicality of this approach by creating what is, to our knowledge, the first LM fine-tuned with collectively sourced public input and evaluating this model against a baseline model trained with established principles from a LM developer. Our quantitative evaluations demonstrate several benefits of our approach: the CCAI-trained model shows lower bias across nine social dimensions compared to the baseline model, while maintaining equivalent performance on language, math, and helpful-harmless evaluations. Qualitative comparisons of the models suggest that the models differ on the basis of their respective constitutions, e.g., when prompted with contentious topics, the CCAI-trained model tends to generate responses that reframe the matter positively instead of a refusal. These results demonstrate a promising, tractable pathway toward publicly informed development of language models.","<method>Collective Constitutional AI (CCAI)</method>, <method>fine-tuning</method>"
2024,https://openalex.org/W2121920665,Psychology,IMPLEMENTASI PENDIDIKAN KARAKTER DI SDIT NURUL ILMI KOTA JAMBI,"This research aims to describe the implementation of character education in SDIT Nurul Ilmi of Jambi include: planning, implementation, and evaluation. The study used a qualitative descriptive approach. The findings of the study are: Character education planning is done by integrating the main character and values matched with the standards of com-petence and basic competence, suitability with the material. Hence, it is translated into a learning device. Character values developed or implemented through habit and exemplary school culture and the implementation of character education in learning is divided into three sections, firstly it is integrated into the subjects, Second in local content, and the third, through the self-development of habit, exemplary, and extracurricular. Character education in extracurricular activities carried out by entering the value of the main characters in each activity option. The Barriers of implementation charactered are, students character, families, and communities. Facilities and supporting facilities play an important role in the integration of the character value. Character education evaluation process carried out during the learning process and when students interact outside the classroom. The research is included (1) a well planned character education is learning device. It is supported by religious and other characters, (2) the implementation of character education in learning is integrated into every subject, local content, and habituation in school; educational character in extracurricular containing noble values; implementation barriers such as student characteristics, family environment and society; carrying adequate infrastructure and facilities, (3) evaluation process conducted continuous character education. Keyword: Implementation, Character Education, SDIT.",No methods found.
2024,https://openalex.org/W3186881383,Psychology,Graph Autoencoders for Embedding Learning in Brain Networks and Major Depressive Disorder Identification,"Brain functional connectivity (FC) networks inferred from functional magnetic resonance imaging (fMRI) have shown altered or aberrant brain functional connectome in various neuropsychiatric disorders. Recent application of deep neural networks to connectome-based classification mostly relies on traditional convolutional neural networks (CNNs) using input FCs on a regular Euclidean grid to learn spatial maps of brain networks neglecting the topological information of the brain networks, leading to potentially sub-optimal performance in brain disorder identification. We propose a novel graph deep learning framework that leverages non-Euclidean information inherent in the graph structure for classifying brain networks in major depressive disorder (MDD). We introduce a novel graph autoencoder (GAE) architecture, built upon graph convolutional networks (GCNs), to embed the topological structure and node content of large fMRI networks into low-dimensional representations. For constructing the brain networks, we employ the Ledoit-Wolf (LDW) shrinkage method to efficiently estimate high-dimensional FC metrics from fMRI data. We explore both supervised and unsupervised techniques for graph embedding learning. The resulting embeddings serve as feature inputs for a deep fully-connected neural network (FCNN) to distinguish MDD from healthy controls (HCs). Evaluating our model on resting-state fMRI MDD dataset, we observe that the GAE-FCNN outperforms several state-of-the-art methods for brain connectome classification, achieving the highest accuracy when using LDW-FC edges as node features. The graph embeddings of fMRI FC networks also reveal significant group differences between MDD and HCs. Our framework demonstrates the feasibility of learning graph embeddings from brain networks, providing valuable discriminative information for diagnosing brain disorders.","<method>deep neural networks</method>, <method>convolutional neural networks (CNNs)</method>, <method>graph deep learning framework</method>, <method>graph autoencoder (GAE)</method>, <method>graph convolutional networks (GCNs)</method>, <method>supervised techniques for graph embedding learning</method>, <method>unsupervised techniques for graph embedding learning</method>, <method>deep fully-connected neural network (FCNN)</method>"
2024,https://openalex.org/W4391560128,Psychology,Analyzing Preceding factors affecting behavioral intention on communicational artificial intelligence as an educational tool,"During the pandemic, artificial intelligence was employed and utilized by students around the globe. Students' conduct changed in a variety of ways when schooling returned to regular instruction. This study aimed to analyze the student's behavioral intention and actual academic use of communicational AI (CAI) as an educational tool. This study identified the variables by utilizing an integrated framework based on the Unified Theory of Acceptance and Use of Technology (UTAUT2) and self-determination theory. Through the use of an online survey and Structural Equation Modeling, data from 533 respondents were analyzed. The results showed that perceived relatedness has the most significant effect on the behavioral intention of students in using CAI as an educational tool, followed by perceived autonomy. It showed that students use CAI based on the objective and the possibility of increasing their productivity, rather than any other purpose in the education setting. Among the UTAUT2 domains, only facilitating conditions, habit, and performance expectancy provided a significant direct effect on behavioral intention and an indirect effect on actual academic use. Further implications were presented. Moreover, the methodology and framework of this study could be extended and applied to educational technology-related studies. Lastly, the outcome of this study may be considered in analyzing the behavioral intention of the students as the teaching-learning environment is still continuously expanding and developing.",<method>Structural Equation Modeling</method>
2024,https://openalex.org/W4391655051,Psychology,Do large language models show decision heuristics similar to humans? A case study using GPT-3.5.,"A Large Language Model (LLM) is an artificial intelligence system trained on vast amounts of natural language data, enabling it to generate human-like responses to written or spoken language input. Generative Pre-Trained Transformer (GPT)-3.5 is an example of an LLM that supports a conversational agent called ChatGPT. In this work, we used a series of novel prompts to determine whether ChatGPT shows heuristics and other context-sensitive responses. We also tested the same prompts on human participants. Across four studies, we found that ChatGPT was influenced by random anchors in making estimates (anchoring, Study 1); it judged the likelihood of two events occurring together to be higher than the likelihood of either event occurring alone, and it was influenced by anecdotal information (representativeness and availability heuristic, Study 2); it found an item to be more efficacious when its features were presented positively rather than negatively-even though both presentations contained statistically equivalent information (framing effect, Study 3); and it valued an owned item more than a newly found item even though the two items were objectively identical (endowment effect, Study 4). In each study, human participants showed similar effects. Heuristics and context-sensitive responses in humans are thought to be driven by cognitive and affective processes such as loss aversion and effort reduction. The fact that an LLM-which lacks these processes-also shows such responses invites consideration of the possibility that language is sufficiently rich to carry these effects and may play a role in generating these effects in humans. (PsycInfo Database Record (c) 2024 APA, all rights reserved).","<method>Large Language Model (LLM)</method>, <method>Generative Pre-Trained Transformer (GPT)-3.5</method>"
2024,https://openalex.org/W4392542342,Psychology,Detecting ChatGPT-Generated Code Submissions in a CS1 Course Using Machine Learning Models,"The emergence of publicly accessible large language models (LLMs) such as ChatGPT poses unprecedented risks of new types of plagiarism and cheating where students use LLMs to solve exercises for them. Detecting this behavior will be a necessary component in introductory computer science (CS1) courses, and educators should be well-equipped with detection tools when the need arises. However, ChatGPT generates code non-deterministically, and thus, traditional similarity detectors might not suffice to detect AI-created code. In this work, we explore the affordances of Machine Learning (ML) models for the detection task. We used an openly available dataset of student programs for CS1 assignments and had ChatGPT generate code for the same assignments, and then evaluated the performance of both traditional machine learning models and Abstract Syntax Tree-based (AST-based) deep learning models in detecting ChatGPT code from student code submissions. Our results suggest that both traditional machine learning models and AST-based deep learning models are effective in identifying ChatGPT-generated code with accuracy above 90%. Since the deployment of such models requires ML knowledge and resources that are not always accessible to instructors, we also explore the patterns detected by deep learning models that indicate possible ChatGPT code signatures, which instructors could possibly use to detect LLM-based cheating manually. We also explore whether explicitly asking ChatGPT to impersonate a novice programmer affects the code produced. We further discuss the potential applications of our proposed models for enhancing introductory computer science instruction.","<method>traditional machine learning models</method>, <method>Abstract Syntax Tree-based (AST-based) deep learning models</method>"
2024,https://openalex.org/W4393078946,Psychology,Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic Motivation Reinforcement Learning Algorithms for Improved Training and Adaptability,"End-to-end multi-task dialogue systems are usually designed with separate modules for the dialogue pipeline. Among these, the policy module is essential for deciding what to do in response to user input. This policy is trained by reinforcement learning algorithms by taking advantage of an environment in which an agent receives feedback in the form of a reward signal. The current dialogue systems, however, only provide meagre and simplistic rewards. Investigating intrinsic motivation reinforcement learning algorithms is the goal of this study. Through this, the agent can quickly accelerate training and improve its capacity to judge the quality of its actions by teaching it an internal incentive system. In particular, we adapt techniques for random network distillation and curiosity-driven reinforcement learning to measure the frequency of state visits and encourage exploration by using semantic similarity between utterances. Experimental results on MultiWOZ, a heterogeneous dataset, show that intrinsic motivation-based debate systems outperform policies that depend on extrinsic incentives. By adopting random network distillation, for example, which is trained using semantic similarity between user-system dialogues, an astounding average success rate of 73% is achieved. This is a significant improvement over the baseline Proximal Policy optimization (PPO), which has an average success rate of 60%. In addition, performance indicators such as booking rates and completion rates show a 10% rise over the baseline. Furthermore, these intrinsic incentive models help improve the system's policy's resilience in an increasing amount of domains. This implies that they could be useful in scaling up to settings that cover a wider range of domains.","<method>reinforcement learning algorithms</method>, <method>intrinsic motivation reinforcement learning algorithms</method>, <method>random network distillation</method>, <method>curiosity-driven reinforcement learning</method>, <method>Proximal Policy Optimization (PPO)</method>"
2024,https://openalex.org/W4394633221,Psychology,An efficient Parkinson's disease detection framework: Leveraging time-frequency representation and AlexNet convolutional neural network,"Parkinson's disease (PD) is a progressive neurodegenerative disorder affecting the quality of life of over 10 million individuals worldwide. Early diagnosis is crucial for timely intervention and better patient outcomes. Electroencephalogram (EEG) signals are commonly used for early PD diagnosis due to their potential in monitoring disease progression. But traditional EEG-based methods lack exploration of brain regions that provide essential information about PD, and their performance falls short for real-time applications. To address these limitations, this study proposes a novel approach using a Time-Frequency Representation (TFR) based AlexNet Convolutional Neural Network (CNN) model to explore EEG channel-based analysis and identify critical brain regions efficiently diagnosing PD from EEG data. The Wavelet Scattering Transform (WST) is employed to capture distinct temporal and spectral characteristics, while AlexNet CNN is utilized to detect complex spatial patterns at different scales, accurately identifying intricate EEG patterns associated with PD. The experiment results on two real-time EEG PD datasets: San Diego dataset and the Iowa dataset demonstrate that frontal and central brain regions, including AF4 and AFz electrodes, contribute significantly to providing more representative features compared to other regions for PD detection. The proposed architecture achieves an impressive accuracy of 99.84% for the San Diego dataset and 95.79% for the Iowa dataset, outperforming existing EEG-based PD detection methods. The findings of this research will assist to create an essential technology for efficient PD diagnosis, enhancing patient care and quality of life.","<method>Time-Frequency Representation (TFR) based AlexNet Convolutional Neural Network (CNN)</method>, <method>Wavelet Scattering Transform (WST)</method>, <method>AlexNet CNN</method>"
2024,https://openalex.org/W4400580797,Psychology,Promoting well-being through happiness at work: a systematic literature review and future research agenda,"Purpose Our study aims to understand what is known about happiness at work (HAW) in terms of publication, citations, dimensions and characteristics, as well as how knowledge about HAW is generated regarding theoretical frameworks, context and methods. Additionally, it explores future directions for HAW research. Design/methodology/approach This paper conducts a systematic literature review of 56 empirical articles published between 2000 and 2022 to comprehensively explore HAW. It examines publication trends, citation patterns, dimensions, characteristics, theoretical frameworks, contextual factors and research methodologies employed in HAW studies. Findings Our findings suggest that while HAW research has gained momentum, there is still a need for exploration, particularly in developing countries. Various theoretical frameworks such as the job demand-resources model, social exchange theory and broaden-and-build theory are identified, with suggestions for the adoption of less popular theories like the positive emotion, engagement, relationships, meaning and accomplishment (PERMA) model and flow theory for future investigations. The review contributes to workplace happiness literature by offering a comprehensive analysis spanning two decades and provides valuable insights for guiding future research toward exploring factors influencing employee well-being. Originality/value Our article offers a structured analysis of HAW literature, emphasizing the necessity for more extensive research, especially in developing nations. It provides valuable insights into the theories and dimensions associated with HAW, guiding future research and assisting organizations in formulating strategies to enhance employee happiness and overall well-being.",No methods found.
2024,https://openalex.org/W4401947187,Psychology,Cooperative thalamocortical circuit mechanism for sensory prediction errors,"Abstract The brain functions as a prediction machine, utilizing an internal model of the world to anticipate sensations and the outcomes of our actions. Discrepancies between expected and actual events, referred to as prediction errors, are leveraged to update the internal model and guide our attention towards unexpected events 1–10 . Despite the importance of prediction-error signals for various neural computations across the brain, surprisingly little is known about the neural circuit mechanisms responsible for their implementation. Here we describe a thalamocortical disinhibitory circuit that is required for generating sensory prediction-error signals in mouse primary visual cortex (V1). We show that violating animals’ predictions by an unexpected visual stimulus preferentially boosts responses of the layer 2/3 V1 neurons that are most selective for that stimulus. Prediction errors specifically amplify the unexpected visual input, rather than representing non-specific surprise or difference signals about how the visual input deviates from the animal’s predictions. This selective amplification is implemented by a cooperative mechanism requiring thalamic input from the pulvinar and cortical vasoactive-intestinal-peptide-expressing (VIP) inhibitory interneurons. In response to prediction errors, VIP neurons inhibit a specific subpopulation of somatostatin-expressing inhibitory interneurons that gate excitatory pulvinar input to V1, resulting in specific pulvinar-driven response amplification of the most stimulus-selective neurons in V1. Therefore, the brain prioritizes unpredicted sensory information by selectively increasing the salience of unpredicted sensory features through the synergistic interaction of thalamic input and neocortical disinhibitory circuits.",No methods found.
2024,https://openalex.org/W4391023547,Psychology,Multimodal diagnosis model of Alzheimer’s disease based on improved Transformer,"Abstract Purpose Recent technological advancements in data acquisition tools allowed neuroscientists to acquire different modality data to diagnosis Alzheimer’s disease (AD). However, how to fuse these enormous amount different modality data to improve recognizing rate and find significance brain regions is still challenging. Methods The algorithm used multimodal medical images [structural magnetic resonance imaging (sMRI) and positron emission tomography (PET)] as experimental data. Deep feature representations of sMRI and PET images are extracted by 3D convolution neural network (3DCNN). An improved Transformer is then used to progressively learn global correlation information among features. Finally, the information from different modalities is fused for identification. A model-based visualization method is used to explain the decisions of the model and identify brain regions related to AD. Results The model attained a noteworthy classification accuracy of 98.1% for Alzheimer’s disease (AD) using the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset. Upon examining the visualization results, distinct brain regions associated with AD diagnosis were observed across different image modalities. Notably, the left parahippocampal region emerged consistently as a prominent and significant brain area. Conclusions A large number of comparative experiments have been carried out for the model, and the experimental results verify the reliability of the model. In addition, the model adopts a visualization analysis method based on the characteristics of the model, which improves the interpretability of the model. Some disease-related brain regions were found in the visualization results, which provides reliable information for AD clinical research.","<method>3D convolution neural network (3DCNN)</method>, <method>improved Transformer</method>, <method>model-based visualization method</method>"
2024,https://openalex.org/W4392053865,Psychology,The intention to adopt metaverse in Islamic banks: an integrated theoretical framework of TAM and religiosity intention model,"Purpose This paper aims to explore the intention to adopt the Metaverse in Islamic banks, with a particular focus on evaluating perceived usefulness, ease of use, user satisfaction and the influence of religiosity. Integrating the technology adoption model (TAM) and religiosity intention model, this study will dissect the multidimensional aspects influencing the acceptance of Metaverse technologies. Design/methodology/approach Surveying Islamic bank professionals in Jordan, this study used a structured questionnaire and data augmentation to analyze Metaverse adoption factors. Using partial least squares-structural equation modeling, the relationships between ease of use, usefulness, religiosity and satisfaction were explored. Findings The study identifies pivotal relationships among perceived usefulness, ease of use, user satisfaction and religiosity in the context of adopting metaverse technologies in Islamic banks in Jordan. Evidence highlights the dominant role of perceived usefulness and ease in influencing both intention to use and satisfaction levels. Religiosity, while not a direct influencer, plays a collaborative role, underscoring a balanced mix of technological and religious elements that will potentially shape the adoption trajectory of metaverse technologies within this specific banking sector. Practical implications Integrating metaverse technologies in Islamic banks necessitates balancing technological advances with Sharia compliance. The study underscores the importance of aligning user experience with religious values and fostering innovation within Islamic ethical guidelines. Originality/value This study uniquely integrates the TAM and religiosity-intention model to explore metaverse adoption in Islamic banks, unveiling a nuanced interplay between technology and religious values. It offers practical insights for tailoring innovations in the Islamic financial ecosystem.",No methods found.
2024,https://openalex.org/W4392190113,Psychology,Factors influencing academic performance and dropout rates in higher education,"The aim of this study was to identify and evaluate the most frequently used research methods and factors influencing academic performance, based on a pool of 95 studies, published after 2012. We considered only peer-reviewed papers containing 78 empirical and 17 meta-analytic studies. Our theoretical background lies in the different approaches of the terms 'university dropout' and 'academic performance'. After the systematic analysis we ascertained the most commonly used methods are Educational Data Mining (EDM) algorithms (decision tree, logistic regression and neural networks) and Structural Equation Modelling (SEM). The strength of the predictive power depends on the dataset, however Support Vector Machines, Multilayer Perceptron, Naïve Bayes algorithm were found to be the most precise in prediction. Regarding factors influencing academic performance we derived our results based on 600,000 university students. Considering the data from meta-analyses and systematic reviews, reaching up to 900 studies, we found grade point average (GPA), obtained credits (ECTS) and gender to be the most consistent and decisive predictors of academic performance. Nevertheless, GPA and ECTS (as output variables) are mediated by student factors (intrinsic motivation, self-regulated learning strategies, self-efficacy, prior education) and throughput factors (work, finances, academic engagement). We had contradictory results on age and family background.","<method>Educational Data Mining (EDM) algorithms</method>, <method>decision tree</method>, <method>logistic regression</method>, <method>neural networks</method>, <method>Structural Equation Modelling (SEM)</method>, <method>Support Vector Machines</method>, <method>Multilayer Perceptron</method>, <method>Naïve Bayes algorithm</method>"
2024,https://openalex.org/W4392891497,Psychology,Learners’ continuance intention in multimodal language learning education: An innovative multiple linear regression model,"Confronted with the unprecedented COVID-19 pandemic, millions of learners have received, are receiving, or will receive multimodal language learning education. This study aims to explore the relationships between various factors influencing learners' continuance intention by proposing an innovative multiple linear regression model in multimodal language learning education. Participants were randomly recruited (N = 334) in China who had received multimodal language learning education by combining Massive Open Online Courses, Rain Classroom, and WeChat. The research instrument, a comprehensive questionnaire, was sent through the online system named Questionnaire Star developed by technical experts. A multiple linear regression analysis was adopted to test the proposed hypotheses and fit the research model. This study confirms the relationships between the Technology Acceptance Model-inclusive constructs such as perceived ease of use, perceived usefulness, attitudes toward multimodal language learning education, and continuance intention of participating in multimodal language learning education. The Technology Acceptance Model is also associated with other constructs, e.g. Task-technology fit, Individual-technology fit, Openness, and Reputation of multimodal language learning educational institutes, and personal investment in multimodal language learning education. However, personal investment neither directly nor indirectly predicts continuance intention. Educators and designers could make every effort to improve multimodal language learning education to enhance personal investment and foster its association with continuance intention of learners.",<method>multiple linear regression</method>
2024,https://openalex.org/W4396853240,Psychology,Research on consumers’ purchase intention of cultural and creative products—Metaphor design based on traditional cultural symbols,"Chinese traditional cultural symbols possess great aesthetic and cultural value, and are widely utilized in product design. In this study, we explore the relationship between metaphor design based on traditional cultural symbols, customer experience and cultural identity, and further estimate how these three variables stimulate consumers’ perceived value to generate consumers’ purchase intention. Based on existing traditional cultural literature and Stimulus-organism-response theory (SOR), we proposed a theoretical research model to characterize the relationship among metaphor design based on traditional cultural symbols, customer experience, cultural identity, perceived value and consumers’ purchase intention. A research survey was conducted and 262 questionnaires were collected in total with 241 valid. We used Smart PLS graph version 3.0 for data analysis. Results indicate that the cognition of metaphor design based on traditional cultural symbols and customer experience has a direct and significant impact on the emotional value thereby, eliciting consumers’ purchase intention, metaphor design based on traditional cultural symbols is directly and indirectly (i.e., through customer experience or perceived value) positively associated with consumers’ purchase intention, also customer experience is directly and indirectly (i.e., through perceived value) associated with consumer purchase intention, cultural identity mediates the indirect effect of customer experience and perceived value on purchase intention, the moderating role of cultural identity between customer experience and perceived value is not significant. Our findings help to expand the existing literature on consumer purchase intentions by rationally using traditional cultural symbols in the product metaphor design.",No methods found.
2024,https://openalex.org/W4390618032,Psychology,An interpretable model based on graph learning for diagnosis of Parkinson’s disease with voice-related EEG,"Abstract Parkinson’s disease (PD) exhibits significant clinical heterogeneity, presenting challenges in the identification of reliable electroencephalogram (EEG) biomarkers. Machine learning techniques have been integrated with resting-state EEG for PD diagnosis, but their practicality is constrained by the interpretable features and the stochastic nature of resting-state EEG. The present study proposes a novel and interpretable deep learning model, graph signal processing-graph convolutional networks (GSP-GCNs), using event-related EEG data obtained from a specific task involving vocal pitch regulation for PD diagnosis. By incorporating both local and global information from single-hop and multi-hop networks, our proposed GSP-GCNs models achieved an averaged classification accuracy of 90.2%, exhibiting a significant improvement of 9.5% over other deep learning models. Moreover, the interpretability analysis revealed discriminative distributions of large-scale EEG networks and topographic map of microstate MS5 learned by our models, primarily located in the left ventral premotor cortex, superior temporal gyrus, and Broca’s area that are implicated in PD-related speech disorders, reflecting our GSP-GCN models’ ability to provide interpretable insights identifying distinctive EEG biomarkers from large-scale networks. These findings demonstrate the potential of interpretable deep learning models coupled with voice-related EEG signals for distinguishing PD patients from healthy controls with accuracy and elucidating the underlying neurobiological mechanisms.","<method>machine learning techniques</method>, <method>deep learning models</method>, <method>graph signal processing-graph convolutional networks (GSP-GCNs)</method>"
2024,https://openalex.org/W4390672472,Psychology,Demystifying the effect of social media usage and eWOM on purchase intention: the mediating role of brand equity,"Purpose The purpose of this study is to examine an integrated model, in which brand equity (BE) mediates the effects of social media usage (SMU) and electronic word of mouth (eWOM) on purchase intentions among Indian consumers of branded apparel. Design/methodology/approach An online questionnaire was used to collect data from 317 Indian customers of branded apparel, and the data were analyzed using the partial least squares structural equation modeling (PLS-SEM) with the help of SmartPLS version 4. Findings First, the results indicated that SMU, eWOM and BE significantly impact consumers purchase intention; at the same time, BE is influenced by SMU and eWOM. Second, results confirmed that BE partially mediates the effects of SMU and eWOM on the purchase intentions of consumers of apparel brands. Research limitations/implications The study's dataset is limited in its generalizability as it is based on specific responses from Indian consumers of branded apparel via an online survey. The results of this study would help marketers and advertisers create customized advertising campaigns for the people who are most likely to buy their products. Marketers can also use social media to promote the uniqueness or point of difference (PoD) of their apparel brands. Originality/value To the best of the authors' knowledge, no study has been conducted on apparel brands in the Indian context that has tested an integrative model, in which BE mediates the effects of SMU and eWOM on the purchase intentions of customers of apparel brands.",No methods found.
2024,https://openalex.org/W4390812034,Psychology,The Utility of AI in Writing a Scientific Review Article on the Impacts of COVID-19 on Musculoskeletal Health,"Abstract Purpose of Review There were two primary purposes to our reviews. First, to provide an update to the scientific community about the impacts of COVID-19 on musculoskeletal health. Second, was to determine the value of using a large language model, ChatGPT 4.0, in the process of writing a scientific review article. To accomplish these objectives, we originally set out to write three review articles on the topic using different methods to produce the initial drafts of the review articles. The first review article was written in the traditional manner by humans, the second was to be written exclusively using ChatGPT (AI-only or AIO), and the third approach was to input the outline and references selected by humans from approach 1 into ChatGPT, using the AI to assist in completing the writing (AI-assisted or AIA). All review articles were extensively fact-checked and edited by all co-authors leading to the final drafts of the manuscripts, which were significantly different from the initial drafts. Recent Findings Unfortunately, during this process, it became clear that approach 2 was not feasible for a very recent topic like COVID-19 as at the time, ChatGPT 4.0 had a cutoff date of September 2021 and all articles published after this date had to be provided to ChatGPT, making approaches 2 and 3 virtually identical. Therefore, only two approaches and two review articles were written (human and AI-assisted). Here we found that the human-only approach took less time to complete than the AI-assisted approach. This was largely due to the number of hours required to fact-check and edit the AI-assisted manuscript. Of note, the AI-assisted approach resulted in inaccurate attributions of references (about 20%) and had a higher similarity index suggesting an increased risk of plagiarism. Summary The main aim of this project was to determine whether the use of AI could improve the process of writing a scientific review article. Based on our experience, with the current state of technology, it would not be advised to solely use AI to write a scientific review article, especially on a recent topic.","<method>large language model, ChatGPT 4.0</method>"
2024,https://openalex.org/W4390881691,Psychology,"Uncertainty Reduction in Flood Susceptibility Mapping Using Random Forest and eXtreme Gradient Boosting Algorithms in Two Tropical Desert Cities, Shibam and Marib, Yemen","Flooding is a natural disaster that coexists with human beings and causes severe loss of life and property worldwide. Although numerous studies for flood susceptibility modelling have been introduced, a notable gap has been the overlooked or reduced consideration of the uncertainty in the accuracy of the produced maps. Challenges such as limited data, uncertainty due to confidence bounds, and the overfitting problem are critical areas for improving accurate models. We focus on the uncertainty in susceptibility mapping, mainly when there is a significant variation in the predictive relevance of the predictor factors. It is also noted that the receiver operating characteristic (ROC) curve may not accurately depict the sensitivity of the resulting susceptibility map to overfitting. Therefore, reducing the overfitting problem was targeted to increase accuracy and improve processing time in flood prediction. This study created a spatial repository to test the models, containing data from historical flooding and twelve topographic and geo-environmental flood conditioning variables. Then, we applied random forest (RF) and extreme gradient boosting (XGB) algorithms to map flood susceptibility, incorporating a variable drop-off in the empirical loop function. The results showed that the drop-off loop function was a crucial method to resolve the model uncertainty associated with the conditioning factors of the susceptibility modelling and methods. The results showed that approximately 8.42% to 9.89% of Marib City and 9.93% to 15.69% of Shibam City areas were highly vulnerable to floods. Furthermore, this study significantly contributes to worldwide endeavors focused on reducing the hazards linked to natural disasters. The approaches used in this study can offer valuable insights and strategies for reducing natural disaster risks, particularly in Yemen.","<method>random forest (RF)</method>, <method>extreme gradient boosting (XGB)</method>"
2024,https://openalex.org/W4390959437,Psychology,Machine learning model (RG-DMML) and ensemble algorithm for prediction of students’ retention and graduation in education,"Automated prediction of students' retention and graduation in education using advanced analytical methods such as artificial intelligence (AI), has recently attracted the attention of educators, both in theory and in practice. Whereas invaluable insights and theories for measuring and testing the topic have been proposed, most of the existing methods do not technically highlight the non-trivial factors behind the renowned challenges and attrition. To this effect, by making use of two categories of data collected in a higher education setting about students (i) retention (n = 52262) and (ii) graduation (n = 53639); this study proposes a machine learning model - RG-DMML (retention and graduation data mining and machine learning) and ensemble algorithm for prediction of students' retention and graduation status in education. This was done by training and testing key features that are technically deemed suitable for measuring the constructs (retention and graduation), such as (i) the Average grade of the previous high school, and (ii) the Entry/admission score. The proposed model (RG-DMML) is designed based on the cross industry standard process for data mining (CRISP-DM) methodology, implemented using supervised machine learning technique such as K-Nearest Neighbor (KNN), and validated using the k-fold cross-validation method. The results show that the executed model and algorithm based on the Bagging method and 10-fold cross-validation are efficient and effective for predicting the student's retention and graduation status, with Precision (retention = 0.909, graduation = 0.822), Recall (retention = 1.000, graduation = 0.957), Accuracy (retention = 0.909, graduation = 0.817), F1-Score (retention = 0.952, graduation = 0.885) showing significant high accuracy levels or performance rate, and low Error-rate (retention = 0.090, graduation = 0.182), respectively. In addition, by considering the individual features selected through the Wrapper method in predicting the outputs, the proposed model proved more effective for predicting the students' retention status in comparison to the graduation data. The implications of the models' output and factors that impact the effective prediction or identification of at-risk students, e.g., for timely intervention, counselling, decision-making, and sustainable educational practice are empirically discussed in the study.","<method>machine learning model - RG-DMML (retention and graduation data mining and machine learning)</method>, <method>ensemble algorithm</method>, <method>cross industry standard process for data mining (CRISP-DM) methodology</method>, <method>supervised machine learning technique</method>, <method>K-Nearest Neighbor (KNN)</method>, <method>k-fold cross-validation method</method>, <method>Bagging method</method>, <method>Wrapper method</method>"
2024,https://openalex.org/W4391062514,Psychology,Translating a value-based framework for resilient e-learning impact in post COVID-19 times: Research-based Evidence from Higher Education in Kuwait,"The covid-19 pandemic has changed people's daily lives and behaviors all across the world and has impacted practically every element of human existence. The introduction of remote education systems and the move toward online learning have had some of the most significant effects. The on-site operations of educational institutions, such as schools, colleges, and universities, have had to be suspended in order to stop the virus' spread. In order to effectively disseminate instructional material and guarantee the unbroken progression of students' academic endeavors, educators have been forced to look for novel approaches. The study used the Value-Based Adoption Model (VAM) as a conceptual framework to look into the factors that affected Kuwait's e-learning outcomes in the midst of the covid-19 pandemic. 382 students at Kuwaiti universities and colleges were the source of quantitative data collection. The findings revealed that peer interaction emerged as the most influential factor in shaping outcomes within the educational context of Kuwait, while instructors and course design factors were not significant. Using the VAM, this study investigated the impact of several factors on students' e-learning results during times of crisis. The research expands the existing knowledge base in the field on this subject and suggests developing a well-organized online learning crisis approach. The main contribution of this work is summarized on (i) An integrated framework for the quality of the e-learning experience in universities in post-covid-19 times and (ii) A resilient higher education institutional learning strategy model in post-covid-19 times. The findings of this paper can be generalizable to other Gulf Corporation Council (GCC) countries such as Kingdom of Saudi Arabia, Qatar, United Arab Emirates (UAE), Bahrain and Oman. This is due to the shared cultural traditions and values, along with similar educational systems among these nations.",No methods found.
2024,https://openalex.org/W4391346549,Psychology,CSR and employee outcomes: a systematic literature review,"Abstract The purpose of this research is to consolidate and extend the current literature on employee outcomes of CSR (referred to as micro-level outcomes). The authors use a systematic review of the literature as a method to summarize and synthesise the different effects of CSR activities on employees based on 270 journal articles. The contribution of this paper is that it provides a comprehensive list of employee outcomes classified into different categories and a conceptual framework that maps desirable and undesirable outcomes of CSR activities on employees. The results show that various dimensions of CSR have different effects on employee outcomes. In addition, we explain mediators of CSR-employee outcomes relationships and moderators that could strengthen or weaken this relationship. The review reveals important gaps and offers a research agenda for the future. We have found only a few studies dealing with the negative impacts of CSR on employees as well as only a few studies that explain how different dimensions of CSR affect employees differently. The study has also practical implications for companies, as understanding different effects of CSR on employees helps organizations to design and implement CSR strategies and policies that foster employees’ positive attitudes and behaviours as well as prevent or reduce the negative effects, and hence create a business value and sustainable growth for the company.",No methods found.
2024,https://openalex.org/W4391649643,Psychology,Diagnosis of Alzheimer's disease via optimized lightweight convolution-attention and structural MRI,"Alzheimer's disease (AD) poses a substantial public health challenge, demanding accurate screening and diagnosis. Identifying AD in its early stages, including mild cognitive impairment (MCI) and healthy control (HC), is crucial given the global aging population. Structural magnetic resonance imaging (sMRI) is essential for understanding the brain's structural changes due to atrophy. While current deep learning networks overlook voxel long-term dependencies, vision transformers (ViT) excel at recognizing such dependencies in images, making them valuable in AD diagnosis. Our proposed method integrates convolution-attention mechanisms in transformer-based classifiers for AD brain datasets, enhancing performance without excessive computing resources. Replacing multi-head attention with lightweight multi-head self-attention (LMHSA), employing inverted residual (IRU) blocks, and introducing local feed-forward networks (LFFN) yields exceptional results. Training on AD datasets with a gradient-centralized optimizer and Adam achieves an impressive accuracy rate of 94.31% for multi-class classification, rising to 95.37% for binary classification (AD vs. HC) and 92.15% for HC vs. MCI. These outcomes surpass existing AD diagnosis approaches, showcasing the model's efficacy. Identifying key brain regions aids future clinical solutions for AD and neurodegenerative diseases. However, this study focused exclusively on the AD Neuroimaging Initiative (ADNI) cohort, emphasizing the need for a more robust, generalizable approach incorporating diverse databases beyond ADNI in future research.","<method>vision transformers (ViT)</method>, <method>convolution-attention mechanisms in transformer-based classifiers</method>, <method>lightweight multi-head self-attention (LMHSA)</method>, <method>inverted residual (IRU) blocks</method>, <method>local feed-forward networks (LFFN)</method>, <method>gradient-centralized optimizer</method>, <method>Adam</method>"
2024,https://openalex.org/W4391755073,Psychology,Specifying cross-system collaboration strategies for implementation: a multi-site qualitative study with child welfare and behavioral health organizations,"Abstract Background Cross-system interventions that integrate health, behavioral health, and social services can improve client outcomes and expand community impact. Successful implementation of these interventions depends on the extent to which service partners can align frontline services and organizational operations. However, collaboration strategies linking multiple implementation contexts have received limited empirical attention. This study identifies, describes, and specifies multi-level collaboration strategies used during the implementation of Ohio Sobriety Treatment and Reducing Trauma (Ohio START), a cross-system intervention that integrates services across two systems (child welfare and evidence-based behavioral health services) for families that are affected by co-occurring child maltreatment and parental substance use disorders. Methods In phase 1, we used a multi-site qualitative design with 17 counties that implemented Ohio START. Qualitative data were gathered from 104 staff from child welfare agencies, behavioral health treatment organizations, and regional behavioral health boards involved in implementation via 48 small group interviews about collaborative approaches to implementation. To examine cross-system collaboration strategies, qualitative data were analyzed using an iterative template approach and content analysis. In phase 2, a 16-member expert panel met to validate and specify the cross-system collaboration strategies identified in the interviews. The panel was comprised of key child welfare and behavioral health partners and scholars. Results In phase 1, we identified seven cross-system collaboration strategies used for implementation. Three strategies were used to staff the program: (1) contract for expertise, (2) provide joint supervision, and (3) co-locate staff. Two strategies were used to promote service access: (4) referral protocols and (5) expedited access agreements. Two strategies were used to align case plans: (6) shared decision-making meetings, and (7) sharing data. In phase 2, expert panelists specified operational details of the cross-system collaboration strategies, and explained the processes by which strategies were perceived to improve implementation and service system outcomes. Conclusions We identified a range of cross-system collaboration strategies that show promise for improving staffing, service access, and case planning. Leaders, supervisors, and frontline staff used these strategies during all phases of implementation. These findings lay the foundation for future experimental and quasi-experimental studies that test the effectiveness of cross-system collaboration strategies.",No methods found.
2024,https://openalex.org/W4392156210,Psychology,Servant leadership style and socially responsible leadership in university context: moderation of promoting sense of community,"Purpose The purpose of this study is to examine the extent to which promoting sense of community moderates the relationship between servant leadership style and socially responsible leadership (SRL) of public universities in Uganda. Design/methodology/approach The study adopted cross-sectional survey design to collect data at one point in time using self-administered questionnaires from 214 respondents to examine the relationship between servant leadership and socially responsible leadership with promoting sense of community as a moderator. The study used statistical package for social scientists (SPSS) PROCESS MACRO to establish clusters among the surveyed public universities and later a model was derived. Findings The study found a significant moderating effect of promoting sense of community on servant leadership and socially responsible leadership. Implying that investment in promoting sense of community creates awareness about the socially responsible leadership in public universities. Practical implications Managers of public universities need to pay keen interest in promoting sense of community to boost socially responsible leadership by building a strong servant leadership style through promoting sense of community for senior managers and leaders especially heads of departments, faculty deans and principals in public universities. Originality/value This study contributes to socially responsible leadership literature by advancing the idea that SRL is an important resource that enhances through instituting servant leadership and promoting sense of community in a complex environment. Ideally, servant leadership and promoting sense of community is one of the drivers of customer value, efficiency and effectiveness of public universities.",No methods found.
2024,https://openalex.org/W4392955615,Psychology,Variation in social media sensitivity across people and contexts,"Abstract Social media impacts people’s wellbeing in different ways, but relatively little is known about why this is the case. Here we introduce the construct of “social media sensitivity” to understand how social media and wellbeing associations differ across people and the contexts in which these platforms are used. In a month-long large-scale intensive longitudinal study (total n = 1632; total number of observations = 120,599), we examined for whom and under which circumstances social media was associated with positive and negative changes in social and affective wellbeing. Applying a combination of frequentist and Bayesian multilevel models, we found a small negative average association between social media use AND subsequent wellbeing, but the associations were heterogenous across people. People with psychologically vulnerable dispositions (e.g., those who were depressed, lonely, not satisfied with life) tended to experience heightened negative social media sensitivity in comparison to people who were not psychologically vulnerable. People also experienced heightened negative social media sensitivity when in certain types of places (e.g., in social places, in nature) and while around certain types of people (e.g., around family members, close ties), as compared to using social media in other contexts. Our results suggest that an understanding of the effects of social media on wellbeing should account for the psychological dispositions of social media users, and the physical and social contexts surrounding their use. We discuss theoretical and practical implications of social media sensitivity for scholars, policymakers, and those in the technology industry.","<method>frequentist multilevel models</method>, <method>Bayesian multilevel models</method>"
2024,https://openalex.org/W4394581105,Psychology,Machine learning-based detection of acute psychosocial stress from body posture and movements,"Abstract Investigating acute stress responses is crucial to understanding the underlying mechanisms of stress. Current stress assessment methods include self-reports that can be biased and biomarkers that are often based on complex laboratory procedures. A promising additional modality for stress assessment might be the observation of body movements, which are affected by negative emotions and threatening situations. In this paper, we investigated the relationship between acute psychosocial stress induction and body posture and movements. We collected motion data from N = 59 individuals over two studies ( Pilot Study : N = 20, Main Study : N = 39) using inertial measurement unit (IMU)-based motion capture suits. In both studies, individuals underwent the Trier Social Stress Test (TSST) and a stress-free control condition (friendly-TSST; f-TSST) in randomized order. Our results show that acute stress induction leads to a reproducible freezing behavior, characterized by less overall motion as well as more and longer periods of no movement. Based on these data, we trained machine learning pipelines to detect acute stress solely from movement information, achieving an accuracy of $${75.0 \pm 17.7}{\%}$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mrow> <mml:mrow> <mml:mn>75.0</mml:mn> <mml:mo>±</mml:mo> <mml:mn>17.7</mml:mn> </mml:mrow> <mml:mo>%</mml:mo> </mml:mrow> </mml:math> ( Pilot Study ) and $${73.4 \pm 7.7}{\%}$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mrow> <mml:mrow> <mml:mn>73.4</mml:mn> <mml:mo>±</mml:mo> <mml:mn>7.7</mml:mn> </mml:mrow> <mml:mo>%</mml:mo> </mml:mrow> </mml:math> ( Main Study ). This, for the first time, suggests that body posture and movements can be used to detect whether individuals are exposed to acute psychosocial stress. While more studies are needed to further validate our approach, we are convinced that motion information can be a valuable extension to the existing biomarkers and can help to obtain a more holistic picture of the human stress response. Our work is the first to systematically explore the use of full-body body posture and movement to gain novel insights into the human stress response and its effects on the body and mind.",<method>machine learning pipelines</method>
2024,https://openalex.org/W4396832329,Psychology,Rethinking Human-AI Collaboration in Complex Medical Decision Making: A Case Study in Sepsis Diagnosis,"Today's AI systems for medical decision support often succeed on benchmark datasets in research papers but fail in real-world deployment. This work focuses on the decision making of sepsis, an acute life-threatening systematic infection that requires an early diagnosis with high uncertainty from the clinician. Our aim is to explore the design requirements for AI systems that can support clinical experts in making better decisions for the early diagnosis of sepsis. The study begins with a formative study investigating why clinical experts abandon an existing AI-powered Sepsis predictive module in their electrical health record (EHR) system. We argue that a human-centered AI system needs to support human experts in the intermediate stages of a medical decision-making process (e.g., generating hypotheses or gathering data), instead of focusing only on the final decision. Therefore, we build SepsisLab based on a state-of-the-art AI algorithm and extend it to predict the future projection of sepsis development, visualize the prediction uncertainty, and propose actionable suggestions (i.e., which additional laboratory tests can be collected) to reduce such uncertainty. Through heuristic evaluation with six clinicians using our prototype system, we demonstrate that SepsisLab enables a promising human-AI collaboration paradigm for the future of AI-assisted sepsis diagnosis and other high-stakes medical decision making.",<method>state-of-the-art AI algorithm</method>
2024,https://openalex.org/W4390508699,Psychology,Burnout among public health workers in Canada: a cross-sectional study,"Abstract Background This study presents the prevalence of burnout among the Canadian public health workforce after three years of the COVID-19 pandemic and its association with work-related factors. Methods Data were collected using an online survey distributed through Canadian public health associations and professional networks between November 2022 and January 2023. Burnout was measured using a modified version of the Oldenburg Burnout Inventory (OLBI). Logistic regressions were used to model the relationship between burnout and work-related factors including years of work experience, redeployment to pandemic response, workplace safety and supports, and harassment. Burnout and the intention to leave or retire as a result of the COVID-19 pandemic was explored using multinomial logistic regressions. Results In 2,079 participants who completed the OLBI, the prevalence of burnout was 78.7%. Additionally, 49.1% of participants reported being harassed because of their work during the pandemic. Burnout was positively associated with years of work experience, redeployment to the pandemic response, being harassed during the pandemic, feeling unsafe in the workplace and not being offered workplace supports. Furthermore, burnout was associated with greater odds of intending to leave public health or retire earlier than anticipated. Conclusion The high levels of burnout among our large sample of Canadian public health workers and its association with work-related factors suggest that public health organizations should consider interventions that mitigate burnout and promote recovery.",No methods found.
2024,https://openalex.org/W4391260578,Psychology,"The effect of teacher self-efficacy, online pedagogical and content knowledge, and emotion regulation on teacher digital burnout: a mediation model","Abstract Background With the increasing prevalence of online teaching, understanding the dynamics that impact educators' well-being and effectiveness is paramount. This study addresses the interconnected relationships among online teaching competence, self-efficacy, emotion regulation, and digital burnout among teachers in the digital learning environment. Objectives The primary objectives of this research are to investigate the direct and mediated effects of online teaching competence and self-efficacy on emotion regulation and digital burnout among teachers. Additionally, the study aims to explore the mediating role of emotion regulation in the relationship between self-efficacy and digital burnout. The overarching goal is to provide comprehensive insights into the factors influencing teacher well-being in the online teaching context. Methodology A cross-sectional survey design was employed, involving a convenience sample of educators from a specific university. Participants responded to validated self-report measures assessing online teaching competence, self-efficacy, emotion regulation, and digital burnout. Statistical analyses, including regression and mediation analyses, were conducted to examine the relationships among the key variables. Results The findings reveal significant relationships and effects among the investigated variables. Online teaching competence is a substantial predictor of emotion regulation and digital burnout. Similarly, self-efficacy significantly impacts emotion regulation and digital burnout. Emotion regulation mediates the relationship between online teaching competence, self-efficacy, and digital burnout. These results highlight the intricate connections shaping teachers' experiences in the digital teaching environment. Conclusions and implications In conclusion, this study provides robust evidence supporting the interconnectedness of online teaching competence, self-efficacy, emotion regulation, and digital burnout among teachers. The implications underscore the importance of fostering these competencies through targeted professional development. Educational institutions and policymakers can use these insights to implement strategies that enhance teacher well-being, ultimately promoting a more effective and sustainable online teaching environment.",No methods found.
2024,https://openalex.org/W4391387532,Psychology,Live streaming E-commerce platform characteristics: Influencing consumer value co-creation and co-destruction behavior,"Live streaming with goods is developing rapidly as an emerging platform economic model. Consumers can interact with the e-commerce platform in real-time and get a good shopping experience. This in turn generates value co-creation behaviors such as word-of-mouth communication. However, due to the lack of effective audit mechanisms on some platforms, the phenomenon of value co-destruction among them is becoming more and more common. This brings value loss to consumers, e-commerce platforms, and other multiple subjects. Based on the SOR model, this paper collects 212 sample data by using a questionnaire survey. The impact of live banding platform characteristics on consumers' value co-creation and value co-destruction behaviors is analyzed through multiple regression and other methods. The results show that platform information quality, functional quality, and interactive quality all positively affect consumers' value co-creation behavior and negatively affect value co-destruction behavior. Perceived value plays a mediating role, and promotional incentives enhance the positive relationship between platform features and perceived value. Therefore, if the live streaming bandwagon platform provides consumers with sufficiently rich, complete, and timely product information, responds quickly to consumer needs, and has high interaction quality. Consumers are more likely to exhibit value co-creation behavior. This study enriches the theoretical results of value co-creation and value co-destruction. It provides theoretical references for live streaming platforms to inhibit consumers' value co-destructive behaviors and promote value co-creation behaviors. In practice, it is pointed out that live streaming platforms should provide consumers with real information on all aspects of the products, update and improve the performance of the platform in a timely manner, and provide regular training to the staff.",No methods found.
2024,https://openalex.org/W4392726867,Psychology,Dropout in online higher education: a systematic literature review,"Abstract The increased availability of technology in higher education has led to the growth of online learning platforms. However, a significant concern exists regarding dropout rates in online higher education (OHE). In this ever-evolving landscape, student attrition poses a complex challenge that demands careful investigation. This systematic literature review presents a comprehensive analysis of the literature to uncover the reasons behind dropout rates in virtual learning environments. Following the PRISMA guidelines, this study systematically identifies and elucidates the risk factors associated with dropout in online higher education. The selection process encompassed articles published between 2013 and June 2023, resulting in the inclusion of 110 relevant articles that significantly contribute to the discourse in this field. We examine demographic, course-related, technology-related, motivational, and support-related aspects that shape students’ decisions in online learning programs. The review highlights key contributors to dropout like the quality of the course, academic preparation, student satisfaction, learner motivation, system attributes, and support services. Conversely, health concerns, financial limitations, technological issues, screen fatigue, isolation, and academic workload, emerge as significant limitations reported by online learners. These insights offer a holistic understanding of dropout dynamics, guiding the development of targeted interventions and strategies to enhance the quality and effectiveness of online education.",No methods found.
2024,https://openalex.org/W4393068914,Psychology,Understanding the Sustainable Development of Community (Social) Disaster Resilience in Serbia: Demographic and Socio-Economic Impacts,"This paper presents the results of quantitative research examining the impacts of demographic and socioeconomic factors on the sustainable development of community disaster resilience. The survey was carried out utilizing a questionnaire distributed to, and subsequently collected online from, 321 participants during January 2024. The study employed an adapted version of the ‘5S’ social resilience framework (62 indicators), encompassing five sub-dimensions—social structure, social capital, social mechanisms, social equity and diversity, and social belief. To explore the relationship between predictors and the sustainable development of community disaster resilience in Serbia, various statistical methods, such as t-tests, one-way ANOVA, Pearson’s correlation, and multivariate linear regression, were used. The results of the multivariate regressions across various community disaster resilience subscales indicate that age emerged as the most significant predictor for the social structure subscale. At the same time, education stood out as the primary predictor for the social capital subscale. Additionally, employment status proved to be the most influential predictor for both social mechanisms and social equity-diversity subscales, with property ownership being the key predictor for the social beliefs subscale. The findings can be used to create strategies and interventions aimed at enhancing the sustainable development of resilience in communities in Serbia by addressing the intricate interplay between demographic characteristics, socio-economic factors, and their ability to withstand, adapt to, and recover from different disasters.",No methods found.
2024,https://openalex.org/W4394770576,Psychology,Exploring Factors That Support Pre-service Teachers’ Engagement in Learning Artificial Intelligence,"Abstract Artificial intelligence (AI) is becoming increasingly relevant, and students need to understand the concept. To design an effective AI program for schools, we need to find ways to expose students to AI knowledge, provide AI learning opportunities, and create engaging AI experiences. However, there is a lack of trained teachers who can facilitate students’ AI learning, so we need to focus on developing the capacity of pre-service teachers to teach AI. Since engagement is known to enhance learning, it is necessary to explore how pre-service teachers engage in learning AI. This study aimed to investigate pre-service teachers’ engagement with learning AI after a 4-week AI program at a university. Thirty-five participants took part in the study and reported their perception of engagement with learning AI on a 7-factor scale. The factors assessed in the survey included engagement (cognitive—critical thinking and creativity, behavioral, and social), attitude towards AI, anxiety towards AI, AI readiness, self-transcendent goals, and confidence in learning AI. We used a structural equation modeling approach to test the relationships in our hypothesized model using SmartPLS 4.0. The results of our study supported all our hypotheses, with attitude, anxiety, readiness, self-transcendent goals, and confidence being found to influence engagement. We discuss our findings and consider their implications for practice and policy.",<method>structural equation modeling</method>
2024,https://openalex.org/W4398130977,Psychology,Imagined Speech-EEG Detection Using Multivariate Swarm Sparse Decomposition-Based Joint Time-Frequency Analysis for Intuitive BCI,"In brain-computer interface (BCI) applications, imagined speech (IMS) decoding based on electroencephalography (EEG) has established a new neuro-paradigm that offers an intuitive communication tool for physically impaired patients.However, existing IMS-EEG-based BCI systems have introduced difficulties in feasible deployment due to nonstationary EEG signals, suboptimal feature extraction, and constrained multi-class scalability.To address these challenges, we have presented a novel approach using the multivariate swarm-sparse decomposition method (MSSDM) for joint time-frequency (JTF) analysis and further developed a feasible end-to-end framework from multichannel IMS-EEG signals for imagined speech detection.MSSDM employs improved multivariate swarm filtering and sparse spectrum techniques to design optimal filter banks for extracting an ensemble of channel-aligned oscillatory components (CAOCs), significantly enhancing IMS activation-related sub-bands.To enhance channelaligned information, multivariate JTF images have been constructed using joint instantaneous frequency and instantaneous amplitude across channels from the obtained CAOCs.Further, JTFbased deep features (JTFDF) were computed using different pretrained neural networks and mapped most discriminant features using two well-known feature correlation techniques: Canonical correlation analysis and Hellinger distance-based correlation.The proposed method has been tested on the 5-class BCI Competition DB and 6-class Coretto DB IMS datasets.The experimental findings on cross-subject reveal that the novel JTFDF feature-based classification model, MSSDM-SqueezeNet-JTFDF, achieved the highest classification performance against all other existing state-of-theart methods in imagined speech recognition.","<method>multivariate swarm-sparse decomposition method (MSSDM)</method>, <method>joint time-frequency (JTF) analysis</method>, <method>multivariate swarm filtering</method>, <method>sparse spectrum techniques</method>, <method>pretrained neural networks</method>, <method>Canonical correlation analysis</method>, <method>Hellinger distance-based correlation</method>, <method>MSSDM-SqueezeNet-JTFDF</method>"
2024,https://openalex.org/W4399516497,Psychology,The role and impact of ChatGPT in educational practices: insights from an Australian higher education case study,"Abstract Artificial intelligence (AI) tools, notably ChatGPT, are increasingly recognised for their transformative potential in higher education. This study employs a detailed case study approach complemented by a survey, delving into ChatGPT’s impact on pedagogical practices, student engagement, and academic performance. It involved 74 undergraduate and postgraduate students enrolled in data analytics courses in Australia. The quantitative analysis highlights ChatGPT’s role in providing personalised and on-demand support, which is highly valued among users for its flexibility and responsiveness, meeting a critical demand in educational settings. Notably, the study identifies a medium effect size ( $$\eta ^2 = 0.173$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mrow> <mml:msup> <mml:mi>η</mml:mi> <mml:mn>2</mml:mn> </mml:msup> <mml:mo>=</mml:mo> <mml:mn>0.173</mml:mn> </mml:mrow> </mml:math> ) in perceived benefits, indicating that ChatGPT accounts for approximately 17.3% of the variance in improved academic outcomes. However, challenges such as ChatGPT’s limited understanding of complex queries and the lack of human interactions are primary concerns, with a medium effect size ( $$\eta ^2 = 0.289$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mrow> <mml:msup> <mml:mi>η</mml:mi> <mml:mn>2</mml:mn> </mml:msup> <mml:mo>=</mml:mo> <mml:mn>0.289</mml:mn> </mml:mrow> </mml:math> ) suggesting significant areas for improvement. Furthermore, statistical analyses reveal a clear relationship between the frequency of ChatGPT usage and the perception of its benefits, underscoring the transformative potential for users who have integrated it into their academic practices. Despite these challenges, the differential impact on users versus non-users highlights the potential for ChatGPT to foster more engaging and effective educational practices. The findings advocate for targeted strategies to epitomise ChatGPT’s integration into educational settings, emphasising the need for ongoing research and the development of comprehensive guidelines to navigate its complexities and maximise its educational benefits.",No methods found.
2024,https://openalex.org/W4399550139,Psychology,Towards Enhancing Place Attachment in Urban Spaces of Vertical Residential Complexes (Bismayah as a Case Study),"The place is a refuge for humans and human activities and interactions. Humans give value and meaning to a place through interaction with it and attachment to it. Positive feelings towards the place and its tangible appearance are place attachment, the most important dimension in the reciprocal relationship between humans and the place that urban designers must pay attention to. As a result of urban growth and the housing crisis, open spaces have lost their value and importance, especially in urban areas in vertical residential complexes. Therefore, the research paper aimed to evaluate the effect of the social and physical aspects of open spaces in vertical residential complexes on enhancing place attachment. This theoretical and practical research relies on literary and library studies and a mixed research strategy. The research used a common methodology by discussing previous literature and extracting the main vocabulary to build the conceptual framework for each. First, the social aspects are represented by demographic and psychological factors and social activities and interactions. Secondly, the material aspects are described by formal and functional characteristics. In the practical part, a field survey was conducted. The questionnaires (150 questionnaires) were distributed using a random method at the selected case study site (Bismayah Residential Complex).The results were analysed to determine the most influential factors based on their priority in enhancing attachment to a place. Thus, the research reached the most prominent indicators that contribute to developing an attachment to a place. with regard to the social aspect Indicators of length of stay, protection and safety, and social activities and interactions had the greatest impact on developing feelings of attachment to a place. This was followed, in varying proportions, by indicators of housing ownership, comfort and tranquility, and cohesion and social communication among the residents of the complex. As for the physical aspect, the indicators of enclosure and accessibility had the largest role, followed by the indicators of dependence on place and visual richness. And arriving at the indicators with the least impact on stimulating residents' feelings towards the place in the social and physical aspects, such as privacy, social homogeneity, clarity, and the human scale",No methods found.
2024,https://openalex.org/W4400225515,Psychology,Impact of heuristic–systematic cues on the purchase intention of the electronic commerce consumer through the perception of product quality,"For electronic commerce (e-commerce) consumers, it is impossible to evaluate the quality of the products on offer as they are unable to physically test them before purchase. Therefore, sellers must convey quality cues that are readily identifiable to such consumers. However, thanks to major technological advances and the development of social interaction systems on e-commerce platforms, individuals can now access large volumes of information (reviews, opinions, ratings) posted directly by fellow consumers, which can also provide cues by which to judge product quality, pre-purchase. Based on dual-process theory and signaling theory, the objective of this study is to analyze the impact of the heuristic or systematic cues related to electronic word-of-mouth on consumer perceived product quality, to determine how this quality affects perceived product performance risk and consumer purchase intention. A quantitative approach was taken using a structured online questionnaire. Data were collected from 835 consumers of e-commerce platforms and analyzed using maximum likelihood structural equation modeling and LISREL software. The results show that the quantity of reviews, source credibility, review usefulness, and brand experience all exert a positive and significant effect on perceived product quality, which, in turn, positively and significantly influences purchase intention. A negative and significant relationship between product performance risk and purchase intention is also found. The findings contribute to the literature by improving our understanding of those determinants of perceived product quality in e-commerce that motivate consumers to make a purchase and can help sellers improve their use of integrated social interaction tools to adequately reflect the quality of their products.",No methods found.
2024,https://openalex.org/W4401403528,Psychology,Financial innovation and gender dynamics: a comparative study of male and female FinTech adoption in emerging economies,"Purpose This paper aims to identify the factors influencing the adoption of financial technology (FinTech) services among Indian residents. Moreover, it compares the awareness levels among both male and female users to offer a comprehensive insight into FinTech adoption. Design/methodology/approach The research comprises two cross-sectional surveys utilizing self-administered questionnaires: Study A involves 411 male participants and Study B involves 473 female users in FinTech adoption. This article used a “Statistical Package for Social Science (SPSS) followed by partial least squares-structural equation modeling (PLS-SEM)” for data analysis. Findings The exciting finding reveals that attitude and personal innovativeness have a significant impact, while technology anxiety shows a statistically insignificant impact on awareness in both studies. Surprisingly, the socio-demographic factor significantly impacts awareness (in Study A) and has an insignificant impact on awareness in Study B. Moreover, both studies reveal that awareness significantly impacts perceived usefulness and ease of use. Additionally, the outcomes confirm a positive relation between awareness, perceived usefulness, ease of use and FinTech adoption in both studies. Practical implications The present research will offer valuable insights to all FinTech service providers and stakeholders, aiding them in planning and designing relevant policies. Originality/value As far as the researchers are aware, this study stands as the initial survey into FinTech that specifically examines the impact of gender on technology adoption. The divergence in awareness and adoption rates between males and females and the authors’ insightful findings illuminate the context's uniqueness. Moreover, this article offers a robust model for using FinTech services from the perspective of a developing economy.",<method>partial least squares-structural equation modeling (PLS-SEM)</method>
2024,https://openalex.org/W4390588030,Psychology,Enterprise social media as enablers of employees' agility: the impact of work stress and enterprise social media visibility,"Purpose According to extensive analysis, employee agility is influenced by teamwork, coordination and the organizational environment. However, less consideration has been given to the role of work stressors (challenge, hindrance) in influencing employee agility. To address this research gap, this study sheds light on how the use of enterprise social media (ESM) for social and work purposes influences employee agility through work stressors. Design/methodology/approach This research also explores how ESM visibility enhances the interaction between work stressors and employee agility by using primary data obtained from Chinese workers. A total of 377 entries were analyzed using AMOS 24.10 tools. All the hypotheses were tested using structural equation modeling (SEM). Findings The findings revealed that ESM use (social and work) negatively impacts challenge and hindrance work stressors. The results also reflect that challenge stressors have a significant impact on employee agility, whereas hindrance stressors are negatively related to it. Furthermore, the outcome also indicated that increased ESM visibility reinforces the connection between challenge stressors and employee agility. However, ESM visibility did not indicate a significant moderating impact on the link between hindrance stressors and employee agility. Originality/value This study describes how ESM usage effects agility of stressed employees. This research also explores how ESM visibility improves the interaction between work stressors and employee agility. The study results contribute to growing research on social media and employee agility and suggest several points of guidance for managers.",No methods found.
2024,https://openalex.org/W4390796562,Psychology,"If we build it together, will they use it? A mixed-methods study evaluating the implementation of Prep-to-Play PRO: an injury prevention programme for women’s elite Australian Football","Objectives We evaluated the implementation of Prep-to-Play PRO, an injury prevention programme for women’s elite Australian Football League (AFLW). Methods The Reach, Effectiveness, Adoption, Implementation and Maintenance (RE-AIM) of Prep-to-Play PRO were assessed based on the proportion of AFLW players and/or staff who: were aware of the programme (R), believed it may reduce anterior cruciate ligament injury (E), attempted to implement any/all programme components (A), implemented all intended components as practically as possible (I) and intended future programme implementation (M). Quantitative and qualitative data were triangulated to assess 58 RE-AIM items (evidence of yes/no/unsure/no evidence) and the 5 RE-AIM dimensions (fully achieved=evidence of yes on &gt;50% dimension items, partially achieved=50% of items evidence of yes and 50% unsure or 50% mix of unsure and unanswered, or not met=evidence of yes on &lt;50% dimension items). Results Multiple sources including AFLW training observations (n=7 total), post-implementation surveys (141 players, 25 staff), semistructured interviews (19 players, 13 staff) and internal programme records (9 staff) contributed to the RE-AIM assessment. After the 2019 season, 8 of 10 (80%) AFLW clubs fully met all five RE-AIM dimensions. All 10 clubs participating in the AFLW fully achieved the reach (R) dimension. One club partially achieved the implementation (I) dimension, and one club partially achieved the effectiveness (E) and adoption (A) dimensions. Conclusion The Prep-to-Play PRO injury prevention programme for the AFLW achieved high implementation, possibly due to the programme’s deliberately flexible approach coupled with our pragmatic definition of implementation. Engaging key stakeholders at multiple ecological levels (organisation, coaches, athletes) throughout programme development and implementation likely enhanced programme implementation.",No methods found.
2024,https://openalex.org/W4390880467,Psychology,Experience-driven well-being: the case of unmanned smart hotels,"Purpose Drawing on the theory of memory-dominant logic, this study aims to examine how the substantive staging of the servicescape, experience co-creation, experiential satisfaction and experience intensification affect experience memorability and hedonic well-being in the case of unmanned smart hotels. Design/methodology/approach An online survey was used, with the target respondents being hotel guests people aged 18 years and older who had been recent guests of the FlyZoo Hotel in Hangzhou, China. Data were collected online from 429 guests who had stayed in the hotel between April and June 2023. Data analysis was undertaken using structural equation modelling. Findings The results suggest that all the proposed four constructs are positive drivers of a memorable unmanned smart hotel experience. The relationship between the memorability of the hotel experience and hedonic well-being was found to be significant and positive. Practical implications Unmanned smart hotels should ensure that all smart technologies function effectively and dependably and offer highly personalised services to guests, allowing them to co-create their experiences. This will lead to the guest receiving a satisfying and memorable experience. To enable experience co-creation using smart technologies, unmanned smart hotels could provide short instructional videos for guests, as well as work closely with manufacturers and suppliers to ensure that smart technology systems are regularly updated. Originality/value This study investigates the antecedents and outcomes of a novel phenomenon and extends the concept of memorable tourism experiences to the context of unmanned smart hotels.",No methods found.
2024,https://openalex.org/W4390953182,Psychology,ERTNet: an interpretable transformer-based framework for EEG emotion recognition,"Background Emotion recognition using EEG signals enables clinicians to assess patients’ emotional states with precision and immediacy. However, the complexity of EEG signal data poses challenges for traditional recognition methods. Deep learning techniques effectively capture the nuanced emotional cues within these signals by leveraging extensive data. Nonetheless, most deep learning techniques lack interpretability while maintaining accuracy. Methods We developed an interpretable end-to-end EEG emotion recognition framework rooted in the hybrid CNN and transformer architecture. Specifically, temporal convolution isolates salient information from EEG signals while filtering out potential high-frequency noise. Spatial convolution discerns the topological connections between channels. Subsequently, the transformer module processes the feature maps to integrate high-level spatiotemporal features, enabling the identification of the prevailing emotional state. Results Experiments’ results demonstrated that our model excels in diverse emotion classification, achieving an accuracy of 74.23% ± 2.59% on the dimensional model (DEAP) and 67.17% ± 1.70% on the discrete model (SEED-V). These results surpass the performances of both CNN and LSTM-based counterparts. Through interpretive analysis, we ascertained that the beta and gamma bands in the EEG signals exert the most significant impact on emotion recognition performance. Notably, our model can independently tailor a Gaussian-like convolution kernel, effectively filtering high-frequency noise from the input EEG data. Discussion Given its robust performance and interpretative capabilities, our proposed framework is a promising tool for EEG-driven emotion brain-computer interface.","<method>hybrid CNN and transformer architecture</method>, <method>temporal convolution</method>, <method>spatial convolution</method>, <method>transformer module</method>, <method>CNN</method>, <method>LSTM</method>"
2024,https://openalex.org/W4391060616,Psychology,When leadership goes awry: the nexus between tyrannical leadership and knowledge hiding,"Purpose Grounded on the conservation of resources (COR) theory, this study examines the relationship between tyrannical leadership and knowledge hiding. Additionally, this study aims to investigate the mediating role of psychological distress and the moderating role of psychological safety. Design/methodology/approach Data was gathered from 435 employees in the corporate sector in China. The study used the partial least squares structural equation modelling approach to assess the proposed connections and analysed the data collected with the help of SmartPLS 4 software. Findings In the study, it was found that there is a positive relationship between tyrannical leadership and knowledge hiding, and this association is mediated by psychological distress. Additionally, the results asserted that the positive effect of tyrannical leadership on knowledge hiding through psychological distress is less pronounced when there is a greater degree of psychological safety. Practical implications Leaders should avoid being tyrannical and adopt a supportive leadership style. They should be aware of the effects of their behaviour on employee well-being, provide resources to help employees cope with distress and foster a culture of psychological safety. This approach promotes knowledge sharing, innovation and employee well-being within the organisation. Originality/value This study contributes to the existing literature by investigating a new factor that influences knowledge hiding: tyrannical leadership. Furthermore, it explains that employees who experience tyrannical leadership are more prone to psychological distress, such as anxiety and fear, and are likelier to engage in knowledge-hiding behaviours. Finally, the study identifies psychological safety as a factor that can mitigate the negative effects of tyrannical leadership on knowledge hiding.",No methods found.
2024,https://openalex.org/W4391103294,Psychology,"Integrated Hydrological Modeling for Watershed Analysis, Flood Prediction, and Mitigation Using Meteorological and Morphometric Data, SCS-CN, HEC-HMS/RAS, and QGIS","Flooding is a natural disaster with extensive impacts. Desert regions face altered flooding patterns owing to climate change, water scarcity, regulations, and rising water demands. This study assessed and predicted flash flood hazards by calculating discharge volume, peak flow, flood depth, and velocity using the Hydrologic Engineering Centre-River Analysis System and Hydrologic Modelling System (HEC-HMS and HEC-RAS) software. We employed meteorological and morphological data analyses, incorporating the soil conservation service (SCS) curve number method for precipitation losses and the SCS-Hydrograph for runoff transformation. The model was applied to two drainage basins (An-Nawayah and Al-Rashrash) in southeastern Cairo, Egypt, which recently encountered several destructive floods. The applied model revealed that 25-, 50-, and 100-year storms produced runoff volumes of 2461.8 × 103, 4299.6 × 103, and 5204.5 × 103 m3 for An-Nawayah and 6212 × 103, 8129.4 × 103, and 10,330.6 × 103 m3 for Al-Rashrash, respectively. Flood risk levels, categorised as high (35.6%), extreme (21.9%), and medium (21.12%) were assessed in low- and very-low-hazard areas. The study highlighted that the areas closer to the Nile River mouth faced greater flood impacts from torrential rain. Our findings demonstrate the effectiveness of these methods in assessing and predicting flood risk. As a mitigation measure, this study recommends the construction of five 10 m high dams to create storage lakes. This integrated approach can be applied to flood risk assessment and mitigation in comparable regions.",No methods found.
2024,https://openalex.org/W4392449443,Psychology,RGBT Tracking via Challenge-Based Appearance Disentanglement and Interaction,"RGB and thermal source data suffer from both shared and specific challenges, and how to explore and exploit them plays a critical role in representing the target appearance in RGBT tracking. In this paper, we propose a novel approach, which performs target appearance representation disentanglement and interaction via both modality-shared and modality-specific challenge attributes, for robust RGBT tracking. In particular, we disentangle the target appearance representations via five challenge-based branches with different structures according to their properties, including three parameter-shared branches to model modality-shared challenges and two parameter-independent branches to model modality-specific challenges. Considering the complementary advantages between modality-specific cues, we propose a guidance interaction module to transfer discriminative features from one modality to another one to enhance the discriminative ability of weak modality. Moreover, we design an aggregation interaction module to combine all challenge-based target representations, which could form more discriminative target representations and fit the challenge-agnostic tracking process. These challenge-based branches are able to model the target appearance under certain challenges so that the target representations can be learned by a few parameters even in the situation of insufficient training data. In addition, to relieve labor costs and avoid label ambiguity, we design a generation strategy to generate training data with different challenge attributes. Comprehensive experiments demonstrate the superiority of the proposed tracker against the state-of-the-art methods on four benchmark datasets.","<method>target appearance representation disentanglement</method>, <method>guidance interaction module</method>, <method>aggregation interaction module</method>, <method>generation strategy to generate training data with different challenge attributes</method>"
2024,https://openalex.org/W4392762829,Psychology,"Nexus between perception, purpose of use, technical challenges and satisfaction for mobile financial services: theory and empirical evidence from Bangladesh","Purpose This study analyzed the relationship between mobile financial services (MFS) usage and customer satisfaction with MFS in Bangladesh, considering perception, purpose of use and technical challenges as the primary factors influencing customer satisfaction with MFS. The aim is to determine the factors most influencing the use of MFS. Design/methodology/approach Data were collected from 400 MFS users through a structured web survey using snowball sampling that is consistent with the nature of MFS users who are difficult to identify or locate. Structural equation modeling (SEM) was used to analyze the data and evaluate the reliability and validity of the measurement model. Findings The results show that customers’ perceptions and satisfaction significantly impact their intention to use MFS. Specifically, customers’ perceptions strongly influence their satisfaction with MFS, and the purpose of use significantly predicts both perception and satisfaction. Technical problems and challenges were found to have no significant impact on satisfaction levels, but other factors were more critical. Furthermore, the integration of innovative technological solutions is crucial for fostering sustainability in MFS, as it enhances reliability and efficiency while minimizing environmental footprints. Research limitations/implications The study was conducted in a single country, relied on self-reported data, and used a cross-sectional design, which limits the ability to draw causal inferences. Future research could explore the factors that influence customer satisfaction with MFS in different countries and regions and incorporate additional variables to provide a more comprehensive understanding of the drivers of customer satisfaction with MFS. Originality/value This study significantly contributes by extending the technology acceptance model (TAM) framework with the innovation resistance theory, offering a nuanced understanding of MFS adoption. The findings challenge conventional wisdom, highlighting the limited impact of technical problems on satisfaction and emphasizing the central role of user perceptions in shaping satisfaction and intention to use.",<method>Structural equation modeling (SEM)</method>
2024,https://openalex.org/W4392858515,Psychology,Key factors influencing the e-government adoption: a systematic literature review,"Purpose This study aimed to identify and analyse the key factors influencing the adoption of e-government services and to discern their implications for various stakeholders, from policymakers to platform developers. Design/methodology/approach Through a comprehensive review of existing literature and detailed analysis of multiple studies, this research organised the influential factors based on their effect: highest, direct and indirect. The study also integrated findings to present a consolidated view of e-government adoption drivers. Findings The research found that users' behaviour, attitude, optimism bias and subjective norms significantly shape their approach to e-government platforms. Trust in e-Government (TEG) emerged as a critical determinant, with security perceptions being of paramount importance. Additionally, non-technical factors, such as cultural, religious and social influences, play a substantial role in e-government adoption decisions. The study also highlighted the importance of performance expectancy, effect expectancy and other determinants influencing e-government adoption. Originality/value While numerous studies have explored e-government adoption, this research offers a novel classification based on the relative effects of each determinant. Integrating findings from diverse studies and emphasising non-technical factors introduce an interdisciplinary approach, bridging the gap between information technology and fields like sociology, anthropology and behavioural sciences. This integrative lens provides a fresh perspective on the topic, encouraging more holistic strategies for enhancing e-government adoption globally.",No methods found.
2024,https://openalex.org/W4393218816,Psychology,A study on smart home use intention of elderly consumers based on technology acceptance models,"Purpose Smart home devices have great potential to improve the quality of life and independence of older people, positively impacting their health, safety, and comfort. However, Chinese research in this field is still in its early stages. Therefore, more comprehensive and in-depth studies are needed to comprehend the various aspects influencing the acceptance and use of smart homes by older users. Patients and methods This study adopted the Technology Acceptance Model (TAM) and included perceived usefulness, perceived ease of use, usage intention, intergenerational technology support, perceived value, and perceived risk as extension variables to delve deeper into the behavioral intentions of older users in smart home services. The study used a convenience sampling method to randomly distribute 236 questionnaires among older adults over the age of 60 in the school’s community and neighboring urban communities who have experience in smart home use and who can complete human-computer interactions either independently or with the help of others, mainly focusing on the four sections: user characteristics, family situation, experience of use, and usage intention. The study used structural equation modeling (SEM) and factor analysis to analyze the completion of questionnaires. Finally, we conducted a validation analysis of the rationality and scientificity of the model and derived the six dimensions of the model of the influencing factors on the use of smart home products by the elderly and the weight sizes of their corresponding 13 influencing factors. Results The results show that perceived usefulness and perceived ease of use have a positive effect on users’ intention to use smart homes. Perceived ease of use has a positive effect on the perceived usefulness of smart homes. In addition, intergenerational technology support, perceived value, and perceived risk impact users’ perceived usefulness and perceived ease of use of the smart home. Conclusion This research aims to describe the factors influencing older users’ willingness to use smart homes. The findings are not only significant for the elderly in China but also of broad value to other regions and countries facing similar demographic challenges. The development of smart homes not only involves the elderly but is also closely related to all segments of society. The government should increase policy support and guide more social forces to participate in the development of the smart home industry. Service providers and designers should fully understand the demand situation and user experience of target users to develop easy-to-use smart home solutions. At the same time, smart homes, as intelligent products for the elderly, need to focus not only on the basic needs of the elderly such as material life and home safety, but also on the spiritual needs of elderly users. Children or caregivers should always pay attention to the psychological state of the elderly and actively guide them to use smart homes to help them realize their self-worth. We look forward to more research focusing on this area in the future and further exploring the specific issues and solutions involved.","<method>Technology Acceptance Model (TAM)</method>, <method>structural equation modeling (SEM)</method>, <method>factor analysis</method>"
2024,https://openalex.org/W4394948580,Psychology,Inductive Cognitive Diagnosis for Fast Student Learning in Web-Based Intelligent Education Systems,"Cognitive diagnosis aims to gauge students' mastery levels based on their response logs.Serving as a pivotal module in web-based online intelligent education systems (WOIESs), it plays an upstream and fundamental role in downstream tasks like learning item recommendation and computerized adaptive testing.WOIESs are open learning environment where numerous new students constantly register and complete exercises.In WOIESs, efficient cognitive diagnosis is crucial to fast feedback and accelerating student learning.However, the existing cognitive diagnosis methods always employ intrinsically transductive student-specific embeddings, which become slow and costly due to retraining when dealing with new students who are unseen during training.To this end, this paper proposes an inductive cognitive diagnosis model (ICDM) for fast new students' mastery levels inference in WOIESs.Specifically, in ICDM, we propose a novel student-centered graph (SCG).Rather than inferring mastery levels through updating student-specific embedding, we derive the inductive mastery levels as the aggregated outcomes of students' neighbors in SCG.Namely, SCG enables to shift the task from finding the most suitable student-specific embedding that fits the response logs to finding the most suitable representations for different node types in SCG, and the latter is more efficient since it no longer requires retraining.To obtain this representation, ICDM consists of a construction-aggregation-generationtransformation process to learn the final representation of students, exercises and concepts.Extensive experiments across real-world datasets show that, compared with the existing cognitive diagnosis methods that are always transductive, ICDM is much more faster while maintains the competitive inference performance for new students.","<method>inductive cognitive diagnosis model (ICDM)</method>, <method>student-centered graph (SCG)</method>"
2024,https://openalex.org/W4400196518,Psychology,"Loyalty toward shared e-scooter: Exploring the role of service quality, satisfaction, and environmental consciousness","Shared electric scooter services (SESS) have gained popularity in many cities as an emerging mobility mode. However, SESS is attributed to low utilization rates in some cities. In this context, the significance of users' satisfaction with the provided services, along with their loyalty, becomes particularly pronounced. Therefore, it is essential to evaluate the service quality (SQ) from the users' perspective and understand its effect on satisfaction and loyalty, which are critical factors for the long-term durability and success of service. Thus, this study examines the determinants of user satisfaction and loyalty toward SESS. For this, we conducted an online survey among the SESS users in Chicago. We analyzed the responses using partial least squares structural equation modeling (PLS-SEM) and reflexive thematic analysis (TA). Our results confirm the quality-value-satisfaction-loyalty paradigm and indicate that users' perceptions of SQ, including safety and security, service availability, interface, and perceived affordability, significantly contribute to user loyalty enhancement toward SESS. Perceived value (hedonic value, utilitarian value, and perceived affordability) and satisfaction mediate the impact of SQ on loyalty. Additionally, the results indicate that those who perceive SESS as cost-effective report higher satisfaction levels. Furthermore, users who have pro-environmental attitudes and view SESS as environmentally friendly are more likely to derive hedonic value from the service and exhibit higher loyalty. The TA uncovers key challenges from the user perspective, including pricing, service timing, end-trip facilities (e.g., parking), street pavement condition, vendor operational zoning, and fleet quality, with a specific emphasis on GPS accuracy. These findings provide valuable insights into user behavioral mechanisms, which can be considered as a manifest for service improvement and enhancing efficiency.","<method>partial least squares structural equation modeling (PLS-SEM)</method>, <method>reflexive thematic analysis (TA)</method>"
2024,https://openalex.org/W4400726844,Psychology,Enhancing Brain Tumor Classification by a Comprehensive Study on Transfer Learning Techniques and Model Efficiency Using MRI Datasets,"Brain tumors, a significant health concern, are a leading cause of mortality globally, with an annual projected increase of 5% by the World Health Organization. This work aims to comprehensively analyze the performance of transfer learning methods in identifying the types of brain tumors, with a particular emphasis on the necessity of prompt identification. The study demonstrates how useful it is to use pre-trained models, including models VGG-16, VGG-19, Inception-v3, ResNet-50, DenseNet, and MobileNet—on MRI datasets and used to obtain a precise classification. Using these methods model accuracy and efficiency have been enhanced. The research aims to contribute to improved treatment planning and patient outcomes by implementing optimal methodologies for precise and automated brain tumor analysis, evaluation framework encompasses vital metrics such as confusion matrices, ROC curves, and the achieved Area Under the Curve (AUC) for each approach. The comprehensive methodology outlined in this paper serves as a systematic guide for the implementation and evaluation of brain tumor classification models utilizing deep learning techniques. The integration of visual representations, code snippets, and performance metrics significantly enhances the clarity and understanding of the proposed approach. Among our proposed algorithms, VGG-16 attains the highest accuracy at 97% and consumes only 22% of time as compared to our previous proposed methodology.","<method>transfer learning</method>, <method>VGG-16</method>, <method>VGG-19</method>, <method>Inception-v3</method>, <method>ResNet-50</method>, <method>DenseNet</method>, <method>MobileNet</method>"
2024,https://openalex.org/W4400952462,Psychology,Exploring drivers of behavioral willingness to use clean energy to reduce environmental emissions in rural China: An extension of the UTAUT2 model,"In the context of the dual-carbon target, the adoption of clean energy in rural areas is an important basis for achieving effective carbon reduction in rural areas. On the basis of the Unified Theory of Technology Acceptance and Use (UTAUT2), this study uses structural equation modeling to investigate the influencing factors of rural clean energy adoption behavior. The results are as follows: (1) Both the expected effect of rural clean energy's adoption and adoption behavior are positively correlated with intention to adopt rural clean energy, whereas the rest are negatively correlated. The willingness to adopt clean energy has the greatest impact on clean energy adoption behavior in rural areas. (2) The expected effect of clean energy's adoption, subjective norms related to clean energy's adoption, facilitation of clean energy adoption support, and habits related to clean energy adoption have partial mediating effects on the impact of clean energy adoption intention. The reliability of clean energy's adoption has a full mediating effect on the impact of clean energy adoption intention. The perceived value of clean energy adoption has no significant mediating effect on the intention to adopt clean energy. (3) Age has a significant moderating effect on perceived value, related behaviors, and intention related to adopting clean energy; education level has no significant moderating effects on facilitation support or intention to adopt clean energy; and region has no significant moderating effects on facilitation support or adoption intention.",No methods found.
2024,https://openalex.org/W4401153291,Psychology,Evaluating user engagement via Metaverse environment through immersive experience for travel and tourism websites,"Purpose The purpose of this paper is to explore user engagement (UE) within the Metaverse (MV) environment, emphasising the crucial role of immersive experiences (IEs). This study aims to understand how IEs influence UE and the mediating effects of hedonic value (HV) and utilitarian value (UV) on this relationship. Additionally, the authors examine the moderating impacts of user perceptions (UPs) such as headset comfort, simulation sickness, prior knowledge and ease of use on the utilisation of the MV. This study seeks to elucidate the dynamics of virtual travel at a pre-experience stage, enhancing the comprehension of how digital platforms can revolutionise UE in travel and tourism. Design/methodology/approach This study used a triangulation methodology to provide a thorough investigation into the factors influencing UE in the MV. A systematic literature review (SLR) was conducted to frame the research context and identify relevant variables. To gather empirical data, 25 interviews were performed with active MV users, supplemented by a survey distributed to 118 participants. The data collected was analysed using structural equation modelling (SEM) to test the hypothesised relationships between IEs, UPs, HV and UV and their combined effect on UE within the MV. Findings The findings from the SEM indicate that engaging in the MV leads to a positive IE, which significantly enhances UE. Additionally, it was discovered that HV and UV play a mediating role in strengthening the link between IEs and UE. Furthermore, UPs, including headset comfort, simulation sickness, prior knowledge and ease of use, are significant moderators in the relationship between IEs and MV usage. These insights provide a nuanced understanding of the variables that contribute to and enhance UE in virtual environments. Originality/value This research contributes original insights into the burgeoning field of digital tourism by focusing on the MV, a rapidly evolving platform. It addresses the gap in the existing literature by delineating the complex interplay between IEs, UPs and value constructs within the MV. By using a mixed-method approach and advanced statistical analysis, this study provides a comprehensive model of UE specific to virtual travel platforms. The findings are particularly valuable for developers and marketers in the hospitality and tourism sectors seeking to capitalise on digital transformation and enhance UE through immersive technologies.",No methods found.
2024,https://openalex.org/W4401465392,Psychology,Exploring ChatGPT literacy in language education: A global perspective and comprehensive approach,"With the widespread use of Generative AI in education, effectively utilizing and integrating it into teaching have become key focal points and challenges in education. Different subjects and target audiences require varied norms and strategies for implementing Generative AI, such as ChatGPT. These differences directly impact the educational integration of Generative AI in various educational contexts. To address these disparities and establish common ground, we propose the concept of ChatGPT literacy to bridge research gaps. In this study, we tailor the concept of ChatGPT literacy specifically for language teachers, aiming to delineate the essential competencies needed to proficiently and ethically use ChatGPT as a language learning and teaching tool. We propose a theoretical framework encompassing six fundamental constructs: benefits, limitations, prompts, evaluation (of ChatGPT responses), assessment (assisted by ChatGPT), and ethics, to comprehensively conceptualise and evaluate ChatGPT literacy. Drawing on both quantitative and qualitative survey data from 492 language teachers across 41 countries, we validate the proposed ChatGPT literacy framework by examining teachers' practices and challenges associated with ChatGPT usage. Our analysis of Likert-scale data, utilizing item and confirmatory techniques, confirms the effectiveness of the six-construct framework in defining ChatGPT literacy. In addition, we collected qualitative data through open questions and conducted thematic analysis, demonstrating that ChatGPT has been integrated throughout the instructional cycle, from material preparation to formative and summative assessment phases. These quantitative and qualitative findings have significant implications for a range of stakeholders, including language educators, learners, AI technology developers, and policymakers, providing valuable insights to inform decisions regarding ChatGPT integration in language education. Ultimately, our study equips relevant stakeholders with the necessary competencies to responsibly exploiting ChatGPT's potential in language and other subject areas.",No methods found.
2024,https://openalex.org/W4390644303,Psychology,A tale of two generations: a time-lag study of career expectations,"Purpose As young individuals transition from educational settings to embark on their career paths, their expectations for their future careers become of paramount importance. Ng et al. (2010) examined the expectations of young people in post-secondary education in 2007; those colloquially referred to as “Millennials” or “GenY”. The present study replicates Ng et al .'s (2010) study among a sample of post-secondary students in 2019 (referred to as Generation Z or GenZ) and compares the expectations of young adults in GenY and GenZ. Design/methodology/approach This study employs a time-lag comparison of GenY and GenZ young career entrants based on data collected in 2007 ( n = 23,413) and 2019 ( n = 16,146). Findings Today's youth seem to have realistic expectations for their first jobs and the analyses suggest that young people continue to seek positive, healthy work environments which make room for work–life balance. Further, young people today are prioritizing job security and are not necessarily mobile due to preference, restlessness or disloyalty, but rather leave employers that are not meeting their current needs or expectations. Practical implications Understanding the career expectations of young people allows educators, employers and policymakers to provide vocational guidance that aligns those expectations with the realities of the labor market and the contemporary career context. Originality/value While GenY was characterized as optimistic with great expectations, GenZ can be described as cautious and pragmatic. The results suggest a shift away from opportunity, towards security, stability, an employer that reflects one's values and a job that is satisfying in the present.",No methods found.
2024,https://openalex.org/W4390771316,Psychology,Building typology classification using convolutional neural networks utilizing multiple ground-level image process for city-scale rapid seismic vulnerability assessment,"Several studies have focused on generating seismic vulnerability maps for earthquake-prone areas, particularly in Indonesia. Building typologies are a key factor in determining vulnerability to earthquakes. However, conducting large-scale field surveys to determine the spatial distribution of building typologies in a city is uneconomical. This paper explores the use of a convolutional neural network (CNN) to automatically detect building typologies from diverse regions in Indonesia, utilizing both conventional and automated building image acquisition processes. In this study, datasets from three distinct image acquisition methods are trained with four unique CNN architectures to identify the best-performing model to classify building typologies. The sample size effect on CNN performance is also investigated. The results showed that randomly sampled Google Street View (GSV) images are the most effective dataset for the CNN model, achieving an f1-score of 84.33%. Among the network architectures tested, MobileNet demonstrated superior performance on the majority of evaluated datasets. As the sample size increases by about 350% in the dataset, there is a positive correlation with up to 2.3% f1-score improvement. Using the best-performing CNN model, two building vulnerability models were employed to assess the spatial distribution of building damage in the urban area of Bandung, considering a hypothetical scenario of an M7 earthquake. Incorporating local construction data, one of the generated maps estimated that approximately 55% of buildings in Bandung would experience moderate to severe structural damage. This study showcases the potential of CNN models in automating regional seismic assessments and providing valuable insights for comprehensive seismic mitigation strategies.","<method>convolutional neural network (CNN)</method>, <method>CNN architectures</method>, <method>MobileNet</method>"
2024,https://openalex.org/W4390817372,Psychology,Brain structure ages—A new biomarker for multi‐disease classification,"Age is an important variable to describe the expected brain's anatomy status across the normal aging trajectory. The deviation from that normative aging trajectory may provide some insights into neurological diseases. In neuroimaging, predicted brain age is widely used to analyze different diseases. However, using only the brain age gap information (i.e., the difference between the chronological age and the estimated age) can be not enough informative for disease classification problems. In this paper, we propose to extend the notion of global brain age by estimating brain structure ages using structural magnetic resonance imaging. To this end, an ensemble of deep learning models is first used to estimate a 3D aging map (i.e., voxel-wise age estimation). Then, a 3D segmentation mask is used to obtain the final brain structure ages. This biomarker can be used in several situations. First, it enables to accurately estimate the brain age for the purpose of anomaly detection at the population level. In this situation, our approach outperforms several state-of-the-art methods. Second, brain structure ages can be used to compute the deviation from the normal aging process of each brain structure. This feature can be used in a multi-disease classification task for an accurate differential diagnosis at the subject level. Finally, the brain structure age deviations of individuals can be visualized, providing some insights about brain abnormality and helping clinicians in real medical contexts.",<method>ensemble of deep learning models</method>
2024,https://openalex.org/W4390847969,Psychology,When young customers co-create value of AI-powered branded app: the mediating role of perceived authenticity,"Purpose Artificial intelligence (AI) allows the brand to co-create value with young customers through mobile apps. However, as many brands claim that their mobile apps are using the most updated AI technology, young customers face app fatigue and start questioning the authenticity of this touchpoint. This paper aims to study the mediating effect of authenticity for the value co-creation of AI-powered branded applications. Design/methodology/approach Drawing from regulatory engagement theory, this study conceptualize authenticity as the key construct in customers’ value experience process, which triggers customer value co-creation. Two scenario-based online experiments are conducted to collect data from 444 young customers. Data analysis is performed using ANOVA and Process Hayes. Findings The results reveal that perceived authenticity is an important mediator between media richness (chatbot vs AI text vs augmented reality) and value co-creation. There is no interaction effect of co-brand fit (high vs low) and source endorsement (doctor vs government) on the relationship between media richness and perceived authenticity, whereas injunctive norms (high vs low) strengthen this relationship. Practical implications The finding provides insights for marketing managers on engaging young customers suffering from app fatigue. Authenticity holds the key to young customers’ technological perceptions. Originality/value This research highlights the importance of perceived authenticity in encouraging young customers to co-create value. Young customers consider authenticity as a motivational force experience that involves customers through the app’s attributes (e.g. media richness) and social standards (e.g. norms), rather than brand factors (e.g. co-brand fit, source endorsement).",No methods found.
2024,https://openalex.org/W4390947155,Psychology,Identifying entrepreneurial opportunities during crises: a qualitative study of Italian firms,"Purpose Recognizing novel entrepreneurial opportunities arising from a crisis is of paramount importance for firms. Hence, understanding the pivotal factors that facilitate firms in this endeavor holds significant value. This study delves into such factors within a representative empirical context impacted by a crisis, drawing insights from existing literature on opportunity recognition during such tumultuous periods. Design/methodology/approach The authors conducted a qualitative inspection of 14 Italian firms during the COVID-19 pandemic crisis. The authors collected a rich body of multi-source qualitative data, including 34 interviews (with senior managers and entrepreneurs) and secondary data (press releases, videos, web interviews, newspapers, reports and academic articles) in two phases (March–August 2020 and September–December 2020). Findings The results suggest the existence of a process model of opportunity recognition during crises based on five entrepreneurial influencing factors (entrepreneurial knowledge, entrepreneurial alertness, entrepreneurial proclivity, entrepreneurial personality and entrepreneurial purpose). Originality/value Various scholars have highlighted that, in times of crises, it is not easy and indeed very challenging for entrepreneurs to identify novel entrepreneurial opportunities. However, recent research has shown that crises can also positively impact entrepreneurs and their capacity to identify new entrepreneurial opportunities. Given these findings, not much research has analyzed the process by which entrepreneurs identify novel entrepreneurial opportunities during crises. This study shows that some entrepreneurial influencing factors are very important to identify new entrepreneurial opportunities during crises.",No methods found.
2024,https://openalex.org/W4391026819,Psychology,Predicting sustainable fashion consumption intentions and practices,"Abstract The fashion industry has a significant impact on the environment, and sustainable fashion consumption (SFC) has become a pressing concern. This study aimed to investigate the factors influencing sustainable fashion consumption behavior (SCB) among Chinese adults, specifically the role of values, attitudes, and norms in shaping such behavior, using the value-belief-norm framework. The study used an online cross-sectional survey design to collect data from 350 participants recruited through a convenience sampling method using social media platforms and email invitations, and the obtained data were analyzed using partial least squares structural equation modelling. The results of the study showed that biospheric (BV), altruistic (AV), and egoistic (EV) values significantly influenced the New ecological paradigm (EP), which, in turn, positively affected awareness of consequences (AC). Personal norms (PN) were positively influenced by EP, AC, and ascription of responsibility (AR). Social norms (SN) and trust in recycling (TR) were also found to positively influence sustainable fashion consumption intentions (SCI). Finally, the study found that SCI and TR were significant predictors of SCB, whereas the moderating effect of TR not statistically significant. The study’s originality lies in its comprehensive investigation of the interplay between various factors (particularly using norms in two facets; PN and SN) in shaping SCB, using a structural equation modeling approach, and exploring the moderating effect of TR. The findings of this study suggest that interventions aimed at promoting SFC should focus on fostering values and beliefs that prioritize the environment, encouraging individuals to take responsibility for their actions, creating an environment in which SFC is normalized, and increasing TR.",No methods found.
2024,https://openalex.org/W4391032529,Psychology,How does sports e-commerce influence consumer behavior through short video live broadcast platforms? Attachment theory perspective,"Purpose Based on the “S-O-R” and attachment theories, this paper constructs a research model of the platform attribute factors of sports short video live e-commerce on consumers' psychological conditions, and explores how platform attributes affect consumer behavior through consumer attachment. Design/methodology/approach The study carried out questionnaire survey through the “snowball” method, and a total of 422 valid questionnaires were collected. The paper uses SPSS 26.0 and AMOS 26.0 to adapt the data and model, and adopts the method of structural equation modeling for analysis. Findings The research results show that the interactivity, identity, personalization and entertainment of live sports e-commerce platforms can actively stimulate consumer attachment, and directly drive their consumption behavior through their attachment. Practical implications Examining the platform attributes and system functions of short video live broadcast e-commerce from the perspective of consumer attachment can help sports e-commerce understand consumers' needs and satisfaction with the functions provided by the platform. Through timely optimization and improvement of system functions, the platform will make the connection between consumers and e-commerce closer, thereby increasing consumer stickiness and promoting the vigorous development of sports e-commerce. Originality/value This study identified and defined the platform attributes contained in the sports live broadcast e-commerce platform, combined the S-O-R model with the attachment theory, expanded the analytical framework of the S-O-R theory and made contributions to the extension of the attachment theory.",No methods found.
2024,https://openalex.org/W4391111398,Psychology,"Characterization of Walking in Mild Parkinson’s Disease: Reliability, Validity and Discriminant Ability of the Six-Minute Walk Test Instrumented with a Single Inertial Sensor","Although the 6-Minute Walk Test (6MWT) is among the recommended clinical tools to assess gait impairments in individuals with Parkinson’s disease (PD), its standard clinical outcome consists only of the distance walked in 6 min. Integrating a single Inertial Measurement Unit (IMU) could provide additional quantitative and objective information about gait quality complementing standard clinical outcome. This study aims to evaluate the test–retest reliability, validity and discriminant ability of gait parameters obtained by a single IMU during the 6MWT in subjects with mild PD. Twenty-two people with mild PD and ten healthy persons performed the 6MWT wearing an IMU placed on the lower trunk. Features belonging to rhythm and pace, variability, regularity, jerkiness, intensity, dynamic instability and symmetry domains were computed. Test–retest reliability was evaluated through the Intraclass Correlation Coefficient (ICC), while concurrent validity was determined by Spearman’s coefficient. Mann–Whitney U test and the Area Under the receiver operating characteristic Curve (AUC) were then applied to assess the discriminant ability of reliable and valid parameters. Results showed an overall high reliability (ICC ≥ 0.75) and multiple significant correlations with clinical scales in all domains. Several features exhibited significant alterations compared to healthy controls. Our findings suggested that the 6MWT instrumented with a single IMU can provide reliable and valid information about gait features in individuals with PD. This offers objective details about gait quality and the possibility of being integrated into clinical evaluations to better define walking rehabilitation strategies in a quick and easy way.",No methods found.
2024,https://openalex.org/W4391144074,Psychology,"The neutrophil-to-lymphocyte ratio, lymphocyte-to-monocyte ratio, and neutrophil-to-high-density-lipoprotein ratio are correlated with the severity of Parkinson’s disease","Background Inflammation plays a pivotal role in the pathogenesis of Parkinson’s disease (PD). However, the correlation between peripheral inflammatory markers and the severity of PD remains unclear. Methods The following items in plasma were collected for assessment among patients with PD ( n = 303) and healthy controls (HCs; n = 303) were assessed for the neutrophil-to-lymphocyte ratio (NLR), lymphocyte-to-monocyte ratio (LMR) and neutrophil-to-high-density-lipoprotein ratio (NHR) in plasma, and neuropsychological assessments were performed for all patients with PD. Spearman rank or Pearson correlation was used to evaluate the correlation between the NLR, the LMR and the NHR and the severity of PD. Receiver operating characteristic (ROC) curves were used to evaluate the diagnostic performance of the NLR, LMR and NHR for PD. Results The plasma NLR and NHR were substantially higher in patients with PD than in HCs, while the plasma LMR was substantially lower. The plasma NLR was positively correlated with Hoehn and Yahr staging scale (H&amp;amp;Y), Unified Parkinson’s Disease Rating Scale (UPDRS), UPDRS-I, UPDRS-II, and UPDRS-III scores. Conversely, it exhibited a negative relationship with Mini-Mental State Examination (MMSE) and Montreal Cognitive Assessment (MoCA) scores. Furthermore, the plasma NHR was positively correlated with H&amp;amp;Y, UPDRS, UPDRS-I, UPDRS-II and UPDRS-III scores. Moreover, negative associations were established between the plasma LMR and H&amp;amp;Y, UPDRS, UPDRS-I, UPDRS-II, and UPDRS-III scores. Finally, based on the ROC curve analysis, the NLR, LMR and NHR exhibited respectable PD discriminating power. Conclusion Our research indicates that a higher NLR and NHR and a lower LMR may be relevant for assessing the severity of PD and appear to be promising disease-state biomarker candidates.",No methods found.
2024,https://openalex.org/W4391350390,Psychology,"Machine learning in physical activity, sedentary, and sleep behavior research","Abstract The nature of human movement and non-movement behaviors is complex and multifaceted, making their study complicated and challenging. Thanks to the availability of wearable activity monitors, we can now monitor the full spectrum of physical activity, sedentary, and sleep behaviors better than ever before—whether the subjects are elite athletes, children, adults, or individuals with pre-existing medical conditions. The increasing volume of generated data, combined with the inherent complexities of human movement and non-movement behaviors, necessitates the development of new data analysis methods for the research of physical activity, sedentary, and sleep behaviors. The characteristics of machine learning (ML) methods, including their ability to deal with complicated data, make them suitable for such analysis and thus can be an alternative tool to deal with data of this nature. ML can potentially be an excellent tool for solving many traditional problems related to the research of physical activity, sedentary, and sleep behaviors such as activity recognition, posture detection, profile analysis, and correlates research. However, despite this potential, ML has not yet been widely utilized for analyzing and studying these behaviors. In this review, we aim to introduce experts in physical activity, sedentary behavior, and sleep research—individuals who may possess limited familiarity with ML—to the potential applications of these techniques for analyzing their data. We begin by explaining the underlying principles of the ML modeling pipeline, highlighting the challenges and issues that need to be considered when applying ML. We then present the types of ML: supervised and unsupervised learning, and introduce a few ML algorithms frequently used in supervised and unsupervised learning. Finally, we highlight three research areas where ML methodologies have already been used in physical activity, sedentary behavior, and sleep behavior research, emphasizing their successes and challenges. This paper serves as a resource for ML in physical activity, sedentary, and sleep behavior research, offering guidance and resources to facilitate its utilization.","<method>supervised learning</method>, <method>unsupervised learning</method>"
2024,https://openalex.org/W4391509840,Psychology,A dynamic Bayesian optimized active recommender system for curiosity-driven partially Human-in-the-loop automated experiments,"Abstract Optimization of experimental materials synthesis and characterization through active learning methods has been growing over the last decade, with examples ranging from measurements of diffraction on combinatorial alloys at synchrotrons, to searches through chemical space with automated synthesis robots for perovskites. In virtually all cases, the target property of interest for optimization is defined a priori with the ability to shift the trajectory of the optimization based on human-identified findings during the experiment is lacking. Thus, to highlight the best of both human operators and AI-driven experiments, here we present the development of a human–AI collaborated experimental workflow, via a Bayesian optimized active recommender system (BOARS), to shape targets on the fly with human real-time feedback. Here, the human guidance overpowers AI at early iteration when prior knowledge (uncertainty) is minimal (higher), while the AI overpowers the human during later iterations to accelerate the process with the human-assessed goal. We showcase examples of this framework applied to pre-acquired piezoresponse force spectroscopy of a ferroelectric thin film, and in real-time on an atomic force microscope, with human assessment to find symmetric hysteresis loops. It is found that such features appear more affected by subsurface defects than the local domain structure. This work shows the utility of human–AI approaches for curiosity driven exploration of systems across experimental domains.","<method>active learning</method>, <method>Bayesian optimized active recommender system (BOARS)</method>"
2024,https://openalex.org/W4391643149,Psychology,Development and psychometric validation of a novel scale for measuring ‘psychedelic preparedness’,"Abstract Preparing participants for psychedelic experiences is crucial for ensuring these experiences are safe and, potentially beneficial. However, there is currently no validated measure to assess the extent to which participants are well-prepared for such experiences. Our study aimed to address this gap by developing, validating, and testing the Psychedelic Preparedness Scale (PPS). Using a novel iterative Delphi-focus group methodology (‘DelFo’), followed by qualitative pre-test interviews, we incorporated the perspectives of expert clinicians/researchers and of psychedelic users to generate items for the scale. Psychometric validation of the PPS was carried out in two large online samples of psychedelic users (N = 516; N = 716), and the scale was also administered to a group of participants before and after a 5–7-day psilocybin retreat (N = 46). Exploratory and confirmatory factor analysis identified four factors from the 20-item PPS: Knowledge-Expectations, Intention-Preparation, Psychophysical-Readiness, and Support-Planning. The PPS demonstrated excellent reliability (ω = 0.954) and evidence supporting convergent, divergent and discriminant validity was also obtained. Significant differences between those scoring high and low (on psychedelic preparedness) before the psychedelic experience were found on measures of mental health/wellbeing outcomes assessed after the experience, suggesting that the scale has predictive utility. By prospectively measuring modifiable pre-treatment preparatory behaviours and attitudes using the PPS, it may be possible to determine whether a participant has generated the appropriate mental ‘set’ and is therefore likely to benefit from a psychedelic experience, or at least, less likely to be harmed.",No methods found.
2024,https://openalex.org/W4391930362,Psychology,Designing AI for mental health diagnosis: challenges from sub-Saharan African value-laden judgements on mental health disorders,"Recently clinicians have become more reliant on technologies such as artificial intelligence (AI) and machine learning (ML) for effective and accurate diagnosis and prognosis of diseases, especially mental health disorders. These remarks, however, apply primarily to Europe, the USA, China and other technologically developed nations. Africa is yet to leverage the potential applications of AI and ML within the medical space. Sub-Saharan African countries are currently disadvantaged economically and infrastructure-wise. Yet precisely, these circumstances create significant opportunities for the deployment of medical AI, which has already been deployed in some places in the continent. However, while AI and ML have come with enormous promises in Africa, there are still challenges when it comes to successfully applying AI and ML designed elsewhere within the African context, especially in diagnosing mental health disorders. We argue, in this paper, that there ought not to be a homogeneous/generic design of AI and ML used in diagnosing mental health disorders. Our claim is grounded on the premise that mental health disorders cannot be diagnosed solely on 'factual evidence' but on both factual evidence and value-laden judgements of what constitutes mental health disorders in sub-Saharan Africa. For ML to play a successful role in diagnosing mental health disorders in sub-Saharan African medical spaces, with a precise focus on South Africa, we allude that it ought to understand what sub-Saharan Africans consider as mental health disorders, that is, the value-laden judgements of some conditions.","<method>artificial intelligence (AI)</method>, <method>machine learning (ML)</method>"
2024,https://openalex.org/W4392130768,Psychology,EEG-based brain-computer interface methods with the aim of rehabilitating advanced stage ALS patients,"Amyotrophic Lateral Sclerosis (ALS) is a neurodegenerative disease that leads to progressive muscle weakness and paralysis, ultimately resulting in the loss of ability to communicate and control the environment. EEG-based Brain-Computer Interface (BCI) methods have shown promise in providing communication and control with the aim of rehabilitating ALS patients. In particular, P300-based BCI has been widely studied and used for ALS rehabilitation. Other EEG-based BCI methods, such as Motor Imagery (MI) based BCI and Hybrid BCI, have also shown promise in ALS rehabilitation. Nonetheless, EEG-based BCI methods hold great potential for improvement. This review article introduces and reviews FFT, WPD, CSP, CSSP, CSP, and GC feature extraction methods. The Common Spatial Pattern (CSP) is an efficient and common technique for extracting data properties used in BCI systems. In addition, Linear Discriminant Analysis (LDA), Support Vector Machine (SVM), Neural Networks (NN), and Deep Learning (DL) classification methods were introduced and reviewed. SVM is the most appropriate classifier due to its insensitivity to the curse of dimensionality. Also, DL is used in the design of BCI systems and is a good choice for BCI systems based on motor imagery with big datasets. Despite the progress made in the field, there are still challenges to overcome, such as improving the accuracy and reliability of EEG signal detection and developing more intuitive and user-friendly interfaces By using BCI, disabled patients can communicate with their caregivers and control their environment using various devices, including wheelchairs, and robotic arms.","<method>FFT</method>, <method>WPD</method>, <method>CSP</method>, <method>CSSP</method>, <method>GC</method>, <method>Linear Discriminant Analysis (LDA)</method>, <method>Support Vector Machine (SVM)</method>, <method>Neural Networks (NN)</method>, <method>Deep Learning (DL)</method>"
2024,https://openalex.org/W4392262143,Psychology,Brain Tumor Detection for Efficient Adaptation and Superior Diagnostic Precision by Utilizing MBConv-Finetuned-B0 and Advanced Deep Learning,"In the rapidly evolving landscape of medical imaging, our proposed work presents an innovative and efficient approach to brain tumor detection through advanced deep learning methodologies.Central to our methodology is the strategic utilization of pre-trained weights from the formidable MBConv-Finetuned-B0 model, initially honed on the expansive ImageNet dataset, providing a foundation rich in general visual knowledge.Our subsequent fine-tuning process targets specific layers relevant to brain tumor detection, introducing two distinct convolutional layers, MBConv 6, 55, and MBConv 6, 30, meticulously added to the MBConv-Finetuned-B0 base model.These layers are intricately designed to extract and refine features specific to brain tumors, ensuring a nuanced understanding of pathology and enhancing the model's discrimination and accuracy.The flexibility of our methodology is exemplified by the thoughtful consideration of two fine-tuning options: one that adjusts all layers of the model and another that selectively fine-tunes only the proposed layers.We conduct a detailed comparative analysis, including homogeneity and median feature values, placing our work in direct comparison with established techniques such as Ensemble Transfer Learning and Quantum Variational Classifier (ETL & QVC), Ultra-Light Deep Learning (ULDL) Model, Deep Convolutional Neural Network (DCNN), and Deep Learning and Image Processing (DLIP).The results showcase the model's proficiency, achieving an accuracy of 94%, precision of 84%, recall of 92%, F1 score of 88%, and an AUC-ROC of 96%.Notably, our model demonstrates superior performance in terms of homogeneity (vE Homogeneity: 0.93, vN Homogeneity: 0.91, Enhancement Homogeneity: 0.97) and median feature values (Median vE Feature Value: 0.82, Median vN Feature Value: 0.87, Median Enhancement Feature Value: 0.80), providing a comprehensive understanding of its effectiveness in capturing subtle nuances in brain tumor images.","<method>MBConv-Finetuned-B0 model</method>, <method>fine-tuning</method>, <method>Ensemble Transfer Learning (ETL)</method>, <method>Quantum Variational Classifier (QVC)</method>, <method>Ultra-Light Deep Learning (ULDL) Model</method>, <method>Deep Convolutional Neural Network (DCNN)</method>, <method>Deep Learning and Image Processing (DLIP)</method>"
2024,https://openalex.org/W4392514027,Psychology,Introducing machine‐learning‐based data fusion methods for analyzing multimodal data: An application of measuring trustworthiness of microenterprises,"Abstract Research Summary Multimodal data, comprising interdependent unstructured text, image, and audio data that collectively characterize the same source, with video being a prominent example, offer a wealth of information for strategy researchers. We emphasize the theoretical importance of capturing the interdependencies between different modalities when evaluating multimodal data. To automate the analysis of video data, we introduce advanced deep machine learning and data fusion methods that comprehensively account for all intra‐ and inter‐modality interdependencies. Through an empirical demonstration focused on measuring the trustworthiness of grassroots sellers in live streaming commerce on Tik Tok, we highlight the crucial role of interpersonal interactions in the business success of microenterprises. We provide access to our data and algorithms to facilitate data fusion in strategy research that relies on multimodal data. Managerial Summary Our study highlights the vital role of both verbal and nonverbal communication in attaining strategic objectives. Through the analysis of multimodal data—incorporating text, images, and audio—we demonstrate the essential nature of interpersonal interactions in bolstering trustworthiness, thus facilitating the success of microenterprises. Leveraging advanced machine learning techniques, such as data fusion for multimodal data and explainable artificial intelligence, we notably enhance predictive accuracy and theoretical interpretability in assessing trustworthiness. By bridging strategic research with cutting‐edge computational techniques, we provide practitioners with actionable strategies for enhancing communication effectiveness and fostering trust‐based relationships. Access our data and code for further exploration.","<method>deep machine learning</method>, <method>data fusion</method>, <method>advanced machine learning techniques</method>, <method>explainable artificial intelligence</method>"
2024,https://openalex.org/W4392651781,Psychology,Leisure time and parenting in Europe: a more difficult equation for mothers?,"Abstract Objective This paper analyzes gender inequalities in leisure time within coresident opposite‐sex couples with and without children at home in five European countries to evaluate the gendered parental impact in leisure time. Background In European societies, women continue to bear much of the physical and mental burden involved in running a household and managing family life resulting in greater levels of stress and time deprivation. Time spent in leisure has been associated with better physical and psychological wellness. Understanding how gender influences the distribution of leisure time among couples living with and without children at home, and how these effects differ across European countries, is important to understand individual and couples' well‐being. Method Drawing on information from the Multinational Time Use Study for 15,024 matched couples residing in Spain, Italy, France, Finland, and the United Kingdom, we conduct a series of ordinary‐least‐squares regression analyses with country fixed effects. Results The general trends reveal that women in Europe allocate less time to leisure and that mothers experience lower leisure of high quality compared to their partners when their children are below the age of 5. However, Finnish and British couples exhibit a more egalitarian distribution of leisure time regardless of their parental status, particularly when compared to Italian and Spanish ones. Conclusion This study unveils a gendered use of leisure time, as well as a gendered parental impact when children are young. However, the impact of women's second shift in leisure time varies across countries, suggesting a cultural and institutional effect. Implications These findings have implications for researchers, health professionals, and policymakers concerned with understanding and alleviating situations of overstress, time poverty, and depression among women, but especially among mothers of young children.",No methods found.
2024,https://openalex.org/W4392655821,Psychology,Agencement of onlife and phygital: smart tech–enabled value co-creation practices,"Purpose In this article, we reflect on how smart technology is transforming service research discourses about service innovation and value co-creation. We adopt the concept of technology smartness’ to refer to the ability of technology to sense, adapt and learn from interactions. Accordingly, we seek to address how smart technologies (i.e. cognitive and distributed technology) can be powerful resources, capable of innovating in relation to actors’ agency, the structure of the service ecosystem and value co-creation practices. Design/methodology/approach This conceptual article integrates evidence from the existing theories with illustrative examples to advance research on service innovation and value co-creation. Findings Through the performative utterances of new tech words, such as onlife and materiality, this article identifies the emergence of innovative forms of agency and structure. Onlife agency entails automated, relational and performative forms, which provide for new decision-making capabilities and expanded opportunities to co-create value. Phygital materiality pertains to new structural features, comprised of new resources and contexts that have distinctive intelligence, autonomy and performativity. The dialectic between onlife agency and phygital materiality (structure) lies in the agencement of smart tech–enabled value co-creation practices based on the notion of becoming that involves not only resources but also actors and contexts. Originality/value This paper proposes a novel conceptual framework that advances a tech-based ecology for service ecosystems, in which value co-creation is enacted by the smartness of technology, which emerges through systemic and performative intra-actions between actors (onlife agency), resources and contexts (phygital materiality and structure).",No methods found.
2024,https://openalex.org/W4392672219,Psychology,"Cultivating the digital citizen: trust, digital literacy and e-government adoption","Purpose This study aims to examine the role of trust and digital literacy in influencing citizens’ adoption of e-government services. Design/methodology/approach Grounded in the technology acceptance model (TAM), a research model was developed focusing on e-filing services adoption. Hypotheses were formulated to assess the moderating effect of digital literacy on the relationship between trust and the key TAM determinants of perceived usefulness and perceived ease of use. A questionnaire-based survey of 876 citizens who have used e-filing using the snow-ball sampling technique was adopted to generate data. The data was analyzed using PLS-SEM through the aid of SmartPLS 4 to assess the measurement model and structural relationships. Findings Trust positively influences perceived usefulness and ease of use, which in turn drive adoption. Additionally, digital literacy significantly moderates the impact of trust on usefulness and ease of use perceptions – the effect is stronger for higher digital literacy. Research limitations/implications The study adopted a single country developing economy context limiting cross-cultural applicability. Second, the focus on e-filing adoption precludes insights across other e-government services. Third, the reliance on perceptual measures risks respondent biases and fourth, the study is a cross-sectional survey design. Practical implications The findings emphasize multifaceted strategies to accelerate e-government adoption. Nurturing citizen trust in e-government systems through enhanced reliability, security and transparency remains vital. Simultaneously, initiatives to cultivate digital access, skills and proficiencies across population segments need to be undertaken. Originality/value This study integrates trust and digital literacy within the theoretical model to provide a more holistic understanding of adoption determinants. It highlights the need for balanced technology-enabled and social interventions to foster acceptance of e-government services.",No methods found.
2024,https://openalex.org/W4393964796,Psychology,Human-machine dialogues unveiled: an in-depth exploration of individual attitudes and adoption patterns toward AI-powered ChatGPT systems,"Purpose ChatGPT is an advanced artificial intelligence (AI) form that can generate human-like text based on large amounts of data. This paper aims to empirically examine the ChatGPT adoption level among Indian individuals by considering the key factors in determining individuals’ attitudes and intentions toward newly emerged AI tools. Design/methodology/approach This paper used “partial least square structural equation modeling” (PLS-SEM) to investigate the relation among several latent factors by applying a representative sample of 351 individuals. Findings This study found that trialability, performance expectancy and personal innovativeness significantly influence individuals' attitudes, while compatibility and effort expectancy do not significantly impact attitudes. Additionally, trialability, performance expectancy, effort expectancy, personal innovativeness and attitude significantly influence behavioral intentions. However, compatibility has an insignificant impact on behavioral intention. Moreover, the research highlights that attitude and behavioral intention directly correlate with actual use. Specifically, the absence of compatibility makes people hesitate to use technology that does not meet their specific needs. Practical implications These unique findings provide valuable insights for technology service providers and government entities. They can use this information to shape their policies, deliver timely and relevant updates and enhance their strategies to boost the adoption of ChatGPT. Originality/value This paper is one of the pioneering attempts to exhibit the research stream to understand the individual acceptance of ChatGPT in an emerging country. Moreover, it gained significant attention from individuals for delivering a unique experience and promising solutions.",<method>partial least square structural equation modeling (PLS-SEM)</method>
2024,https://openalex.org/W4394997844,Psychology,Revisiting drug–protein interaction prediction: a novel global–local perspective,"Abstract Motivation Accurate inference of potential drug–protein interactions (DPIs) aids in understanding drug mechanisms and developing novel treatments. Existing deep learning models, however, struggle with accurate node representation in DPI prediction, limiting their performance. Results We propose a new computational framework that integrates global and local features of nodes in the drug–protein bipartite graph for efficient DPI inference. Initially, we employ pre-trained models to acquire fundamental knowledge of drugs and proteins and to determine their initial features. Subsequently, the MinHash and HyperLogLog algorithms are utilized to estimate the similarity and set cardinality between drug and protein subgraphs, serving as their local features. Then, an energy-constrained diffusion mechanism is integrated into the transformer architecture, capturing interdependencies between nodes in the drug–protein bipartite graph and extracting their global features. Finally, we fuse the local and global features of nodes and employ multilayer perceptrons to predict the likelihood of potential DPIs. A comprehensive and precise node representation guarantees efficient prediction of unknown DPIs by the model. Various experiments validate the accuracy and reliability of our model, with molecular docking results revealing its capability to identify potential DPIs not present in existing databases. This approach is expected to offer valuable insights for furthering drug repurposing and personalized medicine research. Availability and implementation Our code and data are accessible at: https://github.com/ZZCrazy00/DPI.","<method>pre-trained models</method>, <method>transformer architecture</method>, <method>multilayer perceptrons</method>"
2024,https://openalex.org/W4395675470,Psychology,Transforming Landslide Prediction: A Novel Approach Combining Numerical Methods and Advanced Correlation Analysis in Slope Stability Investigation,"Landslides cause significant economic losses and casualties worldwide. However, robust prediction remains challenging due to the complexity of geological factors contributing to slope stability. Advanced correlation analysis methods can improve prediction capabilities. This study aimed to develop a novel landslide prediction approach that combines numerical modeling and correlation analysis (Spearman rho and Kendall tau) to improve displacement-based failure prediction. Simulations generate multi-location displacement data sets on soil and rock slopes under incremental stability reductions. Targeted monitoring points profile local displacement responses. Statistical analyses, including mean/variance and Spearman/Kendall correlations, quantified displacement-stability relationships. For the homogeneous soil slope, monitoring point 2 of the middle section of the slope showed a mean horizontal displacement of 17.65 mm and a mean vertical displacement of 9.72 mm under stability reduction. Spearman’s rho correlation coefficients ranged from 0.31 to 0.76, while Kendall’s tau values ranged from 0.29 to 0.64, indicating variable displacement–stability relationships. The joint rock slope model had strong positive total displacement correlations (Spearman’s and Kendall’s correlation ranges of +1.0 and −1.0) at most points. Horizontal and vertical displacements reached mean maxima of 44.13 mm and 22.17 mm, respectively, at the unstable point 2 of the center section of the slope. The advanced correlation analysis techniques provided superior identification of parameters affecting slope stability compared to standard methods. The generated predictive model dramatically improves landslide prediction capability, allowing preventive measures to be taken to mitigate future losses through this new approach.","<method>numerical modeling</method>, <method>correlation analysis (Spearman rho)</method>, <method>correlation analysis (Kendall tau)</method>"
2024,https://openalex.org/W4397009812,Psychology,"The Development, Testing, and Refinement of Eccles, Wigfield, and Colleagues’ Situated Expectancy-Value Model of Achievement Performance and Choice","Abstract To address the seven guiding questions posed for authors of articles in this special issue, we begin by discussing the development (in the late 1970s-early 1980s) of Eccles’ expectancy-value theory of achievement choice (EEVT), a theory developed to explain the cultural phenomenon of why girls were less likely to participate in STEM courses and careers. We then discuss how we tested key predictions from the theory, notably how expectancies and values relate to achievement choices and performance and how socialization practices at home and in school influence them. Next, we discuss three main refinements: addressing developmental aspects of the theory, refining construct definitions, and renaming the theory situated expectancy value theory. We discuss reasons for that change, and their implications. To illustrate the theory’s practicality, we discuss intervention projects based in the model, and what next steps should be in SEVT-based intervention research. We close with suggestions for future research, emphasizing attaining consensus on how to measure the central constructs, expanding the model to capture better motivation of diverse groups, and the challenges of testing the increasingly complex predictions stemming from the model. Throughout the manuscript, we make suggestions for early career researchers to provide guidance for their own development of theories.",No methods found.
2024,https://openalex.org/W4399563755,Psychology,Exploring the frontier: Transformer-based models in EEG signal analysis for brain-computer interfaces,"This review systematically explores the application of transformer-based models in EEG signal processing and brain-computer interface (BCI) development, with a distinct focus on ensuring methodological rigour and adhering to empirical validations within the existing literature. By examining various transformer architectures, such as the Temporal Spatial Transformer Network (TSTN) and EEG Conformer, this review delineates their capabilities in mitigating challenges intrinsic to EEG data, such as noise and artifacts, and their subsequent implications on decoding and classification accuracies across disparate mental tasks. The analytical scope extends to a meticulous examination of attention mechanisms within transformer models, delineating their role in illuminating critical temporal and spatial EEG features and facilitating interpretability in model decision-making processes. The discourse additionally encapsulates emerging works that substantiate the efficacy of transformer models in noise reduction of EEG signals and diversifying applications beyond the conventional motor imagery paradigm. Furthermore, this review elucidates evident gaps and propounds exploratory avenues in the applications of pre-trained transformers in EEG analysis and the potential expansion into real-time and multi-task BCI applications. Collectively, this review distils extant knowledge, navigates through the empirical findings, and puts forward a structured synthesis, thereby serving as a conduit for informed future research endeavours in transformer-enhanced, EEG-based BCI systems.","<method>transformer-based models</method>, <method>Temporal Spatial Transformer Network (TSTN)</method>, <method>EEG Conformer</method>, <method>attention mechanisms within transformer models</method>"
2024,https://openalex.org/W4399813647,Psychology,A network correspondence toolbox for quantitative evaluation of novel neuroimaging results,"Decades of neuroscience research has shown that macroscale brain dynamics can be reliably decomposed into a subset of large-scale functional networks, but the specific spatial topographies of these networks and the names used to describe them can vary across studies. Such discordance has hampered interpretation and convergence of research findings across the field. To address this problem, we have developed the Network Correspondence Toolbox (NCT) to permit researchers to examine and report spatial correspondence between their novel neuroimaging results and sixteen widely used functional brain atlases, consistent with recommended reporting standards developed by the Organization for Human Brain Mapping. The atlases included in the toolbox show some topographical convergence for specific networks, such as those labeled as default or visual. Network naming varies across atlases, particularly for networks spanning frontoparietal association cortices. For this reason, quantitative comparison with multiple atlases is recommended to benchmark novel neuroimaging findings. We provide several exemplar demonstrations using the Human Connectome Project task fMRI results and UK Biobank independent component analysis maps to illustrate how researchers can use the NCT to report their own findings through quantitative evaluation against multiple published atlases. The NCT provides a convenient means for computing Dice coefficients with spin test permutations to determine the magnitude and statistical significance of correspondence among user-defined maps and existing atlas labels. The NCT also includes functionality to incorporate additional atlases in the future. The adoption of the NCT will make it easier for network neuroscience researchers to report their findings in a standardized manner, thus aiding reproducibility and facilitating comparisons between studies to produce interdisciplinary insights.",No methods found.
2024,https://openalex.org/W4400191428,Psychology,"Impact of leader support on open innovation: The mediating role of organizational culture, intellectual property, and collaboration","This study investigates how open innovation (OI) functions in the Thai food industry, focusing on the direct and indirect effects of leader support (LS) on OI through organizational culture (OC), intellectual property (IP), and collaboration (CO). The study employs a sample size of 380 units and utilizes path analysis and bootstrapping techniques to assess the relationships among the variables. The results indicate that LS has a significant direct impact on OI, with OC, IP, and CO serving as crucial mediators. Specifically, OC acts as a mediator, with IP and CO following, emphasizing their substantial indirect impacts on OI. OC, IP, and CO act as mediators, emphasizing their substantial indirect impacts on OI. These findings underscore the importance of effective LS in fostering a culture of innovation within the Thai food industry, where traditional recipes intersect with modern culinary trends. By elucidating the mechanisms through which LS influences OI, this study offers practical insights for organizational leaders seeking to stimulate creativity, encourage risk-taking, and foster collaboration in order to ultimately drive innovation and maintain competitiveness in the dynamic landscape of the Thai food industry. This research contributes to the body of knowledge by identifying collaboration as a critical mediator, integrating constructs from leadership, collaboration, and innovation literatures, and validating the mediation model in the context of the Thai food industry.",No methods found.
2024,https://openalex.org/W4390475350,Psychology,Flipped classroom with gamified technology and paper-based method for teaching vocabulary,"Abstract While gamified technology integration in vocabulary instruction within a flipped classroom has yielded beneficial teaching outcomes, specific studies have raised concerns about potential adverse effects linked to this approach. As a result, conducting a comparative analysis between gamified technology and conventional paper-based methods within the flipped classroom framework has become essential. This analysis aims to foster the development of a targeted teaching approach that adeptly addresses the unique needs of students. This study employed a sequential explanatory research design to examine the effectiveness of flipped classroom with gamified technology and paper-based method in teaching vocabulary to students with different proficiency levels. Quantitative data was gathered from a pretest and a posttest, whilst qualitative data was collected through teachers’ guided reflection. Using Academic Word List (300 target words), control groups employed a paper-based, while experimental groups applied gamified technology ( Quizlet, Kahoot!, Quizizz, Socrative, and Google Form ), which lasted 10 weeks. The participants were 144 non-English major students who took a general English course in the 2nd semester of 2023. Quantitative data analysis ran in SPSS 25 using Paired Sample t-Test and One-way ANOVA . The qualitative data were analyzed using thematic progression. The results showed that gamified technology did not affect students’ learning outcomes, while the paper-based method resulted conversely. It revealed that the paper-based method is more effective than gamified technology for students in general, with low proficiency and high-proficiency level. Further, teachers’ beliefs admitted distinctive issues that gamified technology was more effective for high-proficiency learners, whereas paper-based was more effective for low-proficiency learners. The difference analysis of quantitative and qualitative data sheds light on discussing threats while implementing gamified technology and possible solutions.",No methods found.
2024,https://openalex.org/W4390725372,Psychology,A Novel EEG-Based Parkinson’s Disease Detection Model Using Multiscale Convolutional Prototype Networks,"Objective and accurate detection of Parkinson's disease (PD) is crucial for timely intervention and treatment. Electroencephalography (EEG) has been proven to characterize PD by measuring brain activity. In recent years, deep learning methods have gained great attention in automated PD detection, but their performance is limited by insufficient data samples. In this article, we propose a novel PD automated detection model named the multiscale convolutional prototype network (MCPNet), which integrates and improves upon multiscale convolutional neural networks (CNNs) and prototype learning. On the one hand, it employs multiscale CNNs to extract brain features from different scales, enhancing feature diversity and utilization. On the other hand, a prototype calibration strategy is introduced to mitigate the effect of data noise on prototype generation, improving the generalization performance of model. Multiple within-dataset and cross-dataset experiments on three different datasets demonstrate the effectiveness of our model in PD detection. The leave-one-subject-out (LOSO) results of within-dataset experiments show that MCPNet achieves an accuracy of 92.5%, a sensitivity of 93.1%, a specificity of 91.9%, and an AUC of 92.4% in cross-subject classification between PD patients and healthy controls. In the cross-dataset classification, the performance of MCPNet is somewhat weakened due to dataset variations. However, this weakening is partially compensated by introducing the prototype calibration strategy. With the introduction of the calibration strategy, the accuracy of cross-dataset classification increases to 90.2%, a 4.0% improvement compared to when it is not used. These results indicate that the proposed model may be a promising tool for automated PD diagnosis.","<method>multiscale convolutional neural networks (CNNs)</method>, <method>prototype learning</method>, <method>prototype calibration strategy</method>"
2024,https://openalex.org/W4390961691,Psychology,"E-government quality from the citizen's perspective: the role of perceived factors, demographic variables and the digital divide","Purpose Governments globally are adopting e-Government services to streamline administrative processes and meet citizens' expectations. This study investigates e-Government service quality from citizens' perspectives in 50 Greek municipalities, using the technology acceptance model (TAM) and cognitive theory. Design/methodology/approach The data from 707 respondents across 50 Greek municipalities are analyzed using structural equation modeling (SEM), ANOVA and moderation analysis. The study assesses the relationships between key factors and citizens' intentions to use e-Government services, examining the impact of demographics and the digital divide. Findings The study reveals that perceived attractiveness (PA), perceived usefulness (PU), perceived ease of use (PEOU) and awareness (AWA) significantly influence citizens' behavioral intentions (BINTs) toward municipal e-Government services. Interestingly, PEOU negatively impacts users' intentions, suggesting dissatisfaction with portal attractiveness and utility. The study explores the influence of demographic variables and the digital divide on citizens' BINTs, highlighting economic activity and income as crucial determinants. Practical implications The study emphasizes the significance of user-friendly design, PU, PEOU and AWA campaigns for the development of effective e-Government platforms. Strategies to address the digital divide and promote citizen engagement are essential for enhancing user experience, service utility and AWA, ultimately fostering a positive attitude toward e-Government. Social implications Addressing demographic differences ensures inclusive e-Government systems, while bridging the digital divide promotes equitable service delivery and citizen engagement. Originality/value This research provides insights into factors influencing citizens' BINTs toward e-Government services. The study's examination of demographic attributes and the digital divide enhances understanding, contributing to the development of citizen-centric e-Government services and supporting inclusive digital transformations.","<method>structural equation modeling (SEM)</method>, <method>ANOVA</method>, <method>moderation analysis</method>"
2024,https://openalex.org/W4391407181,Psychology,DiffMDD: A Diffusion-Based Deep Learning Framework for MDD Diagnosis Using EEG,"Major Depression Disorder (MDD) is a common yet destructive mental disorder that affects millions of people worldwide. Making early and accurate diagnosis of it is very meaningful. Recently, EEG, a non-invasive technique of recording spontaneous electrical activity of brains, has been widely used for MDD diagnosis. However, there are still some challenges in data quality and data size of EEG: (1) A large amount of noise is inevitable during EEG collection, making it difficult to extract discriminative features from raw EEG; (2) It is difficult to recruit a large number of subjects to collect sufficient and diverse data for model training. Both of the challenges cause the overfitting problem, especially for deep learning methods. In this paper, we propose DiffMDD, a diffusion-based deep learning framework for MDD diagnosis using EEG. Specifically, we extract more noise-irrelevant features to improve the model's robustness by designing the Forward Diffusion Noisy Training Module. Then we increase the size and diversity of data to help the model learn more generalized features by designing the Reverse Diffusion Data Augmentation Module. Finally, we re-train the classifier on the augmented dataset for MDD diagnosis. We conducted comprehensive experiments to test the overall performance and each module's effectiveness. The framework was validated on two public MDD diagnosis datasets, achieving the state-of-the-art performance.","<method>diffusion-based deep learning framework</method>, <method>Forward Diffusion Noisy Training Module</method>, <method>Reverse Diffusion Data Augmentation Module</method>"
2024,https://openalex.org/W4391468027,Psychology,Predicting University Student Graduation Using Academic Performance and Machine Learning: A Systematic Literature Review,"Predicting university student graduation is a beneficial tool for both students and institutions. With the help of this predictive capacity, students may make well-informed decisions about their academic and career paths, and institutions can proactively identify students who may not graduate and offer tailored support to ensure their success. The use of machine learning for predicting university student graduation has drawn more attention in recent years. Large datasets of student academic performance data can be used to train machine learning algorithms to identify patterns that are applicable in predicting future outcomes. In accordance with some studies, this approach predicts student graduation with an accuracy rate as high as 90%. Many SLRs have been conducted in this field, but there are still limitations, including not discussing the predictive models and algorithms used, a lack of coverage of the machine learning algorithms applied, small database coverage, keyword selection that does not cover all synonyms relevant to the investigation, and less specific data collection transparency. By delving into the limitations of existing SLRs on this topic, this research not only enhances the understanding of machine learning applications in forecasting student graduation but also fills a crucial gap in the literature. The inclusion of weaknesses in current SLRs provides a foundation for justifying the need for this study, emphasizing the necessity of a more nuanced and comprehensive review to advance the field and guide future research efforts in smart learning environments. This research conducts a thorough systematic review of the existing literature on machine learning-based student graduation prediction models from 70 journal articles from 2018 through 2023 that are pertinent. This review includes the various machine learning algorithms that have been implemented, the various academic performance data that was obtained from students, and the effectiveness of the models that have been developed. It also discusses the difficulties and potential advantages of utilizing machine learning to predict student graduation. The review indicates that the most common approach employed is the prediction of students' academic performance, which relies on data obtained from the Learning Management System and Student Information System. The primary data utilized for prediction purposes consists Student retention and time of academic and behavioral information. Among the various algorithms employed, Support Vector Machine and Random Forest are the most commonly utilized. This study makes a significant contribution to the advancement of learner modules within the smart learning environment.","<method>Support Vector Machine</method>, <method>Random Forest</method>"
2024,https://openalex.org/W4391625720,Psychology,Combining artificial and human intelligence to manage cross-cultural knowledge in humanitarian logistics: a Yin–Yang dialectic systems view of knowledge creation,"Purpose Aiming to resolve cross-cultural paradoxes in combining artificial intelligence (AI) with human intelligence (HI) for international humanitarian logistics, this paper aims to adopt an unorthodox Yin–Yang dialectic approach to address how AI–HI interactions can be interpreted as a sophisticated cross-cultural knowledge creation (KC) system that enables more effective decision-making for providing humanitarian relief across borders. Design/methodology/approach This paper is conceptual and pragmatic in nature, whereas its structure design follows the requirements of a real impact study. Findings Based on experimental information and logical reasoning, the authors first identify three critical cross-cultural challenges in AI–HI collaboration: paradoxes of building a cross-cultural KC system, paradoxes of integrative AI and HI in moral judgement and paradoxes of processing moral-related information with emotions in AI–HI collaboration. Then applying the Yin–Yang dialectic to interpret Klir’s epistemological frame (1993), the authors propose an unconventional stratified system of cross-cultural KC for understanding integrative AI–HI decision-making for humanitarian logistics across cultures. Practical implications This paper aids not only in deeply understanding complex issues stemming from human emotions and cultural cognitions in the context of cross-border humanitarian logistics, but also equips culturally-diverse stakeholders to effectively navigate these challenges and their potential ramifications. It enhances the decision-making process and optimizes the synergy between AI and HI for cross-cultural humanitarian logistics. Originality/value The originality lies in the use of a cognitive methodology of the Yin–Yang dialectic to metaphorize the dynamic genesis of integrative AI-HI KC for international humanitarian logistics. Based on system science and knowledge management, this paper applies game theory, multi-objective optimization and Markov decision process to operationalize the conceptual framework in the context of cross-cultural humanitarian logistics.","<method>game theory</method>, <method>multi-objective optimization</method>, <method>Markov decision process</method>"
2024,https://openalex.org/W4402061574,Psychology,Abstractive Text Summarization Using GAN,"In the field of natural language processing, the task of writing long concepts into short expressions has attracted attention due to its ability to simplify the processing and understanding of information. While traditional transcription techniques are effective to some extent, they often fail to capture the essence and nuances of the original texts. This article explores a new approach to collecting abstract data using artificial neural networks (GANs), a class of deep learning models known for their ability to create patterns of real information. We describe the fundamentals of text collection through a comprehensive review of existing literature and methods and highlight the complexity of GAN-based text. Our goal is to transform complex text into context and meaning by combining the power of GANs with natural language understanding. We detail the design and training of an adaptive GAN model for the text recognition task. We also conduct various experiments and evaluations using established metrics such as ROUGE and BLEU scores to evaluate the effectiveness and efficiency of our approach. The results show that GANs can be used to improve the quality and consistency of generated content, data storage, data analysis paper, etc. It shows its promise in paving the way for advanced applications in fields. Through this research, we aim to contribute to the continued evolution of writing technology, providing insights and innovations that support the field to a new level of well-done.","<method>artificial neural networks (GANs)</method>, <method>GAN-based text</method>, <method>adaptive GAN model</method>"
2024,https://openalex.org/W4391135337,Psychology,ACCORD (ACcurate COnsensus Reporting Document): A reporting guideline for consensus methods in biomedicine developed via a modified Delphi,"Background In biomedical research, it is often desirable to seek consensus among individuals who have differing perspectives and experience. This is important when evidence is emerging, inconsistent, limited, or absent. Even when research evidence is abundant, clinical recommendations, policy decisions, and priority-setting may still require agreement from multiple, sometimes ideologically opposed parties. Despite their prominence and influence on key decisions, consensus methods are often poorly reported. Our aim was to develop the first reporting guideline dedicated to and applicable to all consensus methods used in biomedical research regardless of the objective of the consensus process, called ACCORD (ACcurate COnsensus Reporting Document). Methods and findings We followed methodology recommended by the EQUATOR Network for the development of reporting guidelines: a systematic review was followed by a Delphi process and meetings to finalize the ACCORD checklist. The preliminary checklist was drawn from the systematic review of existing literature on the quality of reporting of consensus methods and suggestions from the Steering Committee. A Delphi panel ( n = 72) was recruited with representation from 6 continents and a broad range of experience, including clinical, research, policy, and patient perspectives. The 3 rounds of the Delphi process were completed by 58, 54, and 51 panelists. The preliminary checklist of 56 items was refined to a final checklist of 35 items relating to the article title ( n = 1), introduction ( n = 3), methods ( n = 21), results ( n = 5), discussion ( n = 2), and other information ( n = 3). Conclusions The ACCORD checklist is the first reporting guideline applicable to all consensus-based studies. It will support authors in writing accurate, detailed manuscripts, thereby improving the completeness and transparency of reporting and providing readers with clarity regarding the methods used to reach agreement. Furthermore, the checklist will make the rigor of the consensus methods used to guide the recommendations clear for readers. Reporting consensus studies with greater clarity and transparency may enhance trust in the recommendations made by consensus panels.",No methods found.
2024,https://openalex.org/W4390608362,Psychology,"Transformative Potential of AI in Healthcare: Definitions, Applications, and Navigating the Ethical Landscape and Public Perspectives","Artificial intelligence (AI) has emerged as a crucial tool in healthcare with the primary aim of improving patient outcomes and optimizing healthcare delivery. By harnessing machine learning algorithms, natural language processing, and computer vision, AI enables the analysis of complex medical data. The integration of AI into healthcare systems aims to support clinicians, personalize patient care, and enhance population health, all while addressing the challenges posed by rising costs and limited resources. As a subdivision of computer science, AI focuses on the development of advanced algorithms capable of performing complex tasks that were once reliant on human intelligence. The ultimate goal is to achieve human-level performance with improved efficiency and accuracy in problem-solving and task execution, thereby reducing the need for human intervention. Various industries, including engineering, media/entertainment, finance, and education, have already reaped significant benefits by incorporating AI systems into their operations. Notably, the healthcare sector has witnessed rapid growth in the utilization of AI technology. Nevertheless, there remains untapped potential for AI to truly revolutionize the industry. It is important to note that despite concerns about job displacement, AI in healthcare should not be viewed as a threat to human workers. Instead, AI systems are designed to augment and support healthcare professionals, freeing up their time to focus on more complex and critical tasks. By automating routine and repetitive tasks, AI can alleviate the burden on healthcare professionals, allowing them to dedicate more attention to patient care and meaningful interactions. However, legal and ethical challenges must be addressed when embracing AI technology in medicine, alongside comprehensive public education to ensure widespread acceptance.","<method>machine learning algorithms</method>, <method>natural language processing</method>, <method>computer vision</method>"
2024,https://openalex.org/W4390919701,Psychology,Chatbots and Large Language Models in Radiology: A Practical Primer for Clinical and Research Applications,"Although chatbots have existed for decades, the emergence of transformer-based large language models (LLMs) has captivated the world through the most recent wave of artificial intelligence chatbots, including ChatGPT. Transformers are a type of neural network architecture that enables better contextual understanding of language and efficient training on massive amounts of unlabeled data, such as unstructured text from the internet. As LLMs have increased in size, their improved performance and emergent abilities have revolutionized natural language processing. Since language is integral to human thought, applications based on LLMs have transformative potential in many industries. In fact, LLM-based chatbots have demonstrated human-level performance on many professional benchmarks, including in radiology. LLMs offer numerous clinical and research applications in radiology, several of which have been explored in the literature with encouraging results. Multimodal LLMs can simultaneously interpret text and images to generate reports, closely mimicking current diagnostic pathways in radiology. Thus, from requisition to report, LLMs have the opportunity to positively impact nearly every step of the radiology journey. Yet, these impressive models are not without limitations. This article reviews the limitations of LLMs and mitigation strategies, as well as potential uses of LLMs, including multimodal models. Also reviewed are existing LLM-based applications that can enhance efficiency in supervised settings.","<method>transformer-based large language models (LLMs)</method>, <method>Transformers</method>, <method>Multimodal LLMs</method>"
2024,https://openalex.org/W4392783116,Psychology,Is it harmful or helpful? Examining the causes and consequences of generative AI usage among university students,"Abstract While the discussion on generative artificial intelligence, such as ChatGPT, is making waves in academia and the popular press, there is a need for more insight into the use of ChatGPT among students and the potential harmful or beneficial consequences associated with its usage. Using samples from two studies, the current research examined the causes and consequences of ChatGPT usage among university students. Study 1 developed and validated an eight-item scale to measure ChatGPT usage by conducting a survey among university students (N = 165). Study 2 used a three-wave time-lagged design to collect data from university students (N = 494) to further validate the scale and test the study’s hypotheses. Study 2 also examined the effects of academic workload, academic time pressure, sensitivity to rewards, and sensitivity to quality on ChatGPT usage. Study 2 further examined the effects of ChatGPT usage on students’ levels of procrastination, memory loss, and academic performance. Study 1 provided evidence for the validity and reliability of the ChatGPT usage scale. Furthermore, study 2 revealed that when students faced higher academic workload and time pressure, they were more likely to use ChatGPT. In contrast, students who were sensitive to rewards were less likely to use ChatGPT. Not surprisingly, use of ChatGPT was likely to develop tendencies for procrastination and memory loss and dampen the students’ academic performance. Finally, academic workload, time pressure, and sensitivity to rewards had indirect effects on students’ outcomes through ChatGPT usage.",No methods found.
2024,https://openalex.org/W4399528455,Psychology,Bias and Fairness in Large Language Models: A Survey,"Abstract Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this article, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.","<method>bias evaluation metrics</method>, <method>bias evaluation datasets</method>, <method>bias mitigation techniques</method>, <method>pre-processing intervention</method>, <method>in-training intervention</method>, <method>intra-processing intervention</method>, <method>post-processing intervention</method>"
2024,https://openalex.org/W4399803256,Psychology,Detecting hallucinations in large language models using semantic entropy,"Abstract Large language model (LLM) systems, such as ChatGPT 1 or Gemini 2 , can show impressive reasoning and question-answering capabilities but often ‘hallucinate’ false outputs and unsubstantiated answers 3,4 . Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents 5 or untrue facts in news articles 6 and even posing a risk to human life in medical domains such as radiology 7 . Encouraging truthfulness through supervision or reinforcement has been only partially successful 8 . Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. Here we develop new methods grounded in statistics, proposing entropy-based uncertainty estimators for LLMs to detect a subset of hallucinations—confabulations—which are arbitrary and incorrect generations. Our method addresses the fact that one idea can be expressed in many ways by computing uncertainty at the level of meaning rather than specific sequences of words. Our method works across datasets and tasks without a priori knowledge of the task, requires no task-specific data and robustly generalizes to new tasks not seen before. By detecting when a prompt is likely to produce a confabulation, our method helps users understand when they must take extra care with LLMs and opens up new possibilities for using LLMs that are otherwise prevented by their unreliability.",<method>entropy-based uncertainty estimators</method>
2024,https://openalex.org/W4390587679,Psychology,"A Systematic Review and Meta-Analysis of Artificial Intelligence Tools in Medicine and Healthcare: Applications, Considerations, Limitations, Motivation and Challenges","Artificial intelligence (AI) has emerged as a transformative force in various sectors, including medicine and healthcare. Large language models like ChatGPT showcase AI’s potential by generating human-like text through prompts. ChatGPT’s adaptability holds promise for reshaping medical practices, improving patient care, and enhancing interactions among healthcare professionals, patients, and data. In pandemic management, ChatGPT rapidly disseminates vital information. It serves as a virtual assistant in surgical consultations, aids dental practices, simplifies medical education, and aids in disease diagnosis. A total of 82 papers were categorised into eight major areas, which are G1: treatment and medicine, G2: buildings and equipment, G3: parts of the human body and areas of the disease, G4: patients, G5: citizens, G6: cellular imaging, radiology, pulse and medical images, G7: doctors and nurses, and G8: tools, devices and administration. Balancing AI’s role with human judgment remains a challenge. A systematic literature review using the PRISMA approach explored AI’s transformative potential in healthcare, highlighting ChatGPT’s versatile applications, limitations, motivation, and challenges. In conclusion, ChatGPT’s diverse medical applications demonstrate its potential for innovation, serving as a valuable resource for students, academics, and researchers in healthcare. Additionally, this study serves as a guide, assisting students, academics, and researchers in the field of medicine and healthcare alike.","<method>Large language models</method>, <method>ChatGPT</method>, <method>systematic literature review using the PRISMA approach</method>"
2024,https://openalex.org/W4392639864,Psychology,Comparing the quality of human and ChatGPT feedback of students’ writing,"Offering students formative feedback on their writing is an effective way to facilitate writing development. Recent advances in AI (i.e., ChatGPT) may function as an automated writing evaluation tool, increasing the amount of feedback students receive and diminishing the burden on teachers to provide frequent feedback to large classes. We examined the ability of generative AI (ChatGPT) to provide formative feedback. We compared the quality of human and AI feedback by scoring the feedback each provided on secondary student essays. We scored the degree to which feedback (a) was criteria-based, (b) provided clear directions for improvement, (c) was accurate, (d) prioritized essential features, and (e) used a supportive tone. 200 pieces of human-generated formative feedback and 200 pieces of AI-generated formative feedback for the same essays. We examined whether ChatGPT and human feedback differed in quality for the whole sample, for compositions that differed in overall quality, and for native English speakers and English learners by comparing descriptive statistics and effect sizes. Human raters were better at providing high-quality feedback to students in all categories other than criteria-based. AI and humans showed differences in feedback quality based on essay quality. Feedback did not vary by language status for humans or AI. Well-trained evaluators provided higher quality feedback than ChatGPT. Considering the ease of generating feedback through ChatGPT and its overall quality, generative AI may still be useful in some contexts, particularly in formative early drafts or instances where a well-trained educator is unavailable.",<method>generative AI (ChatGPT)</method>
2024,https://openalex.org/W4393149408,Psychology,Drivers of generative AI adoption in higher education through the lens of the Theory of Planned Behaviour,"Drawing on the Theory of Planned Behaviour (TPB), this study investigates the relationship between the perceived benefits, strengths, weaknesses, and risks of generative AI (GenAI) tools and the fundamental factors of the TPB model (i.e., attitude, subjective norms, and perceived behavioural control). The study also investigates the structural association between the TPB variables and intention to use GenAI tools, and how the latter might affect the actual usage of GenAI tools in higher education. The paper adopts a quantitative approach, relying on an anonymous self-administered online questionnaire to gather primary data from 130 lecturers and 168 students in higher education institutions (HEIs) in several countries, and PLS-SEM for data analysis. The results indicate that although lecturers' and students' perceptions of the risks and weaknesses of GenAI tools differ, the perceived strengths and advantages of GenAI technologies have a significant and positive impact on their attitudes, subjective norms, and perceived behavioural control. The TPB core variables positively and significantly impact lecturers' and students' intentions to use GenAI tools, which in turn significantly and positively impact their adoption of such tools. This paper advances theory by outlining the factors shaping the adoption of GenAI technologies in HEIs. It provides stakeholders with a variety of managerial and policy implications for how to formulate suitable rules and regulations to utilise the advantages of these tools while mitigating the impacts of their disadvantages. Limitations and future research opportunities are also outlined.",No methods found.
2024,https://openalex.org/W4393252719,Psychology,Application of generative artificial intelligence (GenAI) in language teaching and learning: A scoping literature review,"This scoping literature review examines the application of Generative Artificial Intelligence (GenAI), a disruptive technology, in language teaching and learning. Since its launch in November 2022, GenAI has captured global attention with OpenAI's ChatGPT, powered by the generative pre-trained transformer-3 (GPT-3) large-language model. The emergence of GenAI holds immense implications across various domains, including language education. This review aims to provide an overview of the current state of research and identify research gaps and future directions in this emerging field. The review follows the PRISMA-ScR guidelines and includes eligible publications published between 2017 and July 2023. Four electronic databases were searched and 41 of the 224 initial papers were eventually selected for review. The findings reveal key terms related to GenAI in language education, the most researched language study and education levels, areas of research, attitudes towards GenAI, and the potential benefits and challenges of GenAI application. The review highlights several research gaps, including the need for more empirical studies to assess the effectiveness and impact of GenAI tools, discussion of ethical considerations, targeted interventions for specific language skills, and stakeholder engagement in responsible integration. Educators are encouraged to incorporate GenAI tools into their teaching practices while remaining vigilant about potential risks. Continuous professional development for educators is crucial to ensure informed decision-making and effective integration of GenAI tools. This scoping review contributes to the existing knowledge on the use of GenAI in language education and informs future research and practice in this disruptive and rapidly evolving field.",<method>generative pre-trained transformer-3 (GPT-3)</method>
2024,https://openalex.org/W4390904004,Psychology,The Use of Artificial Intelligence in Writing Scientific Review Articles,"Abstract Purpose of Review With the recent explosion in the use of artificial intelligence (AI) and specifically ChatGPT, we sought to determine whether ChatGPT could be used to assist in writing credible, peer-reviewed, scientific review articles. We also sought to assess, in a scientific study, the advantages and limitations of using ChatGPT for this purpose. To accomplish this, 3 topics of importance in musculoskeletal research were selected: (1) the intersection of Alzheimer’s disease and bone; (2) the neural regulation of fracture healing; and (3) COVID-19 and musculoskeletal health. For each of these topics, 3 approaches to write manuscript drafts were undertaken: (1) human only; (2) ChatGPT only (AI-only); and (3) combination approach of #1 and #2 (AI-assisted). Articles were extensively fact checked and edited to ensure scientific quality, resulting in final manuscripts that were significantly different from the original drafts. Numerous parameters were measured throughout the process to quantitate advantages and disadvantages of approaches. Recent Findings Overall, use of AI decreased the time spent to write the review article, but required more extensive fact checking. With the AI-only approach, up to 70% of the references cited were found to be inaccurate. Interestingly, the AI-assisted approach resulted in the highest similarity indices suggesting a higher likelihood of plagiarism. Finally, although the technology is rapidly changing, at the time of study, ChatGPT 4.0 had a cutoff date of September 2021 rendering identification of recent articles impossible. Therefore, all literature published past the cutoff date was manually provided to ChatGPT, rendering approaches #2 and #3 identical for contemporary citations. As a result, for the COVID-19 and musculoskeletal health topic, approach #2 was abandoned midstream due to the extensive overlap with approach #3. Summary The main objective of this scientific study was to see whether AI could be used in a scientifically appropriate manner to improve the scientific writing process. Indeed, AI reduced the time for writing but had significant inaccuracies. The latter necessitates that AI cannot currently be used alone but could be used with careful oversight by humans to assist in writing scientific review articles.",<method>ChatGPT</method>
2024,https://openalex.org/W4399450035,Psychology,Power Hungry Processing: Watts Driving the Cost of AI Deployment?,"Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of ""generality"" comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and 'general-purpose' models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.","<method>finetuned models</method>, <method>multi-purpose generative architectures</method>"
2024,https://openalex.org/W4392732989,Psychology,Student perspectives on the use of generative artificial intelligence technologies in higher education,"Abstract The aim of this project was to understand student perspectives on generative artificial intelligence (GAI) technologies such as Chat generative Pre-Trained Transformer (ChatGPT), in order to inform changes to the University of Liverpool Academic Integrity code of practice. The survey for this study was created by a library student team and vetted through focus groups. A total of 2555 students participated in the survey. Results showed that only 7% of students who responded had not heard of any GAI technologies, whilst over half had used or considered using these for academic purposes. The majority of students (54.1%) were supportive or somewhat supportive of using tools such as Grammarly, but 70.4% were unsupportive or somewhat unsupportive towards students using tools such as ChatGPT to write their whole essay. Students who had higher levels of confidence in their academic writing were less likely to use or consider using them for academic purposes, and were also less likely to be supportive of other students using them. Most students (41.1%) also thought there should be a university wide policy on when these technologies are or are not appropriate to use. The results of this research suggest that students require clear policies on the use of GAI and that these technologies should not be banned from university, but consideration must be made to ensure different groups of students have equal access to the technologies.",No methods found.
2024,https://openalex.org/W4390833194,Psychology,Towards Conversational Diagnostic AI,"At the heart of medicine lies the physician-patient dialogue, where skillful history-taking paves the way for accurate diagnosis, effective management, and enduring trust. Artificial Intelligence (AI) systems capable of diagnostic dialogue could increase accessibility, consistency, and quality of care. However, approximating clinicians' expertise is an outstanding grand challenge. Here, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large Language Model (LLM) based AI system optimized for diagnostic dialogue. AMIE uses a novel self-play based simulated environment with automated feedback mechanisms for scaling learning across diverse disease conditions, specialties, and contexts. We designed a framework for evaluating clinically-meaningful axes of performance including history-taking, diagnostic accuracy, management reasoning, communication skills, and empathy. We compared AMIE's performance to that of primary care physicians (PCPs) in a randomized, double-blind crossover study of text-based consultations with validated patient actors in the style of an Objective Structured Clinical Examination (OSCE). The study included 149 case scenarios from clinical providers in Canada, the UK, and India, 20 PCPs for comparison with AMIE, and evaluations by specialist physicians and patient actors. AMIE demonstrated greater diagnostic accuracy and superior performance on 28 of 32 axes according to specialist physicians and 24 of 26 axes according to patient actors. Our research has several limitations and should be interpreted with appropriate caution. Clinicians were limited to unfamiliar synchronous text-chat which permits large-scale LLM-patient interactions but is not representative of usual clinical practice. While further research is required before AMIE could be translated to real-world settings, the results represent a milestone towards conversational diagnostic AI.","<method>Large Language Model (LLM)</method>, <method>self-play based simulated environment</method>"
2024,https://openalex.org/W4391776447,Psychology,Theories of motivation: A comprehensive analysis of human behavior drivers,"This paper explores theories of motivation, including instinct theory, arousal theory, incentive theory, intrinsic theory, extrinsic theory, the ARCS model, self-determination theory, expectancy-value theory, and goal-orientation theory. Each theory is described in detail, along with its key concepts, assumptions, and implications for behavior. Intrinsic theory suggests that individuals are motivated by internal factors like enjoyment and satisfaction, while extrinsic theory suggests that external factors like rewards and social pressure drive behavior. Arousal theory says that to feel motivated, people try to keep an optimal level of activation or excitement. Incentive theory suggests that behavior is driven by the promise of rewards or the threat of punishment. The ARCS model, designed to motivate learners, incorporates elements of attention, relevance, confidence, and satisfaction. Self-determination theory proposes that individuals are motivated by their needs for autonomy, competence, and relatedness. The expectation-value theory suggests that behavior is influenced by individuals' beliefs about their ability to succeed and the value they place on the task. The goal-orientation theory suggests that individuals have different goals for engaging in a behavior. By understanding these different theories of motivation, educators, coaches, managers, and individuals may analyze what drives behavior and how to harness it to achieve their goals. In essence, a nuanced comprehension of these diverse motivation theories equips individuals across varied domains with a strategic toolkit to navigate the complex landscape of human behavior, fostering a more profound understanding of what propels actions and how to channel these insights toward the attainment of overarching goals.",No methods found.
2024,https://openalex.org/W4392866787,Psychology,In search of artificial intelligence (AI) literacy in teacher education: A scoping review,"Artificial Intelligence (AI) literacy has recently emerged on the educational agenda raising expectations on teachers' and teacher educators' professional knowledge. This scoping review examines how the scientific literature conceptualises AI literacy in relation to teachers' different forms of professional knowledge relevant for Teacher Education (TE). The search strategy included papers and proceedings from 2000- to 2023 related to AI literacy and TE as well as the intersection of AI and teaching. Thirty-four papers were included in the analysis. The Aristotelian concepts episteme (theoretical-scientific knowledge), techne (practical-productive knowledge), and phronesis (professional judgement) were used as a lens to capture implicit and explicit dimensions of teachers' professional knowledge. Results indicate that AI literacy is a globally emerging research topic in education but almost absent in the context of TE. The literature covers many different topics and draws on different methodological approaches. Computer science and exploratory teaching approaches influence the type of epistemic, practical, and ethical knowledge. Currently, teachers' professional knowledge is not broadly addressed or captured in the research. Questions of ethics are predominantly addressed as a matter of understanding technical configurations of data-driven AI technologies. Teacher's' practical knowledge tends to translate into the adoption of digital resources for teaching about AI or the integration of AI EdTech into teaching. By identifying several research gaps, particularly concerning teachers' practical and ethical knowledge, this paper adds to a more comprehensive understanding of AI literacy in teaching and can contribute to a more well-informed AI literacy education in TE as well as laying the ground for future research related to teachers' professional knowledge.",No methods found.
2024,https://openalex.org/W4391844002,Psychology,Texture Exposure of Unconventional (101)<sub>Zn</sub> Facet: Enabling Dendrite‐Free Zn Deposition on Metallic Zinc Anodes,"Abstract Texturing metallic zinc anodes (MZAs) for selective exposure of (002) Zn plane with high thermodynamical stability is an efficient scheme for dendrite‐free Zn electrodeposition. However, fundamental factors that influence Zn deposition morphology via surface crystallographic texture engineering are not well understood. Herein, different from traditional cognition, MZAs with preferential exposure of (101) Zn facet are demonstrated to be equally effective in promoting dendrite‐free Zn deposition, which is enabled by introducing trace amount (0.01 m ) of theophylline into ZnSO 4 electrolyte. Experimental results and mathematical model corroborate, indicating mechanistically that the theophylline derived cations preferentially adsorb on the (002) Zn crystal plane due to higher adsorption energy, thereby accelerating its growth through increased binding affinity with Zn 2+ ions. Consequently, this phenomenon facilitates the texture exposure of (101) Zn facet to achieve ordered surface crystallographic orientation of MZAs (101‐Zn), thus enabling electrodeposition/dissolution cycling over 650 h under a depth of discharge up to 40% and significantly boosting the rechargeability (76.7% capacity retention after 1000 cycles) of the 101‐Zn||carbon‐cloth@MnO 2 full battery relative to counterpart without theophylline additive (36.3%). The work offers deep insights on the scientific links between the surface crystallographic orientation of MZAs and Zn deposition morphology, while opens up vast untapped opportunities to realize dendrite‐free MZAs.",No methods found.
2024,https://openalex.org/W4394929444,Psychology,AI literacy and its implications for prompt engineering strategies,"Artificial intelligence technologies are rapidly advancing. As part of this development, large language models (LLMs) are increasingly being used when humans interact with systems based on artificial intelligence (AI), posing both new opportunities and challenges. When interacting with LLM-based AI system in a goal-directed manner, prompt engineering has evolved as a skill of formulating precise and well-structured instructions to elicit desired responses or information from the LLM, optimizing the effectiveness of the interaction. However, research on the perspectives of non-experts using LLM-based AI systems through prompt engineering and on how AI literacy affects prompting behavior is lacking. This aspect is particularly important when considering the implications of LLMs in the context of higher education. In this present study, we address this issue, introduce a skill-based approach to prompt engineering, and explicitly consider the role of non-experts' AI literacy (students) in their prompt engineering skills. We also provide qualitative insights into students' intuitive behaviors towards LLM-based AI systems. The results show that higher-quality prompt engineering skills predict the quality of LLM output, suggesting that prompt engineering is indeed a required skill for the goal-directed use of generative AI tools. In addition, the results show that certain aspects of AI literacy can play a role in higher quality prompt engineering and targeted adaptation of LLMs within education. We, therefore, argue for the integration of AI educational content into current curricula to enable a hybrid intelligent society in which students can effectively use generative AI tools such as ChatGPT.",No methods found.
2024,https://openalex.org/W4392767952,Psychology,What are artificial intelligence literacy and competency? A comprehensive framework to support them,"Artificial intelligence (AI) education in K–12 schools is a global initiative, yet planning and executing AI education is challenging. The major frameworks are focused on identifying content and technical knowledge (AI literacy). Most of the current definitions of AI literacy for a non-technical audience are developed from an engineering perspective and may not be appropriate for K–12 education. Teacher perspectives are essential to making sense of this initiative. Literacy is about knowing (knowledge, what skills); competency is about applying the knowledge in a beneficial way (confidence, how well). They are strongly related. This study goes beyond knowledge (AI literacy), and its two main goals are to (i) define AI literacy and competency by adding the aspects of confidence and self-reflective mindsets, and (ii) propose a more comprehensive framework for K–12 AI education. These definitions are needed for this emerging and disruptive technology (e.g., ChatGPT and Sora, generative AI). We used the definitions and the basic curriculum design approaches as the analytical framework and teacher perspectives. Participants included 30 experienced AI teachers from 15 middle schools. We employed an iterative co-design cycle to discuss and revise the framework throughout four cycles. The definition of AI competency has five abilities that take confidence into account, and the proposed framework comprises five key components: technology, impact, ethics, collaboration, and self-reflection. We also identify five effective learning experiences to foster abilities and confidences, and suggest five future research directions: prompt engineering, data literacy, algorithmic literacy, self-reflective mindset, and empirical research.",No methods found.
2024,https://openalex.org/W4392130427,Psychology,Artificial intelligence and machine learning applications in the project lifecycle of the construction industry: A comprehensive review,"The construction industry faces many challenges, including schedule and cost overruns, productivity constraints, and workforce shortages. Compared to other sectors, it lags in digitalization in every project phase. Artificial Intelligence (AI) and Machine Learning (ML) have emerged as transformative technologies revolutionizing the construction sector. However, a discernible gap persists in systematically categorizing the applications of these technologies throughout the various phases of the construction project life cycle. In response to this gap, this research aims to present a thorough assessment of the deployment of AI and ML across diverse phases in construction projects, with the ultimate goal of furnishing valuable insights for the effective integration of these intelligent systems within the construction sector. A thorough literature review was performed to identify AI and ML applications in the building sector. After scrutinizing the literature, the applications of AI and ML were presented based on a construction project life cycle. A critical review of existing literature on AI and ML applications in the building industry showed that AI and ML applications are more frequent in the planning and construction stages. Moreover, the opportunities for AI and ML applications in other stages were discussed based on the life cycle categorization and presented in this study. The practical contribution of the study lies in providing valuable insights for the effective integration of intelligent systems within the construction sector. Academically, the research contributes by conducting a thorough literature review, categorizing AI and ML applications based on the construction project life cycle, and identifying opportunities for their deployment in different stages.",No methods found.
2024,https://openalex.org/W4392791588,Psychology,Can large language models replace humans in systematic reviews? Evaluating <scp>GPT</scp>‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages,"Systematic reviews are vital for guiding practice, research and policy, although they are often slow and labour-intensive. Large language models (LLMs) could speed up and automate systematic reviews, but their performance in such tasks has yet to be comprehensively evaluated against humans, and no study has tested Generative Pre-Trained Transformer (GPT)-4, the biggest LLM so far. This pre-registered study uses a ""human-out-of-the-loop"" approach to evaluate GPT-4's capability in title/abstract screening, full-text review and data extraction across various literature types and languages. Although GPT-4 had accuracy on par with human performance in some tasks, results were skewed by chance agreement and dataset imbalance. Adjusting for these caused performance scores to drop across all stages: for data extraction, performance was moderate, and for screening, it ranged from none in highly balanced literature datasets (~1:1) to moderate in those datasets where the ratio of inclusion to exclusion in studies was imbalanced (~1:3). When screening full-text literature using highly reliable prompts, GPT-4's performance was more robust, reaching ""human-like"" levels. Although our findings indicate that, currently, substantial caution should be exercised if LLMs are being used to conduct systematic reviews, they also offer preliminary evidence that, for certain review tasks delivered under specific conditions, LLMs can rival human performance.","<method>Large language models (LLMs)</method>, <method>Generative Pre-Trained Transformer (GPT)-4</method>"
2024,https://openalex.org/W4391473908,Psychology,Exploring AI-mediated informal digital learning of English (AI-IDLE): a mixed-method investigation of Chinese EFL learners’ AI adoption and experiences,"Recent advancements in natural language processing and large language models have ushered language learning into the age of artificial intelligence (AI). Recognizing the affordances of generative AI tools, this paper aims to examine the degree to which L2 learners accepted and leveraged large language model platforms (e.g. ChatGPT, Bing Chat) for the informal digital learning of English (IDLE) purposes. Employing an explanatory sequential mixed-method design, this study draws on the technology acceptance model (TAM) and collects data via an adapted TAM questionnaire and an interview guide. A total of 867 Chinese EFL (English as a foreign language) learners answered the online survey, while 20 attended the post-survey interviews. Drawing on a validated structural model that elucidates the inter-factor relationships of perceived ease of use, perceived usefulness, intention to use, and actual use, the quantitative analysis provides statistical accounts for EFL learners' adoption of Generative Pre-trained Transformer (GPT) technologies. The qualitative findings, derived from the interview data, reveal three key themes: (1) how perceived usefulness of chatbots for IDLE emerges from hands-on experimentation with these tools; (2) how intention to use increases as learners negotiate chatbot affordances and constraints; and (3) how actual use of chatbots for IDLE involves using these tools as tutors or conversation partners. Connections between quantitative and qualitative findings enhance our understanding of how EFL learners negotiate the affordances and constraints of highly capable AI technologies to participate in creative IDLE practices. By understanding these practices, this study draws attention to the attitudes and practices that constitute AI literacies, ultimately offering implications for future classroom practices and research.",<method>Generative Pre-trained Transformer (GPT)</method>
2024,https://openalex.org/W4401081015,Psychology,A Multi-Criteria Group Decision-Making Approach For Robot Selection Using Interval-Valued Intuitionistic Fuzzy Information And Aczel-Alsina Bonferroni Means,"The process of identifying the most appropriate robot for a particular industrial task has grown challenging and more difficult in the fast-paced environment. It is merely driven by the complex evolution and continuous integration of modern characteristics and advanced features by various suppliers. Industrial robots are now widely available in the marketplace, each possessing a distinctive collection of skills, attributes, and requirements. However, the selection of optimal robots is heavily influenced by factors such as the manufacturing environment, product design, production system, and overall cost considerations. These factors directly impact the decision-making process. The ultimate goal for the decision maker is to pinpoint and choose the most suitable robot, capable of delivering the desired output while minimizing costs and catering to the specific requirements of the industry. So, to consider it, in this paper, the hybrid structure of the Aczel-Alsina (AA) and Bonferroni mean (BM) operators for interval-valued intuitionistic fuzzy (IVIF) environment has been proposed, which can show the interrelationship between the multiple criteria and assist the expertise in decision-making (DM) process. Moreover, the algorithm and methodology for the multi-criteria group decision-making (MCGDM) problem have been defined which is further utilized by solving a real-world problem to demonstrate the effectiveness and validity of the proposed method. At last, the comparison analysis between prior and proposed studies has been displayed, and then followed by the conclusion of the result.",No methods found.
2024,https://openalex.org/W4392343959,Psychology,Investigating factors of students' behavioral intentions to adopt chatbot technologies in higher education: Perspective from expanded diffusion theory of innovation,"With the emergence of emerging 4IR technologies, AI application tools (chatbots) are becoming more and more popular and widespread in various fields, including education. This study investigates the factors that influence undergraduate students' inclination to utilize AI application tools, specifically chatbots, for educational purposes. We applied an expanded diffusion theory of innovation framework to examine the relationships between relative advantages, compatibility, trialability, trust, perceived usefulness, perceived ease of use, and behavioral intention. Data from 842 undergraduate students were collected through a questionnaire using a 7-point scale, and the findings were analyzed using SmartPLS 4.0.9.2 software with a covariance-based structural equation model. The results confirm the hypotheses regarding the relative advantages, compatibility, trialability, perceived usefulness, and trust of chatbots. Students who perceive the benefits of chatbots express a strong intention to use them for academic purposes. The perception of compatibility between students and chatbots positively influences their adoption intention, and those who have the opportunity to try out chatbots are more likely to use them, indicating the importance of trialability. Surprisingly, the study did not find direct relationships between perceived usefulness, perceived ease of use, and behavioral intention, suggesting the presence of other influencing factors or dynamics in the adoption of chatbots for educational purposes. The findings offer practical insights for students and contribute to the theoretical understanding of the diffusion theory of innovation. Future research can further explore these findings to gain deeper insights into the complexities of chatbot adoption and enhance the adoption of AI tools in educational settings.",No methods found.
2024,https://openalex.org/W4394884079,Psychology,TRANSFORMING FINTECH FRAUD DETECTION WITH ADVANCED ARTIFICIAL INTELLIGENCE ALGORITHMS,"The rapid evolution of financial technology (fintech) platforms has exponentially increased the volume and sophistication of financial transactions, concurrently elevating the risk and complexity of fraudulent activities. This necessitates a paradigm shift in fraud detection methodologies towards more agile, accurate, and predictive solutions. This paper presents a comprehensive study on the transformative potential of advanced Artificial Intelligence (AI) algorithms in enhancing fintech fraud detection mechanisms. By leveraging cutting-edge AI techniques including deep learning, machine learning, and natural language processing, this research aims to develop a robust fraud detection framework capable of identifying, analyzing, and preventing fraudulent transactions in real-time.&#x0D; Our methodology encompasses the deployment of several AI algorithms on extensive datasets comprising genuine and fraudulent financial transactions. Through a comparative analysis, we identify the most effective algorithms in terms of accuracy, efficiency, and scalability. Key findings reveal that deep learning models, particularly those employing neural networks, outperform traditional machine learning models in detecting complex and nuanced fraudulent activities. Furthermore, the integration of natural language processing enables the extraction and analysis of unstructured data, significantly enhancing the detection capabilities.&#x0D; Conclusively, this paper underscores the critical role of advanced AI algorithms in revolutionizing fintech fraud detection. It highlights the superior performance of AI-based models over conventional methods, offering fintech platforms a more dynamic and predictive approach to fraud prevention. This research not only contributes to the academic discourse on financial security but also provides practical insights for fintech companies striving to safeguard their operations against fraud.&#x0D; Keywords: Artificial Intelligence, Fintech, Fraud Detection, Ethical Ai, Regulatory Compliance, Data Privacy, Algorithmic Bias, Predictive Analytics, Blockchain Technology, Quantum Computing, Interdisciplinary Collaboration, Innovation, Transparency, Accountability, Continuous Learning, Ethical Principles, Real-Time Processing, Financial Sector.","<method>deep learning</method>, <method>machine learning</method>, <method>natural language processing</method>, <method>neural networks</method>"
2024,https://openalex.org/W4393902165,Psychology,Role of activity-based learning and ChatGPT on students' performance in education,"This study investigates the impact of activity-based learning and the utilization of ChatGPT on students' academic performance within the educational framework. The study aims to assess the effectiveness of activity-based learning in comparison to traditional methods, while also evaluating the potential benefits and drawbacks of integrating ChatGPT as an educational tool. The study employs a comparative approach, analyzing the outcomes of students exposed to activity-based learning versus those using conventional methods. Additionally, the study examines the usage of ChatGPT in education through surveys and trials to determine its contribution to personalized feedback, interactive learning, and innovative teaching methods. The findings reveal that activity-based learning enhances students' engagement, motivation, and critical thinking skills. Students participating in activity-based learning demonstrate improved academic achievement, which is attributed to their active involvement and practical application of knowledge. Similarly, the integration of ChatGPT offers novel avenues for interactive learning and individualized assistance, fostering students' understanding and exploration of complex concepts. In conclusion, activity-based learning proves to be a student-centered approach that enhances learning outcomes by fostering active participation and practical engagement. The utilization of ChatGPT in education showcases its potential to enhance educational experiences through interactive conversations and innovative teaching methodologies, despite considerations regarding potential limitations and ethical implications.",No methods found.
2024,https://openalex.org/W4391572037,Psychology,Machine Learning Applications in Healthcare: Current Trends and Future Prospects,"The integration of machine learning (ML) in healthcare has witnessed remarkable advancements, transforming the landscape of medical diagnosis, treatment, and overall patient care. This article provides a comprehensive review of the current trends and future prospects of machine learning applications in the healthcare domain.The current landscape is characterized by the utilization of ML algorithms for disease diagnosis and risk prediction, personalized treatment plans, and efficient healthcare resource management. Notable applications include image recognition for radiology and pathology, predictive analytics for disease prognosis, and the development of precision medicine tailored to individual patient profiles.This review explores the evolving role of ML in improving patient outcomes, enhancing clinical decision-making, and optimizing healthcare workflows. It delves into the challenges faced in integrating ML into existing healthcare systems, such as data privacy concerns, interpretability of complex models, and the need for robust validation processes.Additionally, the article discusses future prospects and emerging trends in ML healthcare applications, including the potential for predictive analytics to preemptively identify health issues, the integration of wearable devices and remote monitoring for continuous patient care, and the intersection of ML with genomics for personalized medicine.The overarching goal of this article is to provide healthcare professionals, researchers, and policymakers with insights into the current state of ML applications in healthcare, along with an outlook on the transformative potential that machine learning holds for the future of healthcare delivery and patient outcomes.","<method>machine learning (ML) algorithms</method>, <method>image recognition</method>, <method>predictive analytics</method>"
2024,https://openalex.org/W4392503764,Psychology,Mental-LLM,"Advances in large language models (LLMs) have empowered a variety of applications. However, there is still a significant gap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health. In this work, we present a comprehensive evaluation of multiple LLMs on various mental health prediction tasks via online text data, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4. We conduct a broad range of experiments, covering zero-shot prompting, few-shot prompting, and instruction fine-tuning. The results indicate a promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9% on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8%. They further perform on par with the state-of-the-art task-specific language model. We also conduct an exploratory case study on LLMs' capability on mental health reasoning tasks, illustrating the promising capability of certain models such as GPT-4. We summarize our findings into a set of action guidelines for potential methods to enhance LLMs' capability for mental health tasks. Meanwhile, we also emphasize the important limitations before achieving deployability in real-world mental health settings, such as known racial and gender bias. We highlight the important ethical risks accompanying this line of research.","<method>zero-shot prompting</method>, <method>few-shot prompting</method>, <method>instruction fine-tuning</method>"
2024,https://openalex.org/W4392563703,Psychology,Teaching CS50 with AI: Leveraging Generative Artificial Intelligence in Computer Science Education,"In Summer 2023, we developed and integrated a suite of AI-based software tools into CS50 at Harvard University. These tools were initially available to approximately 70 summer students, then to thousands of students online, and finally to several hundred on campus during Fall 2023. Per the course's own policy, we encouraged students to use these course-specific tools and limited the use of commercial AI software such as ChatGPT, GitHub Copilot, and the new Bing. Our goal was to approximate a 1:1 teacher-to-student ratio through software, thereby equipping students with a pedagogically-minded subject-matter expert by their side at all times, designed to guide students toward solutions rather than offer them outright. The tools were received positively by students, who noted that they felt like they had ""a personal tutor.'' Our findings suggest that integrating AI thoughtfully into educational settings enhances the learning experience by providing continuous, customized support and enabling human educators to address more complex pedagogical issues. In this paper, we detail how AI tools have augmented teaching and learning in CS50, specifically in explaining code snippets, improving code style, and accurately responding to curricular and administrative queries on the course's discussion forum. Additionally, we present our methodological approach, implementation details, and guidance for those considering using these tools or AI generally in education.",No methods found.
2024,https://openalex.org/W4392239564,Psychology,Human-AI collaboration patterns in AI-assisted academic writing,"Artificial Intelligence (AI) has increasingly influenced higher education, notably in academic writing where AI-powered assisting tools offer both opportunities and challenges. Recently, the rapid growth of generative AI (GAI) has brought its impacts into sharper focus, yet the dynamics of its utilisation in academic writing remain largely unexplored. This paper focuses on examining the nature of human-AI interactions in academic writing, specifically investigating the strategies doctoral students employ when collaborating with a GAI-powered assisting tool. This study involves 626 recorded activities on how ten doctoral students interact with GAI-powered assisting tool during academic writing. AI-driven learning analytics approach was adopted for three layered analyses: (1) data pre-processing and analysis with quantitative content analysis, (2) sequence analysis with Hidden Markov Model (HMM) and hierarchical sequence clustering, and (3) pattern analysis with process mining. Findings indicate that doctoral students engaging in iterative, highly interactive processes with the GAI-powered assisting tool generally achieve better performance in the writing task. In contrast, those who use GAI merely as a supplementary information source, maintaining a linear writing approach, tend to get lower writing performance. This study points to the need for further investigations into human-AI collaboration in learning in higher education, with implications for tailored educational strategies and solutions.","<method>AI-driven learning analytics</method>, <method>quantitative content analysis</method>, <method>Hidden Markov Model (HMM)</method>, <method>hierarchical sequence clustering</method>, <method>process mining</method>"
2024,https://openalex.org/W4392303127,Psychology,Recent advancements and challenges of NLP-based sentiment analysis: A state-of-the-art review,"Sentiment analysis is a method within natural language processing that evaluates and identifies the emotional tone or mood conveyed in textual data. Scrutinizing words and phrases categorizes them into positive, negative, or neutral sentiments. The significance of sentiment analysis lies in its capacity to derive valuable insights from extensive textual data, empowering businesses to grasp customer sentiments, make informed choices, and enhance their offerings. For the further advancement of sentiment analysis, gaining a deep understanding of its algorithms, applications, current performance, and challenges is imperative. Therefore, in this extensive survey, we began exploring the vast array of application domains for sentiment analysis, scrutinizing them within the context of existing research. We then delved into prevalent pre-processing techniques, datasets, and evaluation metrics to enhance comprehension. We also explored Machine Learning, Deep Learning, Large Language Models and Pre-trained models in sentiment analysis, providing insights into their advantages and drawbacks. Subsequently, we precisely reviewed the experimental results and limitations of recent state-of-the-art articles. Finally, we discussed the diverse challenges encountered in sentiment analysis and proposed future research directions to mitigate these concerns. This extensive review provides a complete understanding of sentiment analysis, covering its models, application domains, results analysis, challenges, and research directions.","<method>Machine Learning</method>, <method>Deep Learning</method>, <method>Large Language Models</method>, <method>Pre-trained models</method>"
2024,https://openalex.org/W4392560365,Psychology,Artificial Intelligence for Clinical Prediction: Exploring Key Domains and Essential Functions,"Clinical prediction is integral to modern healthcare, leveraging current and historical medical data to forecast health outcomes. The integration of Artificial Intelligence (AI) in this field significantly enhances diagnostic accuracy, treatment planning, disease prevention, and personalised care leading to better patient outcomes and healthcare efficiency. This systematic review implemented a structured four-step methodology, including an extensive literature search in academic databases (PubMed, Embase, Google Scholar), applying specific inclusion and exclusion criteria, data extraction focusing on AI techniques and their applications in clinical prediction, and a thorough analysis of the collected information to understand AI's roles in enhancing clinical prediction. Through the analysis of 74 experimental studies, eight key domains, where AI significantly enhances clinical prediction, were identified: 1) Diagnosis and early detection of disease; 2) Prognosis of disease course and outcomes; 3) Risk assessment of future disease; 4) Treatment response for personalised medicine; 5) Disease progression; 6) Readmission risks; 7) Complication risks; and 8) Mortality prediction. Oncology and radiology come on top of the specialties benefiting from AI in clinical prediction. The review highlights AI's transformative impact across various clinical prediction domains, including its role in revolutionising diagnostics, improving prognosis accuracy, aiding in personalised medicine, and enhancing patient safety. AI-driven tools contribute significantly to the efficiency and effectiveness of healthcare delivery. AI's integration in clinical prediction marks a substantial advancement in healthcare. Recommendations include enhancing data quality and accessibility, promoting interdisciplinary collaboration, focusing on ethical AI practices, investing in AI education, expanding clinical trials, developing regulatory oversight, involving patients in the AI integration process, and continuous monitoring and improvement of AI systems.",No methods found.
2024,https://openalex.org/W4401171937,Psychology,Exploring students’ perspectives on Generative AI-assisted academic writing,"Abstract The rapid development of generative artificial intelligence (GenAI), including large language models (LLM), has merged to support students in their academic writing process. Keeping pace with the technical and educational landscape requires careful consideration of the opportunities and challenges that GenAI-assisted systems create within education. This serves as a useful and necessary starting point for fully leveraging its potential for learning and teaching. Hence, it is crucial to gather insights from diverse perspectives and use cases from actual users, particularly the unique voices and needs of student-users. Therefore, this study explored and examined students' perceptions and experiences about GenAI-assisted academic writing by conducting in-depth interviews with 20 Chinese students in higher education after completing academic writing tasks using a ChatGPT4-embedded writing system developed by the research team. The study found that students expected AI to serve multiple roles, including multi-tasking writing assistant, virtual tutor, and digital peer to support multifaceted writing processes and performance. Students perceived that GenAI-assisted writing could benefit them in three areas including the writing process, performance, and their affective domain. Meanwhile, they also identified AI-related, student-related, and task-related challenges that were experienced during the GenAI-assisted writing activity. These findings contribute to a more nuanced understanding of GenAI's impact on academic writing that is inclusive of student perspectives, offering implications for educational AI design and instructional design.","<method>generative artificial intelligence (GenAI)</method>, <method>large language models (LLM)</method>, <method>ChatGPT4-embedded writing system</method>"
2024,https://openalex.org/W4391031493,Psychology,Board gender diversity and ESG performance: The mediating role of temporal orientation in South Africa context,"Prevailing research on the interaction between board gender diversity (BGD) and Environmental, Social, and Governance (ESG) performance presents equivocal findings, particularly in the context of developing countries. This study ventures into an exploratory examination of this association, situated in the socio-cultural milieu of South Africa, a region where the lower social status of women often leads to a bias towards short-term perspectives. Drawing on the role congruity theory of prejudice toward female leaders, this study aims to investigate the mediating role of short-term orientation (SHRT) in the BGD-ESG relationship. We further explore how the preference of female directors toward SHRT varies depending on their tenure on the board and across family and non-family firms. The empirical findings, drawn from an examination of publicly listed non-financial firms on the Johannesburg Stock Exchange (JSE) from 2015 to 2020, indicate a negative relationship between BGD and ESG, with SHRT predominantly mediating this association. Additionally, the tenure of female directors attenuates their preference for SHRT. Notably, we found the effect of BGD on SHRT is less pronounced in family firms, where the choices of female directors are more aligned with the family firm's long-term orientation. Our findings contribute to both theory and practice by advancing our understanding of the BGD-ESG relationship and providing practical implications for organizations, leaders, and policymakers.",No methods found.
2024,https://openalex.org/W4391256306,Psychology,How should we change teaching and assessment in response to increasingly powerful generative Artificial Intelligence? Outcomes of the ChatGPT teacher survey,"Abstract There has been widespread media commentary about the potential impact of generative Artificial Intelligence (AI) such as ChatGPT on the Education field, but little examination at scale of how educators believe teaching and assessment should change as a result of generative AI. This mixed methods study examines the views of educators ( n = 318) from a diverse range of teaching levels, experience levels, discipline areas, and regions about the impact of AI on teaching and assessment, the ways that they believe teaching and assessment should change, and the key motivations for changing their practices. The majority of teachers felt that generative AI would have a major or profound impact on teaching and assessment, though a sizeable minority felt it would have a little or no impact. Teaching level, experience, discipline area, region, and gender all significantly influenced perceived impact of generative AI on teaching and assessment. Higher levels of awareness of generative AI predicted higher perceived impact, pointing to the possibility of an ‘ignorance effect’. Thematic analysis revealed the specific curriculum, pedagogy, and assessment changes that teachers feel are needed as a result of generative AI, which centre around learning with AI, higher-order thinking, ethical values, a focus on learning processes and face-to-face relational learning. Teachers were most motivated to change their teaching and assessment practices to increase the performance expectancy of their students and themselves. We conclude by discussing the implications of these findings in a world with increasingly prevalent AI.",No methods found.
2024,https://openalex.org/W4390638955,Psychology,Artificial intelligence (AI) learning tools in K-12 education: A scoping review,"Abstract Artificial intelligence (AI) literacy is a global strategic objective in education. However, little is known about how AI should be taught. In this paper, 46 studies in academic conferences and journals are reviewed to investigate pedagogical strategies, learning tools, assessment methods in AI literacy education in K-12 contexts, and students’ learning outcomes. The investigation reveals that the promotion of AI literacy education has seen significant progress in the past two decades. This highlights that intelligent agents, including Google’s Teachable Machine, Learning ML, and Machine Learning for Kids, are age-appropriate tools for AI literacy education in K-12 contexts. Kindergarten students can benefit from learning tools such as PopBots, while software devices, such as Scratch and Python, which help to develop the computational thinking of AI algorithms, can be introduced to both primary and secondary schools. The research shows that project-based, human–computer collaborative learning and play- and game-based approaches, with constructivist methodologies, have been applied frequently in AI literacy education. Cognitive, affective, and behavioral learning outcomes, course satisfaction and soft skills acquisition have been reported. The paper informs educators of appropriate learning tools, pedagogical strategies, assessment methodologies in AI literacy education, and students’ learning outcomes. Research implications and future research directions within the K-12 context are also discussed.","<method>intelligent agents</method>, <method>Teachable Machine</method>, <method>Learning ML</method>, <method>Machine Learning for Kids</method>, <method>project-based learning</method>, <method>human–computer collaborative learning</method>, <method>play- and game-based approaches</method>, <method>constructivist methodologies</method>"
2024,https://openalex.org/W4396696202,Psychology,The Reporting of a Disproportionality Analysis for Drug Safety Signal Detection Using Individual Case Safety Reports in PharmacoVigilance (READUS-PV): Development and Statement,"Disproportionality analyses using reports of suspected adverse drug reactions are the most commonly used quantitative methods for detecting safety signals in pharmacovigilance. However, their methods and results are generally poorly reported in published articles and existing guidelines do not capture the specific features of disproportionality analyses. We here describe the development of a guideline (REporting of A Disproportionality analysis for drUg Safety signal detection using individual case safety reports in PharmacoVigilance [READUS-PV]) for reporting the results of disproportionality analyses in articles and abstracts. We established a group of 34 international experts from universities, the pharmaceutical industry, and regulatory agencies, with expertise in pharmacovigilance, disproportionality analyses, and assessment of safety signals. We followed a three-step process to develop the checklist: (1) an open-text survey to generate a first list of items; (2) an online Delphi method to select and rephrase the most important items; (3) a final online consensus meeting. Among the panel members, 33 experts responded to round 1 and 30 to round 2 of the Delphi and 25 participated to the consensus meeting. Overall, 60 recommendations for the main body of the manuscript and 13 recommendations for the abstracts were retained by participants after the Delphi method. After merging of some items together and the online consensus meeting, the READUS-PV guidelines comprise a checklist of 32 recommendations, in 14 items, for the reporting of disproportionality analyses in the main body text and four items, comprising 12 recommendations, for abstracts. The READUS-PV guidelines will support authors, editors, peer-reviewers, and users of disproportionality analyses using individual case safety report databases. Adopting these guidelines will lead to more transparent, comprehensive, and accurate reporting and interpretation of disproportionality analyses, facilitating the integration with other sources of evidence.",No methods found.
2024,https://openalex.org/W4391071215,Psychology,Automatic assessment of text-based responses in post-secondary education: A systematic review,"Text-based open-ended questions in academic formative and summative assessments help students become deep learners and prepare them to understand concepts for a subsequent conceptual assessment. However, grading text-based questions, especially in large (>50 enrolled students) courses, is tedious and time-consuming for instructors. Text processing models continue progressing with the rapid development of Artificial Intelligence (AI) tools and Natural Language Processing (NLP) algorithms. Especially after breakthroughs in Large Language Models (LLM), there is immense potential to automate rapid assessment and feedback of text-based responses in education. This systematic review adopts a scientific and reproducible literature search strategy based on the PRISMA process using explicit inclusion and exclusion criteria to study text-based automatic assessment systems in post-secondary education, screening 838 papers and synthesizing 93 studies. To understand how text-based automatic assessment systems have been developed and applied in education in recent years, three research questions are considered: 1) What types of automated assessment systems can be identified using input, output, and processing framework? 2) What are the educational focus and research motivations of studies with automated assessment systems? 3) What are the reported research outcomes in automated assessment systems and the next steps for educational applications? All included studies are summarized and categorized according to a proposed comprehensive framework, including the input and output of the system, research motivation, and research outcomes, aiming to answer the research questions accordingly. Additionally, the typical studies of automated assessment systems, research methods, and application domains in these studies are investigated and summarized. This systematic review provides an overview of recent educational applications of text-based assessment systems for understanding the latest AI/NLP developments assisting in text-based assessments in higher education. Findings will particularly benefit researchers and educators incorporating LLMs such as ChatGPT into their educational activities.","<method>Natural Language Processing (NLP) algorithms</method>, <method>Large Language Models (LLM)</method>"
2024,https://openalex.org/W4392251648,Psychology,A Comprehensive Survey on Source-Free Domain Adaptation,"Over the past decade, domain adaptation has become a widely studied branch of transfer learning which aims to improve performance on target domains by leveraging knowledge from the source domain. Conventional domain adaptation methods often assume access to both source and target domain data simultaneously, which may not be feasible in real-world scenarios due to privacy and confidentiality concerns. As a result, the research of Source-Free Domain Adaptation (SFDA) has drawn growing attention in recent years, which only utilizes the source-trained model and unlabeled target data to adapt to the target domain. Despite the rapid explosion of SFDA work, there has been no timely and comprehensive survey in the field. To fill this gap, we provide a comprehensive survey of recent advances in SFDA and organize them into a unified categorization scheme based on the framework of transfer learning. Instead of presenting each approach independently, we modularize several components of each method to more clearly illustrate their relationships and mechanisms in light of the composite properties of each method. Furthermore, we compare the results of more than 30 representative SFDA methods on three popular classification benchmarks, namely Office-31, Office-home, and VisDA, to explore the effectiveness of various technical routes and the combination effects among them. Additionally, we briefly introduce the applications of SFDA and related fields. Drawing on our analysis of the challenges confronting SFDA, we offer some insights into future research directions and potential settings.","<method>domain adaptation</method>, <method>transfer learning</method>, <method>Source-Free Domain Adaptation (SFDA)</method>"
2024,https://openalex.org/W4394009485,Psychology,AI-Driven Clinical Decision Support Systems: An Ongoing Pursuit of Potential,"Clinical Decision Support Systems (CDSS) are essential tools in contemporary healthcare, enhancing clinicians' decisions and patient outcomes. The integration of artificial intelligence (AI) is now revolutionizing CDSS even further. This review delves into AI technologies transforming CDSS, their applications in healthcare decision-making, associated challenges, and the potential trajectory toward fully realizing AI-CDSS's potential. The review begins by laying the groundwork with a definition of CDSS and its function within the healthcare field. It then highlights the increasingly significant role that AI is playing in enhancing CDSS effectiveness and efficiency, underlining its evolving prominence in shaping healthcare practices. It examines the integration of AI technologies into CDSS, including machine learning algorithms like neural networks and decision trees, natural language processing, and deep learning. It also addresses the challenges associated with AI integration, such as interpretability and bias. We then shift to AI applications within CDSS, with real-life examples of AI-driven diagnostics, personalized treatment recommendations, risk prediction, early intervention, and AI-assisted clinical documentation. The review emphasizes user-centered design in AI-CDSS integration, addressing usability, trust, workflow, and ethical and legal considerations. It acknowledges prevailing obstacles and suggests strategies for successful AI-CDSS adoption, highlighting the need for workflow alignment and interdisciplinary collaboration. The review concludes by summarizing key findings, underscoring AI's transformative potential in CDSS, and advocating for continued research and innovation. It emphasizes the need for collaborative efforts to realize a future where AI-powered CDSS optimizes healthcare delivery and improves patient outcomes.","<method>neural networks</method>, <method>decision trees</method>, <method>natural language processing</method>, <method>deep learning</method>"
2024,https://openalex.org/W4390667862,Psychology,Integration of Generative AI Techniques and Applications in Student Behavior and Cognitive Achievement in Arab Higher Education,"The integration of Artificial Intelligence (AI) in higher education has the power to revolutionize the learning experience by fostering engagement, personalization, efficiency, and innovation. AI offers a wide range of exciting possibilities where AI-powered tools enable students to receive tailored feedback and guidance, enabling them to learn at their own pace and excel academically. This research aims to investigate the effects of generative AI techniques and applications on students' cognitive achievement through student behavior. Data was collected through surveys in three Arab countries including Oman, Jordan and Yemen. 768 students from these Arab country's universities were participated in completing surveys randomly. Structure Equation Modeling SEM-PLS was adopted to analysis data. Results reveal that generative AI techniques and applications have positive and significant effects on students' cognitive achievement in Arab higher education institutions. Results also reveal that student behavior enhances the relationship among AI techniques, applications and cognitive achievement. These results highlight the crucial role of AI applications among students in higher education while the integration of this emerging technology is still at the first stage, students' interaction with and utility of these applications show high satisfactory level of their impact on students' behavior and cognitive achievement. This research contributes to literature of generative AI applications giving evidence from Arab region and filling the gap regarding usage of these applications in higher education.","<method>generative AI techniques</method>, <method>Structure Equation Modeling SEM-PLS</method>"
2024,https://openalex.org/W4398203672,Psychology,Hallucination Rates and Reference Accuracy of ChatGPT and Bard for Systematic Reviews: Comparative Analysis,"Background Large language models (LLMs) have raised both interest and concern in the academic community. They offer the potential for automating literature search and synthesis for systematic reviews but raise concerns regarding their reliability, as the tendency to generate unsupported (hallucinated) content persist. Objective The aim of the study is to assess the performance of LLMs such as ChatGPT and Bard (subsequently rebranded Gemini) to produce references in the context of scientific writing. Methods The performance of ChatGPT and Bard in replicating the results of human-conducted systematic reviews was assessed. Using systematic reviews pertaining to shoulder rotator cuff pathology, these LLMs were tested by providing the same inclusion criteria and comparing the results with original systematic review references, serving as gold standards. The study used 3 key performance metrics: recall, precision, and F1-score, alongside the hallucination rate. Papers were considered “hallucinated” if any 2 of the following information were wrong: title, first author, or year of publication. Results In total, 11 systematic reviews across 4 fields yielded 33 prompts to LLMs (3 LLMs×11 reviews), with 471 references analyzed. Precision rates for GPT-3.5, GPT-4, and Bard were 9.4% (13/139), 13.4% (16/119), and 0% (0/104) respectively (P&lt;.001). Recall rates were 11.9% (13/109) for GPT-3.5 and 13.7% (15/109) for GPT-4, with Bard failing to retrieve any relevant papers (P&lt;.001). Hallucination rates stood at 39.6% (55/139) for GPT-3.5, 28.6% (34/119) for GPT-4, and 91.4% (95/104) for Bard (P&lt;.001). Further analysis of nonhallucinated papers retrieved by GPT models revealed significant differences in identifying various criteria, such as randomized studies, participant criteria, and intervention criteria. The study also noted the geographical and open-access biases in the papers retrieved by the LLMs. Conclusions Given their current performance, it is not recommended for LLMs to be deployed as the primary or exclusive tool for conducting systematic reviews. Any references generated by such models warrant thorough validation by researchers. The high occurrence of hallucinations in LLMs highlights the necessity for refining their training and functionality before confidently using them for rigorous academic purposes.","<method>ChatGPT</method>, <method>Bard (Gemini)</method>, <method>GPT-3.5</method>, <method>GPT-4</method>"
2024,https://openalex.org/W4391993876,Psychology,"Digital twin simulation modeling, artificial intelligence-based Internet of Manufacturing Things systems, and virtual machine and cognitive computing algorithms in the Industry 4.0-based Slovak labor market","Research background: On the basis of an analysis of the current situation and expectations in the field of implementation of the elements of the Industry 4.0 concept, the purpose of this paper is to identify the effects on the labor market in large manufacturing enterprises in the Slovak Republic. Purpose of the article: The presented work has a theoretical-empirical nature and consists of a theoretical section and a practical section, which includes statistical indicator analysis and quantitative research. In the theoretical section, the paper discusses the issue of Industry 4.0 in general, with a focus on its impact on the labor market, thus laying the groundwork for future research on the subject. Methods: The output of this work is an analysis of selected indicators of the manufacturing industry sector in the Slovak Republic, based on the most recent employment data analysis in the first stage and quantitative research survey in the second stage, with the respondents being manufacturing industry companies operating in the Slovak Republic, and whose primary objective is to determine the current status of the implementation of the elements and technologies of Industry 4.0 in production companies in the Slovak Republic, as well as the factors influencing this situation, such as digital twin simulation modeling, artificial intelligence-based Internet of Manufacturing Things systems, and virtual machine and cognitive computing algorithms. Findings &amp; value added: The research findings indicate that the degree of digitization adopted by businesses in the Slovak Republic is comparatively less robust and more sluggish to adapt. This is primarily attributable to the underdeveloped educational system, population reluctance, self-actualization, and inadequate state support. Recommendations for the Slovak market aim to increase the digital proficiency of businesses and of the general populace through various means, such as reforming legislation, enhancing state support for entrepreneurs, and modifying the education system, constituting the added value of the work.","<method>digital twin simulation modeling</method>, <method>artificial intelligence-based Internet of Manufacturing Things systems</method>, <method>virtual machine algorithms</method>, <method>cognitive computing algorithms</method>"
2024,https://openalex.org/W4392109462,Psychology,Exploring User Adoption of ChatGPT: A Technology Acceptance Model Perspective,"In the rapidly evolving landscape of technology, the emergence of Chat Generative Pre-trained Transformer (ChatGPT) marks a pivotal milestone in the realm of Artificial Intelligence (AI). However, little research has reported the predictors of people's intentions to use ChatGPT. This pioneering study empirically examines user adoption through the lens of the Technology Acceptance Model (TAM) using a convenience sampling method. The study surveyed 784 ChatGPT users in China, of whom 58.93% were males. The results have revealed several key findings: (1) perceived usefulness, perceived ease of use, behavioral intention, and use behavior were positively correlated with each other; (2) behavioral intention acted as a mediating factor in the relationship between perceived usefulness and use behavior, as well as the relationship between perceived ease of use and use behavior; (3) perceived usefulness and behavioral intention played a chain-mediated role between perceived ease of use and use behavior; (4) the relationship between behavioral intention and use behavior exhibited greater strength among females compared to males; (5) the association between behavioral intention and use behavior was found to be stronger among urban users in comparison to their rural counterparts; (6) the connections between perceived ease of use and perceived usefulness, perceived ease of use and behavioral intention, and behavioral intention and use behavior were observed to be stronger among individuals with higher educational backgrounds relative to those with lower educational backgrounds. These findings provide crucial nuanced insights to advance the practical application of ChatGPT, emphasizing the need for enhanced usability and ease of use. However, this study exclusively captured usage behaviors within the Chinese user base. Future investigations could encompass diverse demographics across multiple countries, enabling cross-cultural comparisons.",<method>Technology Acceptance Model (TAM)</method>
2024,https://openalex.org/W4393086054,Psychology,Empowering student self‐regulated learning and science education through <scp>ChatGPT</scp>: A pioneering pilot study,"In recent years, AI technologies have been developed to promote students' self‐regulated learning (SRL) and proactive learning in digital learning environments. This paper discusses a comparative study between generative AI‐based (SRLbot) and rule‐based AI chatbots (Nemobot) in a 3‐week science learning experience with 74 Secondary 4 students in Hong Kong. The experimental group used SRLbot to maintain a regular study habit and facilitate their SRL, while the control group utilized rule‐based AI chatbots. Results showed that SRLbot effectively enhanced students' science knowledge, behavioural engagement and motivation. Quantile regression analysis indicated that the number of interactions significantly predicted variations in SRL. Students appreciated the personalized recommendations and flexibility of SRLbot, which adjusted responses based on their specific learning and SRL scenarios. The ChatGPT‐enhanced instructional design reduced learning anxiety and promoted learning performance, motivation and sustained learning habits. Students' feedback on learning challenges, psychological support and self‐regulation behaviours provided insights into their progress and experience with this technology. SRLbot's adaptability and personalized approach distinguished it from rule‐based chatbots. The findings offer valuable evidence for AI developers and educators to consider generative AI settings and chatbot design, facilitating greater success in online science learning. Practitioner notes What is already known about this topic AI technologies have been used to support student self‐regulated learning (SRL) across subjects. SRL has been identified as an important aspect of student learning that can be developed through technological support. Generative AI technologies like ChatGPT have shown potential for enhancing student learning by providing personalized guidance and feedback. What this paper adds This paper reports on a case study that specifically examines the effectiveness of ChatGPT in promoting SRL among secondary students. The study provides evidence that ChatGPT can enhance students' science knowledge, motivation and SRL compared to a rule‐based AI chatbot. The study offers insights into how ChatGPT can be used as a tool to facilitate SRL and promote sustained learning habits. Implications for practice and/or policy The findings of this study suggest that educators should consider the potential of ChatGPT and other generative AI technologies to support student learning and SRL. Educators and students should be aware of the limitations of AI technologies and ensure that they are used appropriately to generate desired responses. It is also important to equip teachers and students with AI competencies to enable them to use AI for learning and teaching.","<method>generative AI-based chatbot (SRLbot)</method>, <method>rule-based AI chatbot (Nemobot)</method>, <method>ChatGPT-enhanced instructional design</method>"
2024,https://openalex.org/W4393386279,Psychology,A classification tool to foster self-regulated learning with generative artificial intelligence by applying self-determination theory: a case of ChatGPT,"Abstract Generative AI such as ChatGPT provides an instant and individualized learning environment, and may have the potential to motivate student self-regulated learning (SRL), more effectively than other non-AI technologies. However, the impact of ChatGPT on student motivation, SRL, and needs satisfaction is unclear. Motivation and the SRL process can be explained using self-determination theory (SDT) and the three phases of forethought, performance, and self-reflection, respectively. Accordingly, a Delphi design was employed in this study to determine how ChatGPT-based learning activities satisfy students’ each SDT need, and foster each SRL phase from a teacher perspective. We involved 36 SDT school teachers with extensive expertise in technology enhanced learning to develop a classification tool for learning activities that affect student needs satisfaction and SRL phases using ChatGPT. We collaborated with the teachers in three rounds to investigate and identify the activities, and we revised labels, descriptions, and explanations. The major finding is that a classification tool for 20 learning activities using ChatGPT was developed. The tool suggests how ChatGPT better satisfy SDT-based needs, and fosters the three SRL phrases. This classification tool can assist researchers in replicating, implementing, and integrating successful ChatGPT in education research and development projects. The tool can inspire teachers to modify the activities using generative AI for their own teaching, and inform policymakers on how to develop guidelines for AI in education.",No methods found.
2024,https://openalex.org/W4399009670,Psychology,Exploring factors influencing the acceptance of ChatGPT in higher education: A smart education perspective,"AI-powered chatbots hold great promise for enhancing learning experiences and outcomes in today's rapidly evolving education system. However, despite the increasing demand for such technologies, there remains a significant research gap regarding the factors influencing users' acceptance and adoption of AI-powered chatbots in educational contexts. This study aims to address this gap by investigating the factors that shape users' attitudes, intentions, and behaviors towards adopting ChatGPT for smart education systems. This research employed a quantitative research approach, data were collected from 458 of participants through a structured questionnaire designed to measure various constructs related to technology acceptance, including perceived ease of use, perceived usefulness, feedback quality, assessment quality, subject norms, attitude towards use, and behavioral intention to use ChatGPT. Structural model analysis (SEM) Statistical techniques were then utilized to examine the relationships between these constructs. The findings of the study revealed that Perceived ease of use and perceived usefulness emerged as significant predictors of users' attitudes towards ChatGPT for smart education. Additionally, feedback quality, assessment quality, and subject norms were found to positively influence users' behavioral intentions to use ChatGPT for smart educational purposes. Moreover, users' attitudes towards use and behavioral intentions were significantly proved for the actual adoption of ChatGPT. However, a few hypotheses, such as the relationship between trust in ChatGPT and perceived usefulness, were not supported by the data. This study contributes to the existing body information systems applications for the determining factor of technology acceptance in smart education context.",No methods found.
2024,https://openalex.org/W4390674594,Psychology,Open access research outputs receive more diverse citations,"Abstract The goal of open access is to allow more people to read and use research outputs. An observed association between highly cited research outputs and open access has been claimed as evidence of increased usage of the research, but this remains controversial. A higher citation count also does not necessarily imply wider usage such as citations by authors from more places. A knowledge gap exists in our understanding of who gets to use open access research outputs and where users are located. Here we address this gap by examining the association between an output’s open access status and the diversity of research outputs that cite it. By analysing large-scale bibliographic data from 2010 to 2019, we found a robust association between open access and increased diversity of citation sources by institutions, countries, subregions, regions, and fields of research, across outputs with both high and medium–low citation counts. Open access through disciplinary or institutional repositories showed a stronger effect than open access via publisher platforms. This study adds a new perspective to our understanding of how citations can be used to explore the effects of open access. It also provides new evidence at global scale of the benefits of open access as a mechanism for widening the use of research and increasing the diversity of the communities that benefit from it.",No methods found.
2024,https://openalex.org/W4392119583,Psychology,Object detection and tracking in Precision Farming: a systematic review,"Object Detection and Tracking have gained importance in recent years because of the great advances in image and video analysis techniques and the accurate results these technologies are producing. Moreover, they have successfully been applied to multiple fields, including the agricultural domain since they offer real-time monitoring of the status of the crops and animals while counting how many are present within a field/barn. This review aims to review the current literature on Object Detection and Tracking within the field of Precision Farming. For that, over 300 research articles were explored, from which 150 articles from the last five years were systematically reviewed and analysed regarding the algorithms they implemented, the domain they belong to, the difficulties they faced, and which limitations should be tackled in the future. Lastly, it examines potential issues that this approach might have, for instance, the lack of open-source datasets with labelled data. The findings of this study indicate that Object Detection and Tracking are critical techniques to enhance Precision Farming and pave the way for robotization for the agricultural sector since they provide accurate results and insights on crop and animal management, and optimize resource allocation. Future work should focus on the optimal acquisition of the datasets prior to Object Detection and Tracking, along with the consideration of the biophysical environment of the farming scenarios.","<method>Object Detection</method>, <method>Tracking</method>"
2024,https://openalex.org/W4392202731,Psychology,Applying large language models and chain-of-thought for automatic scoring,"This study investigates the application of large language models (LLMs), specifically GPT-3.5 and GPT-4, with Chain-of-Though (CoT) in the automatic scoring of student-written responses to science assessments. We focused on overcoming the challenges of accessibility, technical complexity, and lack of explainability that have previously limited the use of artificial intelligence-based automatic scoring tools among researchers and educators. With a testing dataset comprising six assessment tasks (three binomial and three trinomial) with 1650 student responses, we employed six prompt engineering strategies to automatically score student responses. The six strategies combined zero-shot or few-shot learning with CoT, either alone or alongside item stem and scoring rubrics, developed based on a novel approach, WRVRT (prompt writing, reviewing, validating, revising, and testing). Results indicated that few-shot (acc = 0.67) outperformed zero-shot learning (acc = 0.60), with 12.6% increase. CoT, when used without item stem and scoring rubrics, did not significantly affect scoring accuracy (acc = 0.60). However, CoT prompting paired with contextual item stems and rubrics proved to be a significant contributor to scoring accuracy (13.44% increase for zero-shot; 3.7% increase for few-shot). We found a more balanced accuracy across different proficiency categories when CoT was used with a scoring rubric, highlighting the importance of domain-specific reasoning in enhancing the effectiveness of LLMs in scoring tasks. We also found that GPT-4 demonstrated superior performance over GPT -3.5 in various scoring tasks when combined with the single-call greedy sampling or ensemble voting nucleus sampling strategy, showing 8.64% difference. Particularly, the single-call greedy sampling strategy with GPT-4 outperformed other approaches. This study also demonstrates the potential of LLMs in facilitating explainable and interpretable automatic scoring, emphasizing that CoT enhances accuracy and transparency, particularly when used with item stem and scoring rubrics.","<method>large language models (LLMs)</method>, <method>GPT-3.5</method>, <method>GPT-4</method>, <method>Chain-of-Thought (CoT)</method>, <method>zero-shot learning</method>, <method>few-shot learning</method>, <method>prompt engineering strategies</method>, <method>WRVRT (prompt writing, reviewing, validating, revising, and testing)</method>, <method>single-call greedy sampling strategy</method>, <method>ensemble voting nucleus sampling strategy</method>"
2024,https://openalex.org/W4390584313,Psychology,A Conceptual Model for Inclusive Technology: Advancing Disability Inclusion through Artificial Intelligence,"Artificial intelligence (AI) has ushered in transformative changes, championing inclusion and accessibility for individuals with disabilities. This article delves into the remarkable AI-driven solutions that have revolutionized their lives across various domains. From assistive technologies such as voice recognition and AI-powered smart glasses catering to diverse needs, to healthcare benefiting from early disease detection algorithms and wearable devices that monitor vital signs and alert caregivers in emergencies, AI has steered in significant enhancements. Moreover, AI-driven prosthetics and exoskeletons have substantially improved mobility for those with limb impairments. The realm of education has not been left untouched, with AI tools creating inclusive learning environments that adapt to individual learning styles, paving the way for academic success among students with disabilities. However, the boundless potential of AI also presents ethical concerns and challenges. Issues like safeguarding data privacy, mitigating algorithmic bias, and bridging the digital divide must be thoughtfully addressed to fully harness AI’s potential in empowering individuals with disabilities. To complement these achievements, a robust conceptual model for AI disability inclusion serves as the theoretical framework, guiding the development of tailored AI solutions. By striking a harmonious balance between innovation and ethics, AI has the power to significantly enhance the overall quality of life for individuals with disabilities across a spectrum of vital areas.","<method>voice recognition</method>, <method>early disease detection algorithms</method>"
2024,https://openalex.org/W4390665705,Psychology,The ethical implications of using generative chatbots in higher education,"Incorporating artificial intelligence (AI) into education, specifically through generative chatbots, can transform teaching and learning for education professionals in both administrative and pedagogical ways. However, the ethical implications of using generative chatbots in education must be carefully considered. Ethical concerns about advanced chatbots have yet to be explored in the education sector. This short article introduces the ethical concerns associated with introducing platforms such as ChatGPT in education. The article outlines how handling sensitive student data by chatbots presents significant privacy challenges, thus requiring adherence to data protection regulations, which may not always be possible. It highlights the risk of algorithmic bias in chatbots, which could perpetuate societal biases, which can be problematic. The article also examines the balance between fostering student autonomy in learning and the potential impact on academic self-efficacy, noting the risk of over-reliance on AI for educational purposes. Plagiarism continues to emerge as a critical ethical concern, with AI-generated content threatening academic integrity. The article advocates for comprehensive measures to address these ethical issues, including clear policies, advanced plagiarism detection techniques, and innovative assessment methods. By addressing these ethical challenges, the article argues that educators, AI developers, policymakers, and students can fully harness the potential of chatbots in education, creating a more inclusive, empowering, and ethically sound educational future.",No methods found.
2024,https://openalex.org/W4391103530,Psychology,Transformative Breast Cancer Diagnosis using CNNs with Optimized ReduceLROnPlateau and Early Stopping Enhancements,"Abstract Breast cancer stands as a paramount public health concern worldwide, underscoring an imperative necessity within the research sphere for precision-driven and efficacious methodologies facilitating accurate detection. The existing diagnostic approaches in breast cancer often suffer from limitations in accuracy and efficiency, leading to delayed detection and subsequent challenges in personalized treatment planning. The primary focus of this research is to overcome these shortcomings by harnessing the power of advanced deep learning techniques, thereby revolutionizing the precision and reliability of breast cancer classification. This research addresses the critical need for improved breast cancer diagnostics by introducing a novel Convolutional Neural Network (CNN) model integrated with an Early Stopping callback and ReduceLROnPlateau callback. By enhancing the precision and reliability of breast cancer classification, the study aims to overcome the limitations of existing diagnostic methods, ultimately leading to better patient outcomes and reduced mortality rates. The comprehensive methodology includes diverse datasets, meticulous image preprocessing, robust model training, and validation strategies, emphasizing the model's adaptability and reliability in varied clinical contexts. The findings showcase the CNN model's exceptional performance, achieving a 95.2% accuracy rate in distinguishing cancerous and non-cancerous breast tissue in the integrated dataset, thereby demonstrating its potential for enhancing clinical decision-making and fostering the development of AI-driven diagnostic solutions.","<method>Convolutional Neural Network (CNN)</method>, <method>Early Stopping callback</method>, <method>ReduceLROnPlateau callback</method>"
2024,https://openalex.org/W4393160204,Psychology,Detecting and Preventing Hallucinations in Large Vision Language Models,"Instruction tuned Large Vision Language Models (LVLMs) have significantly advanced in generalizing across a diverse set of multi-modal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of the hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address this, we introduce M-HalDetect, a Multimodal Hallucination Detection Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained annotations on VQA examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions and relationships that are unfaithful. To demonstrate the potential of this dataset for hallucination prevention, we optimize InstructBLIP through our novel Fine-grained Direct Preference Optimization (FDPO). We also train fine-grained multi-modal reward models from InstructBLIP and evaluate their effectiveness with best-of-n rejection sampling (RS). We perform human evaluation on both FDPO and rejection sampling, and find that they reduce hallucination rates in InstructBLIP by 41% and 55% respectively. We also find that our reward model generalizes to other multi-modal models, reducing hallucinations in LLaVA and mPLUG-OWL by 15% and 57% respectively, and has strong correlation with human evaluated accuracy scores. The dataset is available at https://github.com/hendryx-scale/mhal-detect.","<method>Instruction tuned Large Vision Language Models (LVLMs)</method>, <method>Fine-grained Direct Preference Optimization (FDPO)</method>, <method>fine-grained multi-modal reward models</method>, <method>best-of-n rejection sampling (RS)</method>"
2024,https://openalex.org/W4392462461,Psychology,Sentiment Analysis in the Age of Generative AI,"Abstract In the rapidly advancing age of Generative AI, Large Language Models (LLMs) such as ChatGPT stand at the forefront of disrupting marketing practice and research. This paper presents a comprehensive exploration of LLMs’ proficiency in sentiment analysis, a core task in marketing research for understanding consumer emotions, opinions, and perceptions. We benchmark the performance of three state-of-the-art LLMs, i.e., GPT-3.5, GPT-4, and Llama 2, against established, high-performing transfer learning models. Despite their zero-shot nature, our research reveals that LLMs can not only compete with but in some cases also surpass traditional transfer learning methods in terms of sentiment classification accuracy. We investigate the influence of textual data characteristics and analytical procedures on classification accuracy, shedding light on how data origin, text complexity, and prompting techniques impact LLM performance. We find that linguistic features such as the presence of lengthy, content-laden words improve classification performance, while other features such as single-sentence reviews and less structured social media text documents reduce performance. Further, we explore the explainability of sentiment classifications generated by LLMs. The findings indicate that LLMs, especially Llama 2, offer remarkable classification explanations, highlighting their advanced human-like reasoning capabilities. Collectively, this paper enriches the current understanding of sentiment analysis, providing valuable insights and guidance for the selection of suitable methods by marketing researchers and practitioners in the age of Generative AI.","<method>Large Language Models (LLMs)</method>, <method>GPT-3.5</method>, <method>GPT-4</method>, <method>Llama 2</method>, <method>transfer learning models</method>"
2024,https://openalex.org/W4390978664,Psychology,Twenty years of network meta‐analysis: Continuing controversies and recent developments,"Abstract Network meta‐analysis (NMA) is an extension of pairwise meta‐analysis (PMA) which combines evidence from trials on multiple treatments in connected networks. NMA delivers internally consistent estimates of relative treatment efficacy, needed for rational decision making. Over its first 20 years NMA's use has grown exponentially, with applications in both health technology assessment (HTA), primarily re‐imbursement decisions and clinical guideline development, and clinical research publications. This has been a period of transition in meta‐analysis, first from its roots in educational and social psychology, where large heterogeneous datasets could be explored to find effect modifiers, to smaller pairwise meta‐analyses in clinical medicine on average with less than six studies. This has been followed by narrowly‐focused estimation of the effects of specific treatments at specific doses in specific populations in sparse networks, where direct comparisons are unavailable or informed by only one or two studies. NMA is a powerful and well‐established technique but, in spite of the exponential increase in applications, doubts about the reliability and validity of NMA persist. Here we outline the continuing controversies, and review some recent developments. We suggest that heterogeneity should be minimized, as it poses a threat to the reliability of NMA which has not been fully appreciated, perhaps because it has not been seen as a problem in PMA. More research is needed on the extent of heterogeneity and inconsistency in datasets used for decision making, on formal methods for making recommendations based on NMA, and on the further development of multi‐level network meta‐regression.",No methods found.
2024,https://openalex.org/W4392866984,Psychology,Language-based game theory in the age of artificial intelligence,"Understanding human behaviour in decision problems and strategic interactions has wide-ranging applications in economics, psychology, and artificial intelligence. Game theory offers a robust foundation for this understanding, based on the idea that individuals aim to maximize a utility function. However, the exact factors influencing strategy choices remain elusive. While traditional models try to explain human behaviour as a function of the outcomes of available actions, recent experimental research reveals that linguistic content significantly impacts decision-making, thus prompting a paradigm shift from outcome-based to language-based utility functions. This shift is more urgent than ever, given the advancement of generative AI, which has the potential to support humans in making critical decisions through language-based interactions. We propose sentiment analysis as a fundamental tool for this shift and take an initial step by analyzing 61 experimental instructions from the dictator game, an economic game capturing the balance between self-interest and the interest of others, which is at the core of many social interactions. Our meta-analysis shows that sentiment analysis can explain human behaviour beyond economic outcomes. We discuss future research directions. We hope this work sets the stage for a novel game theoretical approach that emphasizes the importance of language in human decisions.",<method>sentiment analysis</method>
2024,https://openalex.org/W4393154152,Psychology,NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models,"Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling. Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent. In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN). At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target. Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goals, integrating commonsense knowledge relevant to navigation task resolution, identifying landmarks from observed scenes, tracking navigation progress, and adapting to exceptions with plan adjustment. Furthermore, we show that LLMs is capable of generating high-quality navigational instructions from observations and actions along a path, as well as drawing accurate top-down metric trajectory given the agent's navigation history. Despite the performance of using NavGPT to zero-shot R2R tasks still falling short of trained models, we suggest adapting multi-modality inputs for LLMs to use as visual navigation agents and applying the explicit reasoning of LLMs to benefit learning-based models. Code is available at: https://github.com/GengzeZhou/NavGPT.","<method>large language models (LLMs)</method>, <method>ChatGPT</method>, <method>GPT-4</method>, <method>NavGPT</method>, <method>zero-shot sequential action prediction</method>"
2024,https://openalex.org/W4394681533,Psychology,REVIEWING THE IMPACT OF HEALTH INFORMATION TECHNOLOGY ON HEALTHCARE MANAGEMENT EFFICIENCY,"This research paper explores the intricate relationship between Health Information Technology (HIT) and healthcare management efficiency, investigating current trends, emerging technologies, and their potential implications. The study encompasses a thorough literature review, highlighting the impact of HIT on operational and clinical aspects of healthcare delivery. Key findings reveal the transformative role of technology in streamlining administrative processes, improving communication, and enhancing overall patient care. Ethical considerations, patient privacy, and regulation compliance are crucial factors in successfully implementing HIT. Looking towards the future, the paper anticipates the integration of emerging technologies such as Artificial Intelligence, Blockchain, and the Internet of Things, signalling a paradigm shift in healthcare management. While acknowledging the potential benefits, the research also underscores the importance of ethical frameworks, transparency, and user-centred design in adopting these technologies. The study concludes with reflections on the limitations of the research, suggesting avenues for future exploration. Recommendations emphasize the need for ongoing research, longitudinal studies, and a global perspective to ensure healthcare organizations effectively leverage technology while maintaining ethical standards. The findings of this research carry implications for healthcare practitioners, policymakers, and technology innovators, encouraging a strategic and ethical approach to the ever-evolving landscape of health information technology.&#x0D; Keywords: Health Information Technology, Healthcare Management Efficiency, Emerging Technologies, Ethical Considerations, Patient Privacy.",<method>Artificial Intelligence</method>
2024,https://openalex.org/W4402780379,Psychology,Investigating Spatial Effects through Machine Learning and Leveraging Explainable AI for Child Malnutrition in Pakistan,"While socioeconomic gradients in regional health inequalities are firmly established, the synergistic interactions between socioeconomic deprivation and climate vulnerability within convenient proximity and neighbourhood locations with health disparities remain poorly explored and thus require deep understanding within a regional context. Furthermore, disregarding the importance of spatial spillover effects and nonlinear effects of covariates on childhood stunting are inevitable in dealing with an enduring issue of regional health inequalities. The present study aims to investigate the spatial inequalities in childhood stunting at the district level in Pakistan and validate the importance of spatial lag in predicting childhood stunting. Furthermore, it examines the presence of any nonlinear relationships among the selected independent features with childhood stunting. The study utilized data related to socioeconomic features from MICS 2017–2018 and climatic data from Integrated Contextual Analysis. A multi-model approach was employed to address the research questions, which included Ordinary Least Squares Regression (OLS), various Spatial Models, Machine Learning Algorithms and Explainable Artificial Intelligence methods. Firstly, OLS was used to analyse and test the linear relationships among selected variables. Secondly, Spatial Durbin Error Model (SDEM) was used to detect and capture the impact of spatial spillover on childhood stunting. Third, XGBoost and Random Forest machine learning algorithms were employed to examine and validate the importance of the spatial lag component. Finally, EXAI methods such as SHapley were utilized to identify potential nonlinear relationships. The study found a clear pattern of spatial clustering and geographical disparities in childhood stunting, with multidimensional poverty, high climate vulnerability and early marriage worsening childhood stunting. In contrast, low climate vulnerability, high exposure to mass media and high women’s literacy were found to reduce childhood stunting. The use of machine learning algorithms, specifically XGBoost and Random Forest, highlighted the significant role played by the average value in the neighbourhood in predicting childhood stunting in nearby districts, confirming that the spatial spillover effect is not bounded by geographical boundaries. Furthermore, EXAI methods such as partial dependency plot reveal the existence of a nonlinear relationship between multidimensional poverty and childhood stunting. The study’s findings provide valuable insights into the spatial distribution of childhood stunting in Pakistan, emphasizing the importance of considering spatial effects in predicting childhood stunting. Individual and household-level factors such as exposure to mass media and women’s literacy have shown positive implications for childhood stunting. It further provides a justification for the usage of EXAI methods to draw better insights and propose customised intervention policies accordingly.","<method>Ordinary Least Squares Regression (OLS)</method>, <method>Spatial Durbin Error Model (SDEM)</method>, <method>XGBoost</method>, <method>Random Forest</method>, <method>Explainable Artificial Intelligence (EXAI) methods</method>, <method>SHapley</method>, <method>partial dependency plot</method>"
2024,https://openalex.org/W4392764062,Psychology,"When artificial intelligence substitutes humans in higher education: the cost of loneliness, student success, and retention","Artificial intelligence (AI) may be the new-new-norm in a post-pandemic learning environment. There is a growing number of university students using AI like ChatGPT and Bard to support their academic experience. Much of the AI in higher education research to date has focused on academic integrity and matters of authorship; yet, there may be unintended consequences beyond these concerns for students. That is, there may be people who reduce their formal social interactions while using these tools. This study evaluates 387 university students and their relationship to – and with – artificial intelligence large-language model-based tools. Using structural equation modelling, the study finds evidence that while AI chatbots designed for information provision may be associated with student performance, when social support, psychological wellbeing, loneliness, and sense of belonging are considered it has a net negative effect on achievement. This study tests an AI-specific form of social support, and the cost it may pose to student success, wellbeing, and retention. Indeed, while AI chatbot usage may be associated with poorer social outcomes, human-substitution activity that may be occurring when a student chooses to seek support from an AI rather than a human (e.g. a librarian, professor, or student advisor) may pose interesting learning and teaching policy implications. We explore the implications of this from the lens of student success and belonging.",<method>structural equation modelling</method>
2024,https://openalex.org/W4390659289,Psychology,Cognition-Driven Structural Prior for Instance-Dependent Label Transition Matrix Estimation,"The label transition matrix has emerged as a widely accepted method for mitigating label noise in machine learning. In recent years, numerous studies have centered on leveraging deep neural networks to estimate the label transition matrix for individual instances within the context of instance-dependent noise. However, these methods suffer from low search efficiency due to the large space of feasible solutions. Behind this drawback, we have explored that the real murderer lies in the invalid class transitions, that is, the actual transition probability between certain classes is zero but is estimated to have a certain value. To mask the <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">invalid class transitions</i> , we introduced a human-cognition-assisted method with structural information from human cognition. Specifically, we introduce a structured transition matrix network ( <bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">STMN</b> ) designed with an adversarial learning process to balance instance features and prior information from human cognition. The proposed method offers two advantages: 1) better estimation effectiveness is obtained by sparing the transition matrix and 2) better estimation accuracy is obtained with the assistance of human cognition. By exploiting these two advantages, our method parametrically estimates a sparse label transition matrix, effectively converting noisy labels into true labels. The efficiency and superiority of our proposed method are substantiated through comprehensive comparisons with state-of-the-art methods on three synthetic datasets and a real-world dataset. Our code will be available at https://github.com/WheatCao/STMN-Pytorch.","<method>label transition matrix</method>, <method>deep neural networks</method>, <method>structured transition matrix network (STMN)</method>, <method>adversarial learning process</method>"
2024,https://openalex.org/W4391044499,Psychology,Artificial intelligence (AI)—it’s the end of the tox as we know it (and I feel fine)*,"The rapid progress of AI impacts diverse scientific disciplines, including toxicology, and has the potential to transform chemical safety evaluation. Toxicology has evolved from an empirical science focused on observing apical outcomes of chemical exposure, to a data-rich field ripe for AI integration. The volume, variety and velocity of toxicological data from legacy studies, literature, high-throughput assays, sensor technologies and omics approaches create opportunities but also complexities that AI can help address. In particular, machine learning is well suited to handle and integrate large, heterogeneous datasets that are both structured and unstructured-a key challenge in modern toxicology. AI methods like deep neural networks, large language models, and natural language processing have successfully predicted toxicity endpoints, analyzed high-throughput data, extracted facts from literature, and generated synthetic data. Beyond automating data capture, analysis, and prediction, AI techniques show promise for accelerating quantitative risk assessment by providing probabilistic outputs to capture uncertainties. AI also enables explanation methods to unravel mechanisms and increase trust in modeled predictions. However, issues like model interpretability, data biases, and transparency currently limit regulatory endorsement of AI. Multidisciplinary collaboration is needed to ensure development of interpretable, robust, and human-centered AI systems. Rather than just automating human tasks at scale, transformative AI can catalyze innovation in how evidence is gathered, data are generated, hypotheses are formed and tested, and tasks are performed to usher new paradigms in chemical safety assessment. Used judiciously, AI has immense potential to advance toxicology into a more predictive, mechanism-based, and evidence-integrated scientific discipline to better safeguard human and environmental wellbeing across diverse populations.","<method>machine learning</method>, <method>deep neural networks</method>, <method>large language models</method>, <method>natural language processing</method>"
2024,https://openalex.org/W4391107516,Psychology,Multiple Classification of Brain MRI Autism Spectrum Disorder by Age and Gender Using Deep Learning,"Abstract The fact that the rapid and definitive diagnosis of autism cannot be made today and that autism cannot be treated provides an impetus to look into novel technological solutions. To contribute to the resolution of this problem through multiple classifications by considering age and gender factors, in this study, two quadruple and one octal classifications were performed using a deep learning (DL) approach. Gender in one of the four classifications and age groups in the other were considered. In the octal classification, classes were created considering gender and age groups. In addition to the diagnosis of ASD (Autism Spectrum Disorders), another goal of this study is to find out the contribution of gender and age factors to the diagnosis of ASD by making multiple classifications based on age and gender for the first time. Brain structural MRI (sMRI) scans of participators with ASD and TD (Typical Development) were pre-processed in the system originally designed for this purpose. Using the Canny Edge Detection (CED) algorithm, the sMRI image data was cropped in the data pre-processing stage, and the data set was enlarged five times with the data augmentation (DA) techniques. The most optimal convolutional neural network (CNN) models were developed using the grid search optimization (GSO) algorism. The proposed DL prediction system was tested with the five-fold cross-validation technique. Three CNN models were designed to be used in the system. The first of these models is the quadruple classification model created by taking gender into account (model 1), the second is the quadruple classification model created by taking into account age (model 2), and the third is the eightfold classification model created by taking into account both gender and age (model 3). ). The accuracy rates obtained for all three designed models are 80.94, 85.42 and 67.94, respectively. These obtained accuracy rates were compared with pre-trained models by using the transfer learning approach. As a result, it was revealed that age and gender factors were effective in the diagnosis of ASD with the system developed for ASD multiple classifications, and higher accuracy rates were achieved compared to pre-trained models.","<method>deep learning (DL) approach</method>, <method>Canny Edge Detection (CED) algorithm</method>, <method>data augmentation (DA) techniques</method>, <method>convolutional neural network (CNN) models</method>, <method>grid search optimization (GSO) algorithm</method>, <method>five-fold cross-validation technique</method>, <method>transfer learning approach</method>"
2024,https://openalex.org/W4396707866,Psychology,Strategies for Integrating Generative AI into Higher Education: Navigating Challenges and Leveraging Opportunities,"The recent emergence of generative AI (GenAI) tools such as ChatGPT, Midjourney, and Gemini have introduced revolutionary capabilities that are predicted to transform numerous facets of society fundamentally. In higher education (HE), the advent of GenAI presents a pivotal moment that may profoundly alter learning and teaching practices in aspects such as inaccuracy, bias, overreliance on technology and algorithms, and limited access to educational AI resources that require in-depth investigation. To evaluate the implications of adopting GenAI in HE, a team of academics and field experts have co-authored this paper, which analyzes the potential for the responsible integration of GenAI into HE and provides recommendations about this integration. This paper recommends strategies for integrating GenAI into HE to create the following positive outcomes: raise awareness about disruptive change, train faculty, change teaching and assessment practices, partner with students, impart AI learning literacies, bridge the digital divide, and conduct applied research. Finally, we propose four preliminary scale levels of a GenAI adoption for faculty. At each level, we suggest courses of action to facilitate progress to the next stage in the adoption of GenAI. This study offers a valuable set of recommendations to decision-makers and faculty, enabling them to prepare for the responsible and judicious integration of GenAI into HE.",No methods found.
2024,https://openalex.org/W4391147961,Psychology,An Incentive Mechanism-Based Minimum Adjustment Consensus Model Under Dynamic Trust Relationship,"In traditional group decision making, the inconsistent experts are usually forced to make compromises toward the group opinion to increase the group consensus level. However, the strategy of reaching group consensus via an incentive mechanism encouraging adjustment of preferences is more effective than forcing, which is the aim of this article. Specifically, this article establishes a novel incentive mechanism to support group consensus under dynamic trust relationship. First, the supremum and infimum incentives-based rule driven by trust relationship is defined. Based on the assumption that if incentive conditions are met, then experts will be willing to adjust their preferences, the incentive behavior-driven minimum adjustment consensus model is developed to generate optimal incentive-based recommendation preferences. Thus, the proposed incentive mechanism can effectively reduce the preference adjustment cost and promote group consensus reaching. Third, the updated trust relationships between experts are shown to be strengthen by the proposed incentive-driven preference revision. Consequently, the optimization model based on trust interaction relationship is constructed to obtain the final group preference matrix. Finally, a supplier selection case of high-end medical equipment is provided to illustrate the proposed method and show the rationality and advantages of the proposed methodology with both a sensitivity analysis and a comparison analysis.",No methods found.
2024,https://openalex.org/W4393359157,Psychology,"A systematic review of AI literacy conceptualization, constructs, and implementation and assessment efforts (2019–2023)","The explosion of AI across all facets of society has given rise to the need for AI education across domains and levels. AI literacy has become an important concept in the current technological landscape, emphasizing the need for individuals to acquire the necessary knowledge and skills to engage with AI systems. This systematic review examined 47 articles published between 2019 to 2023, focusing on recent work to capture new insights and initiatives given the burgeoning of the literature on this topic. In the initial stage, we explored the dataset to identify the themes covered by the selected papers and the target population for AI literacy efforts. We identified that the articles broadly contributed to one of the following themes: a) conceptualizing AI literacy, b) prompting AI literacy efforts, and c) developing AI literacy assessment instruments. We also found that a range of populations, from pre-K students to adults in the workforce, were targeted. In the second stage, we conducted a thorough content analysis to synthesize six key constructs of AI literacy: Recognize, Know and Understand, Use and Apply, Evaluate, Create, and Navigate Ethically. We then applied this framework to categorize a range of empirical studies and identify the prevalence of each construct across the studies. We subsequently review assessment instruments developed for AI literacy and discuss them. The findings of this systematic review are relevant for formal education and workforce preparation and advancement, empowering individuals to leverage AI and drive innovation.",No methods found.
2024,https://openalex.org/W4396723652,Psychology,"Analysis of college students' attitudes toward the use of ChatGPT in their academic activities: effect of intent to use, verification of information and responsible use","Abstract Background In recent years, the use of artificial intelligence (AI) in education has increased worldwide. The launch of the ChatGPT-3 posed great challenges for higher education, given its popularity among university students. The present study aimed to analyze the attitudes of university students toward the use of ChatGPTs in their academic activities. Method This study was oriented toward a quantitative approach and had a nonexperimental design. An online survey was administered to the 499 participants. Results The findings of this study revealed a significant association between various factors and attitudes toward the use of the ChatGPT. The higher beta coefficients for responsible use (β=0.806***), the intention to use frequently (β=0.509***), and acceptance (β=0.441***) suggested that these are the strongest predictors of a positive attitude toward ChatGPT. The presence of positive emotions (β=0.418***) also plays a significant role. Conversely, risk (β=-0.104**) and boredom (β=-0.145**) demonstrate a negative yet less decisive influence. These results provide an enhanced understanding of how students perceive and utilize ChatGPTs, supporting a unified theory of user behavior in educational technology contexts. Conclusion Ease of use, intention to use frequently, acceptance, and intention to verify information influenced the behavioral intention to use ChatGPT responsibly. On the one hand, this study provides suggestions for HEIs to improve their educational curricula to take advantage of the potential benefits of AI and contribute to AI literacy.",No methods found.
2024,https://openalex.org/W4400461591,Psychology,Counterfactual Explanations and Algorithmic Recourses for Machine Learning: A Review,"Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine learning based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this article, we seek to review and categorize research on counterfactual explanations , a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.","<method>counterfactual explanations</method>, <method>counterfactual explanation algorithms</method>"
2024,https://openalex.org/W4401533174,Psychology,The Crowdless Future? Generative AI and Creative Problem-Solving,"The rapid advances in generative artificial intelligence (AI) open up attractive opportunities for creative problem-solving through human-guided AI partnerships. To explore this potential, we initiated a crowdsourcing challenge focused on sustainable, circular economy business ideas generated by the human crowd (HC) and collaborative human-AI efforts using two alternative forms of solution search. The challenge attracted 125 global solvers from various industries, and we used strategic prompt engineering to generate the human-AI solutions. We recruited 300 external human evaluators to judge a randomized selection of 13 out of 234 solutions, totaling 3,900 evaluator-solution pairs. Our results indicate that while human crowd solutions exhibited higher novelty—both on average and for highly novel outcomes—human-AI solutions demonstrated superior strategic viability, financial and environmental value, and overall quality. Notably, human-AI solutions cocreated through differentiated search, where human-guided prompts instructed the large language model to sequentially generate outputs distinct from previous iterations, outperformed solutions generated through independent search. By incorporating “AI in the loop” into human-centered creative problem-solving, our study demonstrates a scalable, cost-effective approach to augment the early innovation phases and lays the groundwork for investigating how integrating human-AI solution search processes can drive more impactful innovations. Funding: This work was supported by Harvard Business School (Division of Research and Faculty Development) and the Laboratory for Innovation Science at Harvard (LISH) at the Digital Data and Design (D 3 ) Institute at Harvard. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2023.18430 .","<method>strategic prompt engineering</method>, <method>large language model</method>"
2024,https://openalex.org/W1578632818,Psychology,Conditioning for Strength and Human Performance,"PART 1: Basic Science Chapter 1: Bioenergetics, T. Jeff Chandler and C. Eric Arnold Chapter 2: The Cardiorespiratory System, Jay R. Hoffman Chapter 3: The Neuromuscular System: Anatomical and Physiological Bases and Adaptations to Training, Jared W. Coburn, Travis W. Beck, Herbert A. deVries, and Terry J. Housh Chapter 4: The Skeletal System, T. Jeff Chandler and Clint Alley Chapter 5: Biomechanics of Conditioning Exercises, Robert U. Newton Chapter 6: Training Responses and Adaptations of the Endocrine System, Andrew C. Fry, and Jay R. Hoffman Chapter 7: Nutrition, Jose Antonio, John Berardi, and Christopher R. Mohr PART 2: Organization and Administration Chapter 8: Test Administration and Interpretation, Lee E. Brown. Daniel Murray, and Patrick Hagerman Chapter 9: Warm-up and Flexibility, Duane V. Knudson Chapter 10: Resistance Exercise Techniques and Spotting, John F. Graham Chapter 11: Facility Administration and Design, Steven Plisk PART 3: Exercise Prescription Chapter 12: Strength and Conditioning for Sport, Michael H. Stone and Meg E. Stone Chapter 13: Resistance Exercise Prescription, Barry A. Spiering and William J. Kraemer Chapter 14: Improving Aerobic Performance, John M. Cissik Chapter 15: Plyometric, Speed, and Agility Exercise Prescription, Jason D. Vescovi PART 4: Special Topics Chapter 16: Foundations of Strength Training for Special Populations, Moh H. Malek, Ann M. York, and Joseph P. Weir Chapter 17: Principles of Injury Prevention and Rehabilitation, Todd S. Ellenbecker, Jake Bleacher, and Anna Thatcher Chapter 18: Ergogenic Aids, Jose Antonio, Tim Ziegenfuss, and Ron Mendel Chapter 19: Implement Training, Allen Hedrick Appendix A: Sample Mission, Goals and Objectives Appendix B: Sample Policies and Procedures Appendix C: Sample Strength and Conditioning Professional Standards and Guidelines Appendix D: Sample Strength and Conditioning Performance Team Development Appendix E: Sample Emergency Care and Planning Appendix F: Sample Protective Legal Documents Index",No methods found.
2024,https://openalex.org/W4391145008,Psychology,Assessing ChatGPT’s Mastery of Bloom’s Taxonomy Using Psychosomatic Medicine Exam Questions: Mixed-Methods Study,"Background Large language models such as GPT-4 (Generative Pre-trained Transformer 4) are being increasingly used in medicine and medical education. However, these models are prone to “hallucinations” (ie, outputs that seem convincing while being factually incorrect). It is currently unknown how these errors by large language models relate to the different cognitive levels defined in Bloom’s taxonomy. Objective This study aims to explore how GPT-4 performs in terms of Bloom’s taxonomy using psychosomatic medicine exam questions. Methods We used a large data set of psychosomatic medicine multiple-choice questions (N=307) with real-world results derived from medical school exams. GPT-4 answered the multiple-choice questions using 2 distinct prompt versions: detailed and short. The answers were analyzed using a quantitative approach and a qualitative approach. Focusing on incorrectly answered questions, we categorized reasoning errors according to the hierarchical framework of Bloom’s taxonomy. Results GPT-4’s performance in answering exam questions yielded a high success rate: 93% (284/307) for the detailed prompt and 91% (278/307) for the short prompt. Questions answered correctly by GPT-4 had a statistically significant higher difficulty than questions answered incorrectly (P=.002 for the detailed prompt and P&lt;.001 for the short prompt). Independent of the prompt, GPT-4’s lowest exam performance was 78.9% (15/19), thereby always surpassing the “pass” threshold. Our qualitative analysis of incorrect answers, based on Bloom’s taxonomy, showed that errors were primarily in the “remember” (29/68) and “understand” (23/68) cognitive levels; specific issues arose in recalling details, understanding conceptual relationships, and adhering to standardized guidelines. Conclusions GPT-4 demonstrated a remarkable success rate when confronted with psychosomatic medicine multiple-choice exam questions, aligning with previous findings. When evaluated through Bloom’s taxonomy, our data revealed that GPT-4 occasionally ignored specific facts (remember), provided illogical reasoning (understand), or failed to apply concepts to a new situation (apply). These errors, which were confidently presented, could be attributed to inherent model biases and the tendency to generate outputs that maximize likelihood.","<method>large language models</method>, <method>GPT-4 (Generative Pre-trained Transformer 4)</method>"
2024,https://openalex.org/W4391750864,Psychology,Efficacy of virtual reality-based training programs and games on the improvement of cognitive disorders in patients: a systematic review and meta-analysis,"Abstract Introduction Cognitive impairments present challenges for patients, impacting memory, attention, and problem-solving abilities. Virtual reality (VR) offers innovative ways to enhance cognitive function and well-being. This study explores the effects of VR-based training programs and games on improving cognitive disorders. Methods PubMed, Scopus, and Web of Science were systematically searched until May 20, 2023. Two researchers selected and extracted data based on inclusion and exclusion criteria, resolving disagreements through consultation with two other authors. Inclusion criteria required studies of individuals with any cognitive disorder engaged in at least one VR-based training session, reporting cognitive impairment data via scales like the MMSE. Only English-published RCTs were considered, while exclusion criteria included materials not primarily focused on the intersection of VR and cognitive disorders. The risk of bias in the included studies was assessed using the MMAT tool. Publication bias was assessed using funnel plots and Egger’s test. The collected data were utilized to calculate the standardized mean differences (Hedges’s g) between the treatment and control groups. The heterogeneity variance was estimated using the Q test and I2 statistic. The analysis was conducted using Stata version 17.0. Results Ten studies were included in the analysis out of a total of 3,157 retrieved articles. VR had a statistically significant improvement in cognitive impairments among patients (Hedges’s g = 0.42, 95% CI: 0.15, 0.68; p _value = 0.05). games (Hedges’s g = 0.61, 95% CI: 0.30, 0.39; p _value = 0.20) had a more significant impact on cognitive impairment improvement compared to cognitive training programs (Hedges’s g = 0.29, 95% CI: -0.11, 0.69; p _value = 0.24). The type of VR intervention was a significant moderator of the heterogeneity between studies. Conclusion VR-based interventions have demonstrated promise in enhancing cognitive function and addressing cognitive impairment, highlighting their potential as valuable tools in improving care for individuals with cognitive disorders. The findings underscore the relevance of incorporating virtual reality into therapeutic approaches for cognitive disorders.",No methods found.
2024,https://openalex.org/W4394964316,Psychology,Higher Education’s Generative Artificial Intelligence Paradox: The Meaning of Chatbot Mania,"Higher education is currently under a significant transformation due to the emergence of generative artificial intelligence (GenAI) technologies, the hype surrounding GenAI and the increasing influence of educational technology business groups over tertiary education. This commentary, prepared for the Special Issue of the Journal of University Teaching &amp; Learning Practice (JUTLP) on “Enhancing student engagement using Artificial Intelligence (AI) and chatbots,” delves into the complex landscape of opportunities and threats that AI chatbots, including ChatGPT, introduce to the realm of higher education. We argue that while GenAI offers promise in enhancing pedagogy, research, administration, and student support, concerns around academic integrity, labour displacement, embedded biases, environmental sustainability, increased commercialisation, and regulatory gaps necessitate a critical approach. Our commentary advocates for the development of critical AI literacy among educators and students, emphasising the necessity to foster an environment of responsible innovation and informed use of AI. We posit that the successful integration of AI in higher education must be grounded in the principles of ethics, equity, and the prioritisation of educational aims and human values. By offering a critical and nuanced exploration of these issues, our commentary aims to contribute to the ongoing discourse on how higher education institutions can navigate the rise of GenAI, ensuring that technological advancements benefit all stakeholders while upholding core academic values.",No methods found.
2024,https://openalex.org/W4391531220,Psychology,An Explainable AI Paradigm for Alzheimer’s Diagnosis Using Deep Transfer Learning,"Alzheimer’s disease (AD) is a progressive neurodegenerative disorder that affects millions of individuals worldwide, causing severe cognitive decline and memory impairment. The early and accurate diagnosis of AD is crucial for effective intervention and disease management. In recent years, deep learning techniques have shown promising results in medical image analysis, including AD diagnosis from neuroimaging data. However, the lack of interpretability in deep learning models hinders their adoption in clinical settings, where explainability is essential for gaining trust and acceptance from healthcare professionals. In this study, we propose an explainable AI (XAI)-based approach for the diagnosis of Alzheimer’s disease, leveraging the power of deep transfer learning and ensemble modeling. The proposed framework aims to enhance the interpretability of deep learning models by incorporating XAI techniques, allowing clinicians to understand the decision-making process and providing valuable insights into disease diagnosis. By leveraging popular pre-trained convolutional neural networks (CNNs) such as VGG16, VGG19, DenseNet169, and DenseNet201, we conducted extensive experiments to evaluate their individual performances on a comprehensive dataset. The proposed ensembles, Ensemble-1 (VGG16 and VGG19) and Ensemble-2 (DenseNet169 and DenseNet201), demonstrated superior accuracy, precision, recall, and F1 scores compared to individual models, reaching up to 95%. In order to enhance interpretability and transparency in Alzheimer’s diagnosis, we introduced a novel model achieving an impressive accuracy of 96%. This model incorporates explainable AI techniques, including saliency maps and grad-CAM (gradient-weighted class activation mapping). The integration of these techniques not only contributes to the model’s exceptional accuracy but also provides clinicians and researchers with visual insights into the neural regions influencing the diagnosis. Our findings showcase the potential of combining deep transfer learning with explainable AI in the realm of Alzheimer’s disease diagnosis, paving the way for more interpretable and clinically relevant AI models in healthcare.","<method>deep learning</method>, <method>explainable AI (XAI)</method>, <method>deep transfer learning</method>, <method>ensemble modeling</method>, <method>pre-trained convolutional neural networks (CNNs)</method>, <method>VGG16</method>, <method>VGG19</method>, <method>DenseNet169</method>, <method>DenseNet201</method>, <method>Ensemble-1 (VGG16 and VGG19)</method>, <method>Ensemble-2 (DenseNet169 and DenseNet201)</method>, <method>saliency maps</method>, <method>grad-CAM (gradient-weighted class activation mapping)</method>"
2024,https://openalex.org/W4391678457,Psychology,AI in personalized learning,"Artificial Intelligence (AI) has revolutionized various sectors, and its impact on education, specifically in personalized learning, is increasingly significant. Personalized learning aims to cater to individual students' unique needs, learning styles, and abilities, enabling them to achieve better educational outcomes. AI technologies offer the potential to transform traditional educational models by providing adaptive and tailored approaches that enhance student engagement, learning efficiency, and overall educational experiences. This abstract provides an overview of the role of AI in personalized learning and highlights its key benefits and challenges. The abstract also explores various AI-powered tools and techniques used in personalized learning environments, such as intelligent tutoring systems, recommendation engines, and adaptive assessments. These tools leverage machine learning algorithms, natural language processing, and data analytics to gather and analyze vast amounts of educational data, providing valuable insights for personalized instruction. AI in personalized learning facilitates adaptive content delivery, enabling students to learn at their own pace and according to their individual strengths and weaknesses. Intelligent tutoring systems, for instance, offer customized feedback, explanations, and recommendations based on real-time student interactions, fostering deeper comprehension and engagement. Moreover, AI-powered recommendation engines provide personalized learning resources and materials that align with students' interests, preferences, and skill levels, creating a more tailored and immersive learning experience. While AI in personalized learning presents numerous benefits, it also poses certain challenges. Privacy concerns, ethical considerations, and potential biases in AI algorithms must be carefully addressed to ensure the responsible and equitable use of these technologies. Furthermore, effective implementation requires adequate infrastructure, access to quality data, and teacher training to leverage AI tools optimally.","<method>intelligent tutoring systems</method>, <method>recommendation engines</method>, <method>adaptive assessments</method>, <method>machine learning algorithms</method>, <method>natural language processing</method>, <method>data analytics</method>"
2024,https://openalex.org/W4392450439,Psychology,Dr. Google to Dr. ChatGPT: assessing the content and quality of artificial intelligence-generated medical information on appendicitis,"Abstract Introduction Generative artificial intelligence (AI) chatbots have recently been posited as potential sources of online medical information for patients making medical decisions. Existing online patient-oriented medical information has repeatedly been shown to be of variable quality and difficult readability. Therefore, we sought to evaluate the content and quality of AI-generated medical information on acute appendicitis. Methods A modified DISCERN assessment tool, comprising 16 distinct criteria each scored on a 5-point Likert scale (score range 16–80), was used to assess AI-generated content. Readability was determined using the Flesch Reading Ease (FRE) and Flesch-Kincaid Grade Level (FKGL) scores. Four popular chatbots, ChatGPT-3.5 and ChatGPT-4, Bard, and Claude-2, were prompted to generate medical information about appendicitis. Three investigators independently scored the generated texts blinded to the identity of the AI platforms. Results ChatGPT-3.5, ChatGPT-4, Bard, and Claude-2 had overall mean (SD) quality scores of 60.7 (1.2), 62.0 (1.0), 62.3 (1.2), and 51.3 (2.3), respectively, on a scale of 16–80. Inter-rater reliability was 0.81, 0.75, 0.81, and 0.72, respectively, indicating substantial agreement. Claude-2 demonstrated a significantly lower mean quality score compared to ChatGPT-4 ( p = 0.001), ChatGPT-3.5 ( p = 0.005), and Bard ( p = 0.001). Bard was the only AI platform that listed verifiable sources, while Claude-2 provided fabricated sources. All chatbots except for Claude-2 advised readers to consult a physician if experiencing symptoms. Regarding readability, FKGL and FRE scores of ChatGPT-3.5, ChatGPT-4, Bard, and Claude-2 were 14.6 and 23.8, 11.9 and 33.9, 8.6 and 52.8, 11.0 and 36.6, respectively, indicating difficulty readability at a college reading skill level. Conclusion AI-generated medical information on appendicitis scored favorably upon quality assessment, but most either fabricated sources or did not provide any altogether. Additionally, overall readability far exceeded recommended levels for the public. Generative AI platforms demonstrate measured potential for patient education and engagement about appendicitis.",No methods found
2024,https://openalex.org/W4392816821,Psychology,Understanding the Mechanism of Conducting Benchmark Test for the Infrastructure of Physical Education Curricula in the Age of Artificial Intelligence,"The study aims to propose a vision for creating a standardized test according to the development of physical education curricula in the era of artificial intelligence, so that we can be able to measure student achievement and master the new curriculum standards. Based on the results, the following can be concluded: directing the teaching and learning process, and determining levels. For students, students’ teaching performance, students’ level of understanding and knowledge, application and achievement of lesson objectives in physical education. There is a strong correlation between most of the skills, knowledge, and behaviors that were measured in the study. This suggests that these skills, knowledge, and behaviors are related to each other, and that students who are high in one are more likely to have a high level in the other skills, knowledge, and behaviors. This is due to the fact that the students who participated in the study had a good level of performance in all the criteria that were measured. Based on the results, the following recommendations can be made: Focus on developing teaching skills and interaction with students, as they are closely related to the level of understanding and knowledge of students. Focus on developing knowledge and understanding of mathematical concepts, mathematical skills, and sporting values and behaviors’, as these skills and knowledge are closely linked to the application and achievement of lesson objectives in physical education.",No methods found.
2024,https://openalex.org/W4393009377,Psychology,Generative Artificial Intelligence in Higher Education: Exploring Ways of Harnessing Pedagogical Practices with the Assistance of ChatGPT,"There is a growing interest in using generative artificial intelligence (AI) for educational purposes within the higher education environments, while AI applications (such as ChatGPT) can transform traditional teaching and learning methods. ChatGPT is an advanced AI tool that generates new content and human-like responses. The purpose of this paper is to use ChatGPT as a research assistant in order to explore ways AI can be harnessed to enhance pedagogical practices in higher education. This is a qualitative study, in which the output-responses generated by ChatGPT provided a starting point for the investigation. AI can be harnessed to enhance pedagogical practices in higher education in various ways including personalized learning, automated assessment and feedback generation, virtual assistants and chatbots, content creation, resource recommendation, time management, language translation and support, research assistance, simulations and virtual labs. Other educational affordances that can strengthen the teaching and learning experience regard collaboration and communication, accessibility and inclusivity, as well as AI literacy. When implementing AI tools such as ChatGPT in higher education, ethical considerations (e.g., data privacy, transparency, accessibility, cultural sensitivity), potential misuses and concerns need to also be addressed. Although ChatGPT can aid the generation of content-ideas for further exploration, it is a complementary-supportive tool, and its output necessitates human evaluation and review. The integration of ChatGPT and other AI tools in the higher educational process/practices has implications for educators, students, design of curricula, and university policy makers. Received: 17 January 2024 | Revised: 27 February 2024 | Accepted: 19 March 2024 Conflicts of Interest The author declares that she has no conflicts of interest to this work. Data Availability Statement Data sharing is not applicable to this article as no new data were created or analyzed in this study.","<method>generative artificial intelligence (AI)</method>, <method>ChatGPT</method>"
2024,https://openalex.org/W4393091384,Psychology,REVIEWING THE TRANSFORMATIONAL IMPACT OF EDGE COMPUTING ON REAL-TIME DATA PROCESSING AND ANALYTICS,"Edge computing has emerged as a pivotal paradigm shift in the realm of data processing and analytics, revolutionizing the way organizations handle real-time data. This review presents a comprehensive review of the transformational impact of edge computing on real-time data processing and analytics. Firstly, the review delves into the fundamental concepts of edge computing, elucidating its architectural framework and highlighting its distinct advantages over traditional cloud-centric approaches. By distributing computational resources closer to data sources, edge computing mitigates latency issues and enhances responsiveness, thereby enabling real-time data processing at the edge. Furthermore, this review explores how edge computing facilitates the seamless integration of analytics capabilities into edge devices, empowering organizations to derive actionable insights at the source of data generation. Leveraging advanced analytics algorithms, such as machine learning and artificial intelligence, edge computing enables autonomous decision-making and predictive analytics in real time, fostering innovation across diverse industry verticals. Moreover, the review examines the transformative implications of edge computing on various sectors, including healthcare, manufacturing, transportation, and smart cities. By enabling localized data processing and analytics, edge computing enhances operational efficiency, ensures data privacy and security, and unlocks new opportunities for business optimization and value creation. This review underscores the profound impact of edge computing on real-time data processing and analytics, revolutionizing the way organizations harness data to drive informed decision-making and gain competitive advantage in today's dynamic business landscape. As edge computing continues to evolve, its transformative potential is poised to redefine the future of data-driven innovation and digital transformation.&#x0D; Keywords: Edge, Computing, Analytics, Data, Impact, Review.","<method>machine learning</method>, <method>artificial intelligence</method>"
2024,https://openalex.org/W4394859632,Psychology,Artificial intelligence in healthcare delivery: Prospects and pitfalls,"This review provides a comprehensive examination of the integration of Artificial Intelligence (AI) into healthcare, focusing on its transformative implications and challenges. Utilising a systematic search strategy across electronic databases such as PubMed, Scopus, Embase, and Sciencedirect, relevant peer-reviewed articles published in English between January 2010 till date were identified. Findings reveal AI's significant impact on healthcare delivery, including its role in enhancing diagnostic precision, enabling treatment personalisation, facilitating predictive analytics, automating tasks, and driving robotics. AI algorithms demonstrate high accuracy in analysing medical images for disease diagnosis and enable the creation of tailored treatment plans based on patient data analysis. Predictive analytics identify high-risk patients for proactive interventions, while AI-powered tools streamline workflows, improving efficiency and patient experience. Additionally, AI-driven robotics automate tasks and enhance care delivery, particularly in rehabilitation and surgery. However, challenges such as data quality, interpretability, bias, and regulatory frameworks must be addressed for responsible AI implementation. Recommendations emphasise the need for robust ethical and legal frameworks, human-AI collaboration, safety validation, education, and comprehensive regulation to ensure the ethical and effective integration of AI in healthcare. This review provides valuable insights into AI's transformative potential in healthcare while advocating for responsible implementation to ensure patient safety and efficacy.","<method>AI algorithms</method>, <method>Predictive analytics</method>, <method>AI-powered tools</method>, <method>AI-driven robotics</method>"
2024,https://openalex.org/W4391399751,Psychology,"Human-in-the-Loop Reinforcement Learning: A Survey and Position on Requirements, Challenges, and Opportunities","Artificial intelligence (AI) and especially reinforcement learning (RL) have the potential to enable agents to learn and perform tasks autonomously with superhuman performance. However, we consider RL as fundamentally a Human-in-the-Loop (HITL) paradigm, even when an agent eventually performs its task autonomously. In cases where the reward function is challenging or impossible to define, HITL approaches are considered particularly advantageous. The application of Reinforcement Learning from Human Feedback (RLHF) in systems such as ChatGPT demonstrates the effectiveness of optimizing for user experience and integrating their feedback into the training loop. In HITL RL, human input is integrated during the agent’s learning process, allowing iterative updates and fine-tuning based on human feedback, thus enhancing the agent’s performance. Since the human is an essential part of this process, we argue that human-centric approaches are the key to successful RL, a fact that has not been adequately considered in the existing literature. This paper aims to inform readers about current explainability methods in HITL RL. It also shows how the application of explainable AI (xAI) and specific improvements to existing explainability approaches can enable a better human-agent interaction in HITL RL for all types of users, whether for lay people, domain experts, or machine learning specialists. Accounting for the workflow in HITL RL and based on software and machine learning methodologies, this article identifies four phases for human involvement for creating HITL RL systems: (1) Agent Development, (2) Agent Learning, (3) Agent Evaluation, and (4) Agent Deployment. We highlight human involvement, explanation requirements, new challenges, and goals for each phase. We furthermore identify low-risk, high-return opportunities for explainability research in HITL RL and present long-term research goals to advance the field. Finally, we propose a vision of human-robot collaboration that allows both parties to reach their full potential and cooperate effectively.","<method>reinforcement learning (RL)</method>, <method>Reinforcement Learning from Human Feedback (RLHF)</method>, <method>explainable AI (xAI)</method>"
2024,https://openalex.org/W4394785902,Psychology,"Evidence-based potential of generative artificial intelligence large language models in orthodontics: a comparative study of ChatGPT, Google Bard, and Microsoft Bing","Summary Background The increasing utilization of large language models (LLMs) in Generative Artificial Intelligence across various medical and dental fields, and specifically orthodontics, raises questions about their accuracy. Objective This study aimed to assess and compare the answers offered by four LLMs: Google’s Bard, OpenAI’s ChatGPT-3.5, and ChatGPT-4, and Microsoft’s Bing, in response to clinically relevant questions within the field of orthodontics. Materials and methods Ten open-type clinical orthodontics-related questions were posed to the LLMs. The responses provided by the LLMs were assessed on a scale ranging from 0 (minimum) to 10 (maximum) points, benchmarked against robust scientific evidence, including consensus statements and systematic reviews, using a predefined rubric. After a 4-week interval from the initial evaluation, the answers were reevaluated to gauge intra-evaluator reliability. Statistical comparisons were conducted on the scores using Friedman’s and Wilcoxon’s tests to identify the model providing the answers with the most comprehensiveness, scientific accuracy, clarity, and relevance. Results Overall, no statistically significant differences between the scores given by the two evaluators, on both scoring occasions, were detected, so an average score for every LLM was computed. The LLM answers scoring the highest, were those of Microsoft Bing Chat (average score = 7.1), followed by ChatGPT 4 (average score = 4.7), Google Bard (average score = 4.6), and finally ChatGPT 3.5 (average score 3.8). While Microsoft Bing Chat statistically outperformed ChatGPT-3.5 (P-value = 0.017) and Google Bard (P-value = 0.029), as well, and Chat GPT-4 outperformed Chat GPT-3.5 (P-value = 0.011), all models occasionally produced answers with a lack of comprehensiveness, scientific accuracy, clarity, and relevance. Limitations The questions asked were indicative and did not cover the entire field of orthodontics. Conclusions Language models (LLMs) show great potential in supporting evidence-based orthodontics. However, their current limitations pose a potential risk of making incorrect healthcare decisions if utilized without careful consideration. Consequently, these tools cannot serve as a substitute for the orthodontist’s essential critical thinking and comprehensive subject knowledge. For effective integration into practice, further research, clinical validation, and enhancements to the models are essential. Clinicians must be mindful of the limitations of LLMs, as their imprudent utilization could have adverse effects on patient care.",<method>large language models (LLMs)</method>
2024,https://openalex.org/W4401844424,Psychology,AlphaFold predictions of fold-switched conformations are driven by structure memorization,"Abstract Recent work suggests that AlphaFold (AF)–a deep learning-based model that can accurately infer protein structure from sequence–may discern important features of folded protein energy landscapes, defined by the diversity and frequency of different conformations in the folded state. Here, we test the limits of its predictive power on fold-switching proteins, which assume two structures with regions of distinct secondary and/or tertiary structure. We find that (1) AF is a weak predictor of fold switching and (2) some of its successes result from memorization of training-set structures rather than learned protein energetics. Combining &gt;280,000 models from several implementations of AF2 and AF3, a 35% success rate was achieved for fold switchers likely in AF’s training sets. AF2’s confidence metrics selected against models consistent with experimentally determined fold-switching structures and failed to discriminate between low and high energy conformations. Further, AF captured only one out of seven experimentally confirmed fold switchers outside of its training sets despite extensive sampling of an additional ~280,000 models. Several observations indicate that AF2 has memorized structural information during training, and AF3 misassigns coevolutionary restraints. These limitations constrain the scope of successful predictions, highlighting the need for physically based methods that readily predict multiple protein conformations.","<method>AlphaFold (AF)</method>, <method>AF2</method>, <method>AF3</method>"
2024,https://openalex.org/W4402827393,Psychology,Larger and more instructable language models become less reliable,"Abstract The prevailing methods to make large language models more powerful and amenable have been based on continuous scaling up (that is, increasing their size, data volume and computational resources 1 ) and bespoke shaping up (including post-filtering 2,3 , fine tuning or use of human feedback 4,5 ). However, larger and more instructable large language models may have become less reliable. By studying the relationship between difficulty concordance, task avoidance and prompting stability of several language model families, here we show that easy instances for human participants are also easy for the models, but scaled-up, shaped-up models do not secure areas of low difficulty in which either the model does not err or human supervision can spot the errors. We also find that early models often avoid user questions but scaled-up, shaped-up models tend to give an apparently sensible yet wrong answer much more often, including errors on difficult questions that human supervisors frequently overlook. Moreover, we observe that stability to different natural phrasings of the same question is improved by scaling-up and shaping-up interventions, but pockets of variability persist across difficulty levels. These findings highlight the need for a fundamental shift in the design and development of general-purpose artificial intelligence, particularly in high-stakes areas for which a predictable distribution of errors is paramount.","<method>continuous scaling up</method>, <method>post-filtering</method>, <method>fine tuning</method>, <method>use of human feedback</method>"
2024,https://openalex.org/W4390975281,Psychology,Semantic and Instance Segmentation in Coastal Urban Spatial Perception: A Multi-Task Learning Framework with an Attention Mechanism,"With the continuous acceleration of urbanization, urban planning and design require more in-depth research and development. Street view images can express rich urban features and guide residents’ emotions toward a city, thereby providing the most intuitive reflection of their perception of the city’s spatial quality. However, current researchers mainly conduct research on urban spatial quality through subjective experiential judgment, which includes problems such as a high cost and a low judgment accuracy. In response to these problems, this study proposes a multi-task learning urban spatial attribute perception model that integrates an attention mechanism. Via this model, the existing attributes of urban street scenes are analyzed. Then, the model is improved by introducing semantic segmentation and instance segmentation to identify and match the qualities of the urban space. The experimental results show that the multi-task learning urban spatial attribute perception model with an integrated attention mechanism has prediction accuracies of 79.54%, 78.62%, 79.68%, 77.42%, 78.45%, and 76.98% for the urban spatial attributes of beauty, boredom, depression, liveliness, safety, and richness, respectively. The accuracy of the multi-task learning urban spatial scene feature image segmentation model with an integrated attention mechanism is 95.4, 94.8, 96.2, 92.1, and 96.7 for roads, walls, sky, vehicles, and buildings, respectively. The multi-task learning urban spatial scene feature image segmentation model with an integrated attention mechanism has a higher recognition accuracy for urban spatial buildings than other models. These research results indicate the model’s effectiveness in matching urban spatial quality with public perception.","<method>multi-task learning</method>, <method>attention mechanism</method>, <method>semantic segmentation</method>, <method>instance segmentation</method>"
2024,https://openalex.org/W4391126287,Psychology,Evaluating the ChatGPT family of models for biomedical reasoning and classification,"Abstract Objective Large language models (LLMs) have shown impressive ability in biomedical question-answering, but have not been adequately investigated for more specific biomedical applications. This study investigates ChatGPT family of models (GPT-3.5, GPT-4) in biomedical tasks beyond question-answering. Materials and Methods We evaluated model performance with 11 122 samples for two fundamental tasks in the biomedical domain—classification (n = 8676) and reasoning (n = 2446). The first task involves classifying health advice in scientific literature, while the second task is detecting causal relations in biomedical literature. We used 20% of the dataset for prompt development, including zero- and few-shot settings with and without chain-of-thought (CoT). We then evaluated the best prompts from each setting on the remaining dataset, comparing them to models using simple features (BoW with logistic regression) and fine-tuned BioBERT models. Results Fine-tuning BioBERT produced the best classification (F1: 0.800-0.902) and reasoning (F1: 0.851) results. Among LLM approaches, few-shot CoT achieved the best classification (F1: 0.671-0.770) and reasoning (F1: 0.682) results, comparable to the BoW model (F1: 0.602-0.753 and 0.675 for classification and reasoning, respectively). It took 78 h to obtain the best LLM results, compared to 0.078 and 0.008 h for the top-performing BioBERT and BoW models, respectively. Discussion The simple BoW model performed similarly to the most complex LLM prompting. Prompt engineering required significant investment. Conclusion Despite the excitement around viral ChatGPT, fine-tuning for two fundamental biomedical natural language processing tasks remained the best strategy.","<method>ChatGPT family of models (GPT-3.5, GPT-4)</method>, <method>zero-shot prompting</method>, <method>few-shot prompting</method>, <method>chain-of-thought (CoT) prompting</method>, <method>Bag of Words (BoW) with logistic regression</method>, <method>fine-tuned BioBERT</method>"
2024,https://openalex.org/W4391482140,Psychology,The implementation of the cognitive theory of multimedia learning in the design and evaluation of an AI educational video assistant utilizing large language models,"The integration of Artificial Intelligence (AI) holds immense potential for revolutionizing education; especially, in contexts where multimodal learning experiences are designed. This paper investigated the potential benefits of Generative Artificial Intelligence (AI) in education, concentrating on the design and evaluation of an AI Educational Video Assistant tailored for multimodal learning experiences. The tool, utilizing the principles of the Cognitive Theory of Multimedia Learning (CTML), comprises three modules: Transcription, Engagement, and Reinforcement, each focusing on distinct aspects of the learning process. Integration of Automatic Speech Recognition (ASR) using OpenAI's Whisper and Google's Large Language Model (LLM) Bard. Our twofold objective includes both the development of this AI assistant tool and the assessment of its effect on improving the learning experiences. For the evaluation, a mixed methods approach was adopted, combining human evaluation by nine educational experts with automatic metrics. Participants provided their perceptions on the tool's effectiveness in terms of engagement, content organization, clarity, and usability. Additionally, automatic metrics including Content Distinctiveness and Readability scores were computed. The results from the human evaluation suggest positive impacts across all assessed domains. The automatic metrics further proved the tool's ability in content generation and readability. Collectively, these preliminary results highlight the tool's potential to revolutionize educational design and provide personalized and engaging learning experiences.","<method>Generative Artificial Intelligence (AI)</method>, <method>Automatic Speech Recognition (ASR)</method>, <method>Large Language Model (LLM)</method>"
2024,https://openalex.org/W4391574742,Psychology,Optimum tuned mass damper inerter under near-fault pulse-like ground motions of buildings including soil-structure interaction,"This study investigates the effectiveness of the tuned mass damper inerter (TMDI) in mitigating building response, considering the soil structure interaction (SSI). Three types of models are examined: single degree of freedom (SDOF), low-rise multi-degree of freedom (MDOF), and high-rise MDOF. Additionally, the natural period of the SDOF model is varied to explore the TMDI's efficacy across different ranges. Frequency and time domain analysis are conducted under pulse-like ground motions. The H2 and genetic algorithm (GA) are used to optimize the parameters of the TMDI. In this optimization method the transfer function for displacement response is minimized. In time domain analysis we used Newmark's integration method to solve the equation of motion for all the cases considered. It is found that the optimized TMDI proves highly effective in mitigating the displacement response of the buildings, accounting for SSI. Notably, its efficiency is more pronounced when pulse period aligns closely with the buildings' natural period. In addition, a notable pattern emerges, wherein the TMDI excels in mitigating response for buildings experiencing large motion, thereby enhancing safety under severe conditions. These findings offer valuable insights into the application and optimization of the TMDI to enhance seismic performance in various buildings, while considering complex interaction with the soil.",<method>genetic algorithm (GA)</method>
2024,https://openalex.org/W4391263683,Psychology,Physical Activity and Cognitive Functioning,"Neuroscience applied to motor activity is a growing area that aims to understand the effects of motor activity on the structures and functions of the Central Nervous System. Attention has been paid to this multidisciplinary field of investigation by the scientific community both because it is of great importance in the treatment of many chronic diseases and because of its potential applications in the Movement Sciences. Motor activity during a developmental age is, in fact, an indispensable tool for the physical and mental growth of children, both able-bodied and disabled. Through movement, individuals can improve their physical efficiency and promote their own better health, establish relationships with the environment and others, express themselves and their emotions, form their identity and develop cognitive processes. This literature review aims, therefore, to highlight how an adequate practice of motor activity offers extraordinary possibilities for everyone in relation to learning, from the perspective of an integral development of the person, and, consequently, can raise the awareness of those involved in the training and growth, especially the youngest, towards the educational value of motor and sports activities. According to this review, and in line with the modern neuroscientific approach toward the relationships between motor activities and cognitive functions, it is possible to claim that hypokinesia tends to inhibit learning. Therefore, it now seems more topical than ever to draw attention to the need to introduce working proposals that integrate brain-based motor activity programs into the school curriculum.",No methods found.
2024,https://openalex.org/W4392285688,Psychology,Machine Learning and Digital Biomarkers Can Detect Early Stages of Neurodegenerative Diseases,"Neurodegenerative diseases (NDs) such as Alzheimer’s Disease (AD) and Parkinson’s Disease (PD) are devastating conditions that can develop without noticeable symptoms, causing irreversible damage to neurons before any signs become clinically evident. NDs are a major cause of disability and mortality worldwide. Currently, there are no cures or treatments to halt their progression. Therefore, the development of early detection methods is urgently needed to delay neuronal loss as soon as possible. Despite advancements in Medtech, the early diagnosis of NDs remains a challenge at the intersection of medical, IT, and regulatory fields. Thus, this review explores “digital biomarkers” (tools designed for remote neurocognitive data collection and AI analysis) as a potential solution. The review summarizes that recent studies combining AI with digital biomarkers suggest the possibility of identifying pre-symptomatic indicators of NDs. For instance, research utilizing convolutional neural networks for eye tracking has achieved significant diagnostic accuracies. ROC-AUC scores reached up to 0.88, indicating high model performance in differentiating between PD patients and healthy controls. Similarly, advancements in facial expression analysis through tools have demonstrated significant potential in detecting emotional changes in ND patients, with some models reaching an accuracy of 0.89 and a precision of 0.85. This review follows a structured approach to article selection, starting with a comprehensive database search and culminating in a rigorous quality assessment and meaning for NDs of the different methods. The process is visualized in 10 tables with 54 parameters describing different approaches and their consequences for understanding various mechanisms in ND changes. However, these methods also face challenges related to data accuracy and privacy concerns. To address these issues, this review proposes strategies that emphasize the need for rigorous validation and rapid integration into clinical practice. Such integration could transform ND diagnostics, making early detection tools more cost-effective and globally accessible. In conclusion, this review underscores the urgent need to incorporate validated digital health tools into mainstream medical practice. This integration could indicate a new era in the early diagnosis of neurodegenerative diseases, potentially altering the trajectory of these conditions for millions worldwide. Thus, by highlighting specific and statistically significant findings, this review demonstrates the current progress in this field and the potential impact of these advancements on the global management of NDs.",<method>convolutional neural networks</method>
2024,https://openalex.org/W4392404912,Psychology,The Impact of Artificial Intelligence on Students' Learning Experience,"The integration of artificial intelligence (AI) in education has the potential to revolutionize the learning experience for students. This abstract provides an overview of the impact of AI on students' learning experience, highlighting its benefits and potential challenges.AI technologies such as machine learning, natural language processing, and data analytics have been increasingly adopted in educational settings. These technologies enable personalized and adaptive learning experiences, providing students with tailored content and feedback based on their individual needs and learning styles. AI-powered educational platforms can analyze vast amounts of data to identify patterns and offer personalized recommendations, thereby enhancing students' engagement and motivation.One of the significant benefits of AI in education is its ability to provide immediate and constructive feedback to students. Automated grading systems powered by AI algorithms can assess and provide feedback on assignments, quizzes, and exams promptly, allowing students to understand their strengths and weaknesses in real-time. This timely feedback facilitates self-reflection and enables students to make necessary improvements, leading to enhanced learning outcomes.Furthermore, AI can support collaborative learning environments. Intelligent tutoring systems and virtual learning assistants can facilitate group discussions, provide guidance, and foster collaboration among students. These AI-powered tools can promote active participation, critical thinking, and problem-solving skills, creating a dynamic learning environment that mirrors real-world scenarios.However, the integration of AI in education also poses challenges that need to be addressed. Privacy and ethical concerns arise when dealing with student data, as AI relies on collecting and analyzing personal information to provide personalized experiences. Safeguarding student data privacy and ensuring ethical use of AI technologies are essential considerations for educators and policymakers.Additionally, there is a potential risk of over-reliance on AI technologies, leading to a passive learning experience for students. Balancing the use of AI with human instruction and guidance is crucial to maintain meaningful interactions and promote deeper understanding.","<method>machine learning</method>, <method>natural language processing</method>, <method>AI algorithms</method>"
2024,https://openalex.org/W4395028987,Psychology,A Human-Centered Learning and Teaching Framework Using Generative Artificial Intelligence for Self-Regulated Learning Development Through Domain Knowledge Learning in K–12 Settings,"The advent of generative artificial intelligence (AI) has ignited an increase in discussions about generative AI tools in education. In this study, a human-centred learning and teaching framework (HCLTF) that uses generative AI tools for self-regulated learning development through domain knowledge learning was proposed to catalyse changes in educational practices. The framework illustrates how generative AI tools can revolutionise educational practices and transform the processes of teaching and learning to become human-centred. It emphasises the evolving roles of teachers, who increasingly become skilful facilitators and humanistic storytellers who craft differentiated instructions and attempt to develop students' individualised learning. Drawing upon insights from neuroscience, the framework guides students to employ generative AI tools to augment their attentiveness, stimulate active engagement in learning, receive immediate feedback, and encourage self-reflection. The pedagogical approach is also reimagined; teachers equipped with generative AI tools and AI literacy can refine their teaching strategies to better equip students to meet future challenges. The practical application of the framework is demonstrated in a case study involving the development of Chinese language writing ability among primary students within a K–12 educational context. This paper also reports the results of a 60-hour development programme for teachers. Specifically, providing in-service teachers with cases involving uses of the proposed framework helped them to better understand the generative AI concepts and integrate them into their teaching and learning and increased their perceived ability to design AI-integrated courses that would enhance students' attention, engagement, confidence, and satisfaction.",No methods found.
2024,https://openalex.org/W4399857583,Psychology,Integrating artificial intelligence to assess emotions in learning environments: a systematic literature review,"Introduction Artificial Intelligence (AI) is transforming multiple sectors within our society, including education. In this context, emotions play a fundamental role in the teaching-learning process given that they influence academic performance, motivation, information retention, and student well-being. Thus, the integration of AI in emotional assessment within educational environments offers several advantages that can transform how we understand and address the socio-emotional development of students. However, there remains a lack of comprehensive approach that systematizes advancements, challenges, and opportunities in this field. Aim This systematic literature review aims to explore how artificial intelligence (AI) is used to evaluate emotions within educational settings. We provide a comprehensive overview of the current state of research, focusing on advancements, challenges, and opportunities in the domain of AI-driven emotional assessment within educational settings. Method The review involved a search across the following academic databases: Pubmed, Web of Science, PsycINFO and Scopus. Forty-one articles were selected that meet the established inclusion criteria. These articles were analyzed to extract key insights related to the integration of AI and emotional assessment within educational environments. Results The findings reveal a variety of AI-driven approaches that were developed to capture and analyze students’ emotional states during learning activities. The findings are summarized in four fundamental topics: (1) emotion recognition in education, (2) technology integration and learning outcomes, (3) special education and assistive technology, (4) affective computing. Among the key AI techniques employed are machine learning and facial recognition, which are used to assess emotions. These approaches demonstrate promising potential in enhancing pedagogical strategies and creating adaptive learning environments that cater to individual emotional needs. The review identified emerging factors that, while important, require further investigation to understand their relationships and implications fully. These elements could significantly enhance the use of AI in assessing emotions within educational settings. Specifically, we are referring to: (1) federated learning, (2) convolutional neural network (CNN), (3) recurrent neural network (RNN), (4) facial expression databases, and (5) ethics in the development of intelligent systems. Conclusion This systematic literature review showcases the significance of AI in revolutionizing educational practices through emotion assessment. While advancements are evident, challenges related to accuracy, privacy, and cross-cultural validity were also identified. The synthesis of existing research highlights the need for further research into refining AI models for emotion recognition and emphasizes the importance of ethical considerations in implementing AI technologies within educational contexts.","<method>machine learning</method>, <method>facial recognition</method>, <method>federated learning</method>, <method>convolutional neural network (CNN)</method>, <method>recurrent neural network (RNN)</method>"
2024,https://openalex.org/W4400916341,Psychology,Reviewing the current state of virtual reality integration in medical education - a scoping review,"Abstract Background In medical education, new technologies like Virtual Reality (VR) are increasingly integrated to enhance digital learning. Originally used to train surgical procedures, now use cases also cover emergency scenarios and non-technical skills like clinical decision-making. This scoping review aims to provide an overview of VR in medical education, including requirements, advantages, disadvantages, as well as evaluation methods and respective study results to establish a foundation for future VR integration into medical curricula. Methods This review follows the updated JBI methodology for scoping reviews and adheres to the respective PRISMA extension. We included reviews in English or German language from 2012 to March 2022 that examine the use of VR in education for medical and nursing students, registered nurses, and qualified physicians. Data extraction focused on medical specialties, subjects, curricula, technical/didactic requirements, evaluation methods and study outcomes as well as advantages and disadvantages of VR. Results A total of 763 records were identified. After eligibility assessment, 69 studies were included. Nearly half of them were published between 2021 and 2022, predominantly from high-income countries. Most reviews focused on surgical training in laparoscopic and minimally invasive procedures (43.5%) and included studies with qualified physicians as participants (43.5%). Technical, didactic and organisational requirements were highlighted and evaluations covering performance time and quality, skills acquisition and validity, often showed positive outcomes. Accessibility, repeatability, cost-effectiveness, and improved skill development were reported as advantages, while financial challenges, technical limitations, lack of scientific evidence, and potential user discomfort were cited as disadvantages. Discussion Despite a high potential of VR in medical education, there are mandatory requirements for its integration into medical curricula addressing challenges related to finances, technical limitations, and didactic aspects. The reported lack of standardised and validated guidelines for evaluating VR training must be overcome to enable high-quality evidence for VR usage in medical education. Interdisciplinary teams of software developers, AI experts, designers, medical didactics experts and end users are required to design useful VR courses. Technical issues and compromised realism can be mitigated by further technological advancements.",No methods found.
2024,https://openalex.org/W4390490725,Psychology,Evaluating LLM-generated Worked Examples in an Introductory Programming Course,"Worked examples, which illustrate the process for solving a problem step-by-step, are a well-established pedagogical technique that has been widely studied in computing classrooms. However, creating high-quality worked examples is very time-intensive for educators, and thus learners tend not to have access to a broad range of such examples. The recent emergence of powerful large language models (LLMs), which appear capable of generating high-quality human-like content, may offer a solution. Separate strands of recent work have shown that LLMs can accurately generate code suitable for a novice audience, and that they can generate high-quality explanations of code. Therefore, LLMs may be well suited to creating a broad range of worked examples, overcoming the bottleneck of manual effort that is currently required. In this work, we present a novel tool, 'WorkedGen', which uses an LLM to generate interactive worked examples. We evaluate this tool with both an expert assessment of the content, and a user study involving students in a first-year Python programming course (n = ~400). We find that prompt chaining and one-shot learning are useful strategies for optimising the output of an LLM when producing worked examples. Our expert analysis suggests that LLMs generate clear explanations, and our classroom deployment revealed that students find the LLM-generated worked examples useful for their learning. We propose several avenues for future work, including investigating WorkedGen's value in a range of programming languages, and with more complex questions suitable for more advanced courses.","<method>large language models (LLMs)</method>, <method>prompt chaining</method>, <method>one-shot learning</method>"
2024,https://openalex.org/W4390590719,Psychology,Terms of debate: Consensus definitions to guide the scientific discourse on visual distraction,"Hypothesis-driven research rests on clearly articulated scientific theories. The building blocks for communicating these theories are scientific terms. Obviously, communication - and thus, scientific progress - is hampered if the meaning of these terms varies idiosyncratically across (sub)fields and even across individual researchers within the same subfield. We have formed an international group of experts representing various theoretical stances with the goal to homogenize the use of the terms that are most relevant to fundamental research on visual distraction in visual search. Our discussions revealed striking heterogeneity and we had to invest much time and effort to increase our mutual understanding of each other's use of central terms, which turned out to be strongly related to our respective theoretical positions. We present the outcomes of these discussions in a glossary and provide some context in several essays. Specifically, we explicate how central terms are used in the distraction literature and consensually sharpen their definitions in order to enable communication across theoretical standpoints. Where applicable, we also explain how the respective constructs can be measured. We believe that this novel type of adversarial collaboration can serve as a model for other fields of psychological research that strive to build a solid groundwork for theorizing and communicating by establishing a common language. For the field of visual distraction, the present paper should facilitate communication across theoretical standpoints and may serve as an introduction and reference text for newcomers.",No methods found.
2024,https://openalex.org/W4391070180,Psychology,AI in medical diagnosis: AI prediction &amp; human judgment,"AI has long been regarded as a panacea for decision-making and many other aspects of knowledge work; as something that will help humans get rid of their shortcomings. We believe that AI can be a useful asset to support decision-makers, but not that it should replace decision-makers. Decision-making uses algorithmic analysis, but it is not solely algorithmic analysis; it also involves other factors, many of which are very human, such as creativity, intuition, emotions, feelings, and value judgments. We have conducted semi-structured open-ended research interviews with 17 dermatologists to understand what they expect from an AI application to deliver to medical diagnosis. We have found four aggregate dimensions along which the thinking of dermatologists can be described: the ways in which our participants chose to interact with AI, responsibility, 'explainability', and the new way of thinking (mindset) needed for working with AI. We believe that our findings will help physicians who might consider using AI in their diagnosis to understand how to use AI beneficially. It will also be useful for AI vendors in improving their understanding of how medics want to use AI in diagnosis. Further research will be needed to examine if our findings have relevance in the wider medical field and beyond.",No methods found.
2024,https://openalex.org/W4392301491,Psychology,Graduate instructors navigating the AI frontier: The role of ChatGPT in higher education,"This research study explores the use of artificial intelligence (AI) in undergraduate assessments, specifically focusing on the ability of graduate teaching assistants (GTAs) to identify AI-generated assessments and the performance of ChatGPT, an AI model, in producing high-quality work. The study examines four guiding research questions and hypotheses related to the accuracy of GTA identification, the achievement of AI-generated work compared to student marks, the impact of GTA characteristics on identification accuracy, and the variation in identification and assessment across different subject areas. The study incorporates ten AI-generated assessments across seven classes taught by five GTAs. The findings reveal that ChatGPT consistently excelled the average student in all classes receiving 10 scores of A or higher out of 11 and receiving the top mark in 8 of the ten classes. GTAs accurately identified 50% of the AI-generated assessments, with results suggesting a potential connection between class size and GTA accuracy in identifying AI-generated work. GTAs with prior experience and familiarity with ChatGPT demonstrated higher accuracy in identifying AI-generated assessments. However, further research is needed to explore this comprehensively. This study also reviews the effectiveness of TurnItin's new AI detector, highlighting an accuracy of 92% across the ten assessments. The study highlights the adaptability of ChatGPT across different subject areas and assessment types, producing assessments that align with diverse educational contexts. In conclusion, this research study contributes to understanding the effectiveness and adaptability of AI in undergraduate assessments. It underscores the need to further explore and develop AI technologies in education.",No methods found.
2024,https://openalex.org/W4393250851,Psychology,Evaluating the subjective perceptions of streetscapes using street-view images,"Developing a model to evaluate urban streetscapes based on subjective perceptions is important for quantitative understanding. However, previous studies have only considered limited types of subjective perceptions, neglecting the relationships between them. Further, accurately measuring subjective perception with low computational costs for large-scale urban regions at high spatial resolutions has been difficult. We present a deep-learning-based multilabel classification model that can measure 22 subjective perceptions scores from street-view images. This model uses the results of a web questionnaire survey encompassing 22 subjective perceptions, with 8.8 million responses. Our model demonstrates high accuracy (0.80–0.91) in measuring subjective perception scores from street-view images and achieves low computational cost by training on 22 subjective perception relationships. The 22 subjective perceptions were analyzed using PCA and k-means analysis. By categorizing the 22 subjective perceptions into a two-dimensional space visualized and grouped into distinct groups—positive, negative, calm, and lively—we unearthed vital insights into the intricate nuances of human perception. In addition, the study used semantic segmentation to extract landscape elements from street-view images and applied ℓ1-regularized sparse modeling to identify the landscape elements structurally correlating with each subjective perception class. The analysis revealed that only seven out of nineteen landscape elements significantly correlated with subjective impressions, and these effects varied by class. Notably, sky coverage positively influences positive subjective perceptions, such as attractiveness and calmness, but negatively affects lively impressions. The proposed model can be used to map the overall image of a city and identify landscape design issues in community development design.","<method>deep-learning-based multilabel classification model</method>, <method>PCA</method>, <method>k-means analysis</method>, <method>semantic segmentation</method>, <method>ℓ1-regularized sparse modeling</method>"
2024,https://openalex.org/W4392812555,Psychology,Artificial intelligence and human translation: A contrastive study based on legal texts,"Artificial intelligence has advanced significantly in recent years, affecting multiple aspects of life. In particular, this has had an impact on the machine translation of texts, reducing or removing human interaction. Artificial intelligence (AI)-based translation software models have thus become widely available, and these now include Google Translate, Bing, Microsoft Translator, DeepL, Reverso, Systran Translate, and Amazon Translate. Several computer-aided translation (CAT) tools such as Memoq, Trados, Smartcat, Lokalise, Smartling, Crowdin, TextUnited, and Memsource are also available. More recently, artificial intelligence has been applied in the development of applications such as ChatGPT, ChatSonic, GPT-3 Playground, Chat GPT 4 and YouChat, which simulate conversational responses to researchers' inquiries, mimicking human interactions more directly. This study thus aimed to examine any remaining contrasts between human and AI translation in the legal field to investigate the potential hypothesis that there is now no difference between human and AI translation. The paper thus also examined concerns about whether the need for human translators will decline in the face of AI development, as well as beginning to assess whether it will ever be possible for those in the legal field to depend only on machine translation. To achieve this, a collection of legal texts from various contracts was chosen, and these pieces were both allocated to legal translators and subjected to AI translation systems. Using a contrastive methodology, the study thus examined the differences between AI and human translation, examining the strengths and weaknesses of both approaches and discussing the situations in which each approach might be most effective.","<method>Artificial intelligence (AI)-based translation software models</method>, <method>ChatGPT</method>, <method>ChatSonic</method>, <method>GPT-3 Playground</method>, <method>Chat GPT 4</method>, <method>YouChat</method>"
2024,https://openalex.org/W4396908686,Psychology,Firefighter Skill Advancement through IoT-Enabled Virtual Reality and CNN-Based Training,"To maintain the safety and efficacy of firefighters in various circumstances, modern firefighting necessitates constantly improving skills and training techniques. Utilizing the Internet of Things (IoT), virtual reality (VR), and convolutional neural networks (CNN), this paper details a novel method for training firefighters. The proposed system collects real-time data on ambient variables, equipment state, and firefighter biometrics via integrating IoT sensors into firefighting equipment and training settings. Using this information, it can develop lifelike VR training simulations of difficult and potentially dangerous scenarios. To make the training settings more realistic and malleable, CNN-based algorithms are used to assess the data. The capacity to simulate a wide variety of firefighting situations, customize training difficulty depending on individual and team performance, and provide instant feedback and performance metrics to trainees are all major benefits of this method. The method also allows teachers to check in and evaluate their learners remotely, improving instruction quality. An IoT-enabled VR and CNN-based training technique has shown promising preliminary results in pilot trials, suggesting it might greatly enhance firefighter competence, situational awareness, and decision-making ability. Because of this, it has the potential to completely alter the way firefighters are informed and prepared for the ever-changing dangers users may encounter on the job.",<method>convolutional neural networks (CNN)</method>
2024,https://openalex.org/W4399182459,Psychology,Exploring Students’ Generative AI-Assisted Writing Processes: Perceptions and Experiences from Native and Nonnative English Speakers,"Abstract Generative artificial intelligence (AI) can create sophisticated textual and multimodal content readily available to students. Writing intensive courses and disciplines that use writing as a major form of assessment are significantly impacted by advancements in generative AI, as the technology has the potential to revolutionize how students write and how they perceive writing as a fundamental literacy skill. However, educators are still at the beginning stage of understanding students’ integration of generative AI in their actual writing process. This study addresses the urgent need to uncover how students engage with ChatGPT throughout different components of their writing processes and their perceptions of the opportunities and challenges of generative AI. Adopting a phenomenological research design, the study explored the writing practices of six students, including both native and nonnative English speakers, in a first-year writing class at a higher education institution in the US. Thematic analysis of students’ written products, self-reflections, and interviews suggests that students utilized ChatGPT for brainstorming and organizing ideas as well as assisting with both global (e.g., argument, structure, coherence) and local issues of writing (e.g., syntax, diction, grammar), while they also had various ethical and practical concerns about the use of ChatGPT. The study brought to front two dilemmas encountered by students in their generative AI-assisted writing: (1) the challenging balance between incorporating AI to enhance writing and maintaining their authentic voice, and (2) the dilemma of weighing the potential loss of learning experiences against the emergence of new learning opportunities accompanying AI integration. These dilemmas highlight the need to rethink learning in an increasingly AI-mediated educational context, emphasizing the importance of fostering students’ critical AI literacy to promote their authorial voice and learning in AI-human collaboration.",No methods found.
2024,https://openalex.org/W4404134492,Psychology,Bias in medical AI: Implications for clinical decision-making,"Biases in medical artificial intelligence (AI) arise and compound throughout the AI lifecycle. These biases can have significant clinical consequences, especially in applications that involve clinical decision-making. Left unaddressed, biased medical AI can lead to substandard clinical decisions and the perpetuation and exacerbation of longstanding healthcare disparities. We discuss potential biases that can arise at different stages in the AI development pipeline and how they can affect AI algorithms and clinical decision-making. Bias can occur in data features and labels, model development and evaluation, deployment, and publication. Insufficient sample sizes for certain patient groups can result in suboptimal performance, algorithm underestimation, and clinically unmeaningful predictions. Missing patient findings can also produce biased model behavior, including capturable but nonrandomly missing data, such as diagnosis codes, and data that is not usually or not easily captured, such as social determinants of health. Expertly annotated labels used to train supervised learning models may reflect implicit cognitive biases or substandard care practices. Overreliance on performance metrics during model development may obscure bias and diminish a model's clinical utility. When applied to data outside the training cohort, model performance can deteriorate from previous validation and can do so differentially across subgroups. How end users interact with deployed solutions can introduce bias. Finally, where models are developed and published, and by whom, impacts the trajectories and priorities of future medical AI development. Solutions to mitigate bias must be implemented with care, which include the collection of large and diverse data sets, statistical debiasing methods, thorough model evaluation, emphasis on model interpretability, and standardized bias reporting and transparency requirements. Prior to real-world implementation in clinical settings, rigorous validation through clinical trials is critical to demonstrate unbiased application. Addressing biases across model development stages is crucial for ensuring all patients benefit equitably from the future of medical AI.","<method>supervised learning</method>, <method>statistical debiasing methods</method>"
2024,https://openalex.org/W4390821180,Psychology,Can ChatGPT effectively complement teacher assessment of undergraduate students’ academic writing?,"The integration of ChatGPT as a supplementary tool for writing instruction has gained traction. However, uncertainties persist regarding how ChatGPT complements teacher assessment and the overall effectiveness of this combined approach. To address this, we conducted a mixed-methods investigation involving 46 undergraduate students from a research university in southern China, engaging them in a Chinese academic writing task. The intraclass correlation coefficient results revealed ChatGPT's efficiency in scoring students' writing, showing moderate to good consistency with teacher evaluations. A paired sample t-test unveiled significant differences in feedback quantity and types between ChatGPT and teacher assessments. Drawing from both interview data and quantitative findings, the study uncovers three ways in which ChatGPT complements teacher assessment, benefiting students with various writing proficiency levels: (1) fostering deeper comprehension of teacher assessments among students, (2) encouraging students to make judgments regarding feedback, and (3) promoting independent thinking about revisions. This study contributes to a more comprehensive understanding of the role of ChatGPT within the context of a combined assessment approach. It underscores that certain inherent weaknesses in ChatGPT's functioning can paradoxically lead to favorable outcomes. By shedding light on the synergy between ChatGPT and teacher assessments, this research seeks to inform and enhance writing instruction in higher education.",No methods found.
2024,https://openalex.org/W4391574066,Psychology,Artificial Intelligence Bringing Improvements to Adaptive Learning in Education: A Case Study,"Despite promising outcomes in higher education, the widespread adoption of learning analytics remains elusive in various educational settings, with primary and secondary schools displaying considerable reluctance to embrace these tools. This hesitancy poses a significant obstacle, particularly given the prevalence of educational technology and the abundance of data generated in these environments. In contrast to higher education institutions that readily integrate learning analytics tools into their educational governance, high schools often harbor skepticism regarding the tools’ impact and returns. To overcome these challenges, this work aims to harness learning analytics to address critical areas, such as school dropout rates, the need to foster student collaboration, improving argumentation and writing skills, and the need to enhance computational thinking across all age groups. The goal is to empower teachers and decision makers with learning analytics tools that will equip them to identify learners in vulnerable or exceptional situations, enabling educational authorities to take suitable actions that are aligned with students’ needs; this could potentially involve adapting learning processes and organizational structures to meet the needs of students. This work also seeks to evaluate the impact of such analytics tools on education within a multi-dimensional and scalable domain, ranging from individual learners to teachers and principals, and extending to broader governing bodies. The primary objective is articulated through the development of a user-friendly AI-based dashboard for learning. This prototype aims to provide robust support for teachers and principals who are dedicated to enhancing the education they provide within the intricate and multifaceted social domain of the school.",No methods found.
2024,https://openalex.org/W4391752835,Psychology,Immersive virtual reality and augmented reality in anatomy education: A systematic review and meta‐analysis,"Abstract The purpose of this review was to (1) analyze the effectiveness of immersive virtual reality (iVR) and augmented reality (AR) as teaching/learning resources (collectively called XR‐technologies) for gaining anatomy knowledge compared to traditional approaches and (2) gauge students' perceptions of the usefulness of these technologies as learning tools. This meta‐analysis, previously registered in PROSPERO (CRD42023423017), followed PRISMA guidelines. A systematic bibliographical search, without time parameters, was conducted through four databases until June 2023. A meta‐analytic approach investigated knowledge gains and XR's usefulness for learning. Pooled effect sizes were estimated using Cohen's standardized mean difference (SMD) and 95% confidence intervals (95% CI). A single‐group proportional meta‐analysis was conducted to quantify the percentage of students who considered XR devices useful for their learning. Twenty‐seven experimental studies, reporting data from 2199 health sciences students, were included for analysis. XR‐technologies yielded higher knowledge gains than traditional approaches (SMD = 0.40; 95% CI = 0.22 to 0.60), especially when used as supplemental/complementary learning resources (SMD = 0.52; 95% CI = 0.40 to 0.63). Specifically, knowledge performance using XR devices outperformed textbooks and atlases (SMD = 0.32; 95% CI = 0.10 to 0.54) and didactic lectures (SMD = 1.00; 95% CI = 0.57 to 1.42), especially among undergraduate students (SMD = 0.41; 95% CI = 0.20 to 0.62). XR devices were perceived to be more useful for learning than traditional approaches (SMD = 0.54; 95% CI = 0.04 to 1), and 80% of all students who used XR devices reported these devices as useful for learning anatomy. Learners using XR technologies demonstrated increased anatomy knowledge gains and considered these technologies useful for learning anatomy.",No methods found.
2024,https://openalex.org/W4391753792,Psychology,Flood Detection with SAR: A Review of Techniques and Datasets,"Floods are among the most severe and impacting natural disasters. Their occurrence rate and intensity have been significantly increasing worldwide in the last years due to climate change and urbanization, bringing unprecedented effects on human lives and activities. Hence, providing a prompt response to flooding events is of crucial relevance for humanitarian, social and economic reasons. Satellite remote sensing using synthetic aperture radar (SAR) offers a great deal of support in facing flood events and mitigating their effects on a global scale. As opposed to multi-spectral sensors, SAR offers important advantages, as it enables Earth’s surface imaging regardless of weather and sunlight illumination conditions. In the last decade, the increasing availability of SAR data, even at no cost, thanks to the efforts of international and national space agencies, has been deeply stimulating research activities in every Earth observation field, including flood mapping and monitoring, where advanced processing paradigms, e.g., fuzzy logic, machine learning, data fusion, have been applied, demonstrating their superiority with respect to traditional classification strategies. However, a fair assessment of the performance and reliability of flood mapping techniques is of key importance for an efficient disasters response and, hence, should be addressed carefully and on a quantitative basis trough synthetic quality metrics and high-quality reference data. To this end, the recent development of open SAR datasets specifically covering flood events with related ground-truth reference data can support thorough and objective validation as well as reproducibility of results. Notwithstanding, SAR-based flood monitoring still suffers from severe limitations, especially in vegetated and urban areas, where complex scattering mechanisms can impair an accurate extraction of water regions. All such aspects, including classification methodologies, SAR datasets, validation strategies, challenges and future perspectives for SAR-based flood mapping are described and discussed.","<method>fuzzy logic</method>, <method>machine learning</method>, <method>data fusion</method>"
2024,https://openalex.org/W4394933962,Psychology,Connectome-driven neural inventory of a complete visual system,"Vision provides animals with detailed information about their surroundings, conveying diverse features such as color, form, and movement across the visual scene. Computing these parallel spatial features requires a large and diverse network of neurons, such that in animals as distant as flies and humans, visual regions comprise half the brain's volume. These visual brain regions often reveal remarkable structure-function relationships, with neurons organized along spatial maps with shapes that directly relate to their roles in visual processing. To unravel the stunning diversity of a complex visual system, a careful mapping of the neural architecture matched to tools for targeted exploration of that circuitry is essential. Here, we report a new connectome of the right optic lobe from a male Drosophila central nervous system FIB-SEM volume and a comprehensive inventory of the fly's visual neurons. We developed a computational framework to quantify the anatomy of visual neurons, establishing a basis for interpreting how their shapes relate to spatial vision. By integrating this analysis with connectivity information, neurotransmitter identity, and expert curation, we classified the ~53,000 neurons into 727 types, about half of which are systematically described and named for the first time. Finally, we share an extensive collection of split-GAL4 lines matched to our neuron type catalog. Together, this comprehensive set of tools and data unlock new possibilities for systematic investigations of vision in Drosophila, a foundation for a deeper understanding of sensory processing.",No methods found.
2024,https://openalex.org/W4396622564,Psychology,Unveiling the shadows: Beyond the hype of AI in education,"Despite the wave of enthusiasm for the role of Artificial Intelligence (AI) in reshaping education, critical voices urge a more tempered approach. This study investigates the less-discussed 'shadows' of AI implementation in educational settings, focusing on potential negatives that may accompany its integration. Through a multi-phased exploration consisting of content analysis and survey research, the study develops and validates a theoretical model that pinpoints several areas of concern. The initial phase, a systematic literature review, yielded 56 relevant studies from which the model was crafted. The subsequent survey with 260 participants from a Saudi Arabian university aimed to validate the model. Findings confirm concerns about human connection, data privacy and security, algorithmic bias, transparency, critical thinking, access equity, ethical issues, teacher development, reliability, and the consequences of AI-generated content. They also highlight correlations between various AI-associated concerns, suggesting intertwined consequences rather than isolated issues. For instance, enhancements in AI transparency could simultaneously support teacher professional development and foster better student outcomes. Furthermore, the study acknowledges the transformative potential of AI but cautions against its unexamined adoption in education. It advocates for comprehensive strategies to maintain human connections, ensure data privacy and security, mitigate biases, enhance system transparency, foster creativity, reduce access disparities, emphasize ethics, prepare teachers, ensure system reliability, and regulate AI-generated content. Such strategies underscore the need for holistic policymaking to leverage AI's benefits while safeguarding against its disadvantages.",No methods found.
2024,https://openalex.org/W4390753190,Psychology,Investigating the impact of motion in the scanner on brain age predictions,"Abstract Brain Age Gap (BAG) is defined as the difference between the brain’s predicted age and the chronological age of an individual. Magnetic resonance imaging (MRI)-based BAG can quantify acceleration of brain aging, and is used to infer brain health as aging and disease interact. Motion in the scanner is a common occurrence that can affect the acquired MRI data and act as a major confound in the derived models. As such, age-related changes in head motion may impact the observed age-related differences. However, the relationship between head motion and BAG as estimated by structural MRI has not been systematically examined. The aim of this study is to assess the impact of motion on voxel-based morphometry (VBM) based BAG. Data were obtained from two sources: i) T1-weighted (T1w) MRIs from the Cambridge Centre for Ageing and Neuroscience (CamCAN) were used to train the brain age prediction model, and ii) T1w MRIs from the Movement-related artifacts (MR-ART) dataset were used to assess the impact of motion on BAG. MR-ART includes one motion-free and two motion-affected (one low and one high) 3D T1w MRIs. We also visually rated the motion levels of the MR-ART MRIs from 0 to 5, with 0 meaning no motion and 5 high motion levels. All images were pre-processed through a standard VBM pipeline. GM density across cortical and subcortical regions were then used to train the brain age prediction model and assess the relationship between BAG and MRI motion. Principal component analysis was used to perform dimension reduction and extract the VBM-based features. BAG was estimated by regressing out the portion of delta age explained by chronological age. Linear mixed-effects models were used to investigate the relationship between BAG and motion session as well as motion severity, including participant IDs as random effects. We repeated the same analysis using cortical thickness based on FreeSurfer 7.4.1 and to compare the results for volumetric versus surface-based measures of brain morphometry. In contrast with the session with no induced motion, predicted delta age was significantly higher for high motion sessions 2.35 years (t = 5.17, p &amp;lt; 0.0001), with marginal effect for low motion sessions 0.95 years (t = 2.11, p = 0.035) for VBM analysis as well as 3.46 years (t = 11.45, p &amp;lt; 0.0001) for high motion and 2.28 years (t = 7.54, p &amp;lt; 0.0001) for low motion based on cortical thickness. In addition, delta age was significantly associated with motion severity as evaluated by visual rating 0.45 years per rating level (t = 4.59, p &amp;lt; 0.0001) for VBM analysis and 0.83 years per motion level (t = 12.89, p &amp;lt; 0.0001) for cortical thickness analysis. Motion in the scanner can significantly impact brain age estimates, and needs to be accounted for as a confound, particularly when studying populations that are known to have higher levels of motion in the scanner. These results have significant implications for brain age studies in aging and neurodegeneration. Based on these findings, we recommend assessment and inclusion of visual motion ratings in such studies. In cases that the visual rating proves prohibitive, we recommend the inclusion of normalized Euler number from FreeSurfer as defined in the manuscript as a covariate in the models.","<method>brain age prediction model</method>, <method>principal component analysis</method>, <method>regression</method>, <method>linear mixed-effects models</method>"
2024,https://openalex.org/W4390917192,Psychology,Exploring the influence of ChatGPT on tourism behavior using the technology acceptance model,"Purpose The present study’s aims are twofold: 1) to contribute to theory development by accounting for both personality and trust in the conceptualization of technology acceptance using the technology acceptance model (TAM) as the theoretical framework; and 2) to explore the influence of ChatGPT-integrated chatbots on tourism behavior. Design/methodology/approach The target population for this study was travelers who previously used technology (website/ app) to plan their holiday abroad. An online survey questionnaire created with Google Forms was distributed via a panel company (iPanel). A screening question was included to filter out respondents who have not previously used technological means to plan their holiday abroad. A panel company (iPanel) was hired to collect data from a convenience sample of 305 Israeli tourists who met the above criterion between August 22 and 27, 2023, and were at least 18. Findings A significant and positive relationship was observed between trust in ChaptGPT and perceived usefulness. Furthermore, a significant and positive association was observed between perceived ease of use and intentions to use ChatGPT-integrated chatbots to plan future holidays. Post hoc analyses suggest that perceived ease of use mediates the relationship between extraversion and trust, trust mediates the relationship between perceived ease of use and perceived usefulness and age moderates the relationship between perceived ease of use and behavioral intentions. Research limitations/implications Data was collected from a convenience sample of Israeli travelers. Hence, generalizations to other countries, nationalities and cultures should be treated carefully; the study is cross-sectional and thus represents respondents’ beliefs and behavioral intentions at a particular time; and the study is based on one of several theoretical frameworks that can be used to conceptualize behaviors associated with using AI by tourists. Practical implications The findings of the present study point to the importance of accounting for tourists’ personal factors, such as personality and age, in developing AI products in the tourism industry. chief executive officers and relevant shareholders would benefit from conducting market research to obtain insights into the factors that may enhance or hamper tourists’ adoption of AI-based technology for planning their holidays abroad. Originality/value Previous work falls short of accounting for personality traits and trust in a single model using the TAM framework. To the best of the authors’ knowledge, this is the first study empirically investigating tourism behavior related to ChatGPT based chatbots as a tool to plan future holidays abroad. Furthermore, the possible role of age as a moderating variable was overlooked in past research.",No methods found.
2024,https://openalex.org/W4390987311,Psychology,Reliability of ChatGPT for performing triage task in the emergency department using the Korean Triage and Acuity Scale,"Background Artificial intelligence (AI) technology can enable more efficient decision-making in healthcare settings. There is a growing interest in improving the speed and accuracy of AI systems in providing responses for given tasks in healthcare settings. Objective This study aimed to assess the reliability of ChatGPT in determining emergency department (ED) triage accuracy using the Korean Triage and Acuity Scale (KTAS). Methods Two hundred and two virtual patient cases were built. The gold standard triage classification for each case was established by an experienced ED physician. Three other human raters (ED paramedics) were involved and rated the virtual cases individually. The virtual cases were also rated by two different versions of the chat generative pre-trained transformer (ChatGPT, 3.5 and 4.0). Inter-rater reliability was examined using Fleiss’ kappa and intra-class correlation coefficient (ICC). Results The kappa values for the agreement between the four human raters and ChatGPTs were .523 (version 4.0) and .320 (version 3.5). Of the five levels, the performance was poor when rating patients at levels 1 and 5, as well as case scenarios with additional text descriptions. There were differences in the accuracy of the different versions of GPTs. The ICC between version 3.5 and the gold standard was .520, and that between version 4.0 and the gold standard was .802. Conclusions A substantial level of inter-rater reliability was revealed when GPTs were used as KTAS raters. The current study showed the potential of using GPT in emergency healthcare settings. Considering the shortage of experienced manpower, this AI method may help improve triaging accuracy.","<method>chat generative pre-trained transformer (ChatGPT, 3.5 and 4.0)</method>"
2024,https://openalex.org/W4391061837,Psychology,Approaches and game elements used to tailor digital gamification for learning: A systematic literature review,"The systematic review examined research on tailored digital gamification for learning based on 43 peer-reviewed articles published between 2013 and 2022. The study aimed to investigate tailored approaches and game elements, contributing to the use of tailored digital gamification in educational settings. The tailored approaches were categorized as personalization, adaptation, and recommendation, with user modeling as their basis. Five clusters of game elements were employed when using these tailored approaches in digital gamified classes. The findings imply that most of the articles in this review were still in the stage of class preparation and focused on what information can be used to tailor. More empirical studies need to be conducted to examine the motivating effects of tailored digital gamifying classes, using the approaches of personalization, adaptation, and recommendation. Additionally, twenty-three game elements were found in this review study, among which reward was the most often used. Then these game elements were grouped into five clusters based on their functions, that is, performance, personal, social, ecological, and fictional cluster. A variety of game element clusters reflect multiple aspects of gamification. The use of them in each tailored approach might contribute to a better understanding and selection of game elements when tailoring digital gamification. These findings provide a holistic picture of common approaches and related game elements in tailored digital gamifying classes. Teachers and curriculum designers can benefit from this study by considering appropriate approaches and game elements.","<method>personalization</method>, <method>adaptation</method>, <method>recommendation</method>, <method>user modeling</method>"
2024,https://openalex.org/W4391573023,Psychology,Deep Reinforcement Learning Unleashing the Power of AI in Decision-Making,"Deep Reinforcement Learning (DRL) has emerged as a transformative paradigm in the field of artificial intelligence (AI), offering unprecedented capabilities in decision-making across diverse domains. This article explores the profound impact of DRL on enhancing the decision-making capabilities of AI systems, elucidating its underlying principles, applications, and implications.DRL represents a fusion of deep learning and reinforcement learning, enabling machines to learn complex behaviors and make decisions by interacting with their environment. The utilization of neural networks allows DRL algorithms to handle high-dimensional input spaces, making it well-suited for tasks that involve intricate decision-making processes.One of the key strengths of DRL lies in its ability to address problems with sparse and delayed rewards, common challenges in traditional reinforcement learning. Through a process of trial and error, DRL algorithms can learn optimal decision strategies by navigating through a vast decision space, adapting to dynamic environments, and maximizing cumulative rewards over time.The applications of DRL span various domains, including robotics, finance, healthcare, gaming, and autonomous systems. In robotics, DRL facilitates the development of intelligent agents capable of autonomously navigating complex environments, performing intricate tasks, and adapting to unforeseen circumstances. In finance, DRL is leveraged for portfolio optimization, algorithmic trading, and risk management, demonstrating its potential to revolutionize traditional financial strategies.","<method>Deep Reinforcement Learning (DRL)</method>, <method>deep learning</method>, <method>reinforcement learning</method>"
2024,https://openalex.org/W4396712983,Psychology,3WC-GBNRS++: A novel three-way classifier with granular-ball neighborhood rough sets based on uncertainty,"Three-way decision with neighborhood rough sets (3WDNRS) is adept at addressing uncertain problems involving continuous data by configuring the neighborhood radius. However, on one hand, the inputs of 3WDNRS are individual neighborhood granules, which reduce the decision efficiency and generality; on other hand, the thresholds of 3WDNRS require prior knowledge to be approximately set in advance, making it difficult to apply in cases where such knowledge is unavailable. To address these issues, we introduce granular-ball computing (GBC) into 3WDNRS from the perspective of uncertainty. Firstly, we propose an enhanced granular-ball generation method based on DBSCAN called DBGBC. Subsequently, we present an improved granular-ball neighborhood rough sets model (GBNRS++) by combining DBGBC with a quality index. Furthermore, we construct a three-way classifier with granular-ball neighborhood rough sets (3WC-GBNRS++) based on the principle of minimum fuzziness loss. This approach provides an objective and efficient way to determine the thresholds. To further enhance classification accuracy, we design an adaptive granular-ball neighborhood within the subsequent classification process of 3WC-GBNRS++. Finally, experimental results demonstrate that, 3WC-GBNRS++ almost outperformed other comparison methods in terms of effectiveness and robustness, including 4 state-of-the-art granular-balls-based classifiers and 5 classical machine learning classifiers on 12 public benchmark datasets. Moreover, we discuss the limitations of our work and the outlook for future research.","<method>three-way decision with neighborhood rough sets (3WDNRS)</method>, <method>granular-ball computing (GBC)</method>, <method>DBSCAN</method>, <method>granular-ball neighborhood rough sets model (GBNRS++)</method>, <method>three-way classifier with granular-ball neighborhood rough sets (3WC-GBNRS++)</method>, <method>adaptive granular-ball neighborhood</method>"
2024,https://openalex.org/W4396723505,Psychology,MentaLLaMA: Interpretable Mental Health Analysis on Social Media with Large Language Models,"As an integral part of people's daily lives, social media is becoming a rich source for automatic mental health analysis.As traditional discriminative methods bear poor generalization ability and low interpretability, the recent large language models (LLMs) have been explored for interpretable mental health analysis on social media, which aims to provide detailed explanations along with predictions in zero-shot or few-shot settings.The results show that LLMs still achieve unsatisfactory classification performance in a zero-shot/few-shot manner, which further significantly affects the quality of the generated explanations.Domain-specific finetuning is an effective solution, but faces two critical challenges: 1) lack of high-quality training data.2) no open-source foundation LLMs.To alleviate these problems, we formally model interpretable mental health analysis as a text generation task, and build the first multi-task and multi-source interpretable mental health instruction (IMHI) dataset with 105K data samples to support LLM instruction tuning and evaluation.The raw social media data are collected from 10 existing sources covering 8 mental health analysis tasks.We prompt ChatGPT with expert-designed few-shot prompts to obtain explanations.To ensure the reliability of the explanations, we perform strict automatic and human evaluations on the correctness, consistency, and quality of generated data.Based on the IMHI dataset and LLaMA2 foundation models, we train MentaLLaMA, the first open-source instruction-following LLM series for interpretable mental health analysis on social media.We evaluate Men-taLLaMA and other advanced methods on the IMHI benchmark, the first holistic evaluation benchmark for interpretable mental health analysis.The results show that MentaLLaMA approaches state-of-the-art discriminative methods in correctness and generates human-level explanations.MentaLLaMA models also show strong generalizability to unseen tasks.The project is available at https://github.com/SteveKGYang/MentaLLaMA.","<method>large language models (LLMs)</method>, <method>zero-shot learning</method>, <method>few-shot learning</method>, <method>domain-specific finetuning</method>, <method>instruction tuning</method>, <method>instruction-following large language models</method>"
2024,https://openalex.org/W4390577959,Psychology,MixFormer: End-to-End Tracking With Iterative Mixed Attention,"Visual object tracking often employs a multi-stage pipeline of feature extraction, target information integration, and bounding box estimation. To simplify this pipeline and unify the process of feature extraction and target information integration, in this paper, we present a compact tracking framework, termed as MixFormer, built upon transformers. Our core design is to utilize the flexibility of attention operations, and we propose a Mixed Attention Module (MAM) for simultaneous feature extraction and target information integration. This synchronous modeling scheme allows us to extract target-specific discriminative features and perform extensive communication between target and search area. Based on MAM, we build our MixFormer trackers simply by stacking multiple MAMs and placing a localization head on top. Specifically, we instantiate two types of MixFormer trackers, a hierarchical tracker MixCvT, and a non-hierarchical simple tracker MixViT. For these two trackers, we investigate a series of pre-training methods and uncover the different behaviors between supervised pre-training and self-supervised pre-training in our MixFormer trackers. We also extend the masked autoencoder pre-training to our MixFormer trackers and design the new competitive TrackMAE pre-training technique. Finally, to handle multiple target templates during online tracking, we devise an asymmetric attention scheme in MAM to reduce computational cost, and propose an effective score prediction module to select high-quality templates. Our MixFormer trackers set a new state-of-the-art performance on seven tracking benchmarks, including LaSOT, TrackingNet, VOT2020, GOT-10 k, OTB100, TOTB and UAV123. In particular, our MixViT-L achieves AUC scores of 73.3% on LaSOT, 86.1% on TrackingNet and 82.8% on TOTB.","<method>transformers</method>, <method>Mixed Attention Module (MAM)</method>, <method>MixFormer trackers</method>, <method>hierarchical tracker MixCvT</method>, <method>non-hierarchical simple tracker MixViT</method>, <method>supervised pre-training</method>, <method>self-supervised pre-training</method>, <method>masked autoencoder pre-training</method>, <method>TrackMAE pre-training technique</method>, <method>asymmetric attention scheme</method>, <method>score prediction module</method>"
2024,https://openalex.org/W4390952274,Psychology,The factors influencing teacher education students’ willingness to adopt artificial intelligence technology for information-based teaching,"This study, rooted in the Technology Acceptance Model (TAM), investigates the multifaceted factors that influence teacher education students in Information-Based Teaching to embrace artificial intelligence technologies. To enrich the TAM framework, we have incorporated elements such as Artificial Intelligence Literacy (AIL), Subjective Norms (SN), and Output Quality (OQ), with the aim of examining their respective effects on the willingness of teacher education students to adopt AI technologies. To substantiate this theoretical framework, we conducted empirical research involving teacher education students from various Chinese universities. Our findings affirm the robustness of the TAM in explaining the inclination of teacher education students, engaged in the actual teaching process within a digitized educational environment, to adopt AI technologies. Through this model, our study underscores the pivotal role of Artificial Intelligence Literacy (AIL) in influencing educators' acceptance of AI technologies, establishing a foundational cornerstone for subsequent explorations within the theoretical landscape of the TAM. In this study, we identify Perceived Usefulness (PU) and Artificial Intelligence Literacy (AIL) as the primary factors affecting Behavioral Intention (BI) to use AI technologies. Consequently, to foster broader adoption of AI technologies by educators, it is essential to emphasize their tangible benefits and superiority in teaching, with the goal of promoting the extended utilization of AI in digitalized instruction.",No methods found.
2024,https://openalex.org/W4391429431,Psychology,The impact of chatbots based on large language models on second language vocabulary acquisition,"In recent years, the integration of artificial intelligence (AI) and machine learning (ML) into education, particularly for Personalized Language Learning (PLL), has garnered significant attention. This approach tailors interventions to address the unique challenges faced by individual learners. Large Language Models (LLMs), including Chatbots, have demonstrated a substantial potential in automating and enhancing educational tasks, effectively capturing the complexity and diversity of human language. In this study, 52 foreign language students were randomly divided into two groups: one with the assistance of a Chatbot based on LLMs and one without. Both groups learned the same series of target words over eight weeks. Post-treatment assessments, including systematic observation and quantitative tests assessing both receptive and productive vocabulary knowledge, were conducted immediately after the study and again two weeks later. The findings demonstrate that employing an AI Chatbot based on LLMs significantly aids students in acquiring both receptive and productive vocabulary knowledge during their second language learning journey. Notably, Chatbots contribute to the long-term retention of productive vocabulary and facilitate incidental vocabulary learning. This study offers valuable insights into the practical benefits of LLM-based tools in language learning, with a specific emphasis on vocabulary development. Chatbots utilizing LLMs emerge as effective language learning aids. It emphasizes the importance of educators understanding the potential of these technologies in L2 vocabulary instruction and encourages the adoption of strategic teaching methods incorporating such tools.","<method>Large Language Models (LLMs)</method>, <method>Chatbots based on LLMs</method>"
2024,https://openalex.org/W4392173880,Psychology,Supporting Teachers’ Professional Development With Generative AI: The Effects on Higher Order Thinking and Self-Efficacy,"Generative AI has emerged as a noteworthy milestone and a consequential advancement in the annals of major disciplines within the domains of human science and technology. This study aims to explore the effects of generative AI-assisted pre-service teaching skills training on pre-service teachers' self-efficacy and higher order thinking. The participants of this study were 215 pre-service mathematics, science, and computer teachers from a university in China. Firstly, a pretest-posttest quasi-experimental design was implemented for an experimental group (teaching skills training by generative AI) and a control group (teaching skills training by traditional methods) by investigating the teacher self-efficacy and higher order thinking of the two groups before and after the experiment. Secondly, a semi-structured interview comprising open-ended questions was administered to 25 pre-service teachers within the experimental group to present their views on generative AI-assisted teaching. The results showed that the scores of pre-service teachers in the experimental group, who used generative AI for teachers' professional development, were considerably higher than those of the control group, both in teacher self-efficacy (F = 8.589, p = 0.0084< 0.05) and higher order thinking (F = 7.217, p = 0.008 < 0.05). It revealed that generative AI can be effective in supporting teachers' professional development. This study produced a practical teachers' professional development method for pre-service teachers with generative AI.",<method>generative AI</method>
2024,https://openalex.org/W4392816828,Psychology,Role of Artificial Intelligence in Higher Education- An Empirical Investigation,"The importance of artificial intelligence (AI) is growing in all economic sectors and thus also in higher education. In recent years, there have been significant developments in this concept of ""Artificial Intelligence in Education (AIED)"". The purpose of this study was to find out how the concept of artificial intelligence can be applied to teaching and learning in higher education and the implications of the use of artificial intelligence in higher education. The impact of the development of technologies on learning is often studied on the methods and scope of learning and teaching. Artificial intelligence enables higher education services to become easily accessible with extraordinary speed, not only in the classroom but also outside the classroom. This report seeks to explore how AI will become an integral part of universities and seeks to examine its immediate and future impact on various aspects of higher education. The challenges of implementing AI in these institutes were also explored. As artificial intelligence (AI) research in education increases, many researchers in the field believe that the role of teachers, schools and leaders in education will change. In this regard, the aim of this study is to investigate which are the possible scenarios for the arrival of artificial intelligence in education and what impact it can have on the future of schools. In this research, it confirmed that artificial intelligence has been widely adopted and used in various forms in education, especially educational institutions. Artificial intelligence was initially implemented in the form of computers and computer-related technologies, moving to web-based and web-based intelligent educational systems, and finally with the use of embedded computing systems and other technologies such as humanoid robots and web-based chatbots teachers &amp; tasks and assignments independently or with tutors. With these platforms, teachers could perform various administrative tasks such as grading and Work more effectively and efficiently and achieve higher quality in your learning activities. On the other hand, because the systems use machine learning and adaptability, the curriculum and content are adapted which improved uptake and retention, which improved the student experience and the overall quality of education.",<method>machine learning</method>
2024,https://openalex.org/W4396802056,Psychology,"""I'm Not Sure, But..."": Examining the Impact of Large Language Models' Uncertainty Expression on User Reliance and Trust","Widely deployed large language models (LLMs) can produce convincing yet incorrect outputs, potentially misleading users who may rely on them as if they were correct. To reduce such overreliance, there have been calls for LLMs to communicate their uncertainty to end users. However, there has been little empirical work examining how users perceive and act upon LLMs' expressions of uncertainty. We explore this question through a large-scale, pre-registered, human-subject experiment (N=404) in which participants answer medical questions with or without access to responses from a fictional LLM-infused search engine. Using both behavioral and self-reported measures, we examine how different natural language expressions of uncertainty impact participants' reliance, trust, and overall task performance. We find that first-person expressions (e.g., ""I'm not sure, but..."") decrease participants' confidence in the system and tendency to agree with the system's answers, while increasing participants' accuracy. An exploratory analysis suggests that this increase can be attributed to reduced (but not fully eliminated) overreliance on incorrect answers. While we observe similar effects for uncertainty expressed from a general perspective (e.g., ""It's not clear, but...""), these effects are weaker and not statistically significant. Our findings suggest that using natural language expressions of uncertainty may be an effective approach for reducing overreliance on LLMs, but that the precise language used matters. This highlights the importance of user testing before deploying LLMs at scale.",No methods found.
2024,https://openalex.org/W4397026358,Psychology,Automated Classification of Cognitive Visual Objects Using Multivariate Swarm Sparse Decomposition From Multichannel EEG-MEG Signals,"In visual object decoding, magnetoencephalogram (MEG) and electroencephalogram (EEG) activation patterns demonstrate the utmost discriminative cognitive analysis due to their multivariate oscillatory nature. However, high noise in the recorded EEG-MEG signals and subject-specific variability make it extremely difficult to classify subject's cognitive responses to different visual stimuli. The proposed method is a multivariate extension of the swarm sparse decomposition method (MSSDM) for multivariate pattern analysis of EEG-MEG-based visual activation signals. In comparison, it is an advanced technique for decomposing nonstationary multicomponent signals into a finite number of channel-aligned oscillatory components that significantly enhance visual activation-related sub-bands. The MSSDM method adopts multivariate swarm filtering and sparse spectrum to automatically deliver optimal frequency bands in channel-specific sparse spectrums, resulting in improved filter banks. By combining the advantages of the multivariate SSDM and Riemann's correlation-assisted fusion feature (RCFF), the MSSDM-RCFF algorithm is investigated to improve the visual object recognition ability of EEG-MEG signals. We have also proposed time–frequency representation based on MSSDM to analyze discriminative cognitive patterns of different visual object classes from multichannel EEG-MEG signals. A proposed MSSDM is evaluated on multivariate synthetic signals and multivariate EEG-MEG signals using five classifiers. The proposed fusion feature and linear discriminant analysis classifier-based framework outperformed all existing state-of-the-art methods used for visual object detection and achieved the highest accuracy of 86.42% using tenfold cross-validation on EEG-MEG multichannel signals.","<method>swarm sparse decomposition method (MSSDM)</method>, <method>multivariate swarm filtering</method>, <method>sparse spectrum</method>, <method>Riemann's correlation-assisted fusion feature (RCFF)</method>, <method>MSSDM-RCFF algorithm</method>, <method>time–frequency representation based on MSSDM</method>, <method>linear discriminant analysis classifier</method>"
2024,https://openalex.org/W4399604220,Psychology,Cheating in the age of generative AI: A high school survey study of cheating behaviors before and after the release of ChatGPT,"The public release of ChatGPT and other generative AI chatbot technologies has been accompanied by questions about how academic integrity and student cheating behaviors will be impacted. We analyzed anonymous survey data from three high schools to see if self-reported cheating numbers changed following the introduction of ChatGPT and similar technologies. This survey data set is unique in that data on cheating had been collected with this set of schools both before and after November 2022, when ChatGPT was publicly released and drew attention to these educational concerns. The results suggested that cheating behaviors remained relatively stable after the introduction of this current generation of generative AI chatbot technology. However, some changes in reported behaviors differed depending on the type of cheating (social cheating, AI-related cheating, etc.). Additional survey questions about high school students' AI chatbot usage and the perceived allowability of such technology revealed mixed opinions on the acceptability of using AI for various academic-related tasks. Most students did not think that using a chatbot to produce an entire paper or complete an entire assignment should be allowable. However, there was support for using AI chatbots to help students to start on assignments and papers and to help explain new concepts to them.",No methods found.
2024,https://openalex.org/W4399704837,Psychology,Advanced surveillance and detection systems using deep learning to combat human trafficking,"Human trafficking remains one of the most heinous crimes, often hidden in plain sight, making it a complex challenge for law enforcement worldwide. The integration of deep learning into advanced surveillance and detection systems presents a promising frontier in the fight against this global issue. This review article explores the transformative impact of deep learning algorithms on surveillance technologies designed to detect patterns and anomalies indicative of human trafficking activities. We delve into various case studies where artificial intelligence (AI)-powered surveillance has not only facilitated the identification and rescue of victims but also significantly hindered the operational capabilities of trafficking networks. By analyzing the deployment of these systems in different contexts, this article assesses their effectiveness, the ethical implications of surveillance, the balance between privacy and security, and the future potential for scaling these technologies. Additionally, we explore the collaborative dynamics between AI technology developers and law enforcement agencies, emphasizing the need for a synergistic approach to maximize the impact of these technologies. This review aims to provide a comprehensive understanding of how cutting-edge deep learning applications are becoming crucial tools in the strategic arsenal against human trafficking, offering a beacon of hope for victims and a significant challenge to traffickers.","<method>deep learning algorithms</method>, <method>artificial intelligence (AI)-powered surveillance</method>"
2024,https://openalex.org/W4403619648,Psychology,Enhancing black-box models: Advances in explainable artificial intelligence for ethical decision-making,"Transparency, trust, and accountability are among the issues raised by artificial intelligence's (AI) growing reliance on black-box models, especially in high-stakes industries like healthcare, finance, and criminal justice. These models, which are frequently distinguished by their intricacy and opacity, are capable of producing extremely accurate forecasts, but users and decision-makers are still unable to fully understand how they operate. In response to this challenge, the field of Explainable AI (XAI) has emerged with the goal of demystifying these models by offering insights into their decision-making processes. Our ability to interpret model behavior has greatly improved with recent developments in XAI techniques, such as SHAP (Shapley Additive Explanations), LIME (Local Interpretable Model-agnostic Explanations), and counterfactual explanations. These instruments make it easier to recognize bias, promote trust, and guarantee adherence to moral principles and laws like the GDPR and the AI Act. Modern XAI techniques are reviewed in this research along with how they are used in moral decision-making. It looks at how explainability can improve fairness, reduce the risks of AI bias and discrimination, and assist well-informed decision-making in a variety of industries. It also examines the trade-offs between performance and interpretability of models, as well as the growing trends toward user-centric explainability techniques. In order to ensure responsible AI development and deployment, XAI's role in fostering accountability and transparency will become increasingly important as AI becomes more integrated into critical systems.","<method>SHAP (Shapley Additive Explanations)</method>, <method>LIME (Local Interpretable Model-agnostic Explanations)</method>, <method>counterfactual explanations</method>"
2024,https://openalex.org/W223178600,Psychology,Occupational Therapy Models for Intervention with Children and Families,"Therapy Models for Intervention with Children and Families explores recent theoretical models that enable occupational therapists to practice and interact with families in a more holistic and occupation-centered manner. This comprehensive and dynamic text offers the latest information on viewing the broader contexts of environment and family in order to meet diverse occupational needs in a range of settings. Edited by Sandee Dunbar, the text presents a variety of case scenarios that feature culturally diverse populations and varying diagnoses of children with occupational needs. With contributions from 11 renowned leaders in occupational therapy, this comprehensive text is designed to increase awareness and understanding of theoretical models and their relationship to current occupational therapy practice with today's children and families. Inside this dynamic text, traditional frames of reference in pediatric practice are explored, including Sensory Integration and Neurodevelopmental Therapy. Some current theoretical models discussed include the Model of Human Occupation, the Person-Environment-Occupation Model, the Ecology of Human Performance Model, and the Occupational Adaptation Model. Incorporated throughout the text is the new Occupational Therapy Practice Framework. Employing a practical approach to this significant aspect of pediatric practice in occupational therapy, Therapy Models for Intervention with Children and Families is an invaluable tool for students at all curriculum levels.",No methods found.
2024,https://openalex.org/W2605915357,Psychology,Integrating Creative Problem Solving And Engineering Design,"Abstract NOTE: The first page of text has been automatically extracted and included below in lieu of an abstract Session 2225 Integrating Creative Problem Solving and Engineering Design Edward Lumsdaine, Michigan Technological University J. William Shelnutt, University of North Carolina at Charlotte Monika Lumsdaine, E&M Lumsdaine Solar Consultants, Inc. Abstract ""Engineering design is the communication of a set of rational decisions obtained with creative problem solving for accomplishing certain stated objectives within prescribed constraints."" How can engineering design be taught within the framework of this definition—what are the goals and building blocks? An innovative textbook demonstrates an integrated approach usable at the fresh- man and senior levels and for multi-level, mutidisciplinary projects. The textbook will be pub- lished by mid-June through McGraw-Hill's College Custom Series. The paper will describe the approach and discuss experiences with different parts of the course content. By conference time, additional feedback from senior projects in technology will be available. The integrated approach has a double focus: • Develop the required thinking skills: visualization, cognitive models, communication, team- work, and creative problem solving. Industry as well as the ABET 2000 Criteria demand that engineers have these foundational skills. • Apply the skills in the twelve steps to quality by design. The textbook provides many practical ""how to"" guidelines, planning and economic analysis tool templates (attached on a CD-ROM), and a library of design documentation samples to enable instructors and students to focus on optimizing their design projects and solutions and prevent dysfunctional teams. A teaching manual accompanies the textbook and will be available from a web site. It includes sample syllabi for a variety of courses from pre-college programs and freshman engineering orien- tation to senior capstone design and workshops to enhance creativity and innovation in the work- place. This broad range is possible by shifting the emphasis from learning the process of creative problem solving to achieving a quality design product. Also, the textbook is built on the knowledge creation cycle and thus addresses different thinking preferences and learning styles. Formats range from 15-hour course modules or seminars to quarter or semester courses or sequences. Motivation The purpose of this integrated textbook is to enable engineers and technologists to be more innova- tive in conceptual design. This book has a strong focus on creative thinking and problem solving, visualization, teamwork, and communication. It responds to the needs of industry for employees who have these foundational skills needed for concurrent engineering and to the ABET Criteria 2000 (which require that engineering and technology students are able to work on multidisciplinary teams and understand the global context of their work). The unique integrated approach enables",No methods found.
2024,https://openalex.org/W4390562274,Psychology,Active inference as a theory of sentient behavior,"This review paper offers an overview of the history and future of active inference—a unifying perspective on action and perception. Active inference is based upon the idea that sentient behavior depends upon our brains' implicit use of internal models to predict, infer, and direct action. Our focus is upon the conceptual roots and development of this theory of (basic) sentience and does not follow a rigid chronological narrative. We trace the evolution from Helmholtzian ideas on unconscious inference, through to a contemporary understanding of action and perception. In doing so, we touch upon related perspectives, the neural underpinnings of active inference, and the opportunities for future development. Key steps in this development include the formulation of predictive coding models and related theories of neuronal message passing, the use of sequential models for planning and policy optimization, and the importance of hierarchical (temporally) deep internal (i.e., generative or world) models. Active inference has been used to account for aspects of anatomy and neurophysiology, to offer theories of psychopathology in terms of aberrant precision control, and to unify extant psychological theories. We anticipate further development in all these areas and note the exciting early work applying active inference beyond neuroscience. This suggests a future not just in biology, but in robotics, machine learning, and artificial intelligence.","<method>active inference</method>, <method>predictive coding models</method>, <method>sequential models for planning and policy optimization</method>, <method>hierarchical (temporally) deep internal (generative or world) models</method>"
2024,https://openalex.org/W4391345489,Psychology,CLARUS: An interactive explainable AI platform for manual counterfactuals in graph neural networks,"Lack of trust in artificial intelligence (AI) models in medicine is still the key blockage for the use of AI in clinical decision support systems (CDSS). Although AI models are already performing excellently in systems medicine, their black-box nature entails that patient-specific decisions are incomprehensible for the physician. Explainable AI (XAI) algorithms aim to ""explain"" to a human domain expert, which input features influenced a specific recommendation. However, in the clinical domain, these explanations must lead to some degree of causal understanding by a clinician. We developed the CLARUS platform, aiming to promote human understanding of graph neural network (GNN) predictions. CLARUS enables the visualisation of patient-specific networks, as well as, relevance values for genes and interactions, computed by XAI methods, such as GNNExplainer. This enables domain experts to gain deeper insights into the network and more importantly, the expert can interactively alter the patient-specific network based on the acquired understanding and initiate re-prediction or retraining. This interactivity allows us to ask manual counterfactual questions and analyse the effects on the GNN prediction. We present the first interactive XAI platform prototype, CLARUS, that allows not only the evaluation of specific human counterfactual questions based on user-defined alterations of patient networks and a re-prediction of the clinical outcome but also a retraining of the entire GNN after changing the underlying graph structures. The platform is currently hosted by the GWDG on https://rshiny.gwdg.de/apps/clarus/.","<method>graph neural network (GNN)</method>, <method>Explainable AI (XAI) algorithms</method>, <method>GNNExplainer</method>"
2024,https://openalex.org/W4393157467,Psychology,Visual Adversarial Examples Jailbreak Aligned Large Language Models,"Warning: this paper contains data, prompts, and model outputs that are offensive in nature. Recently, there has been a surge of interest in integrating vision into Large Language Models (LLMs), exemplified by Visual Language Models (VLMs) such as Flamingo and GPT-4. This paper sheds light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated LLMs. Second, we highlight that the versatility of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. As an illustration, we present a case study in which we exploit visual adversarial examples to circumvent the safety guardrail of aligned LLMs with integrated vision. Intriguingly, we discover that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions (that it otherwise would not) and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example. Our study underscores the escalating adversarial risks associated with the pursuit of multimodality. Our findings also connect the long-studied adversarial vulnerabilities of neural networks to the nascent field of AI alignment. The presented attack suggests a fundamental adversarial challenge for AI alignment, especially in light of the emerging trend toward multimodality in frontier foundation models.","<method>Visual Language Models (VLMs)</method>, <method>Flamingo</method>, <method>GPT-4</method>, <method>visual adversarial examples</method>"
2024,https://openalex.org/W4400134137,Psychology,Computational intelligence-based classification system for the diagnosis of memory impairment in psychoactive substance users,"Abstract Computational intelligence techniques have emerged as a promising approach for diagnosing various medical conditions, including memory impairment. Increased abuse of psychoactive drugs poses a global public health burden, as repeated exposure to these substances can cause neurodegeneration, premature aging, and negatively affect memory impairment. Many studies in the literature relied on statistical studies, but they remained inaccurate. Some studies relied on physical data because the time factor was not considered, until Artificial Intelligence (AI) techniques came along that proved their worth in this diagnosis. The variable deep neural network method was used to adapt to the intermediate results and re-process the intermediate in case the result is undesirable. Computational intelligence was used in this study to classify a brain image from MRI or CT scans and to show the effectiveness of the dose ratio on health with treatment time, and to diagnose memory impairment in users of psychoactive substances. Understanding the neurotoxic profiles of psychoactive substances and the underlying pathways is hypothesized to be of great importance in improving the risk assessment and treatment of substance use disorders. The results proved the worth of the proposed method in terms of the accuracy of recognition rate as well as the possibility of diagnosis. It can be concluded that the diagnostic efficiency is increased by increasing the number of hidden layers in the neural network and controlling the weights and variables that control the deep learning algorithm. Thus, we conclude that good classification in this field may save human life or early detection of memory impairment.","<method>Artificial Intelligence (AI) techniques</method>, <method>variable deep neural network method</method>, <method>computational intelligence</method>, <method>deep learning algorithm</method>"
2024,https://openalex.org/W4400981456,Psychology,Multimodal data integration for oncology in the era of deep neural networks: a review,"Cancer research encompasses data across various scales, modalities, and resolutions, from screening and diagnostic imaging to digitized histopathology slides to various types of molecular data and clinical records. The integration of these diverse data types for personalized cancer care and predictive modeling holds the promise of enhancing the accuracy and reliability of cancer screening, diagnosis, and treatment. Traditional analytical methods, which often focus on isolated or unimodal information, fall short of capturing the complex and heterogeneous nature of cancer data. The advent of deep neural networks has spurred the development of sophisticated multimodal data fusion techniques capable of extracting and synthesizing information from disparate sources. Among these, Graph Neural Networks (GNNs) and Transformers have emerged as powerful tools for multimodal learning, demonstrating significant success. This review presents the foundational principles of multimodal learning including oncology data modalities, taxonomy of multimodal learning, and fusion strategies. We delve into the recent advancements in GNNs and Transformers for the fusion of multimodal data in oncology, spotlighting key studies and their pivotal findings. We discuss the unique challenges of multimodal learning, such as data heterogeneity and integration complexities, alongside the opportunities it presents for a more nuanced and comprehensive understanding of cancer. Finally, we present some of the latest comprehensive multimodal pan-cancer data sources. By surveying the landscape of multimodal data integration in oncology, our goal is to underline the transformative potential of multimodal GNNs and Transformers. Through technological advancements and the methodological innovations presented in this review, we aim to chart a course for future research in this promising field. This review may be the first that highlights the current state of multimodal modeling applications in cancer using GNNs and transformers, presents comprehensive multimodal oncology data sources, and sets the stage for multimodal evolution, encouraging further exploration and development in personalized cancer care.","<method>deep neural networks</method>, <method>Graph Neural Networks (GNNs)</method>, <method>Transformers</method>"
2024,https://openalex.org/W4390721566,Psychology,Thousands of AI Authors on the Future of AI,"In the largest survey of its kind, 2,778 researchers who had published in top-tier artificial intelligence (AI) venues gave predictions on the pace of AI progress and the nature and impacts of advanced AI systems The aggregate forecasts give at least a 50% chance of AI systems achieving several milestones by 2028, including autonomously constructing a payment processing site from scratch, creating a song indistinguishable from a new song by a popular musician, and autonomously downloading and fine-tuning a large language model. If science continues undisrupted, the chance of unaided machines outperforming humans in every possible task was estimated at 10% by 2027, and 50% by 2047. The latter estimate is 13 years earlier than that reached in a similar survey we conducted only one year earlier [Grace et al., 2022]. However, the chance of all human occupations becoming fully automatable was forecast to reach 10% by 2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey). Most respondents expressed substantial uncertainty about the long-term value of AI progress: While 68.3% thought good outcomes from superhuman AI are more likely than bad, of these net optimists 48% gave at least a 5% chance of extremely bad outcomes such as human extinction, and 59% of net pessimists gave 5% or more to extremely good outcomes. Between 38% and 51% of respondents gave at least a 10% chance to advanced AI leading to outcomes as bad as human extinction. More than half suggested that ""substantial"" or ""extreme"" concern is warranted about six different AI-related scenarios, including misinformation, authoritarian control, and inequality. There was disagreement about whether faster or slower AI progress would be better for the future of humanity. However, there was broad agreement that research aimed at minimizing potential risks from AI systems ought to be prioritized more.",No methods found.
2024,https://openalex.org/W4391774550,Psychology,Role of machine learning and deep learning techniques in EEG-based BCI emotion recognition system: a review,"Abstract Emotion is a subjective psychophysiological reaction coming from external stimuli which impacts every aspect of our daily lives. Due to the continuing development of non-invasive and portable sensor technologies, such as brain-computer interfaces (BCI), intellectuals from several fields have been interested in emotion recognition techniques. Human emotions can be recognised using a variety of behavioural cues, including gestures and body language, voice, and physiological markers. The first three, however, might be ineffective because people sometimes conceal their genuine emotions either intentionally or unknowingly. More precise and objective emotion recognition can be accomplished using physiological signals. Among other physiological signals, Electroencephalogram (EEG) is more responsive and sensitive to variation in affective states. Various EEG-based emotion recognition methods have recently been introduced. This study reviews EEG-based BCIs for emotion identification and gives an outline of the progress made in this field. A summary of the datasets and techniques utilised to evoke human emotions and various emotion models is also given. We discuss several EEG feature extractions, feature selection/reduction, machine learning, and deep learning algorithms in accordance with standard emotional identification process. We provide an overview of the human brain's EEG rhythms, which are closely related to emotional states. We also go over a number of EEG-based emotion identification research and compare numerous machine learning and deep learning techniques. In conclusion, this study highlights the applications, challenges and potential areas for future research in identification and classification of human emotional states.","<method>feature extraction</method>, <method>feature selection/reduction</method>, <method>machine learning algorithms</method>, <method>deep learning algorithms</method>"
2024,https://openalex.org/W4391811765,Psychology,Usefulness and Accuracy of Artificial Intelligence Chatbot Responses to Patient Questions for Neurosurgical Procedures,"BACKGROUND AND OBJECTIVES: The Internet has become a primary source of health information, leading patients to seek answers online before consulting health care providers. This study aims to evaluate the implementation of Chat Generative Pre-Trained Transformer (ChatGPT) in neurosurgery by assessing the accuracy and helpfulness of artificial intelligence (AI)–generated responses to common postsurgical questions. METHODS: A list of 60 commonly asked questions regarding neurosurgical procedures was developed. ChatGPT-3.0, ChatGPT-3.5, and ChatGPT-4.0 responses to these questions were recorded and graded by numerous practitioners for accuracy and helpfulness. The understandability and actionability of the answers were assessed using the Patient Education Materials Assessment Tool. Readability analysis was conducted using established scales. RESULTS: A total of 1080 responses were evaluated, equally divided among ChatGPT-3.0, 3.5, and 4.0, each contributing 360 responses. The mean helpfulness score across the 3 subsections was 3.511 ± 0.647 while the accuracy score was 4.165 ± 0.567. The Patient Education Materials Assessment Tool analysis revealed that the AI-generated responses had higher actionability scores than understandability. This indicates that the answers provided practical guidance and recommendations that patients could apply effectively. On the other hand, the mean Flesch Reading Ease score was 33.5, suggesting that the readability level of the responses was relatively complex. The Raygor Readability Estimate scores ranged within the graduate level, with an average score of the 15th grade. CONCLUSION: The artificial intelligence chatbot's responses, although factually accurate, were not rated highly beneficial, with only marginal differences in perceived helpfulness and accuracy between ChatGPT-3.0 and ChatGPT-3.5 versions. Despite this, the responses from ChatGPT-4.0 showed a notable improvement in understandability, indicating enhanced readability over earlier versions.","<method>Chat Generative Pre-Trained Transformer (ChatGPT-3.0)</method>, <method>Chat Generative Pre-Trained Transformer (ChatGPT-3.5)</method>, <method>Chat Generative Pre-Trained Transformer (ChatGPT-4.0)</method>"
2024,https://openalex.org/W4392241969,Psychology,All models are wrong and yours are useless: making clinical prediction models impactful for patients,"All models are wrong and yours are useless: making clinical prediction models impactful for patients Florian MarkowetzCheck for updates Most published clinical prediction models are never used in clinical practice and there is a huge gap between academic research and clinical implementation.Here, I propose ways for academic researchers to be proactive partners in improving clinical practice and to design models in ways that ultimately benefit patients.""All models are wrong, but some are useful"" is an aphorism attributed to the statistician George Box.There is humility in claiming your model is wrong, but there is also bravado in implying your model might be useful.And, honestly, I don't think it is.I think your model is useless.How would I know?I don't even know who you are.Well, it is a bet.A bet I am willing to take because the odds are ridiculously in my favour.I will explain what I mean in the context of clinical prediction models.My points apply to a wide range of preclinical models, both computational and biological, but my own core expertise is with clinical prediction tools.These are computational models from statistics, machine learning or AI that try to predict clinically relevant variables and ultimately aim to help doctors to treat patients better.The papers describing them make claims like ""this model can be used in the clinic""; generally softened with words like ""might"", ""could"", ""potential"", ""promise"", or other techniques to reduce accountability.The Box quote offers a yardstick to measure the success of these models; not by how correctly they describe reality but by how useful they are in helping patients.And in general, almost none of these tools ever help anyone.There is a wealth of systematic reviews in different fields to show how many models have been proposed and how few have even been validated, let alone been adopted in the clinic.For example, 408(!) models for chronic obstructive pulmonary disease were systematically reviewed 1 and as a summary the authors bleakly note ""several methodological pitfalls in their development and a low rate of external validation"".And whatever biomedical area you work in, your experiences will mirror this resultmany novel prediction models, little help for patients.I believe that a model designed to be used for patients is useless unless it is actually used for patients.",No methods found.
2024,https://openalex.org/W4392349077,Psychology,Tracking and mapping in medical computer vision: A review,"As computer vision algorithms increase in capability, their applications in clinical systems will become more pervasive. These applications include: diagnostics, such as colonoscopy and bronchoscopy; guiding biopsies, minimally invasive interventions, and surgery; automating instrument motion; and providing image guidance using pre-operative scans. Many of these applications depend on the specific visual nature of medical scenes and require designing algorithms to perform in this environment. In this review, we provide an update to the field of camera-based tracking and scene mapping in surgery and diagnostics in medical computer vision. We begin with describing our review process, which results in a final list of 515 papers that we cover. We then give a high-level summary of the state of the art and provide relevant background for those who need tracking and mapping for their clinical applications. After which, we review datasets provided in the field and the clinical needs that motivate their design. Then, we delve into the algorithmic side, and summarize recent developments. This summary should be especially useful for algorithm designers and to those looking to understand the capability of off-the-shelf methods. We maintain focus on algorithms for deformable environments while also reviewing the essential building blocks in rigid tracking and mapping since there is a large amount of crossover in methods. With the field summarized, we discuss the current state of the tracking and mapping methods along with needs for future algorithms, needs for quantification, and the viability of clinical applications. We then provide some research directions and questions. We conclude that new methods need to be designed or combined to support clinical applications in deformable environments, and more focus needs to be put into collecting datasets for training and evaluation.",No methods found.
2024,https://openalex.org/W4392504616,Psychology,Bumblebees socially learn behaviour too complex to innovate alone,"Abstract Culture refers to behaviours that are socially learned and persist within a population over time. Increasing evidence suggests that animal culture can, like human culture, be cumulative: characterized by sequential innovations that build on previous ones 1 . However, human cumulative culture involves behaviours so complex that they lie beyond the capacity of any individual to independently discover during their lifetime 1–3 . To our knowledge, no study has so far demonstrated this phenomenon in an invertebrate. Here we show that bumblebees can learn from trained demonstrator bees to open a novel two-step puzzle box to obtain food rewards, even though they fail to do so independently. Experimenters were unable to train demonstrator bees to perform the unrewarded first step without providing a temporary reward linked to this action, which was removed during later stages of training. However, a third of naive observer bees learned to open the two-step box from these demonstrators, without ever being rewarded after the first step. This suggests that social learning might permit the acquisition of behaviours too complex to ‘re-innovate’ through individual learning. Furthermore, naive bees failed to open the box despite extended exposure for up to 24 days. This finding challenges a common opinion in the field: that the capacity to socially learn behaviours that cannot be innovated through individual trial and error is unique to humans.",No methods found.
2024,https://openalex.org/W4393971547,Psychology,A self-attention-based CNN-Bi-LSTM model for accurate state-of-charge estimation of lithium-ion batteries,"In the quest for clean and efficient energy solutions, lithium-ion batteries have emerged at the forefront of technological innovation. Accurate state-of-charge (SOC) estimation across a broad temperature range is essential for extending battery longevity, and enduring effective management of overcharge and over-discharge conditions. However, prevailing challenges persist in achieving precise SOC estimates and generalizing across a wide temperature range, particularly at lower temperatures. Our comparative analysis reveals that, while a single-layer bidirectional LSTM model with a self-attention mechanism achieves remarkable SOC estimation accuracy at room temperature, the intricacies of SOC estimation at lower temperatures necessitate the incorporation of more hidden layers and more complex network architecture to capture intricate features influencing battery dynamics. Hence, we propose a deep learning model, based on convolutional neural networks integrating bidirectional long short-term memory and self-attention mechanism (CNN-Bi-LSTM-AM), specifically designed to tackle the challenges of achieving accurate SOC estimations across a wide temperature range. The proposed model demonstrates proficiency in capturing both spatial and temporal dependencies critical for lithium-ion battery SOC estimation. Furthermore, the integration of a self-attention mechanism enhances the model's adeptness to discern pertinent features and patterns within the dataset, thereby improving its overall performance and robustness, even in sub-room temperature environments.","<method>single-layer bidirectional LSTM model with a self-attention mechanism</method>, <method>deep learning model based on convolutional neural networks integrating bidirectional long short-term memory and self-attention mechanism (CNN-Bi-LSTM-AM)</method>"
2024,https://openalex.org/W4396762959,Psychology,Assessing Visual Hallucinations in Vision-Enabled Large Language Models,"<title>Abstract</title> Recent advancements in vision-enabled large language models have prompted a renewed interest in evaluating their capabilities and limitations when interpreting complex visual data. The current research employs ImageNet-A, a dataset specifically designed with adversarially selected images that challenge standard AI models, to test the visual processing robustness of three prominent models: GPT-4 Vision, Google Gemini 1.5, and Anthropic Claude 3. Quantitative analyses revealed notable disparities in misclassification rates and types of errors among these models, indicating a variation in their ability to handle adversarial inputs effectively. GPT-4 Vision demonstrated a commendable robustness, whereas Google Gemini 1.5 excelled in processing speed and efficiency. Anthropic Claude 3, while showing intermediate accuracy levels, displayed a significant propensity for contextual misinterpretations. Qualitative evaluations further assessed the relevance and plausibility of the models' visual hallucinations, uncovering challenges in achieving human-like understanding of ambiguous or complex scenes. The findings emphasize the necessity for further improvements in semantic accuracy and contextual understanding. Future research directions include enhancing adversarial robustness, refining evaluation metrics to better capture the qualitative aspects of visual understanding, and fostering interdisciplinary collaborations to develop AI systems with more nuanced interpretive abilities. The study underscores the ongoing journey towards AI models that can match human perceptual skills, highlighting both the progress made and the considerable challenges that remain.",No methods found.
2024,https://openalex.org/W4399054302,Psychology,Artificial Intelligence in Point-of-Care Biosensing: Challenges and Opportunities,"The integration of artificial intelligence (AI) into point-of-care (POC) biosensing has the potential to revolutionize diagnostic methodologies by offering rapid, accurate, and accessible health assessment directly at the patient level. This review paper explores the transformative impact of AI technologies on POC biosensing, emphasizing recent computational advancements, ongoing challenges, and future prospects in the field. We provide an overview of core biosensing technologies and their use at the POC, highlighting ongoing issues and challenges that may be solved with AI. We follow with an overview of AI methodologies that can be applied to biosensing, including machine learning algorithms, neural networks, and data processing frameworks that facilitate real-time analytical decision-making. We explore the applications of AI at each stage of the biosensor development process, highlighting the diverse opportunities beyond simple data analysis procedures. We include a thorough analysis of outstanding challenges in the field of AI-assisted biosensing, focusing on the technical and ethical challenges regarding the widespread adoption of these technologies, such as data security, algorithmic bias, and regulatory compliance. Through this review, we aim to emphasize the role of AI in advancing POC biosensing and inform researchers, clinicians, and policymakers about the potential of these technologies in reshaping global healthcare landscapes.","<method>machine learning algorithms</method>, <method>neural networks</method>"
2024,https://openalex.org/W4399426804,Psychology,Evaluating the persuasive influence of political microtargeting with large language models,"Recent advancements in large language models (LLMs) have raised the prospect of scalable, automated, and fine-grained political microtargeting on a scale previously unseen; however, the persuasive influence of microtargeting with LLMs remains unclear. Here, we build a custom web application capable of integrating self-reported demographic and political data into GPT-4 prompts in real-time, facilitating the live creation of unique messages tailored to persuade individual users on four political issues. We then deploy this application in a preregistered randomized control experiment ( n = 8,587) to investigate the extent to which access to individual-level data increases the persuasive influence of GPT-4. Our approach yields two key findings. First, messages generated by GPT-4 were broadly persuasive, in some cases increasing support for an issue stance by up to 12 percentage points. Second, in aggregate, the persuasive impact of microtargeted messages was not statistically different from that of non-microtargeted messages (4.83 vs. 6.20 percentage points, respectively, P = 0.226). These trends hold even when manipulating the type and number of attributes used to tailor the message. These findings suggest—contrary to widespread speculation—that the influence of current LLMs may reside not in their ability to tailor messages to individuals but rather in the persuasiveness of their generic, nontargeted messages. We release our experimental dataset, GPTarget2024 , as an empirical baseline for future research.",<method>GPT-4</method>
2024,https://openalex.org/W4396832329,Psychology,Rethinking Human-AI Collaboration in Complex Medical Decision Making: A Case Study in Sepsis Diagnosis,"Today's AI systems for medical decision support often succeed on benchmark datasets in research papers but fail in real-world deployment. This work focuses on the decision making of sepsis, an acute life-threatening systematic infection that requires an early diagnosis with high uncertainty from the clinician. Our aim is to explore the design requirements for AI systems that can support clinical experts in making better decisions for the early diagnosis of sepsis. The study begins with a formative study investigating why clinical experts abandon an existing AI-powered Sepsis predictive module in their electrical health record (EHR) system. We argue that a human-centered AI system needs to support human experts in the intermediate stages of a medical decision-making process (e.g., generating hypotheses or gathering data), instead of focusing only on the final decision. Therefore, we build SepsisLab based on a state-of-the-art AI algorithm and extend it to predict the future projection of sepsis development, visualize the prediction uncertainty, and propose actionable suggestions (i.e., which additional laboratory tests can be collected) to reduce such uncertainty. Through heuristic evaluation with six clinicians using our prototype system, we demonstrate that SepsisLab enables a promising human-AI collaboration paradigm for the future of AI-assisted sepsis diagnosis and other high-stakes medical decision making.",<method>state-of-the-art AI algorithm</method>
2024,https://openalex.org/W4391144074,Psychology,"The neutrophil-to-lymphocyte ratio, lymphocyte-to-monocyte ratio, and neutrophil-to-high-density-lipoprotein ratio are correlated with the severity of Parkinson’s disease","Background Inflammation plays a pivotal role in the pathogenesis of Parkinson’s disease (PD). However, the correlation between peripheral inflammatory markers and the severity of PD remains unclear. Methods The following items in plasma were collected for assessment among patients with PD ( n = 303) and healthy controls (HCs; n = 303) were assessed for the neutrophil-to-lymphocyte ratio (NLR), lymphocyte-to-monocyte ratio (LMR) and neutrophil-to-high-density-lipoprotein ratio (NHR) in plasma, and neuropsychological assessments were performed for all patients with PD. Spearman rank or Pearson correlation was used to evaluate the correlation between the NLR, the LMR and the NHR and the severity of PD. Receiver operating characteristic (ROC) curves were used to evaluate the diagnostic performance of the NLR, LMR and NHR for PD. Results The plasma NLR and NHR were substantially higher in patients with PD than in HCs, while the plasma LMR was substantially lower. The plasma NLR was positively correlated with Hoehn and Yahr staging scale (H&amp;amp;Y), Unified Parkinson’s Disease Rating Scale (UPDRS), UPDRS-I, UPDRS-II, and UPDRS-III scores. Conversely, it exhibited a negative relationship with Mini-Mental State Examination (MMSE) and Montreal Cognitive Assessment (MoCA) scores. Furthermore, the plasma NHR was positively correlated with H&amp;amp;Y, UPDRS, UPDRS-I, UPDRS-II and UPDRS-III scores. Moreover, negative associations were established between the plasma LMR and H&amp;amp;Y, UPDRS, UPDRS-I, UPDRS-II, and UPDRS-III scores. Finally, based on the ROC curve analysis, the NLR, LMR and NHR exhibited respectable PD discriminating power. Conclusion Our research indicates that a higher NLR and NHR and a lower LMR may be relevant for assessing the severity of PD and appear to be promising disease-state biomarker candidates.",No methods found.
2024,https://openalex.org/W4391232768,Psychology,EEG components of inhibitory control ability in internet gaming disorder: A systematic review and meta‐analysis of randomized controlled trials,"Abstract Background Inhibitory control ability is a crucial cognitive function that enables individuals to regulate their impulses and behaviors in a goal‐directed manner. However, with the increasing prevalence of internet gaming disorder (IGD), there has been growing concern about the impact of excessive gaming on inhibitory control ability. Despite the accumulating evidence on this topic, the research conclusion on whether people with IGD have worse inhibition control ability than healthy controls remains inconsistent, and the lack of effective electroencephalography prediction indicators further complicates this issue. To address this research gap, the present study aimed to investigate whether N2 event‐related potential (ERP) and P3 ERP components could serve as reliable indicators of inhibitory control ability in individuals with IGD. Methods To achieve this goal, a systematic literature search was conducted in several databases, including Web of Science, ScienceDirect (EBSCO), SpringerLink, PubMed, and Wiley Online Library. The inclusion criteria were strictly implemented to ensure the quality of the studies included in the meta‐analysis. In the end, a total of 5 studies, with 139 participants diagnosed with IGD and 139 healthy controls, were included in the analysis. Results Meta‐analysis revealed large effect sizes of N2 and P3 amplitudes in individuals with IGD, indicating that these two ERP components could be potential indicators of inhibitory control ability. Specifically, the N2 and P3 amplitude was significantly larger in individuals with IGD than in the healthy control group, suggesting deficits in inhibitory control function and increased impulsivity in the IGD group. In the inhibition control task, the IGD group required more cognitive resources to suppress impulsive responses. Conclusion Overall, the findings of this meta‐analysis shed light on the potential use of N2 and P3 amplitudes as reliable indicators of inhibitory control ability in individuals with IGD. The results provide crucial insights into the neural mechanisms underlying inhibitory control impairment in IGD, which could inform the development of effective interventions for this condition. Further research is needed to explore the functional significance of these ERP components and their potential clinical applications in the diagnosis and treatment of IGD.",No methods found.
2024,https://openalex.org/W4391070618,Psychology,Testing an extended theory of planned behaviour in predicting Covid-19 vaccination intention over the course of the pandemic: A three-wave repeated cross-sectional study,"BackgroundMass vaccination against Covid-19 has been recognised as the most effective strategy for overcoming the pandemic emergency and remains crucial in the ongoing efforts to mitigate the impact of the virus. The present study aimed to test the efficacy of an extended Theory of Planned Behaviour (TPB) model in predicting vaccination intention in three different phases of the pandemic. Understanding how psychological drivers of vaccine acceptance may have changed throughout the pandemic is essential for informing public health strategies and addressing vaccine hesitancy, even in the current post-pandemic context.MethodsUsing a repeated cross-sectional survey, we tested the hypothesised extended TPB model (intention, attitude, subjective norms, perceived behavioural control, anticipated affective reactions, risk perception, trust in science, trust in institutions and religiosity) across three independent samples: before (T1: November–December 2020; N = 657), during (T2: March–May 2021; N = 818), and after (T3: February–March 2022; N = 605) the start of the vaccination campaign in Italy.ResultsResults indicated significant differences between the time points in all investigated variables, pointing to a general trend of improvement in vaccine acceptability levels at T2 compared to T1, and a worsening at T3 compared to the other two time points. Interestingly, net of these differences, a multi-group SEM analysis supported the invariance, across time, of the structural relationships examined within the extended TPB.ConclusionFindings demonstrated the efficacy of the TPB in predicting Covid-19 vaccination intention at different stages of the pandemic, suggesting that the model, in its extended version, represents a valuable framework for designing interventions aimed at promoting vaccine uptake.",No methods found.
2024,https://openalex.org/W4394845368,Psychology,Perspectives in the Development of Tools to Assess Vaccine Literacy,"Vaccine literacy (VL) is the ability to find, understand, and evaluate vaccination-related information to make appropriate decisions about immunization. The tools developed so far for its evaluation have produced consistent results. However, some dimensions may be underestimated due to the complexity of factors influencing VL. Moreover, the heterogeneity of methods used in studies employing these tools hinders a comprehensive understanding of its role even more. To overcome these limitations, a path has been sought to propose new instruments. This has necessitated updating earlier literature reviews on VL and related tools, exploring its relationship with vaccine hesitancy (VH), and examining associated variables like beliefs, attitudes, and behaviors towards immunization. Based on the current literature, and supported by the re-analysis of a dataset from an earlier study, we propose a theoretical framework to serve as the foundation for creating future assessment tools. These instruments should not only evaluate the psychological factors underlying the motivational aspect of VL, but also encompass knowledge and competencies. The positioning of VL in the framework at the intersection between sociodemographic antecedents and attitudes, leading to behaviors and outcomes, explains why and how VL can directly or indirectly influence vaccination decisions by countering VH and operating at personal, as well as at organizational and community levels.",No methods found.
2024,https://openalex.org/W4393904218,Psychology,Multilevel analysis of COVID-19 vaccination intention: the moderating role of economic and cultural country characteristics,"Abstract Background Predictors of COVID-19 (coronavirus) vaccination have been extensively researched; however, the contextual factors contributing to understanding vaccination intention remain largely unexplored. The present study aimed to investigate the moderating role of economic development (Gross domestic product - GDP per capita), economic inequality (Gini index), the perceived corruption index and Hofstede’s measurements of cultural values—index of individualism/collectivism and power distance index—in the relationship between determinants of satisfaction with the healthcare system, trust in political institutions, conspiracy beliefs and COVID-19 vaccination intention. Methods A multilevel modelling approach was employed on a sample of approximately 51 000 individuals nested within 26 countries. Data were drawn from the European Social Survey Round 10. The model examined the effect of individual- and country-level predictors and their interaction on vaccination intention. Results Satisfaction with the healthcare system had a stronger positive effect on intention to get vaccinated in countries with lower perceived corruption and more individualistic countries. Trust in political institutions had a stronger positive effect on vaccination intention in countries with higher economic development and lower perceived corruption, while a negative effect of conspiracy beliefs on vaccination intention was stronger in countries with lower economic development, higher perceived corruption and a more collectivistic cultural orientation. Conclusion Our findings highlight the importance of considering individual and contextual factors when addressing vaccination intention.",<method>multilevel modelling</method>
2024,https://openalex.org/W4400061112,Psychology,Cognitive Control and Neural Activity during Human Development: Evidence for Synaptic Pruning,"The modern world is full of distractions that steal our attention from important tasks. As a result, the world places ever-growing demands on cognitive control: the ability to coordinate mental resources to achieve our goals (Koechlin et al., 2003). A key mediator of cognitive control is the prefrontal cortex (PFC), which is heavily involved in processes like learning and memory, attention, and inhibition of improper responses (Koechlin et al., 2003). Yet the PFC, with its sophisticated circuitry that enables these roles, is one of the last brain regions to develop (Hill et al., 2010; Kolk and Rakic, 2022). Indeed, the maturation of the PFC is an important component of adolescent development (Larsen and Luna, 2018), as is the development of cognitive control. This delayed development of the PFC may lead to undesired outcomes, such as risky behaviors in teenagers (Larsen and Luna, 2018), which may be linked specifically to deficits in response inhibition or the suppression of competing responses during a given task or everyday situation (Luna and Sweeney, 2004). Therefore, understanding the developmental changes in the circuitry that underlies cognitive control, particularly response inhibition, is an important area of investigation. A major component of PFC maturation is the removal of excess connections via synaptic pruning (Kolk and Rakic, 2022). During early childhood, the PFC undergoes rapid expansion, with dorsolateral and medial PFC expanding to nearly double the surface area of other cortical regions such as occipital and insular cortices (Hill et al., 2010). Underlying this rapid expansion is a period of extensive synaptic growth (synaptogenesis), which begins prenatally but peaks postnatally (Teffer and Semendeferi, 2012), at ∼3.5 years of age in humans (Kolk and Rakic, 2022). PFC cortical thickness, which can serve as a measure of synaptic density, also shows rapid growth early in … Correspondence should be addressed to Leland L. Fleming at llfleming{at}mclean.harvard.edu.",No methods found.
2024,https://openalex.org/W4401634453,Psychology,"“I Thought It Was Better to Be Safe Than Sorry”: Factors Influencing Parental Decisions on HPV and Other Adolescent Vaccinations for Students with Intellectual Disability and/or Autism in New South Wales, Australia","The uptake of human papilloma virus (HPV) and other adolescent vaccinations in special schools for young people with disability is significantly lower than in mainstream settings. This study explored the factors believed to influence parental decision making regarding vaccine uptake for students with intellectual disability and/or on the autism spectrum attending special schools in New South Wales, Australia, from the perspective of all stakeholders involved in the program. Focus groups and interviews were conducted with 40 participants, including parents, school staff, and immunisation providers. The thematic analysis identified two themes: (1) appreciating diverse parental attitudes towards vaccination and (2) educating parents and managing vaccination questions and concerns. While most parents were described as pro-vaccination, others were anti-vaccination or vaccination-hesitant, articulating a marked protectiveness regarding their child’s health. Reasons for vaccine hesitancy included beliefs that vaccines cause autism, concerns that the vaccination may be traumatic for the child, vaccination fatigue following COVID-19, and assumptions that children with disability will not be sexually active. Special school staff regarded the vaccination information pack as inadequate for families, and nurses described limited educational impact resulting from minimal direct communication with parents. More effective communication strategies are needed to address vaccine hesitancy among parents with children with disability.",No methods found.
2024,https://openalex.org/W4391613813,Psychology,Combined Effect of Maternal Separation and Early-Life Immune Activation on Brain and Behaviour of Rat Offspring,"Adversity during early life, a critical period for brain development, increases vulnerability and can have a lasting impact on the brain and behaviour of a child. However, the long-term effects of cumulative early-life stressors on brain and behaviour are not well known. We studied a 2-hit rat model of early-life adversity using maternal separation (MS) and immune activation (lipopolysaccharide (LPS)). Rat pups underwent MS for 15 (control) or 180 (MS) minutes per day from postnatal day (P)2-14 and were administered saline or LPS (intraperitoneal) on P3. Open-field (OFT) and object-place recognition tests were performed on rat offspring at P33-35 and P42-50, respectively. The pre-frontal cortex (PFC) and hippocampus were removed at the experimental endpoint (P52-55) for mRNA expression. MS induced anxiety-like behaviour in OFT in male and reduced locomotor activity in both male and female offspring. LPS induced a subtle decline in memory in the object-place recognition test in male offspring. MS increased glial fibrillary acidic protein (GFAP) and brain-derived neurotrophic factor expression in PFC and ionised calcium-binding adapter molecule-1 expression in male hippocampus. MS and LPS resulted in distinct behavioural phenotypes in a sex-specific manner. The combination of MS and LPS had a synergistic effect on the anxiety-like behaviour, locomotor activity, and GFAP mRNA expression outcomes.",No methods found.
2024,https://openalex.org/W4391881153,Psychology,Determinants of HPV vaccine uptake intentions in Chinese clinical interns: an extended theory of planned behavior approach,"This study aims to utilize the extended Theory of Planned Behavior (TPB) model to examine the intentions of clinical interns in China towards Human papillomaviruses (HPV) vaccination. It also fills a significant gap in the literature concerning vaccine acceptance in this specific population. This cross-sectional study was carried out with clinical interns in Shandong Province, China, with a total of 1,619 participants. Data were collected through self-reported questionnaires, including demographic characteristics, TPB variables, and HPV-related health knowledge. Hierarchical regression analysis was employed to identify key factors influencing vaccination intentions, and Structural Equation Modeling (SEM) was used to analyze the interrelationships between these factors. This study initially identified key predictors affecting clinical interns' intentions to receive the HPV vaccine through hierarchical regression analysis. The preliminary model, which accounted for demographic factors, revealed foundational impacts of household income and HPV-related clinical experience on intentions. After integrating TPB variables-attitude, subjective norm, perceived behavioral control, and HPV-related health knowledge-the model's explanatory power was enhanced to 37.30%. SEM analysis focused on the interplay among TPB constructs and extended variables, confirming their significance in forming vaccination intentions, with subjective norm having the most substantial impact (β = 0.375, p < 0.001). The extended TPB model explained over half of the variance in vaccination intentions, substantiating the hypotheses and revealing the psychological determinants behind clinical interns' decision-making for HPV vaccination. The extended TPB model from this study effectively explains the vaccination intentions among clinical interns for HPV, offering theoretical support for public health strategies and educational interventions targeting this group. These findings are of significant importance for public health practice and future health promotion strategies.",No methods found.
2024,https://openalex.org/W4390506881,Medicine,Discovering biomarkers associated and predicting cardiovascular disease with high accuracy using a novel nexus of machine learning techniques for precision medicine,"Abstract Personalized interventions are deemed vital given the intricate characteristics, advancement, inherent genetic composition, and diversity of cardiovascular diseases (CVDs). The appropriate utilization of artificial intelligence (AI) and machine learning (ML) methodologies can yield novel understandings of CVDs, enabling improved personalized treatments through predictive analysis and deep phenotyping. In this study, we proposed and employed a novel approach combining traditional statistics and a nexus of cutting-edge AI/ML techniques to identify significant biomarkers for our predictive engine by analyzing the complete transcriptome of CVD patients. After robust gene expression data pre-processing, we utilized three statistical tests (Pearson correlation, Chi-square test, and ANOVA) to assess the differences in transcriptomic expression and clinical characteristics between healthy individuals and CVD patients. Next, the recursive feature elimination classifier assigned rankings to transcriptomic features based on their relation to the case–control variable. The top ten percent of commonly observed significant biomarkers were evaluated using four unique ML classifiers (Random Forest, Support Vector Machine, Xtreme Gradient Boosting Decision Trees, and k-Nearest Neighbors). After optimizing hyperparameters, the ensembled models, which were implemented using a soft voting classifier, accurately differentiated between patients and healthy individuals. We have uncovered 18 transcriptomic biomarkers that are highly significant in the CVD population that were used to predict disease with up to 96% accuracy. Additionally, we cross-validated our results with clinical records collected from patients in our cohort. The identified biomarkers served as potential indicators for early detection of CVDs. With its successful implementation, our newly developed predictive engine provides a valuable framework for identifying patients with CVDs based on their biomarker profiles.","<method>recursive feature elimination classifier</method>, <method>Random Forest</method>, <method>Support Vector Machine</method>, <method>Xtreme Gradient Boosting Decision Trees</method>, <method>k-Nearest Neighbors</method>, <method>soft voting classifier</method>"
2024,https://openalex.org/W4391292768,Medicine,Improving large language models for clinical named entity recognition via prompt engineering,"Abstract Importance The study highlights the potential of large language models, specifically GPT-3.5 and GPT-4, in processing complex clinical data and extracting meaningful information with minimal training data. By developing and refining prompt-based strategies, we can significantly enhance the models’ performance, making them viable tools for clinical NER tasks and possibly reducing the reliance on extensive annotated datasets. Objectives This study quantifies the capabilities of GPT-3.5 and GPT-4 for clinical named entity recognition (NER) tasks and proposes task-specific prompts to improve their performance. Materials and Methods We evaluated these models on 2 clinical NER tasks: (1) to extract medical problems, treatments, and tests from clinical notes in the MTSamples corpus, following the 2010 i2b2 concept extraction shared task, and (2) to identify nervous system disorder-related adverse events from safety reports in the vaccine adverse event reporting system (VAERS). To improve the GPT models' performance, we developed a clinical task-specific prompt framework that includes (1) baseline prompts with task description and format specification, (2) annotation guideline-based prompts, (3) error analysis-based instructions, and (4) annotated samples for few-shot learning. We assessed each prompt's effectiveness and compared the models to BioClinicalBERT. Results Using baseline prompts, GPT-3.5 and GPT-4 achieved relaxed F1 scores of 0.634, 0.804 for MTSamples and 0.301, 0.593 for VAERS. Additional prompt components consistently improved model performance. When all 4 components were used, GPT-3.5 and GPT-4 achieved relaxed F1 socres of 0.794, 0.861 for MTSamples and 0.676, 0.736 for VAERS, demonstrating the effectiveness of our prompt framework. Although these results trail BioClinicalBERT (F1 of 0.901 for the MTSamples dataset and 0.802 for the VAERS), it is very promising considering few training samples are needed. Discussion The study’s findings suggest a promising direction in leveraging LLMs for clinical NER tasks. However, while the performance of GPT models improved with task-specific prompts, there's a need for further development and refinement. LLMs like GPT-4 show potential in achieving close performance to state-of-the-art models like BioClinicalBERT, but they still require careful prompt engineering and understanding of task-specific knowledge. The study also underscores the importance of evaluation schemas that accurately reflect the capabilities and performance of LLMs in clinical settings. Conclusion While direct application of GPT models to clinical NER tasks falls short of optimal performance, our task-specific prompt framework, incorporating medical knowledge and training samples, significantly enhances GPT models' feasibility for potential clinical applications.","<method>large language models (GPT-3.5)</method>, <method>large language models (GPT-4)</method>, <method>prompt-based strategies</method>, <method>task-specific prompt framework</method>, <method>few-shot learning</method>, <method>BioClinicalBERT</method>"
2024,https://openalex.org/W4391221083,Medicine,FDA-Approved Artificial Intelligence and Machine Learning (AI/ML)-Enabled Medical Devices: An Updated Landscape,"As artificial intelligence (AI) has been highly advancing in the last decade, machine learning (ML)-enabled medical devices are increasingly used in healthcare. In this study, we collected publicly available information on AI/ML-enabled medical devices approved by the FDA in the United States, as of the latest update on 19 October 2023. We performed comprehensive analysis of a total of 691 FDA-approved artificial intelligence and machine learning (AI/ML)-enabled medical devices and offer an in-depth analysis of clearance pathways, approval timeline, regulation type, medical specialty, decision type, recall history, etc. We found a significant surge in approvals since 2018, with clear dominance of the radiology specialty in the application of machine learning tools, attributed to the abundant data from routine clinical data. The study also reveals a reliance on the 510(k)-clearance pathway, emphasizing its basis on substantial equivalence and often bypassing the need for new clinical trials. Also, it notes an underrepresentation of pediatric-focused devices and trials, suggesting an opportunity for expansion in this demographic. Moreover, the geographical limitation of clinical trials, primarily within the United States, points to a need for more globally inclusive trials to encompass diverse patient demographics. This analysis not only maps the current landscape of AI/ML-enabled medical devices but also pinpoints trends, potential gaps, and areas for future exploration, clinical trial practices, and regulatory approaches. In conclusion, our analysis sheds light on the current state of FDA-approved AI/ML-enabled medical devices and prevailing trends, contributing to a wider comprehension.",No methods found.
2024,https://openalex.org/W4391137578,Medicine,A multinational study on the factors influencing university students’ attitudes and usage of ChatGPT,"Abstract Artificial intelligence models, like ChatGPT, have the potential to revolutionize higher education when implemented properly. This study aimed to investigate the factors influencing university students’ attitudes and usage of ChatGPT in Arab countries. The survey instrument “TAME-ChatGPT” was administered to 2240 participants from Iraq, Kuwait, Egypt, Lebanon, and Jordan. Of those, 46.8% heard of ChatGPT, and 52.6% used it before the study. The results indicated that a positive attitude and usage of ChatGPT were determined by factors like ease of use, positive attitude towards technology, social influence, perceived usefulness, behavioral/cognitive influences, low perceived risks, and low anxiety. Confirmatory factor analysis indicated the adequacy of the “TAME-ChatGPT” constructs. Multivariate analysis demonstrated that the attitude towards ChatGPT usage was significantly influenced by country of residence, age, university type, and recent academic performance. This study validated “TAME-ChatGPT” as a useful tool for assessing ChatGPT adoption among university students. The successful integration of ChatGPT in higher education relies on the perceived ease of use, perceived usefulness, positive attitude towards technology, social influence, behavioral/cognitive elements, low anxiety, and minimal perceived risks. Policies for ChatGPT adoption in higher education should be tailored to individual contexts, considering the variations in student attitudes observed in this study.",No methods found.
2024,https://openalex.org/W4391925496,Medicine,"Selection, optimization and validation of ten chronic disease polygenic risk scores for clinical implementation in diverse US populations","Polygenic risk scores (PRSs) have improved in predictive performance, but several challenges remain to be addressed before PRSs can be implemented in the clinic, including reduced predictive performance of PRSs in diverse populations, and the interpretation and communication of genetic results to both providers and patients. To address these challenges, the National Human Genome Research Institute-funded Electronic Medical Records and Genomics (eMERGE) Network has developed a framework and pipeline for return of a PRS-based genome-informed risk assessment to 25,000 diverse adults and children as part of a clinical study. From an initial list of 23 conditions, ten were selected for implementation based on PRS performance, medical actionability and potential clinical utility, including cardiometabolic diseases and cancer. Standardized metrics were considered in the selection process, with additional consideration given to strength of evidence in African and Hispanic populations. We then developed a pipeline for clinical PRS implementation (score transfer to a clinical laboratory, validation and verification of score performance), and used genetic ancestry to calibrate PRS mean and variance, utilizing genetically diverse data from 13,475 participants of the All of Us Research Program cohort to train and test model parameters. Finally, we created a framework for regulatory compliance and developed a PRS clinical report for return to providers and for inclusion in an additional genome-informed risk assessment. The initial experience from eMERGE can inform the approach needed to implement PRS-based testing in diverse clinical settings.",No methods found.
2024,https://openalex.org/W4392754729,Medicine,"Revolutionizing agriculture with artificial intelligence: plant disease detection methods, applications, and their limitations","Accurate and rapid plant disease detection is critical for enhancing long-term agricultural yield. Disease infection poses the most significant challenge in crop production, potentially leading to economic losses. Viruses, fungi, bacteria, and other infectious organisms can affect numerous plant parts, including roots, stems, and leaves. Traditional techniques for plant disease detection are time-consuming, require expertise, and are resource-intensive. Therefore, automated leaf disease diagnosis using artificial intelligence (AI) with Internet of Things (IoT) sensors methodologies are considered for the analysis and detection. This research examines four crop diseases: tomato, chilli, potato, and cucumber. It also highlights the most prevalent diseases and infections in these four types of vegetables, along with their symptoms. This review provides detailed predetermined steps to predict plant diseases using AI. Predetermined steps include image acquisition, preprocessing, segmentation, feature selection, and classification. Machine learning (ML) and deep understanding (DL) detection models are discussed. A comprehensive examination of various existing ML and DL-based studies to detect the disease of the following four crops is discussed, including the datasets used to evaluate these studies. We also provided the list of plant disease detection datasets. Finally, different ML and DL application problems are identified and discussed, along with future research prospects, by combining AI with IoT platforms like smart drones for field-based disease detection and monitoring. This work will help other practitioners in surveying different plant disease detection strategies and the limits of present systems.","<method>artificial intelligence (AI)</method>, <method>machine learning (ML)</method>, <method>deep learning (DL)</method>"
2024,https://openalex.org/W4392791588,Medicine,Can large language models replace humans in systematic reviews? Evaluating <scp>GPT</scp>‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages,"Systematic reviews are vital for guiding practice, research and policy, although they are often slow and labour-intensive. Large language models (LLMs) could speed up and automate systematic reviews, but their performance in such tasks has yet to be comprehensively evaluated against humans, and no study has tested Generative Pre-Trained Transformer (GPT)-4, the biggest LLM so far. This pre-registered study uses a ""human-out-of-the-loop"" approach to evaluate GPT-4's capability in title/abstract screening, full-text review and data extraction across various literature types and languages. Although GPT-4 had accuracy on par with human performance in some tasks, results were skewed by chance agreement and dataset imbalance. Adjusting for these caused performance scores to drop across all stages: for data extraction, performance was moderate, and for screening, it ranged from none in highly balanced literature datasets (~1:1) to moderate in those datasets where the ratio of inclusion to exclusion in studies was imbalanced (~1:3). When screening full-text literature using highly reliable prompts, GPT-4's performance was more robust, reaching ""human-like"" levels. Although our findings indicate that, currently, substantial caution should be exercised if LLMs are being used to conduct systematic reviews, they also offer preliminary evidence that, for certain review tasks delivered under specific conditions, LLMs can rival human performance.","<method>Large language models (LLMs)</method>, <method>Generative Pre-Trained Transformer (GPT)-4</method>"
2024,https://openalex.org/W4395050972,Medicine,Optimization of hepatological clinical guidelines interpretation by large language models: a retrieval augmented generation-based framework,"Abstract Large language models (LLMs) can potentially transform healthcare, particularly in providing the right information to the right provider at the right time in the hospital workflow. This study investigates the integration of LLMs into healthcare, specifically focusing on improving clinical decision support systems (CDSSs) through accurate interpretation of medical guidelines for chronic Hepatitis C Virus infection management. Utilizing OpenAI’s GPT-4 Turbo model, we developed a customized LLM framework that incorporates retrieval augmented generation (RAG) and prompt engineering. Our framework involved guideline conversion into the best-structured format that can be efficiently processed by LLMs to provide the most accurate output. An ablation study was conducted to evaluate the impact of different formatting and learning strategies on the LLM’s answer generation accuracy. The baseline GPT-4 Turbo model’s performance was compared against five experimental setups with increasing levels of complexity: inclusion of in-context guidelines, guideline reformatting, and implementation of few-shot learning. Our primary outcome was the qualitative assessment of accuracy based on expert review, while secondary outcomes included the quantitative measurement of similarity of LLM-generated responses to expert-provided answers using text-similarity scores. The results showed a significant improvement in accuracy from 43 to 99% ( p &lt; 0.001), when guidelines were provided as context in a coherent corpus of text and non-text sources were converted into text. In addition, few-shot learning did not seem to improve overall accuracy. The study highlights that structured guideline reformatting and advanced prompt engineering (data quality vs. data quantity) can enhance the efficacy of LLM integrations to CDSSs for guideline delivery.","<method>Large language models (LLMs)</method>, <method>OpenAI’s GPT-4 Turbo model</method>, <method>retrieval augmented generation (RAG)</method>, <method>prompt engineering</method>, <method>few-shot learning</method>"
2024,https://openalex.org/W4393247259,Medicine,Employing deep learning and transfer learning for accurate brain tumor detection,"Abstract Artificial intelligence-powered deep learning methods are being used to diagnose brain tumors with high accuracy, owing to their ability to process large amounts of data. Magnetic resonance imaging stands as the gold standard for brain tumor diagnosis using machine vision, surpassing computed tomography, ultrasound, and X-ray imaging in its effectiveness. Despite this, brain tumor diagnosis remains a challenging endeavour due to the intricate structure of the brain. This study delves into the potential of deep transfer learning architectures to elevate the accuracy of brain tumor diagnosis. Transfer learning is a machine learning technique that allows us to repurpose pre-trained models on new tasks. This can be particularly useful for medical imaging tasks, where labelled data is often scarce. Four distinct transfer learning architectures were assessed in this study: ResNet152, VGG19, DenseNet169, and MobileNetv3. The models were trained and validated on a dataset from benchmark database: Kaggle. Five-fold cross validation was adopted for training and testing. To enhance the balance of the dataset and improve the performance of the models, image enhancement techniques were applied to the data for the four categories: pituitary, normal, meningioma, and glioma. MobileNetv3 achieved the highest accuracy of 99.75%, significantly outperforming other existing methods. This demonstrates the potential of deep transfer learning architectures to revolutionize the field of brain tumor diagnosis.","<method>deep learning</method>, <method>machine vision</method>, <method>deep transfer learning</method>, <method>transfer learning</method>, <method>ResNet152</method>, <method>VGG19</method>, <method>DenseNet169</method>, <method>MobileNetv3</method>, <method>five-fold cross validation</method>"
2024,https://openalex.org/W4391103530,Medicine,Transformative Breast Cancer Diagnosis using CNNs with Optimized ReduceLROnPlateau and Early Stopping Enhancements,"Abstract Breast cancer stands as a paramount public health concern worldwide, underscoring an imperative necessity within the research sphere for precision-driven and efficacious methodologies facilitating accurate detection. The existing diagnostic approaches in breast cancer often suffer from limitations in accuracy and efficiency, leading to delayed detection and subsequent challenges in personalized treatment planning. The primary focus of this research is to overcome these shortcomings by harnessing the power of advanced deep learning techniques, thereby revolutionizing the precision and reliability of breast cancer classification. This research addresses the critical need for improved breast cancer diagnostics by introducing a novel Convolutional Neural Network (CNN) model integrated with an Early Stopping callback and ReduceLROnPlateau callback. By enhancing the precision and reliability of breast cancer classification, the study aims to overcome the limitations of existing diagnostic methods, ultimately leading to better patient outcomes and reduced mortality rates. The comprehensive methodology includes diverse datasets, meticulous image preprocessing, robust model training, and validation strategies, emphasizing the model's adaptability and reliability in varied clinical contexts. The findings showcase the CNN model's exceptional performance, achieving a 95.2% accuracy rate in distinguishing cancerous and non-cancerous breast tissue in the integrated dataset, thereby demonstrating its potential for enhancing clinical decision-making and fostering the development of AI-driven diagnostic solutions.","<method>Convolutional Neural Network (CNN)</method>, <method>Early Stopping callback</method>, <method>ReduceLROnPlateau callback</method>"
2024,https://openalex.org/W4391528827,Medicine,Deep learning-aided decision support for diagnosis of skin disease across skin tones,"Abstract Although advances in deep learning systems for image-based medical diagnosis demonstrate their potential to augment clinical decision-making, the effectiveness of physician–machine partnerships remains an open question, in part because physicians and algorithms are both susceptible to systematic errors, especially for diagnosis of underrepresented populations. Here we present results from a large-scale digital experiment involving board-certified dermatologists ( n = 389) and primary-care physicians ( n = 459) from 39 countries to evaluate the accuracy of diagnoses submitted by physicians in a store-and-forward teledermatology simulation. In this experiment, physicians were presented with 364 images spanning 46 skin diseases and asked to submit up to four differential diagnoses. Specialists and generalists achieved diagnostic accuracies of 38% and 19%, respectively, but both specialists and generalists were four percentage points less accurate for the diagnosis of images of dark skin as compared to light skin. Fair deep learning system decision support improved the diagnostic accuracy of both specialists and generalists by more than 33%, but exacerbated the gap in the diagnostic accuracy of generalists across skin tones. These results demonstrate that well-designed physician–machine partnerships can enhance the diagnostic accuracy of physicians, illustrating that success in improving overall diagnostic accuracy does not necessarily address bias.",<method>deep learning system decision support</method>
2024,https://openalex.org/W4392816775,Medicine,Virtual reality and augmented reality in medical education: an umbrella review,"Objective This umbrella review aims to ascertain the extent to which immersive Virtual Reality (VR) and Augmented Reality (AR) technologies improve specific competencies in healthcare professionals within medical education and training, in contrast to traditional educational methods or no intervention. Methods Adhering to PRISMA guidelines and the PICOS approach, a systematic literature search was conducted across major databases to identify studies examining the use of VR and AR in medical education. Eligible studies were screened and categorized based on the PICOS criteria. Descriptive statistics and chi-square tests were employed to analyze the data, supplemented by the Fisher test for small sample sizes or specific conditions. Analysis The analysis involved cross-tabulating the stages of work (Development and Testing, Results, Evaluated) and variables of interest (Performance, Engagement, Performance and Engagement, Effectiveness, no evaluated) against the types of technologies used. Chi-square tests assessed the associations between these categorical variables. Results A total of 28 studies were included, with the majority reporting increased or positive effects from the use of immersive technologies. VR was the most frequently studied technology, particularly in the “Performance” and “Results” stages. The chi-square analysis, with a Pearson value close to significance ( p = 0.052), suggested a non-significant trend toward the association of VR with improved outcomes. Conclusions The results indicate that VR is a prevalent tool in the research landscape of medical education technologies, with a positive trend toward enhancing educational outcomes. However, the statistical analysis did not reveal a significant association, suggesting the need for further research with larger sample sizes. This review underscores the potential of immersive technologies to enhance medical training yet calls for more rigorous studies to establish definitive evidence of their efficacy.",No methods found.
2024,https://openalex.org/W4402780379,Medicine,Investigating Spatial Effects through Machine Learning and Leveraging Explainable AI for Child Malnutrition in Pakistan,"While socioeconomic gradients in regional health inequalities are firmly established, the synergistic interactions between socioeconomic deprivation and climate vulnerability within convenient proximity and neighbourhood locations with health disparities remain poorly explored and thus require deep understanding within a regional context. Furthermore, disregarding the importance of spatial spillover effects and nonlinear effects of covariates on childhood stunting are inevitable in dealing with an enduring issue of regional health inequalities. The present study aims to investigate the spatial inequalities in childhood stunting at the district level in Pakistan and validate the importance of spatial lag in predicting childhood stunting. Furthermore, it examines the presence of any nonlinear relationships among the selected independent features with childhood stunting. The study utilized data related to socioeconomic features from MICS 2017–2018 and climatic data from Integrated Contextual Analysis. A multi-model approach was employed to address the research questions, which included Ordinary Least Squares Regression (OLS), various Spatial Models, Machine Learning Algorithms and Explainable Artificial Intelligence methods. Firstly, OLS was used to analyse and test the linear relationships among selected variables. Secondly, Spatial Durbin Error Model (SDEM) was used to detect and capture the impact of spatial spillover on childhood stunting. Third, XGBoost and Random Forest machine learning algorithms were employed to examine and validate the importance of the spatial lag component. Finally, EXAI methods such as SHapley were utilized to identify potential nonlinear relationships. The study found a clear pattern of spatial clustering and geographical disparities in childhood stunting, with multidimensional poverty, high climate vulnerability and early marriage worsening childhood stunting. In contrast, low climate vulnerability, high exposure to mass media and high women’s literacy were found to reduce childhood stunting. The use of machine learning algorithms, specifically XGBoost and Random Forest, highlighted the significant role played by the average value in the neighbourhood in predicting childhood stunting in nearby districts, confirming that the spatial spillover effect is not bounded by geographical boundaries. Furthermore, EXAI methods such as partial dependency plot reveal the existence of a nonlinear relationship between multidimensional poverty and childhood stunting. The study’s findings provide valuable insights into the spatial distribution of childhood stunting in Pakistan, emphasizing the importance of considering spatial effects in predicting childhood stunting. Individual and household-level factors such as exposure to mass media and women’s literacy have shown positive implications for childhood stunting. It further provides a justification for the usage of EXAI methods to draw better insights and propose customised intervention policies accordingly.","<method>Ordinary Least Squares Regression (OLS)</method>, <method>Spatial Durbin Error Model (SDEM)</method>, <method>XGBoost</method>, <method>Random Forest</method>, <method>Explainable Artificial Intelligence (EXAI) methods</method>, <method>SHapley</method>, <method>partial dependency plot</method>"
2024,https://openalex.org/W4393119757,Medicine,Unmasking bias in artificial intelligence: a systematic review of bias detection and mitigation strategies in electronic health record-based models,"Abstract Objectives Leveraging artificial intelligence (AI) in conjunction with electronic health records (EHRs) holds transformative potential to improve healthcare. However, addressing bias in AI, which risks worsening healthcare disparities, cannot be overlooked. This study reviews methods to handle various biases in AI models developed using EHR data. Materials and Methods We conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and Meta-analyses guidelines, analyzing articles from PubMed, Web of Science, and IEEE published between January 01, 2010 and December 17, 2023. The review identified key biases, outlined strategies for detecting and mitigating bias throughout the AI model development, and analyzed metrics for bias assessment. Results Of the 450 articles retrieved, 20 met our criteria, revealing 6 major bias types: algorithmic, confounding, implicit, measurement, selection, and temporal. The AI models were primarily developed for predictive tasks, yet none have been deployed in real-world healthcare settings. Five studies concentrated on the detection of implicit and algorithmic biases employing fairness metrics like statistical parity, equal opportunity, and predictive equity. Fifteen studies proposed strategies for mitigating biases, especially targeting implicit and selection biases. These strategies, evaluated through both performance and fairness metrics, predominantly involved data collection and preprocessing techniques like resampling and reweighting. Discussion This review highlights evolving strategies to mitigate bias in EHR-based AI models, emphasizing the urgent need for both standardized and detailed reporting of the methodologies and systematic real-world testing and evaluation. Such measures are essential for gauging models’ practical impact and fostering ethical AI that ensures fairness and equity in healthcare.","<method>resampling</method>, <method>reweighting</method>"
2024,https://openalex.org/W4391235397,Medicine,Real-Time Plant Disease Dataset Development and Detection of Plant Disease Using Deep Learning,"Agriculture plays a significant role in meeting food needs and providing food security for the increasingly growing global population, which has increased by 0.88% since 2022. Plant diseases can reduce food production and affect food security. Worldwide crop loss due to plant disease is estimated to be around 14.1%. The lack of proper identification of plant disease at the early stages of infection can result in inappropriate disease control measures. Therefore, the automatic identification and diagnosis of plant diseases are highly recommended. Lack of availability of large amounts of data that are not processed to a large extent is one of the main challenges in plant disease diagnosis. In the current manuscript, we developed datasets for food grains specifically for rice, wheat, and maize to address the identified challenges. The developed datasets consider the common diseases (two bacterial diseases and two fungal diseases of rice, four fungal diseases of maize, and four fungal diseases of wheat) that affect crop yields and cause damage to the whole plant. The datasets developed were applied to eight fine-tuned deep learning models with the same training hyperparameters. The experimental results based on eight fine-tuned deep learning models show that, while recognizing maize leaf diseases, the models Xception and MobileNet performed best with a testing accuracy of 0.9580 and 0.9464 respectively. Similarly, while recognizing the wheat leaf diseases, the models MobileNetV2 and MobileNet performed best with a testing accuracy of 0.9632 and 0.9628 respectively. The Xception and Inception V3 models performed best, with a testing accuracy of 0.9728 and 0.9620, respectively, for recognizing rice leaf diseases. The research also proposes a new convolutional neural network (CNN) model trained from scratch on all three food grain datasets developed. The proposed model performs well and shows a testing accuracy of 0.9704, 0.9706, and 0.9808 respectively on the maize, rice, and wheat datasets.","<method>fine-tuned deep learning models</method>, <method>Xception</method>, <method>MobileNet</method>, <method>MobileNetV2</method>, <method>Inception V3</method>, <method>convolutional neural network (CNN) model trained from scratch</method>"
2024,https://openalex.org/W4396494945,Medicine,Vision–language foundation model for echocardiogram interpretation,"Abstract The development of robust artificial intelligence models for echocardiography has been limited by the availability of annotated clinical data. Here, to address this challenge and improve the performance of cardiac imaging models, we developed EchoCLIP, a vision–language foundation model for echocardiography, that learns the relationship between cardiac ultrasound images and the interpretations of expert cardiologists across a wide range of patients and indications for imaging. After training on 1,032,975 cardiac ultrasound videos and corresponding expert text, EchoCLIP performs well on a diverse range of benchmarks for cardiac image interpretation, despite not having been explicitly trained for individual interpretation tasks. EchoCLIP can assess cardiac function (mean absolute error of 7.1% when predicting left ventricular ejection fraction in an external validation dataset) and identify implanted intracardiac devices (area under the curve (AUC) of 0.84, 0.92 and 0.97 for pacemakers, percutaneous mitral valve repair and artificial aortic valves, respectively). We also developed a long-context variant (EchoCLIP-R) using a custom tokenizer based on common echocardiography concepts. EchoCLIP-R accurately identified unique patients across multiple videos (AUC of 0.86), identified clinical transitions such as heart transplants (AUC of 0.79) and cardiac surgery (AUC 0.77) and enabled robust image-to-text search (mean cross-modal retrieval rank in the top 1% of candidate text reports). These capabilities represent a substantial step toward understanding and applying foundation models in cardiovascular imaging for preliminary interpretation of echocardiographic findings.","<method>vision–language foundation model</method>, <method>EchoCLIP</method>, <method>long-context variant (EchoCLIP-R)</method>, <method>custom tokenizer</method>"
2024,https://openalex.org/W4391320803,Medicine,Oral squamous cell carcinoma detection using EfficientNet on histopathological images,"Introduction Oral Squamous Cell Carcinoma (OSCC) poses a significant challenge in oncology due to the absence of precise diagnostic tools, leading to delays in identifying the condition. Current diagnostic methods for OSCC have limitations in accuracy and efficiency, highlighting the need for more reliable approaches. This study aims to explore the discriminative potential of histopathological images of oral epithelium and OSCC. By utilizing a database containing 1224 images from 230 patients, captured at varying magnifications and publicly available, a customized deep learning model based on EfficientNetB3 was developed. The model’s objective was to differentiate between normal epithelium and OSCC tissues by employing advanced techniques such as data augmentation, regularization, and optimization. Methods The research utilized a histopathological imaging database for Oral Cancer analysis, incorporating 1224 images from 230 patients. These images, taken at various magnifications, formed the basis for training a specialized deep learning model built upon the EfficientNetB3 architecture. The model underwent training to distinguish between normal epithelium and OSCC tissues, employing sophisticated methodologies including data augmentation, regularization techniques, and optimization strategies. Results The customized deep learning model achieved significant success, showcasing a remarkable 99% accuracy when tested on the dataset. This high accuracy underscores the model’s efficacy in effectively discerning between normal epithelium and OSCC tissues. Furthermore, the model exhibited impressive precision, recall, and F1-score metrics, reinforcing its potential as a robust diagnostic tool for OSCC. Discussion This research demonstrates the promising potential of employing deep learning models to address the diagnostic challenges associated with OSCC. The model’s ability to achieve a 99% accuracy rate on the test dataset signifies a considerable leap forward in earlier and more accurate detection of OSCC. Leveraging advanced techniques in machine learning, such as data augmentation and optimization, has shown promising results in improving patient outcomes through timely and precise identification of OSCC.","<method>deep learning model based on EfficientNetB3</method>, <method>data augmentation</method>, <method>regularization</method>, <method>optimization</method>"
2024,https://openalex.org/W4396636942,Medicine,Artificial intelligence in digital pathology: a systematic review and meta-analysis of diagnostic test accuracy,"Abstract Ensuring diagnostic performance of artificial intelligence (AI) before introduction into clinical practice is essential. Growing numbers of studies using AI for digital pathology have been reported over recent years. The aim of this work is to examine the diagnostic accuracy of AI in digital pathology images for any disease. This systematic review and meta-analysis included diagnostic accuracy studies using any type of AI applied to whole slide images (WSIs) for any disease. The reference standard was diagnosis by histopathological assessment and/or immunohistochemistry. Searches were conducted in PubMed, EMBASE and CENTRAL in June 2022. Risk of bias and concerns of applicability were assessed using the QUADAS-2 tool. Data extraction was conducted by two investigators and meta-analysis was performed using a bivariate random effects model, with additional subgroup analyses also performed. Of 2976 identified studies, 100 were included in the review and 48 in the meta-analysis. Studies were from a range of countries, including over 152,000 whole slide images (WSIs), representing many diseases. These studies reported a mean sensitivity of 96.3% (CI 94.1–97.7) and mean specificity of 93.3% (CI 90.5–95.4). There was heterogeneity in study design and 99% of studies identified for inclusion had at least one area at high or unclear risk of bias or applicability concerns. Details on selection of cases, division of model development and validation data and raw performance data were frequently ambiguous or missing. AI is reported as having high diagnostic accuracy in the reported areas but requires more rigorous evaluation of its performance.",No methods found.
2024,https://openalex.org/W4400993192,Medicine,Damage identification of steel bridge based on data augmentation and adaptive optimization neural network,"With the advancement of deep learning, data-driven structural damage identification (SDI) has shown considerable development. However, collecting vibration signals related to structural damage poses certain challenges, which can undermine the accuracy of the identification results produced by data-driven SDI methods in scenarios where data is scarce. This paper introduces an innovative approach to bridge SDI in a few-shot context by integrating an adaptive simulated annealing particle swarm optimization-convolutional neural network (ASAPSO-CNN) as the foundational framework, augmented by data enhancement techniques. Firstly, three specific types of noise are introduced to augment the source signals used for training. Subsequently, the source signals and augmented signals are recombined to construct a four-dimensional matrix as the input to the CNN, while defining the damage feature vector as the output. Secondly, a CNN is constructed to establish the mapping relationship between the input and output. Then, an adaptive fitness function is proposed that simultaneously considers the accuracy of SDI, model complexity, and training efficiency. The ASAPSO is employed to adaptively optimize the hyperparameters of the CNN. The proposed method is validated on an experimental model of a three-span continuous beam. It is compared with four other data-driven methods, demonstrating good effectiveness and robustness of SDI under cases of scarce data. Finally, the effectiveness of this SDI method is validated in a real-world case of a steel truss bridge.","<method>adaptive simulated annealing particle swarm optimization-convolutional neural network (ASAPSO-CNN)</method>, <method>data enhancement techniques</method>, <method>convolutional neural network (CNN)</method>, <method>adaptive simulated annealing particle swarm optimization (ASAPSO)</method>"
2024,https://openalex.org/W4390588437,Medicine,Enhancing heart disease prediction using a self-attention-based transformer model,"Abstract Cardiovascular diseases (CVDs) continue to be the leading cause of more than 17 million mortalities worldwide. The early detection of heart failure with high accuracy is crucial for clinical trials and therapy. Patients will be categorized into various types of heart disease based on characteristics like blood pressure, cholesterol levels, heart rate, and other characteristics. With the use of an automatic system, we can provide early diagnoses for those who are prone to heart failure by analyzing their characteristics. In this work, we deploy a novel self-attention-based transformer model, that combines self-attention mechanisms and transformer networks to predict CVD risk. The self-attention layers capture contextual information and generate representations that effectively model complex patterns in the data. Self-attention mechanisms provide interpretability by giving each component of the input sequence a certain amount of attention weight. This includes adjusting the input and output layers, incorporating more layers, and modifying the attention processes to collect relevant information. This also makes it possible for physicians to comprehend which features of the data contributed to the model's predictions. The proposed model is tested on the Cleveland dataset, a benchmark dataset of the University of California Irvine (UCI) machine learning (ML) repository. Comparing the proposed model to several baseline approaches, we achieved the highest accuracy of 96.51%. Furthermore, the outcomes of our experiments demonstrate that the prediction rate of our model is higher than that of other cutting-edge approaches used for heart disease prediction.","<method>self-attention-based transformer model</method>, <method>self-attention mechanisms</method>, <method>transformer networks</method>"
2024,https://openalex.org/W4391609543,Medicine,Does AI-Driven Technostress Promote or Hinder Employees’ Artificial Intelligence Adoption Intention? A Moderated Mediation Model of Affective Reactions and Technical Self-Efficacy,"Purpose: The increasing integration of Artificial Intelligence (AI) within enterprises is generates significant technostress among employees, potentially influencing their intention to adopt AI. However, existing research on the psychological effects of this phenomenon remains inconclusive. Drawing on the Affective Events Theory (AET) and the Challenge–Hindrance Stressor Framework (CHSF), the current study aims to explore the “black box” between challenge and hindrance technology stressors and employees’ intention to adopt AI, as well as the boundary conditions of this mediation relationship. Methods: The study employs a quantitative approach and utilizes three-wave data. Data were collected through the snowball sampling technique and a structured questionnaire survey. The sample comprises employees from 11 distinct organizations located in Guangdong Province, China. We received 301 valid questionnaires, representing an overall response rate of 75%. The theoretical model was tested through confirmatory factor analysis and regression analyses using Mplus and the Process macro for SPSS. Results: The results indicate that positive affect mediates the positive relationship between challenge technology stressors and AI adoption intention, whereas AI anxiety mediates the negative relationship between hindrance technology stressors and AI adoption intention. Furthermore, the results reveal that technical self-efficacy moderates the effects of challenge and hindrance technology stressors on affective reactions and the indirect effects of challenge and hindrance technology stressors on AI adoption intention through positive affect and AI anxiety, respectively. Conclusion: Overall, our study suggests that AI-driven challenge technology stressors positively impact AI adoption intention through the cultivation of positive affect, while hindrance technology stressors impede AI adoption intention by triggering AI anxiety. Additionally, technical self-efficacy emerges as a crucial moderator in shaping these relationships. This research has the potential to make a meaningful contribution to the literature on AI adoption intention, deepening our holistic understanding of the influential mechanisms involved. Furthermore, the study affirms the applicability and relevance of Affective Events Theory (AET) and the Challenge-Hindrance Stressor Framework (CHSF). In practical terms, the research provides actionable insights for organizations to effectively manage employees’ AI adoption intention. Keywords: challenge and hindrance technology stressors, AI adoption intention, positive affect, AI anxiety, technical self-efficacy",No methods found.
2024,https://openalex.org/W4400937555,Medicine,The diagnostic and triage accuracy of the GPT-3 artificial intelligence model: an observational study,"BackgroundArtificial intelligence (AI) applications in health care have been effective in many areas of medicine, but they are often trained for a single task using labelled data, making deployment and generalisability challenging. How well a general-purpose AI language model performs diagnosis and triage relative to physicians and laypeople is not well understood.MethodsWe compared the predictive accuracy of Generative Pre-trained Transformer 3 (GPT-3)'s diagnostic and triage ability for 48 validated synthetic case vignettes (<50 words; sixth-grade reading level or below) of both common (eg, viral illness) and severe (eg, heart attack) conditions to a nationally representative sample of 5000 lay people from the USA who could use the internet to find the correct options and 21 practising physicians at Harvard Medical School. There were 12 vignettes for each of four triage categories: emergent, within one day, within 1 week, and self-care. The correct diagnosis and triage category (ie, ground truth) for each vignette was determined by two general internists at Harvard Medical School. For each vignette, human respondents and GPT-3 were prompted to list diagnoses in order of likelihood, and the vignette was marked as correct if the ground-truth diagnosis was in the top three of the listed diagnoses. For triage accuracy, we examined whether the human respondents' and GPT-3's selected triage was exactly correct according to the four triage categories, or matched a dichotomised triage variable (emergent or within 1 day vs within 1 week or self-care). We estimated GPT-3's diagnostic and triage confidence on a given vignette using a modified bootstrap resampling procedure, and examined how well calibrated GPT-3's confidence was by computing calibration curves and Brier scores. We also performed subgroup analysis by case acuity, and an error analysis for triage advice to characterise how its advice might affect patients using this tool to decide if they should seek medical care immediately.FindingsAmong all cases, GPT-3 replied with the correct diagnosis in its top three for 88% (42/48, 95% CI 75–94) of cases, compared with 54% (2700/5000, 53–55) for lay individuals (p<0.0001) and 96% (637/666, 94–97) for physicians (p=0·012). GPT-3 triaged 70% correct (34/48, 57–82) versus 74% (3706/5000, 73–75; p=0.60) for lay individuals and 91% (608/666, 89–93%; p<0.0001) for physicians. As measured by the Brier score, GPT-3 confidence in its top prediction was reasonably well calibrated for diagnosis (Brier score=0·18) and triage (Brier score=0·22). We observed an inverse relationship between case acuity and GPT-3 accuracy (p<0·0001) with a fitted trend line of –8·33% decrease in accuracy for every level of increase in case acuity. For triage error analysis, GPT-3 deprioritised truly emergent cases in seven instances.InterpretationA general-purpose AI language model without any content-specific training could perform diagnosis at levels close to, but below, physicians and better than lay individuals. We found that GPT-3's performance was inferior to physicians for triage, sometimes by a large margin, and its performance was closer to that of lay individuals. Although the diagnostic performance of GPT-3 was comparable to physicians, it was significantly better than a typical person using a search engine.FundingThe National Heart, Lung, and Blood Institute.","<method>Generative Pre-trained Transformer 3 (GPT-3)</method>, <method>modified bootstrap resampling procedure</method>"
2024,https://openalex.org/W4391097427,Medicine,"Quantifying the direct and indirect effects of terrain, climate and human activity on the spatial pattern of kNDVI-based vegetation growth: A case study from the Minjiang River Basin, Southeast China","In the context of global change, it is vital to comprehensively understand the spatial pattern and driving mechanism of vegetation growth to maintain the stability of watershed ecosystems. Previous research has focused mainly on identifying the main drivers of vegetation growth, while the direct and indirect effects of climate, terrain, and human activity on vegetation growth have rarely been explored. This study used the Minjiang River Basin (MRB), an important ecological barrier and the largest watershed in southeastern China, as an example. The kernel normalized difference vegetation index (kNDVI) was calculated on the Google Earth Engine (GEE) platform to examine the spatial pattern and evolution characteristics of vegetation growth. The optimal parameter-based geographical detector (OPGD) and partial least squares structural equation modeling (PLS-SEM) were used to analyze how terrain, climate, and human activity influenced the spatial pattern of the kNDVI. (1) From 2001 to 2020, vegetation growth in the MRB was predominantly rated as excellent or good, and 88.93% of the area showed an increasing trend of vegetation growth. (2) The OPGD revealed that the primary drivers influencing the spatial distribution of the kNDVI in the MRB included population density, nighttime light, elevation and temperature, which explained >40% of the variation in the kNDVI. The interaction of all paired drivers enhanced the explanatory power of the kNDVI, among which the strongest interaction was between population density and elevation, and the second interaction was between population density and temperature. (3) PLS-SEM revealed that human activity had a direct negative effect on the kNDVI, while terrain and climate had direct and indirect positive effects on the kNDVI. Overall, the total effects of terrain, climate and human activity on the kNDVI were 0.594, 0.233 and − 0.495, respectively, indicating that the positive effect of terrain outweighed the negative effect of human activity on vegetation growth in the MRB. These findings not only provide scientific evidence for ecological conservation and management in the MRB but also offer a useful reference for other regions exploring the complex causes of spatial patterns of vegetation growth.","<method>optimal parameter-based geographical detector (OPGD)</method>, <method>partial least squares structural equation modeling (PLS-SEM)</method>"
2024,https://openalex.org/W4392285688,Medicine,Machine Learning and Digital Biomarkers Can Detect Early Stages of Neurodegenerative Diseases,"Neurodegenerative diseases (NDs) such as Alzheimer’s Disease (AD) and Parkinson’s Disease (PD) are devastating conditions that can develop without noticeable symptoms, causing irreversible damage to neurons before any signs become clinically evident. NDs are a major cause of disability and mortality worldwide. Currently, there are no cures or treatments to halt their progression. Therefore, the development of early detection methods is urgently needed to delay neuronal loss as soon as possible. Despite advancements in Medtech, the early diagnosis of NDs remains a challenge at the intersection of medical, IT, and regulatory fields. Thus, this review explores “digital biomarkers” (tools designed for remote neurocognitive data collection and AI analysis) as a potential solution. The review summarizes that recent studies combining AI with digital biomarkers suggest the possibility of identifying pre-symptomatic indicators of NDs. For instance, research utilizing convolutional neural networks for eye tracking has achieved significant diagnostic accuracies. ROC-AUC scores reached up to 0.88, indicating high model performance in differentiating between PD patients and healthy controls. Similarly, advancements in facial expression analysis through tools have demonstrated significant potential in detecting emotional changes in ND patients, with some models reaching an accuracy of 0.89 and a precision of 0.85. This review follows a structured approach to article selection, starting with a comprehensive database search and culminating in a rigorous quality assessment and meaning for NDs of the different methods. The process is visualized in 10 tables with 54 parameters describing different approaches and their consequences for understanding various mechanisms in ND changes. However, these methods also face challenges related to data accuracy and privacy concerns. To address these issues, this review proposes strategies that emphasize the need for rigorous validation and rapid integration into clinical practice. Such integration could transform ND diagnostics, making early detection tools more cost-effective and globally accessible. In conclusion, this review underscores the urgent need to incorporate validated digital health tools into mainstream medical practice. This integration could indicate a new era in the early diagnosis of neurodegenerative diseases, potentially altering the trajectory of these conditions for millions worldwide. Thus, by highlighting specific and statistically significant findings, this review demonstrates the current progress in this field and the potential impact of these advancements on the global management of NDs.",<method>convolutional neural networks</method>
2024,https://openalex.org/W4393167823,Medicine,Risk analysis and assessment of water resource carrying capacity based on weighted gray model with improved entropy weighting method in the central plains region of China,"The issue of global water shortage is a serious concern. The scientific evaluation of water resource carrying capacity (WRCC) serves as the foundation for implementing measures to protect water resources. In addition, most of the studies are based on the analysis and research of regional WRCC from the aspects of water quantity and water quality. There are few studies on the four aspects of water resources endowment conditions, society, economy and ecological environment, which is difficult to scientifically and accurately reflect the analysis and evaluation of regional WRCC by the four systems. Therefore, it is necessary to conduct a deeper discussion and Analysis on this topic. This study presents a WRCC index system and corresponding ranking criteria based on 20 influencing factors from four aspects: water resources endowment (WRE), economy, society, and ecological environment. In addition, by combining the improved entropy weighting method (EWM) with gray correlation analysis, the weighted gray technique for order preference by similarity to an ideal solution (TOPSIS) model is proposed for analyzing and assessing WRCC risk. Finally, the WRCC of the study area from 2012 to 2021 is comprehensively evaluated in the central plains region of China (CPROC) as an example. The results show that the comprehensive evaluation obtained a multi-year average value of 0.2935, and the water resources shortage in the CPROC is generally in grade III status. The comprehensive average value of Beijing is 0.345, and the comprehensive average value of Henan is 0.397. The overall degree of water resources shortage is in the state of grade V shortage, Shaanxi is in the state of grade IV shortage, and the degree of water resources in Tianjin and Shanxi is relatively good. This study provides corresponding scientific basis and methodological guidance for the sustainable utilization of water resources and healthy socio-economic performance in the CPROC.","<method>improved entropy weighting method (EWM)</method>, <method>gray correlation analysis</method>, <method>weighted gray technique for order preference by similarity to an ideal solution (TOPSIS) model</method>"
2024,https://openalex.org/W4402757343,Medicine,"Global prevalence, trend and projection of myopia in children and adolescents from 1990 to 2050: a comprehensive systematic review and meta-analysis","Background Myopia is a pervasive global public health concern, particularly among the younger population. However, the escalating prevalence of myopia remains uncertain. Hence, our research aims to ascertain the global and regional prevalence of myopia, along with its occurrence within specific demographic groups. Methods An exhaustive literature search was performed on several databases covering the period from their inception to 27 June 2023. The global prevalence of myopia was determined by employing pooled estimates with a 95% CI, and further analysis was conducted to assess variations in prevalence estimates across different subgroups. Additionally, a time series model was utilised to forecast and fit accurately the future prevalence of myopia for the next three decades. Results This study encompasses a comprehensive analysis of 276 studies, involving a total of 5 410 945 participants from 50 countries across all six continents. The findings revealed a gradual increase in pooled prevalence of myopia, ranging from 24.32% (95% CI 15.23% to 33.40%) to 35.81% (95% CI 31.70% to 39.91%), observed from 1990 to 2023, and projections indicate that this prevalence is expected to reach 36.59% in 2040 and 39.80% in 2050. Notably, individuals residing in East Asia (35.22%) or in urban areas (28.55%), female gender (33.57%), adolescents (47.00%), and high school students (45.71%) exhibit a higher proportion of myopia prevalence. Conclusion The global prevalence of childhood myopia is substantial, affecting approximately one-third of children and adolescents, with notable variations in prevalence across different demographic groups. It is anticipated that the global incidence of myopia will exceed 740 million cases by 2050.",<method>time series model</method>
2024,https://openalex.org/W4390607226,Medicine,RanMerFormer: Randomized vision transformer with token merging for brain tumor classification,"Brains are the control center of the nervous system in human bodies, and brain tumor is one of the most deadly diseases. Currently, magnetic resonance imaging (MRI) is the most effective way to brain tumors early detection in clinical diagnoses due to its superior imaging quality for soft tissues. Manual analysis of brain MRI is error-prone which depends on empirical experience and the fatigue state of the radiologists to a large extent. Computer-aided diagnosis (CAD) systems are becoming more and more impactful because they can provide accurate prediction results based on medical images with advanced techniques from computer vision. Therefore, a novel CAD method for brain tumor classification named RanMerFormer is presented in this paper. A pre-trained vision transformer is used as the backbone model. Then, a merging mechanism is proposed to remove the redundant tokens in the vision transformer, which improves computing efficiency substantially. Finally, a randomized vector functional-link serves as the head in the proposed RanMerFormer, which can be trained swiftly. All the simulation results are obtained from two public benchmark datasets, which reveal that the proposed RanMerFormer can achieve state-of-the-art performance for brain tumor classification. The trained RanMerFormer can be applied in real-world scenarios to assist in brain tumor diagnosis.","<method>vision transformer</method>, <method>randomized vector functional-link</method>"
2024,https://openalex.org/W4391023920,Medicine,A hybrid deep CNN model for brain tumor image multi-classification,"Abstract The current approach to diagnosing and classifying brain tumors relies on the histological evaluation of biopsy samples, which is invasive, time-consuming, and susceptible to manual errors. These limitations underscore the pressing need for a fully automated, deep-learning-based multi-classification system for brain malignancies. This article aims to leverage a deep convolutional neural network (CNN) to enhance early detection and presents three distinct CNN models designed for different types of classification tasks. The first CNN model achieves an impressive detection accuracy of 99.53% for brain tumors. The second CNN model, with an accuracy of 93.81%, proficiently categorizes brain tumors into five distinct types: normal, glioma, meningioma, pituitary, and metastatic. Furthermore, the third CNN model demonstrates an accuracy of 98.56% in accurately classifying brain tumors into their different grades. To ensure optimal performance, a grid search optimization approach is employed to automatically fine-tune all the relevant hyperparameters of the CNN models. The utilization of large, publicly accessible clinical datasets results in robust and reliable classification outcomes. This article conducts a comprehensive comparison of the proposed models against classical models, such as AlexNet, DenseNet121, ResNet-101, VGG-19, and GoogleNet, reaffirming the superiority of the deep CNN-based approach in advancing the field of brain tumor classification and early detection.","<method>deep convolutional neural network (CNN)</method>, <method>grid search optimization</method>, <method>AlexNet</method>, <method>DenseNet121</method>, <method>ResNet-101</method>, <method>VGG-19</method>, <method>GoogleNet</method>"
2024,https://openalex.org/W4390706643,Medicine,Present and Future Innovations in AI and Cardiac MRI,"Cardiac MRI is used to diagnose and treat patients with a multitude of cardiovascular diseases. Despite the growth of clinical cardiac MRI, complicated image prescriptions and long acquisition protocols limit the specialty and restrain its impact on the practice of medicine. Artificial intelligence (AI)-the ability to mimic human intelligence in learning and performing tasks-will impact nearly all aspects of MRI. Deep learning (DL) primarily uses an artificial neural network to learn a specific task from example data sets. Self-driving scanners are increasingly available, where AI automatically controls cardiac image prescriptions. These scanners offer faster image collection with higher spatial and temporal resolution, eliminating the need for cardiac triggering or breath holding. In the future, fully automated inline image analysis will most likely provide all contour drawings and initial measurements to the reader. Advanced analysis using radiomic or DL features may provide new insights and information not typically extracted in the current analysis workflow. AI may further help integrate these features with clinical, genetic, wearable-device, and ""omics"" data to improve patient outcomes. This article presents an overview of AI and its application in cardiac MRI, including in image acquisition, reconstruction, and processing, and opportunities for more personalized cardiovascular care through extraction of novel imaging markers.","<method>Artificial intelligence (AI)</method>, <method>Deep learning (DL)</method>, <method>artificial neural network</method>, <method>radiomic features</method>, <method>DL features</method>"
2024,https://openalex.org/W4391508432,Medicine,Artificial intelligence-driven virtual rehabilitation for people living in the community: A scoping review,"Abstract Virtual Rehabilitation (VRehab) is a promising approach to improving the physical and mental functioning of patients living in the community. The use of VRehab technology results in the generation of multi-modal datasets collected through various devices. This presents opportunities for the development of Artificial Intelligence (AI) techniques in VRehab, namely the measurement, detection, and prediction of various patients’ health outcomes. The objective of this scoping review was to explore the applications and effectiveness of incorporating AI into home-based VRehab programs. PubMed/MEDLINE, Embase, IEEE Xplore, Web of Science databases, and Google Scholar were searched from inception until June 2023 for studies that applied AI for the delivery of VRehab programs to the homes of adult patients. After screening 2172 unique titles and abstracts and 51 full-text studies, 13 studies were included in the review. A variety of AI algorithms were applied to analyze data collected from various sensors and make inferences about patients’ health outcomes, most involving evaluating patients’ exercise quality and providing feedback to patients. The AI algorithms used in the studies were mostly fuzzy rule-based methods, template matching, and deep neural networks. Despite the growing body of literature on the use of AI in VRehab, very few studies have examined its use in patients’ homes. Current research suggests that integrating AI with home-based VRehab can lead to improved rehabilitation outcomes for patients. However, further research is required to fully assess the effectiveness of various forms of AI-driven home-based VRehab, taking into account its unique challenges and using standardized metrics.","<method>fuzzy rule-based methods</method>, <method>template matching</method>, <method>deep neural networks</method>"
2024,https://openalex.org/W4395037579,Medicine,Assessing ChatGPT 4.0’s test performance and clinical diagnostic accuracy on USMLE STEP 2 CK and clinical case reports,"Abstract While there is data assessing the test performance of artificial intelligence (AI) chatbots, including the Generative Pre-trained Transformer 4.0 (GPT 4) chatbot (ChatGPT 4.0), there is scarce data on its diagnostic accuracy of clinical cases. We assessed the large language model (LLM), ChatGPT 4.0, on its ability to answer questions from the United States Medical Licensing Exam (USMLE) Step 2, as well as its ability to generate a differential diagnosis based on corresponding clinical vignettes from published case reports. A total of 109 Step 2 Clinical Knowledge (CK) practice questions were inputted into both ChatGPT 3.5 and ChatGPT 4.0, asking ChatGPT to pick the correct answer. Compared to its previous version, ChatGPT 3.5, we found improved accuracy of ChatGPT 4.0 when answering these questions, from 47.7 to 87.2% ( p = 0.035) respectively. Utilizing the topics tested on Step 2 CK questions, we additionally found 63 corresponding published case report vignettes and asked ChatGPT 4.0 to come up with its top three differential diagnosis. ChatGPT 4.0 accurately created a shortlist of differential diagnoses in 74.6% of the 63 case reports (74.6%). We analyzed ChatGPT 4.0’s confidence in its diagnosis by asking it to rank its top three differentials from most to least likely. Out of the 47 correct diagnoses, 33 were the first (70.2%) on the differential diagnosis list, 11 were second (23.4%), and three were third (6.4%). Our study shows the continued iterative improvement in ChatGPT’s ability to answer standardized USMLE questions accurately and provides insights into ChatGPT’s clinical diagnostic accuracy.","<method>Generative Pre-trained Transformer 4.0 (GPT 4) chatbot (ChatGPT 4.0)</method>, <method>ChatGPT 3.5</method>, <method>large language model (LLM)</method>"
2024,https://openalex.org/W4402521185,Medicine,Advanced Ensemble Machine Learning Techniques for Optimizing Diabetes Mellitus Prognostication: A Detailed Examination of Hospital Data,"Diabetes is a chronic disease that affects millions of people worldwide. Early diagnosis and effective management are crucial for reducing its complications. Diabetes is the fourth-highest cause of mortality due to its association with various comorbidities, including heart disease, nerve damage, blood vessel damage, and blindness. The potential of machine learning algorithms in predicting Diabetes and related conditions is significant, and mining diabetes data is an efficient method for extracting new insights.The primary objective of this study is to develop an enhanced ensemble model to predict Diabetes with improved accuracy by leveraging various machine learning algorithms.This study tested several popular machine learning algorithms commonly used in diabetes prediction, including Naive Bayes (NB), Generalized Linear Model (GLM), Logistic Regression (LR), Fast Large Margin (FLM), Deep Learning (DL), Decision Tree (DT), Random Forest (RF), Gradient Boosted Trees (GBT), and Support Vector Machine (SVM). The performance of these algorithms was compared, and two different ensemble techniques—stacking and voting—were used to build a more accurate predictive model.The top three algorithms based on accuracy were Deep Learning, Naive Bayes, and Gradient Boosted Trees. The machine learning algorithms revealed that individuals with Diabetes are significantly affected by the number of chronic conditions they have, as well as their gender and age. The ensemble models, particularly the stacking method, provided higher accuracy than individual algorithms. The stacking ensemble model achieved a slightly better accuracy of 99.94% compared to 99.34% for the voting method.Building an ensemble model significantly increased the accuracy of predicting Diabetes and related conditions. The stacking ensemble model, in particular, demonstrated superior performance, highlighting the importance of combining multiple machine learning approaches to enhance predictive accuracy","<method>Naive Bayes (NB)</method>, <method>Generalized Linear Model (GLM)</method>, <method>Logistic Regression (LR)</method>, <method>Fast Large Margin (FLM)</method>, <method>Deep Learning (DL)</method>, <method>Decision Tree (DT)</method>, <method>Random Forest (RF)</method>, <method>Gradient Boosted Trees (GBT)</method>, <method>Support Vector Machine (SVM)</method>, <method>stacking ensemble</method>, <method>voting ensemble</method>"
2024,https://openalex.org/W4396526654,Medicine,"Recent advancement of remaining useful life prediction of lithium-ion battery in electric vehicle applications: A review of modelling mechanisms, network configurations, factors, and outstanding issues","The remaining useful life (RUL) prediction of lithium-ion batteries (LIBs) plays a crucial role in battery management, safety assurance, and the anticipation of maintenance needs for reliable electric vehicle (EV) operation. An efficient prediction of RUL can ensure its safe operation and prevent both internal and external failures, as well as avoid any unwanted catastrophic events. However, achieving precise RUL prediction for electric vehicles presents a challenging task due to several issues related to intricate operational characteristics and dynamic shifts in model parameters throughout the aging process, battery parameters data extraction, data preprocessing, and hyperparameters tuning of the prediction model. This phenomenon significantly impacts the advancement of electric vehicle technology. To address these challenges, this study offers a comprehensive overview of various RUL prediction methods, presenting a comparative analysis of their outcomes, advantages, drawbacks, and associated research constraints. Emphasis is placed on the necessity of a battery management system (BMS) to ensure the safe and reliable functioning of LIBs. The review delves into crucial implementation factors, including battery test bench considerations, data selection, feature extraction, data preprocessing, performance evaluation indicators, and hyperparameter tuning. Additionally, the issues and challenges related to RUL prediction approaches such as; thermal runaway, material selection, cell balancing, battery aging, relaxation impact, training algorithms, data acquisition, and hyperparameter tuning were outlined to provide an in-depth understanding of the recent situations. The outcome of this review comprehensively examines various methods for predicting the RUL of LIB in EV applications, offering insights into their advantages, limitations, and research challenges. Recommendations for future trends in LIBs technology comprise enhancing prognostic accuracy and developing robust approaches to guarantee sustainable operation and management.",No methods found.
2024,https://openalex.org/W4390987311,Medicine,Reliability of ChatGPT for performing triage task in the emergency department using the Korean Triage and Acuity Scale,"Background Artificial intelligence (AI) technology can enable more efficient decision-making in healthcare settings. There is a growing interest in improving the speed and accuracy of AI systems in providing responses for given tasks in healthcare settings. Objective This study aimed to assess the reliability of ChatGPT in determining emergency department (ED) triage accuracy using the Korean Triage and Acuity Scale (KTAS). Methods Two hundred and two virtual patient cases were built. The gold standard triage classification for each case was established by an experienced ED physician. Three other human raters (ED paramedics) were involved and rated the virtual cases individually. The virtual cases were also rated by two different versions of the chat generative pre-trained transformer (ChatGPT, 3.5 and 4.0). Inter-rater reliability was examined using Fleiss’ kappa and intra-class correlation coefficient (ICC). Results The kappa values for the agreement between the four human raters and ChatGPTs were .523 (version 4.0) and .320 (version 3.5). Of the five levels, the performance was poor when rating patients at levels 1 and 5, as well as case scenarios with additional text descriptions. There were differences in the accuracy of the different versions of GPTs. The ICC between version 3.5 and the gold standard was .520, and that between version 4.0 and the gold standard was .802. Conclusions A substantial level of inter-rater reliability was revealed when GPTs were used as KTAS raters. The current study showed the potential of using GPT in emergency healthcare settings. Considering the shortage of experienced manpower, this AI method may help improve triaging accuracy.","<method>chat generative pre-trained transformer (ChatGPT, 3.5 and 4.0)</method>"
2024,https://openalex.org/W4391145465,Medicine,Assessing generalisability of deep learning-based polyp detection and segmentation methods through a computer vision challenge,"Abstract Polyps are well-known cancer precursors identified by colonoscopy. However, variability in their size, appearance, and location makes the detection of polyps challenging. Moreover, colonoscopy surveillance and removal of polyps are highly operator-dependent procedures and occur in a highly complex organ topology. There exists a high missed detection rate and incomplete removal of colonic polyps. To assist in clinical procedures and reduce missed rates, automated methods for detecting and segmenting polyps using machine learning have been achieved in past years. However, the major drawback in most of these methods is their ability to generalise to out-of-sample unseen datasets from different centres, populations, modalities, and acquisition systems. To test this hypothesis rigorously, we, together with expert gastroenterologists, curated a multi-centre and multi-population dataset acquired from six different colonoscopy systems and challenged the computational expert teams to develop robust automated detection and segmentation methods in a crowd-sourcing Endoscopic computer vision challenge. This work put forward rigorous generalisability tests and assesses the usability of devised deep learning methods in dynamic and actual clinical colonoscopy procedures. We analyse the results of four top performing teams for the detection task and five top performing teams for the segmentation task. Our analyses demonstrate that the top-ranking teams concentrated mainly on accuracy over the real-time performance required for clinical applicability. We further dissect the devised methods and provide an experiment-based hypothesis that reveals the need for improved generalisability to tackle diversity present in multi-centre datasets and routine clinical procedures.","<method>machine learning</method>, <method>deep learning</method>"
2024,https://openalex.org/W4391531696,Medicine,Artificial intelligence in the risk prediction models of cardiovascular disease and development of an independent validation screening tool: a systematic review,"Abstract Background A comprehensive overview of artificial intelligence (AI) for cardiovascular disease (CVD) prediction and a screening tool of AI models (AI-Ms) for independent external validation are lacking. This systematic review aims to identify, describe, and appraise AI-Ms of CVD prediction in the general and special populations and develop a new independent validation score (IVS) for AI-Ms replicability evaluation. Methods PubMed, Web of Science, Embase, and IEEE library were searched up to July 2021. Data extraction and analysis were performed for the populations, distribution, predictors, algorithms, etc. The risk of bias was evaluated with the prediction risk of bias assessment tool (PROBAST). Subsequently, we designed IVS for model replicability evaluation with five steps in five items, including transparency of algorithms, performance of models, feasibility of reproduction, risk of reproduction, and clinical implication, respectively. The review is registered in PROSPERO (No. CRD42021271789). Results In 20,887 screened references, 79 articles (82.5% in 2017–2021) were included, which contained 114 datasets (67 in Europe and North America, but 0 in Africa). We identified 486 AI-Ms, of which the majority were in development ( n = 380), but none of them had undergone independent external validation. A total of 66 idiographic algorithms were found; however, 36.4% were used only once and only 39.4% over three times. A large number of different predictors (range 5–52,000, median 21) and large-span sample size (range 80–3,660,000, median 4466) were observed. All models were at high risk of bias according to PROBAST, primarily due to the incorrect use of statistical methods. IVS analysis confirmed only 10 models as “recommended”; however, 281 and 187 were “not recommended” and “warning,” respectively. Conclusion AI has led the digital revolution in the field of CVD prediction, but is still in the early stage of development as the defects of research design, report, and evaluation systems. The IVS we developed may contribute to independent external validation and the development of this field.",No methods found.
2024,https://openalex.org/W4399800741,Medicine,Do competitive strategies affect working capital management efficiency?,"Purpose This study examines the effects of CLS and DS on companies' WCME and analyses the differences in WCME at company and market levels. Design/methodology/approach This study adopts the DEA approach, regression, differences, and additional analyses to achieve its objectives. This study employs 235 non-financial companies and 1,175 company-year observations from eight active industries in the United States from 2016 to 2020. Findings The findings indicate that CLS and DS strategies positively influence companies' WCME. Additionally, WCME differed across size categories and industries, with large companies and those operating in the communication services industry showing better WCME. By contrast, WCME did not differ between the periods before and during the COVID-19 pandemic. Practical implications This study scrutinizes the impact of CLS and DS strategies on companies' WCME to bridge the gap in this field. It extends the investigation of competitive strategies as explanatory variables for a company's WCME and examines the differences in companies' WCME at the company and market levels, which may assist decision-makers in improving their strategies and efficiencies for continuous improvement. Originality/value This study enhances current knowledge by uncovering the influence of CLS and DS strategies on improving companies' WCME, an underexplored topic. It also explores companies' WCME trends and patterns regarding company size, industry type, and the pandemic period to draw interesting conclusions about the essence of WCME.",No methods found.
2024,https://openalex.org/W4391878291,Medicine,Effective lung nodule detection using deep CNN with dual attention mechanisms,"Abstract Novel methods are required to enhance lung cancer detection, which has overtaken other cancer-related causes of death as the major cause of cancer-related mortality. Radiologists have long-standing methods for locating lung nodules in patients with lung cancer, such as computed tomography (CT) scans. Radiologists must manually review a significant amount of CT scan pictures, which makes the process time-consuming and prone to human error. Computer-aided diagnosis (CAD) systems have been created to help radiologists with their evaluations in order to overcome these difficulties. These systems make use of cutting-edge deep learning architectures. These CAD systems are designed to improve lung nodule diagnosis efficiency and accuracy. In this study, a bespoke convolutional neural network (CNN) with a dual attention mechanism was created, which was especially crafted to concentrate on the most important elements in images of lung nodules. The CNN model extracts informative features from the images, while the attention module incorporates both channel attention and spatial attention mechanisms to selectively highlight significant features. After the attention module, global average pooling is applied to summarize the spatial information. To evaluate the performance of the proposed model, extensive experiments were conducted using benchmark dataset of lung nodules. The results of these experiments demonstrated that our model surpasses recent models and achieves state-of-the-art accuracy in lung nodule detection and classification tasks.","<method>convolutional neural network (CNN)</method>, <method>dual attention mechanism</method>, <method>channel attention</method>, <method>spatial attention</method>, <method>global average pooling</method>"
2024,https://openalex.org/W4392004069,Medicine,A precise model for skin cancer diagnosis using hybrid U-Net and improved MobileNet-V3 with hyperparameters optimization,"Abstract Skin cancer is a frequently occurring and possibly deadly disease that necessitates prompt and precise diagnosis in order to ensure efficacious treatment. This paper introduces an innovative approach for accurately identifying skin cancer by utilizing Convolution Neural Network architecture and optimizing hyperparameters. The proposed approach aims to increase the precision and efficacy of skin cancer recognition and consequently enhance patients' experiences. This investigation aims to tackle various significant challenges in skin cancer recognition, encompassing feature extraction, model architecture design, and optimizing hyperparameters. The proposed model utilizes advanced deep-learning methodologies to extract complex features and patterns from skin cancer images. We enhance the learning procedure of deep learning by integrating Standard U-Net and Improved MobileNet-V3 with optimization techniques, allowing the model to differentiate malignant and benign skin cancers. Also substituted the crossed-entropy loss function of the Mobilenet-v3 mathematical framework with a bias loss function to enhance the accuracy. The model's squeeze and excitation component was replaced with the practical channel attention component to achieve parameter reduction. Integrating cross-layer connections among Mobile modules has been proposed to leverage synthetic features effectively. The dilated convolutions were incorporated into the model to enhance the receptive field. The optimization of hyperparameters is of utmost importance in improving the efficiency of deep learning models. To fine-tune the model's hyperparameter, we employ sophisticated optimization methods such as the Bayesian optimization method using pre-trained CNN architecture MobileNet-V3. The proposed model is compared with existing models, i.e., MobileNet, VGG-16, MobileNet-V2, Resnet-152v2 and VGG-19 on the “HAM-10000 Melanoma Skin Cancer dataset"". The empirical findings illustrate that the proposed optimized hybrid MobileNet-V3 model outperforms existing skin cancer detection and segmentation techniques based on high precision of 97.84%, sensitivity of 96.35%, accuracy of 98.86% and specificity of 97.32%. The enhanced performance of this research resulted in timelier and more precise diagnoses, potentially contributing to life-saving outcomes and mitigating healthcare expenditures.","<method>Convolution Neural Network</method>, <method>deep-learning methodologies</method>, <method>Standard U-Net</method>, <method>Improved MobileNet-V3</method>, <method>bias loss function</method>, <method>practical channel attention component</method>, <method>cross-layer connections among Mobile modules</method>, <method>dilated convolutions</method>, <method>Bayesian optimization method</method>, <method>pre-trained CNN architecture MobileNet-V3</method>"
2024,https://openalex.org/W4391718168,Medicine,A comparative study of explainable ensemble learning and logistic regression for predicting in-hospital mortality in the emergency department,"Abstract This study addresses the challenges associated with emergency department (ED) overcrowding and emphasizes the need for efficient risk stratification tools to identify high-risk patients for early intervention. While several scoring systems, often based on logistic regression (LR) models, have been proposed to indicate patient illness severity, this study aims to compare the predictive performance of ensemble learning (EL) models with LR for in-hospital mortality in the ED. A cross-sectional single-center study was conducted at the ED of Imam Reza Hospital in northeast Iran from March 2016 to March 2017. The study included adult patients with one to three levels of emergency severity index. EL models using Bagging, AdaBoost, random forests (RF), Stacking and extreme gradient boosting (XGB) algorithms, along with an LR model, were constructed. The training and validation visits from the ED were randomly divided into 80% and 20%, respectively. After training the proposed models using tenfold cross-validation, their predictive performance was evaluated. Model performance was compared using the Brier score (BS), The area under the receiver operating characteristics curve (AUROC), The area and precision–recall curve (AUCPR), Hosmer–Lemeshow (H–L) goodness-of-fit test, precision, sensitivity, accuracy, F1-score, and Matthews correlation coefficient (MCC). The study included 2025 unique patients admitted to the hospital’s ED, with a total percentage of hospital deaths at approximately 19%. In the training group and the validation group, 274 of 1476 (18.6%) and 152 of 728 (20.8%) patients died during hospitalization, respectively. According to the evaluation of the presented framework, EL models, particularly Bagging, predicted in-hospital mortality with the highest AUROC (0.839, CI (0.802–0.875)) and AUCPR = 0.64 comparable in terms of discrimination power with LR (AUROC (0.826, CI (0.787–0.864)) and AUCPR = 0.61). XGB achieved the highest precision (0.83), sensitivity (0.831), accuracy (0.842), F1-score (0.833), and the highest MCC (0.48). Additionally, the most accurate models in the unbalanced dataset belonged to RF with the lowest BS (0.128). Although all studied models overestimate mortality risk and have insufficient calibration ( P &gt; 0.05), stacking demonstrated relatively good agreement between predicted and actual mortality. EL models are not superior to LR in predicting in-hospital mortality in the ED. Both EL and LR models can be considered as screening tools to identify patients at risk of mortality.","<method>logistic regression (LR)</method>, <method>ensemble learning (EL)</method>, <method>Bagging</method>, <method>AdaBoost</method>, <method>random forests (RF)</method>, <method>Stacking</method>, <method>extreme gradient boosting (XGB)</method>"
2024,https://openalex.org/W4394580101,Medicine,A systematic review and multivariate meta-analysis of the physical and mental health benefits of touch interventions,"Receiving touch is of critical importance, as many studies have shown that touch promotes mental and physical well-being. We conducted a pre-registered (PROSPERO: CRD42022304281) systematic review and multilevel meta-analysis encompassing 137 studies in the meta-analysis and 75 additional studies in the systematic review (n = 12,966 individuals, search via Google Scholar, PubMed and Web of Science until 1 October 2022) to identify critical factors moderating touch intervention efficacy. Included studies always featured a touch versus no touch control intervention with diverse health outcomes as dependent variables. Risk of bias was assessed via small study, randomization, sequencing, performance and attrition bias. Touch interventions were especially effective in regulating cortisol levels (Hedges' g = 0.78, 95% confidence interval (CI) 0.24 to 1.31) and increasing weight (0.65, 95% CI 0.37 to 0.94) in newborns as well as in reducing pain (0.69, 95% CI 0.48 to 0.89), feelings of depression (0.59, 95% CI 0.40 to 0.78) and state (0.64, 95% CI 0.44 to 0.84) or trait anxiety (0.59, 95% CI 0.40 to 0.77) for adults. Comparing touch interventions involving objects or robots resulted in similar physical (0.56, 95% CI 0.24 to 0.88 versus 0.51, 95% CI 0.38 to 0.64) but lower mental health benefits (0.34, 95% CI 0.19 to 0.49 versus 0.58, 95% CI 0.43 to 0.73). Adult clinical cohorts profited more strongly in mental health domains compared with healthy individuals (0.63, 95% CI 0.46 to 0.80 versus 0.37, 95% CI 0.20 to 0.55). We found no difference in health benefits in adults when comparing touch applied by a familiar person or a health care professional (0.51, 95% CI 0.29 to 0.73 versus 0.50, 95% CI 0.38 to 0.61), but parental touch was more beneficial in newborns (0.69, 95% CI 0.50 to 0.88 versus 0.39, 95% CI 0.18 to 0.61). Small but significant small study bias and the impossibility to blind experimental conditions need to be considered. Leveraging factors that influence touch intervention efficacy will help maximize the benefits of future interventions and focus research in this field.",No methods found.
2024,https://openalex.org/W4396905964,Medicine,A systematic review and meta-analysis of artificial intelligence versus clinicians for skin cancer diagnosis,"Scientific research of artificial intelligence (AI) in dermatology has increased exponentially. The objective of this study was to perform a systematic review and meta-analysis to evaluate the performance of AI algorithms for skin cancer classification in comparison to clinicians with different levels of expertise. Based on PRISMA guidelines, 3 electronic databases (PubMed, Embase, and Cochrane Library) were screened for relevant articles up to August 2022. The quality of the studies was assessed using QUADAS-2. A meta-analysis of sensitivity and specificity was performed for the accuracy of AI and clinicians. Fifty-three studies were included in the systematic review, and 19 met the inclusion criteria for the meta-analysis. Considering all studies and all subgroups of clinicians, we found a sensitivity (Sn) and specificity (Sp) of 87.0% and 77.1% for AI algorithms, respectively, and a Sn of 79.78% and Sp of 73.6% for all clinicians (overall); differences were statistically significant for both Sn and Sp. The difference between AI performance (Sn 92.5%, Sp 66.5%) vs. generalists (Sn 64.6%, Sp 72.8%), was greater, when compared with expert clinicians. Performance between AI algorithms (Sn 86.3%, Sp 78.4%) vs expert dermatologists (Sn 84.2%, Sp 74.4%) was clinically comparable. Limitations of AI algorithms in clinical practice should be considered, and future studies should focus on real-world settings, and towards AI-assistance.",No methods found.
2024,https://openalex.org/W4400981456,Medicine,Multimodal data integration for oncology in the era of deep neural networks: a review,"Cancer research encompasses data across various scales, modalities, and resolutions, from screening and diagnostic imaging to digitized histopathology slides to various types of molecular data and clinical records. The integration of these diverse data types for personalized cancer care and predictive modeling holds the promise of enhancing the accuracy and reliability of cancer screening, diagnosis, and treatment. Traditional analytical methods, which often focus on isolated or unimodal information, fall short of capturing the complex and heterogeneous nature of cancer data. The advent of deep neural networks has spurred the development of sophisticated multimodal data fusion techniques capable of extracting and synthesizing information from disparate sources. Among these, Graph Neural Networks (GNNs) and Transformers have emerged as powerful tools for multimodal learning, demonstrating significant success. This review presents the foundational principles of multimodal learning including oncology data modalities, taxonomy of multimodal learning, and fusion strategies. We delve into the recent advancements in GNNs and Transformers for the fusion of multimodal data in oncology, spotlighting key studies and their pivotal findings. We discuss the unique challenges of multimodal learning, such as data heterogeneity and integration complexities, alongside the opportunities it presents for a more nuanced and comprehensive understanding of cancer. Finally, we present some of the latest comprehensive multimodal pan-cancer data sources. By surveying the landscape of multimodal data integration in oncology, our goal is to underline the transformative potential of multimodal GNNs and Transformers. Through technological advancements and the methodological innovations presented in this review, we aim to chart a course for future research in this promising field. This review may be the first that highlights the current state of multimodal modeling applications in cancer using GNNs and transformers, presents comprehensive multimodal oncology data sources, and sets the stage for multimodal evolution, encouraging further exploration and development in personalized cancer care.","<method>deep neural networks</method>, <method>Graph Neural Networks (GNNs)</method>, <method>Transformers</method>"
2024,https://openalex.org/W4390708138,Medicine,Accuracy of GPT-4 in histopathological image detection and classification of colorectal adenomas,"Aims To evaluate the accuracy of Chat Generative Pre-trained Transformer (ChatGPT) powered by GPT-4 in histopathological image detection and classification of colorectal adenomas using the diagnostic consensus provided by pathologists as a reference standard. Methods A study was conducted with 100 colorectal polyp photomicrographs, comprising an equal number of adenomas and non-adenomas, classified by two pathologists. These images were analysed by classic GPT-4 for 1 time in October 2023 and custom GPT-4 for 20 times in December 2023. GPT-4’s responses were compared against the reference standard through statistical measures to evaluate its proficiency in histopathological diagnosis, with the pathologists further assessing the model’s descriptive accuracy. Results GPT-4 demonstrated a median sensitivity of 74% and specificity of 36% for adenoma detection. The median accuracy of polyp classification varied, ranging from 16% for non-specific changes to 36% for tubular adenomas. Its diagnostic consistency, indicated by low kappa values ranging from 0.06 to 0.11, suggested only poor to slight agreement. All of the microscopic descriptions corresponded with their diagnoses. GPT-4 also commented about the limitations in its diagnoses (eg, slide diagnosis best done by pathologists, the inadequacy of single-image diagnostic conclusions, the need for clinical data and a higher magnification view). Conclusions GPT-4 showed high sensitivity but low specificity in detecting adenomas and varied accuracy for polyp classification. However, its diagnostic consistency was low. This artificial intelligence tool acknowledged its diagnostic limitations, emphasising the need for a pathologist’s expertise and additional clinical context.","<method>Chat Generative Pre-trained Transformer (ChatGPT) powered by GPT-4</method>, <method>classic GPT-4</method>, <method>custom GPT-4</method>"
2024,https://openalex.org/W4390870882,Medicine,A domain adaptation approach to damage classification with an application to bridge monitoring,"Data-driven machine-learning algorithms generally suffer from a lack of labelled health-state data, mainly those referring to damage conditions. To address such an issue, population-based structural health monitoring seeks to enrich the original dataset by transferring knowledge from a population of monitored structures. Within this context, this paper presents a transfer learning approach, based on domain adaptation, to leverage information from completely-labelled bridge structure data to accurately predict new instances of an unknown target domain. Since intrinsic structural differences may cause distribution shifts, domain adaptation attempts to minimise the distance between the domains and to learn a mapping within a shared feature space. Specifically, the methodology involves the long-term acquisition of natural frequencies from several structural scenarios. Such damage-sensitive features are then aligned via domain adaptation so that a machine-learning algorithm can effectively utilise the labelled source domain data and generalise well to the unlabelled target-domain data. The described procedure is applied to two case studies, including the Z24 and the S101 benchmark bridges and their finite element models, respectively. The results demonstrate the successful exchange of health-state labels to identify the damage class within a population of bridges equipped with SHM systems, showing potential to reduce computational efforts and to deal with scarce or poor data sets in application to bridge network monitoring.","<method>transfer learning</method>, <method>domain adaptation</method>, <method>machine-learning algorithm</method>"
2024,https://openalex.org/W4392200867,Medicine,Revolutionizing core muscle analysis in female sexual dysfunction based on machine learning,"Abstract The purpose of this study is to investigate the role of core muscles in female sexual dysfunction (FSD) and develop comprehensive rehabilitation programs to address this issue. We aim to answer the following research questions: what are the roles of core muscles in FSD, and how can machine and deep learning models accurately predict changes in core muscles during FSD? FSD is a common condition that affects women of all ages, characterized by symptoms such as decreased libido, difficulty achieving orgasm, and pain during intercourse. We conducted a comprehensive analysis of changes in core muscles during FSD using machine and deep learning. We evaluated the performance of multiple models, including multi-layer perceptron (MLP), long short-term memory (LSTM), convolutional neural network (CNN), recurrent neural network (RNN), ElasticNetCV, random forest regressor, SVR, and Bagging regressor. The models were evaluated based on mean squared error (MSE), mean absolute error (MAE), and R-squared (R 2 ) score. Our results show that CNN and random forest regressor are the most accurate models for predicting changes in core muscles during FSD. CNN achieved the lowest MSE (0.002) and the highest R 2 score (0.988), while random forest regressor also performed well with an MSE of 0.0021 and an R 2 score of 0.9905. Our study demonstrates that machine and deep learning models can accurately predict changes in core muscles during FSD. The neglected core muscles play a significant role in FSD, highlighting the need for comprehensive rehabilitation programs that address these muscles. By developing these programs, we can improve the quality of life for women with FSD and help them achieve optimal sexual health.","<method>multi-layer perceptron (MLP)</method>, <method>long short-term memory (LSTM)</method>, <method>convolutional neural network (CNN)</method>, <method>recurrent neural network (RNN)</method>, <method>ElasticNetCV</method>, <method>random forest regressor</method>, <method>SVR</method>, <method>Bagging regressor</method>"
2024,https://openalex.org/W4392241969,Medicine,All models are wrong and yours are useless: making clinical prediction models impactful for patients,"All models are wrong and yours are useless: making clinical prediction models impactful for patients Florian MarkowetzCheck for updates Most published clinical prediction models are never used in clinical practice and there is a huge gap between academic research and clinical implementation.Here, I propose ways for academic researchers to be proactive partners in improving clinical practice and to design models in ways that ultimately benefit patients.""All models are wrong, but some are useful"" is an aphorism attributed to the statistician George Box.There is humility in claiming your model is wrong, but there is also bravado in implying your model might be useful.And, honestly, I don't think it is.I think your model is useless.How would I know?I don't even know who you are.Well, it is a bet.A bet I am willing to take because the odds are ridiculously in my favour.I will explain what I mean in the context of clinical prediction models.My points apply to a wide range of preclinical models, both computational and biological, but my own core expertise is with clinical prediction tools.These are computational models from statistics, machine learning or AI that try to predict clinically relevant variables and ultimately aim to help doctors to treat patients better.The papers describing them make claims like ""this model can be used in the clinic""; generally softened with words like ""might"", ""could"", ""potential"", ""promise"", or other techniques to reduce accountability.The Box quote offers a yardstick to measure the success of these models; not by how correctly they describe reality but by how useful they are in helping patients.And in general, almost none of these tools ever help anyone.There is a wealth of systematic reviews in different fields to show how many models have been proposed and how few have even been validated, let alone been adopted in the clinic.For example, 408(!) models for chronic obstructive pulmonary disease were systematically reviewed 1 and as a summary the authors bleakly note ""several methodological pitfalls in their development and a low rate of external validation"".And whatever biomedical area you work in, your experiences will mirror this resultmany novel prediction models, little help for patients.I believe that a model designed to be used for patients is useless unless it is actually used for patients.",No methods found.
2024,https://openalex.org/W4392356867,Medicine,The SAFE procedure: a practical stopping heuristic for active learning-based screening in systematic reviews and meta-analyses,"Abstract Active learning has become an increasingly popular method for screening large amounts of data in systematic reviews and meta-analyses. The active learning process continually improves its predictions on the remaining unlabeled records, with the goal of identifying all relevant records as early as possible. However, determining the optimal point at which to stop the active learning process is a challenge. The cost of additional labeling of records by the reviewer must be balanced against the cost of erroneous exclusions. This paper introduces the SAFE procedure, a practical and conservative set of stopping heuristics that offers a clear guideline for determining when to end the active learning process in screening software like ASReview. The eclectic mix of stopping heuristics helps to minimize the risk of missing relevant papers in the screening process. The proposed stopping heuristic balances the costs of continued screening with the risk of missing relevant records, providing a practical solution for reviewers to make informed decisions on when to stop screening. Although active learning can significantly enhance the quality and efficiency of screening, this method may be more applicable to certain types of datasets and problems. Ultimately, the decision to stop the active learning process depends on careful consideration of the trade-off between the costs of additional record labeling against the potential errors of the current model for the specific dataset and context.",<method>active learning</method>
2024,https://openalex.org/W4403545332,Medicine,Evaluating AI and Machine Learning Models in Breast Cancer Detection: A Review of Convolutional Neural Networks (CNN) and Global Research Trends,"Numerous studies have highlighted the significance of artificial intelligence (AI) in breast cancer diagnosis. However, systematic reviews of AI applications in this field often lack cohesion, with each study adopting a unique approach. The aim of this study is to provide a detailed examination of AI's role in breast cancer diagnosis through citation analysis, helping to categorize the key areas that attract academic attention. It also includes a thematic analysis to identify the specific research topics within each category. A total of 30,200 studies related to breast cancer and AI, published between 2015 and 2024, were sourced from databases such as IEEE, Scopus, PubMed, Springer, and Google Scholar. After applying inclusion and exclusion criteria, 32 relevant studies were identified. Most of these studies utilized classification models for breast cancer prediction, with high accuracy being the most commonly reported performance metric. Convolutional Neural Networks (CNN) emerged as the preferred model in many studies. The findings indicate that both the quantity and quality of AI-based algorithms in breast cancer diagnosis are increases in the given years. AI is increasingly seen as a complement to healthcare sector and clinical expertise, with the target of enhancing the accessibility and affordability of quality healthcare worldwide.","<method>classification models</method>, <method>Convolutional Neural Networks (CNN)</method>"
2024,https://openalex.org/W4390777660,Medicine,Unlocking the Potential of XAI for Improved Alzheimer’s Disease Detection and Classification Using a ViT-GRU Model,"Alzheimer's Disease (AD) is a significant cause of dementia worldwide, and its progression from mild to severe affects an individual's ability to perform daily activities independently. The accurate and early diagnosis of AD is crucial for effective clinical intervention. However, interpreting AD from medical images can be challenging, even for experienced radiologists. Therefore, there is a need for an automatic diagnosis of AD, and researchers have investigated the potential of utilizing Artificial Intelligence (AI) techniques, particularly deep learning models, to address this challenge. This study proposes a framework that combines a Vision Transformer (ViT) and a Gated Recurrent Unit (GRU) to detect AD characteristics from Magnetic Resonance Imaging (MRI) images accurately and reliably. The ViT identifies crucial features from the input image, and the GRU establishes clear correlations between these features. The proposed model overcomes the class imbalance issue in the MRI image dataset and achieves superior accuracy and performance compared to existing methods. The model was trained on the Alzheimer's MRI Preprocessed Dataset obtained from Kaggle, achieving notable accuracies of 99.53% for 4-class and 99.69% for binary classification. It also demonstrated a high accuracy of 99.26% for 3-class on the AD Neuroimaging Initiative (ADNI) Baseline Database. These results were validated through a thorough 10-fold cross-validation process. Furthermore, Explainable AI (XAI) techniques were incorporated to make the model interpretable and explainable. This allows clinicians to understand the model's decision-making process and gain insights into the underlying factors driving the AD diagnosis.","<method>Vision Transformer (ViT)</method>, <method>Gated Recurrent Unit (GRU)</method>, <method>Explainable AI (XAI) techniques</method>"
2024,https://openalex.org/W4391166899,Medicine,"Prediction of atmospheric PM2.5 level by machine learning techniques in Isfahan, Iran","Abstract With increasing levels of air pollution, air quality prediction has attracted more attention. Mathematical models are being developed by researchers to achieve precise predictions. Monitoring and prediction of atmospheric PM 2.5 levels, as a predominant pollutant, is essential in emission mitigation programs. In this study, meteorological datasets from 9 years in Isfahan city, a large metropolis of Iran, were applied to predict the PM 2.5 levels, using four machine learning algorithms including Artificial Neural |Networks (ANNs), K-Nearest-Neighbors (KNN), Support Vector |Machines (SVMs) and ensembles of classification trees Random Forest (RF). The data from 7 air quality monitoring stations located in Isfahan City were taken into consideration. The Confusion Matrix and Cross-Entropy Loss were used to analyze the performance of classification models. Several parameters, including sensitivity, specificity, accuracy, F1 score, precision, and the area under the curve (AUC), are computed to assess model performance. Finally, by introducing the predicted data for 2020 into ArcGIS software and using the IDW (Inverse Distance Weighting) method, interpolation was conducted for the area of Isfahan city and the pollution map was illustrated for each month of the year. The results showed that, based on the accuracy percentage, the ANN model has a better performance (90.1%) in predicting PM 2.5 grades compared to the other models for the applied meteorological dataset, followed by RF (86.1%), SVM (84.6%) and KNN (82.2%) models, respectively. Therefore, ANN modelling provides a feasible procedure for the managerial planning of air pollution control.","<method>Artificial Neural Networks (ANNs)</method>, <method>K-Nearest-Neighbors (KNN)</method>, <method>Support Vector Machines (SVMs)</method>, <method>Random Forest (RF)</method>"
2024,https://openalex.org/W4391751277,Medicine,Digital health technologies and machine learning augment patient reported outcomes to remotely characterise rheumatoid arthritis,"Digital measures of health status captured during daily life could greatly augment current in-clinic assessments for rheumatoid arthritis (RA), to enable better assessment of disease progression and impact. This work presents results from weaRAble-PRO, a 14-day observational study, which aimed to investigate how digital health technologies (DHT), such as smartphones and wearables, could augment patient reported outcomes (PRO) to determine RA status and severity in a study of 30 moderate-to-severe RA patients, compared to 30 matched healthy controls (HC). Sensor-based measures of health status, mobility, dexterity, fatigue, and other RA specific symptoms were extracted from daily iPhone guided tests (GT), as well as actigraphy and heart rate sensor data, which was passively recorded from patients' Apple smartwatch continuously over the study duration. We subsequently developed a machine learning (ML) framework to distinguish RA status and to estimate RA severity. It was found that daily wearable sensor-outcomes robustly distinguished RA from HC participants (F1, 0.807). Furthermore, by day 7 of the study (half-way), a sufficient volume of data had been collected to reliably capture the characteristics of RA participants. In addition, we observed that the detection of RA severity levels could be improved by augmenting standard patient reported outcomes with sensor-based features (F1, 0.833) in comparison to using PRO assessments alone (F1, 0.759), and that the combination of modalities could reliability measure continuous RA severity, as determined by the clinician-assessed RAPID-3 score at baseline (r",<method>machine learning (ML) framework</method>
2024,https://openalex.org/W4391810207,Medicine,Machine Learning–Based Prediction of Suicidality in Adolescents With Allergic Rhinitis: Derivation and Validation in 2 Independent Nationwide Cohorts,"Background Given the additional risk of suicide-related behaviors in adolescents with allergic rhinitis (AR), it is important to use the growing field of machine learning (ML) to evaluate this risk. Objective This study aims to evaluate the validity and usefulness of an ML model for predicting suicide risk in patients with AR. Methods We used data from 2 independent survey studies, Korea Youth Risk Behavior Web-based Survey (KYRBS; n=299,468) for the original data set and Korea National Health and Nutrition Examination Survey (KNHANES; n=833) for the external validation data set, to predict suicide risks of AR in adolescents aged 13 to 18 years, with 3.45% (10,341/299,468) and 1.4% (12/833) of the patients attempting suicide in the KYRBS and KNHANES studies, respectively. The outcome of interest was the suicide attempt risks. We selected various ML-based models with hyperparameter tuning in the discovery and performed an area under the receiver operating characteristic curve (AUROC) analysis in the train, test, and external validation data. Results The study data set included 299,468 (KYRBS; original data set) and 833 (KNHANES; external validation data set) patients with AR recruited between 2005 and 2022. The best-performing ML model was the random forest model with a mean AUROC of 84.12% (95% CI 83.98%-84.27%) in the original data set. Applying this result to the external validation data set revealed the best performance among the models, with an AUROC of 89.87% (sensitivity 83.33%, specificity 82.58%, accuracy 82.59%, and balanced accuracy 82.96%). While looking at feature importance, the 5 most important features in predicting suicide attempts in adolescent patients with AR are depression, stress status, academic achievement, age, and alcohol consumption. Conclusions This study emphasizes the potential of ML models in predicting suicide risks in patients with AR, encouraging further application of these models in other conditions to enhance adolescent health and decrease suicide rates.",<method>random forest</method>
2024,https://openalex.org/W4392450360,Medicine,Geographically weighted machine learning for modeling spatial heterogeneity in traffic crash frequency and determinants in US,"Spatial analyses of traffic crashes have drawn much interest due to the nature of the spatial dependence and spatial heterogeneity in the crash data. This study makes the best of Geographically Weighted Random Forest (GW-RF) model to explore the local associations between crash frequency and various influencing factors in the US, including road network attributes, socio-economic characteristics, and land use factors collected from multiple data sources. Special emphasis is put on modeling the spatial heterogeneity in the effects of a factor on crash frequency in different geographical areas in a data-driven way. The GW-RF model outperforms global models (e.g. Random Forest) and conventional geographically weighted regression, demonstrating superior predictive accuracy and elucidating spatial variations. The GW-RF model reveals spatial distinctions in the effects of certain factors on crash frequency. For example, the importance of intersection density varies significantly across regions, with high significance in the southern and northeastern areas. Low-grade road density emerges as influential in specific cities. The findings highlight the significance of different factors in influencing crash frequency across zones. Road network factors, particularly intersection density, exhibit high importance universally, while socioeconomic variables demonstrate moderate effects. Interestingly, land use variables show relatively lower importance. The outcomes could help to allocate resources and implement tailored interventions to reduce the likelihood of crashes.","<method>Geographically Weighted Random Forest (GW-RF)</method>, <method>Random Forest</method>, <method>geographically weighted regression</method>"
2024,https://openalex.org/W4393405326,Medicine,"Developing Deep LSTMs With Later Temporal Attention for Predicting COVID-19 Severity, Clinical Outcome, and Antibody Level by Screening Serological Indicators Over Time","Objective: The clinical course of COVID-19, as well as the immunological reaction, is notable for its extreme variability. Identifying the main associated factors might help understand the disease progression and physiological status of COVID-19 patients. The dynamic changes of the antibody against Spike protein are crucial for understanding the immune response. This work explores a temporal attention (TA) mechanism of deep learning to predict COVID-19 disease severity, clinical outcomes, and Spike antibody levels by screening serological indicators over time. Methods: We use feature selection techniques to filter feature subsets that are highly correlated with the target. The specific deep Long Short-Term Memory (LSTM) models are employed to capture the dynamic changes of disease severity, clinical outcome, and Spike antibody level. We also propose deep LSTMs with a TA mechanism to emphasize the later blood test records because later records often attract more attention from doctors. Results: Risk factors highly correlated with COVID-19 are revealed. LSTM achieves the highest classification accuracy for disease severity prediction. Temporal Attention Long Short-Term Memory (TA-LSTM) achieves the best performance for clinical outcome prediction. For Spike antibody level prediction, LSTM achieves the best permanence. Conclusion: The experimental results demonstrate the effectiveness of the proposed models. The proposed models can provide a computer-aided medical diagnostics system by simply using time series of serological indicators.","<method>feature selection techniques</method>, <method>deep Long Short-Term Memory (LSTM) models</method>, <method>deep LSTMs with a temporal attention (TA) mechanism</method>, <method>Temporal Attention Long Short-Term Memory (TA-LSTM)</method>"
2024,https://openalex.org/W4394618925,Medicine,Optimal homeostatic stress to maximize the homogeneity of adaptations to interval interventions in soccer players,"This study examined the uniformity of adaptations in cardiorespiratory fitness and bio-motor abilities by analyzing individual responses to measures representing the mentioned qualities. Twenty-four male well-trained soccer players (Age = 26 ± 4 years; stature = 181 ± 3.8; Weight = 84 ± 6.1) were randomized to two groups performing short sprint interval training [sSIT (3 sets of 10 × 4 s all-out sprints with 20 s of recovery between efforts and 3 min of rest intervals between sets)] or a time-matched small-sided game [SSG (3 sets of 3 v 3 efforts in a 20 × 15 m area with 3 min of relief in-between)]. Before and after the 6-week training period, aerobic fitness indices, cardiac hemodynamics, and anaerobic power were assessed through a graded exercise test utilizing a gas collection system, noninvasive impedance cardiography, and a lower-body Wingate test, respectively. Also, sport-specific bio-motor abilities were determined by measuring linear speed, change of direction, and jumping ability. Comparing inter-individual variability in the adaptive changes by analyzing residuals in individual adaptations indicated that sSIT induces more uniform changes in the first and second ventilatory threshold (VT 1 &amp;amp; VT 2 ), stroke volume, and peak power output across team members than SSG. SSG also yielded lower proportions of responders in <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" id=""m1""><mml:mrow><mml:mover accent=""true""><mml:mi mathvariant=""normal"">V</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:msub><mml:mi mathvariant=""normal"">O</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁡</mml:mo><mml:mi>max</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math> , VT 1 , VT 2 , peak, and average power output compared to sSIT. Additionally, the coefficient of variation in mean group changes in measures of aerobic fitness and bio-motor abilities in response to sSIT were lower than in SSG. Short sprint interval training induces more homogenized adaptations in measures of cardiorespiratory fitness and anaerobic power than small-sided games across team members.",No methods found.
2024,https://openalex.org/W4401537518,Medicine,A New Brain Network Construction Paradigm for Brain Disorder via Diffusion-Based Graph Contrastive Learning,"Brain network analysis plays an increasingly important role in studying brain function and the exploring of disease mechanisms. However, existing brain network construction tools have some limitations, including dependency on empirical users, weak consistency in repeated experiments and time-consuming processes. In this work, a diffusion-based brain network pipeline, DGCL is designed for end-to-end construction of brain networks. Initially, the brain region-aware module (BRAM) precisely determines the spatial locations of brain regions by the diffusion process, avoiding subjective parameter selection. Subsequently, DGCL employs graph contrastive learning to optimize brain connections by eliminating individual differences in redundant connections unrelated to diseases, thereby enhancing the consistency of brain networks within the same group. Finally, the node-graph contrastive loss and classification loss jointly constrain the learning process of the model to obtain the reconstructed brain network, which is then used to analyze important brain connections. Validation on two datasets, ADNI and ABIDE, demonstrates that DGCL surpasses traditional methods and other deep learning models in predicting disease development stages. Significantly, the proposed model improves the efficiency and generalization of brain network construction. In summary, the proposed DGCL can be served as a universal brain network construction scheme, which can effectively identify important brain connections through generative paradigms and has the potential to provide disease interpretability support for neuroscience research.","<method>diffusion process</method>, <method>graph contrastive learning</method>, <method>node-graph contrastive loss</method>, <method>classification loss</method>"
2024,https://openalex.org/W4390819402,Medicine,Development and Validation of a Machine Learning Model to Predict Weekly Risk of Hypoglycemia in Patients with Type 1 Diabetes Based on Continuous Glucose Monitoring,"Aim: The aim of this study was to develop and validate a prediction model based on CGM data to identify a week-to-week risk profile of excessive hypoglycemia. Methods: We analyzed, trained, and internally tested two prediction models using CGM data from 205 type 1 diabetes patients with long-term CGM monitoring. A binary classification approach (XGBoost) combined with feature engineering deployed on the CGM signals was utilized to predict excessive hypoglycemia risk defined by two targets (TBR > 4% and the upper TBR 90th percentile limit) of time below range (TBR) the following week. The models were validated in two independent cohorts with a total of 253 additional patients. Results: A total of 61,470 weeks of CGM data were included in the analysis. The XGBoost models had a ROC-AUC of 0.83-0.87 (95% confidence interval [CI]; 0.83-0.88) in the test dataset. The external validation showed ROC-AUCs of 0.81-0.90. The most discriminative features included the low blood glucose index (LBGI), the glycemic risk assessment diabetes equation (GRADE), hypoglycemia, the TBR, waveform length, the CV and mean glucose during the previous week. This highlights that the pattern of hypoglycemia combined with glucose variability during the past week contains information on the risk of future hypoglycemia. Conclusion: Prediction models based on real-world CGM data can be used to predict the risk of hypoglycemia in the forthcoming week. The models showed good performance in both the internal and external validation cohorts.",<method>XGBoost</method>
2024,https://openalex.org/W4390882397,Medicine,"Regional disparities, age-related changes and sex-related differences in knee osteoarthritis","Abstract Background The objective of the study is to analyse the regions, age and sex differences in the incidence of knee osteoarthritis (KOA). Methods Data were extracted from the global burden of diseases (GBD) 2019 study, including incidence, years lived with disability (YLD), disability-adjusted life-years (DALYs) and risk factors. Estimated annual percentage changes (EAPCs) were calculated to quantify the temporal trends in age standardized rate (ASR) of KOA. Paired t-test, paired Wilcoxon signed-rank test and spearman correlation were performed to analyze the association of sex disparity in KOA and socio-demographic index (SDI). Results There were significant regional differences in the incidence of knee osteoarthritis. In 2019, South Korea had the highest incidence of knee osteoarthritis (474.85,95%UI:413.34–539.64) and Thailand had the highest increase in incidence of knee osteoarthritis (EAPC = 0.56, 95%CI = 0.54–0.58). Notably, higher incidence, YLD and DALYs of knee osteoarthritis were associated with areas with a high socio-demographic index ( r = 0.336, p &lt; 0.001; r = 0.324, p &lt; 0.001; r = 0.324, p &lt; 0.001). In terms of age differences, the greatest increase in the incidence of knee osteoarthritis was between the 35–39 and 40–44 age groups. (EAPC = 0.52, 95%CI = 0.40–0.63; 0.47, 95%CI = 0.36–0.58). In addition, there were significant sex differences in the disease burden of knee osteoarthritis ( P &lt; 0.001). Conclusions The incidence of knee osteoarthritis is significantly different with regions, age and sex.",No methods found.
2024,https://openalex.org/W4391243967,Medicine,Reviews and syntheses: Remotely sensed optical time series for monitoring vegetation productivity,"Abstract. Vegetation productivity is a critical indicator of global ecosystem health and is impacted by human activities and climate change. A wide range of optical sensing platforms, from ground-based to airborne and satellite, provide spatially continuous information on terrestrial vegetation status and functioning. As optical Earth observation (EO) data are usually routinely acquired, vegetation can be monitored repeatedly over time, reflecting seasonal vegetation patterns and trends in vegetation productivity metrics. Such metrics include gross primary productivity, net primary productivity, biomass, or yield. To summarize current knowledge, in this paper we systematically reviewed time series (TS) literature for assessing state-of-the-art vegetation productivity monitoring approaches for different ecosystems based on optical remote sensing (RS) data. As the integration of solar-induced fluorescence (SIF) data in vegetation productivity processing chains has emerged as a promising source, we also include this relatively recent sensor modality. We define three methodological categories to derive productivity metrics from remotely sensed TS of vegetation indices or quantitative traits: (i) trend analysis and anomaly detection, (ii) land surface phenology, and (iii) integration and assimilation of TS-derived metrics into statistical and process-based dynamic vegetation models (DVMs). Although the majority of used TS data streams originate from data acquired from satellite platforms, TS data from aircraft and unoccupied aerial vehicles have found their way into productivity monitoring studies. To facilitate processing, we provide a list of common toolboxes for inferring productivity metrics and information from TS data. We further discuss validation strategies of the RS data derived productivity metrics: (1) using in situ measured data, such as yield; (2) sensor networks of distinct sensors, including spectroradiometers, flux towers, or phenological cameras; and (3) inter-comparison of different productivity metrics. Finally, we address current challenges and propose a conceptual framework for productivity metrics derivation, including fully integrated DVMs and radiative transfer models here labelled as “Digital Twin”. This novel framework meets the requirements of multiple ecosystems and enables both an improved understanding of vegetation temporal dynamics in response to climate and environmental drivers and enhances the accuracy of vegetation productivity monitoring.","<method>trend analysis and anomaly detection</method>, <method>land surface phenology</method>, <method>integration and assimilation of time series-derived metrics into statistical and process-based dynamic vegetation models (DVMs)</method>, <method>fully integrated dynamic vegetation models (DVMs) and radiative transfer models (“Digital Twin” framework)</method>"
2024,https://openalex.org/W4391437034,Medicine,A methodical exploration of imaging modalities from dataset to detection through machine learning paradigms in prominent lung disease diagnosis: a review,"Abstract Background Lung diseases, both infectious and non-infectious, are the most prevalent cause of mortality overall in the world. Medical research has identified pneumonia, lung cancer, and Corona Virus Disease 2019 (COVID-19) as prominent lung diseases prioritized over others. Imaging modalities, including X-rays, computer tomography (CT) scans, magnetic resonance imaging (MRIs), positron emission tomography (PET) scans, and others, are primarily employed in medical assessments because they provide computed data that can be utilized as input datasets for computer-assisted diagnostic systems. Imaging datasets are used to develop and evaluate machine learning (ML) methods to analyze and predict prominent lung diseases. Objective This review analyzes ML paradigms, imaging modalities' utilization, and recent developments for prominent lung diseases. Furthermore, the research also explores various datasets available publically that are being used for prominent lung diseases. Methods The well-known databases of academic studies that have been subjected to peer review, namely ScienceDirect, arXiv, IEEE Xplore, MDPI, and many more, were used for the search of relevant articles. Applied keywords and combinations used to search procedures with primary considerations for review, such as pneumonia, lung cancer, COVID-19, various imaging modalities, ML, convolutional neural networks (CNNs), transfer learning, and ensemble learning. Results This research finding indicates that X-ray datasets are preferred for detecting pneumonia, while CT scan datasets are predominantly favored for detecting lung cancer. Furthermore, in COVID-19 detection, X-ray datasets are prioritized over CT scan datasets. The analysis reveals that X-rays and CT scans have surpassed all other imaging techniques. It has been observed that using CNNs yields a high degree of accuracy and practicability in identifying prominent lung diseases. Transfer learning and ensemble learning are complementary techniques to CNNs to facilitate analysis. Furthermore, accuracy is the most favored metric for assessment.","<method>machine learning (ML)</method>, <method>convolutional neural networks (CNNs)</method>, <method>transfer learning</method>, <method>ensemble learning</method>"
2024,https://openalex.org/W4392056032,Medicine,A novel fusion framework of deep bottleneck residual convolutional neural network for breast cancer classification from mammogram images,"With over 2.1 million new cases of breast cancer diagnosed annually, the incidence and mortality rate of this disease pose severe global health issues for women. Identifying the disease’s influence is the only practical way to lessen it immediately. Numerous research works have developed automated methods using different medical imaging to identify BC. Still, the precision of each strategy differs based on the available resources, the issue’s nature, and the dataset being used. We proposed a novel deep bottleneck convolutional neural network with a quantum optimization algorithm for breast cancer classification and diagnosis from mammogram images. Two novel deep architectures named three-residual blocks bottleneck and four-residual blocks bottle have been proposed with parallel and single paths. Bayesian Optimization (BO) has been employed to initialize hyperparameter values and train the architectures on the selected dataset. Deep features are extracted from the global average pool layer of both models. After that, a kernel-based canonical correlation analysis and entropy technique is proposed for the extracted deep features fusion. The fused feature set is further refined using an optimization technique named quantum generalized normal distribution optimization. The selected features are finally classified using several neural network classifiers, such as bi-layered and wide-neural networks. The experimental process was conducted on a publicly available mammogram imaging dataset named INbreast, and a maximum accuracy of 96.5% was obtained. Moreover, for the proposed method, the sensitivity rate is 96.45, the precision rate is 96.5, the F1 score value is 96.64, the MCC value is 92.97%, and the Kappa value is 92.97%, respectively. The proposed architectures are further utilized for the diagnosis process of infected regions. In addition, a detailed comparison has been conducted with a few recent techniques showing the proposed framework’s higher accuracy and precision rate.","<method>deep bottleneck convolutional neural network</method>, <method>quantum optimization algorithm</method>, <method>three-residual blocks bottleneck architecture</method>, <method>four-residual blocks bottleneck architecture</method>, <method>Bayesian Optimization (BO)</method>, <method>kernel-based canonical correlation analysis</method>, <method>entropy technique</method>, <method>quantum generalized normal distribution optimization</method>, <method>bi-layered neural network classifier</method>, <method>wide-neural network classifier</method>"
2024,https://openalex.org/W4392139441,Medicine,Artificial intelligence for radiographic imaging detection of caries lesions: a systematic review,"Abstract Background The aim of this systematic review is to evaluate the diagnostic performance of Artificial Intelligence (AI) models designed for the detection of caries lesion (CL). Materials and methods An electronic literature search was conducted on PubMed, Web of Science, SCOPUS, LILACS and Embase databases for retrospective, prospective and cross-sectional studies published until January 2023, using the following keywords: artificial intelligence (AI), machine learning (ML), deep learning (DL), artificial neural networks (ANN), convolutional neural networks (CNN), deep convolutional neural networks (DCNN), radiology, detection, diagnosis and dental caries (DC). The quality assessment was performed using the guidelines of QUADAS-2. Results Twenty articles that met the selection criteria were evaluated. Five studies were performed on periapical radiographs, nine on bitewings, and six on orthopantomography. The number of imaging examinations included ranged from 15 to 2900. Four studies investigated ANN models, fifteen CNN models, and two DCNN models. Twelve were retrospective studies, six cross-sectional and two prospective. The following diagnostic performance was achieved in detecting CL: sensitivity from 0.44 to 0.86, specificity from 0.85 to 0.98, precision from 0.50 to 0.94, PPV (Positive Predictive Value) 0.86, NPV (Negative Predictive Value) 0.95, accuracy from 0.73 to 0.98, area under the curve (AUC) from 0.84 to 0.98, intersection over union of 0.3–0.4 and 0.78, Dice coefficient 0.66 and 0.88, F1-score from 0.64 to 0.92. According to the QUADAS-2 evaluation, most studies exhibited a low risk of bias. Conclusion AI-based models have demonstrated good diagnostic performance, potentially being an important aid in CL detection. Some limitations of these studies are related to the size and heterogeneity of the datasets. Future studies need to rely on comparable, large, and clinically meaningful datasets. Protocol PROSPERO identifier: CRD42023470708","<method>Artificial Neural Networks (ANN)</method>, <method>Convolutional Neural Networks (CNN)</method>, <method>Deep Convolutional Neural Networks (DCNN)</method>"
2024,https://openalex.org/W4400227316,Medicine,FPSO/LNG hawser system lifetime assessment by Gaidai multivariate risk assessment method,"Abstract Floating Production Storage and Offloading (FPSO) unit being an offshore vessel, storing and producing crude oil, prior to crude oil being transported by accompanying shuttle tanker. Critical mooring/hawser strains during offloading operation have to be accurately predicted, in order to maintain operational safety and reliability. During certain types of offloading, excessive hawser tensions may occur, causing operational risks. Current study examines FPSO vessel’s dynamic reactions to hydrodynamic wave-induced loads, given realistic in situ environmental conditions, utilizing the AQWA software package. Current study advocates novel multi-dimensional spatiotemporal risks assessment approach, that is particularly well suited for large dataset analysis, based on numerical simulations (or measurements). Advocated multivariate reliability methodology may be useful for a variety of marine and offshore systems that must endure severe environmental stressors during their intended operational lifespan. Methodology, presented in this study provides advanced capability to efficiently, yet accurately evaluate dynamic system failure, hazard and damage risks, given representative dynamic record of multidimensional system’s inter-correlated critical components. Gaidai risk assessment method being novel dynamic multidimensional system’s lifetime assessment methodology. In order to validate and benchmark Gaidai risk assessment method, in this study it was applied to FPSO and potentially LNG (i.e., Liquid Natural Gas) vessels dynamics. Major advantage of the advocated approach is that there are no existing alternative risk assessment methods, able to tackle unlimited number of system’s dimensions. Accurate multi-dimensional risk assessment had been carried out, based on numerically simulated data, partially verified by available laboratory experiments. Confidence intervals had been given for predicted dynamic high-dimensional system risk levels.",<method>Gaidai risk assessment method</method>
2024,https://openalex.org/W4403839497,Medicine,When combinations of humans and AI are useful: A systematic review and meta-analysis,"Abstract Inspired by the increasing use of artificial intelligence (AI) to augment humans, researchers have studied human–AI systems involving different tasks, systems and populations. Despite such a large body of work, we lack a broad conceptual understanding of when combinations of humans and AI are better than either alone. Here we addressed this question by conducting a preregistered systematic review and meta-analysis of 106 experimental studies reporting 370 effect sizes. We searched an interdisciplinary set of databases (the Association for Computing Machinery Digital Library, the Web of Science and the Association for Information Systems eLibrary) for studies published between 1 January 2020 and 30 June 2023. Each study was required to include an original human-participants experiment that evaluated the performance of humans alone, AI alone and human–AI combinations. First, we found that, on average, human–AI combinations performed significantly worse than the best of humans or AI alone (Hedges’ g = −0.23; 95% confidence interval, −0.39 to −0.07). Second, we found performance losses in tasks that involved making decisions and significantly greater gains in tasks that involved creating content. Finally, when humans outperformed AI alone, we found performance gains in the combination, but when AI outperformed humans alone, we found losses. Limitations of the evidence assessed here include possible publication bias and variations in the study designs analysed. Overall, these findings highlight the heterogeneity of the effects of human–AI collaboration and point to promising avenues for improving human–AI systems.",No methods found.
2024,https://openalex.org/W4391174596,Medicine,Generative Large Language Models for Detection of Speech Recognition Errors in Radiology Reports,"This study evaluated the ability of generative large language models (LLMs) to detect speech recognition errors in radiology reports. A dataset of 3233 CT and MRI reports was assessed by radiologists for speech recognition errors. Errors were categorized as clinically significant or not clinically significant. Performances of five generative LLMs—GPT-3.5-turbo, GPT-4, text-davinci-003, Llama-v2–70B-chat, and Bard—were compared in detecting these errors, using manual error detection as the reference standard. Prompt engineering was used to optimize model performance. GPT-4 demonstrated high accuracy in detecting clinically significant errors (precision, 76.9%; recall, 100%; F1 score, 86.9%) and not clinically significant errors (precision, 93.9%; recall, 94.7%; F1 score, 94.3%). Text-davinci-003 achieved F1 scores of 72% and 46.6% for clinically significant and not clinically significant errors, respectively. GPT-3.5-turbo obtained 59.1% and 32.2% F1 scores, while Llama-v2–70B-chat scored 72.8% and 47.7%. Bard showed the lowest accuracy, with F1 scores of 47.5% and 20.9%. GPT-4 effectively identified challenging errors of nonsense phrases and internally inconsistent statements. Longer reports, resident dictation, and overnight shifts were associated with higher error rates. In conclusion, advanced generative LLMs show potential for automatic detection of speech recognition errors in radiology reports. Keywords: CT, Large Language Model, Machine Learning, MRI, Natural Language Processing, Radiology Reports, Speech, Unsupervised Learning Supplemental material is available for this article. © RSNA, 2024","<method>generative large language models (LLMs)</method>, <method>GPT-3.5-turbo</method>, <method>GPT-4</method>, <method>text-davinci-003</method>, <method>Llama-v2–70B-chat</method>, <method>Bard</method>, <method>prompt engineering</method>, <method>unsupervised learning</method>"
2024,https://openalex.org/W4391480252,Medicine,Performance of convolutional neural networks for the classification of brain tumors using magnetic resonance imaging,"Brain tumors are a diverse group of neoplasms that are challenging to detect and classify due to their varying characteristics. Deep learning techniques have proven to be effective in tumor classification. However, there is a lack of studies that compare these techniques using a common methodology. This work aims to analyze the performance of convolutional neural networks in the classification of brain tumors. We propose a network consisting of a few convolutional layers, batch normalization, and max-pooling. Then, we explore recent deep architectures, such as VGG, ResNet, EfficientNet, or ConvNeXt. The study relies on two magnetic resonance imaging datasets with over 3000 images of three types of tumors –gliomas, meningiomas, and pituitary tumors–, as well as images without tumors. We determine the optimal hyperparameters of the networks using the training and validation sets. The training and test sets are used to assess the performance of the models from different perspectives, including training from scratch, data augmentation, transfer learning, and fine-tuning. The experiments are performed using the TensorFlow and Keras libraries in Python. We compare the accuracy of the models and analyze their complexity based on the capacity of the networks, their training times, and image throughput. Several networks achieve high accuracy rates on both datasets, with the best model achieving 98.7% accuracy, which is on par with state-of-the-art methods. The average precision for each type of tumor is 94.3% for gliomas, 93.8% for meningiomas, 97.9% for pituitary tumors, and 95.3% for images without tumors. VGG is the largest model with over 171 million parameters, whereas MobileNet and EfficientNetB0 are the smallest ones with 3.2 and 5.9 million parameters, respectively. These two neural networks are also the fastest to train with 23.7 and 25.4 seconds per epoch, respectively. On the other hand, ConvNext is the slowest model with 58.2 seconds per epoch. Our custom model obtained the highest image throughput with 234.37 images per second, followed by MobileNet with 226 images per second. ConvNext obtained the smallest throughput with 97.35 images per second. ResNet, MobileNet, and EfficientNet are the most accurate networks, with MobileNet and EfficientNet demonstrating superior performance in terms of complexity. Most models achieve the best accuracy using transfer learning followed by a fine-tuning step. However, data augmentation does not contribute to increasing the accuracy of the models in general.","<method>convolutional neural networks</method>, <method>batch normalization</method>, <method>max-pooling</method>, <method>VGG</method>, <method>ResNet</method>, <method>EfficientNet</method>, <method>ConvNeXt</method>, <method>training from scratch</method>, <method>data augmentation</method>, <method>transfer learning</method>, <method>fine-tuning</method>"
2024,https://openalex.org/W4391641063,Medicine,Predictors for estimating subcortical EEG responses to continuous speech,"Perception of sounds and speech involves structures in the auditory brainstem that rapidly process ongoing auditory stimuli. The role of these structures in speech processing can be investigated by measuring their electrical activity using scalp-mounted electrodes. However, typical analysis methods involve averaging neural responses to many short repetitive stimuli that bear little relevance to daily listening environments. Recently, subcortical responses to more ecologically relevant continuous speech were detected using linear encoding models. These methods estimate the temporal response function (TRF), which is a regression model that minimises the error between the measured neural signal and a predictor derived from the stimulus. Using predictors that model the highly non-linear peripheral auditory system may improve linear TRF estimation accuracy and peak detection. Here, we compare predictors from both simple and complex peripheral auditory models for estimating brainstem TRFs on electroencephalography (EEG) data from 24 participants listening to continuous speech. We also investigate the data length required for estimating subcortical TRFs, and find that around 12 minutes of data is sufficient for clear wave V peaks (&gt;3 dB SNR) to be seen in nearly all participants. Interestingly, predictors derived from simple filterbank-based models of the peripheral auditory system yield TRF wave V peak SNRs that are not significantly different from those estimated using a complex model of the auditory nerve, provided that the nonlinear effects of adaptation in the auditory system are appropriately modelled. Crucially, computing predictors from these simpler models is more than 50 times faster compared to the complex model. This work paves the way for efficient modelling and detection of subcortical processing of continuous speech, which may lead to improved diagnosis metrics for hearing impairment and assistive hearing technology.","<method>linear encoding models</method>, <method>temporal response function (TRF)</method>, <method>regression model</method>"
2024,https://openalex.org/W4393044095,Medicine,"Comparative performance analysis of Boruta, SHAP, and Borutashap for disease diagnosis: A study with multiple machine learning algorithms","Interpretable machine learning models are instrumental in disease diagnosis and clinical decision-making, shedding light on relevant features. Notably, Boruta, SHAP (SHapley Additive exPlanations), and BorutaShap were employed for feature selection, each contributing to the identification of crucial features. These selected features were then utilized to train six machine learning algorithms, including LR, SVM, ETC, AdaBoost, RF, and LR, using diverse medical datasets obtained from public sources after rigorous preprocessing. The performance of each feature selection technique was evaluated across multiple ML models, assessing accuracy, precision, recall, and F1-score metrics. Among these, SHAP showcased superior performance, achieving average accuracies of 80.17%, 85.13%, 90.00%, and 99.55% across diabetes, cardiovascular, statlog, and thyroid disease datasets, respectively. Notably, the LGBM emerged as the most effective algorithm, boasting an average accuracy of 91.00% for most disease states. Moreover, SHAP enhanced the interpretability of the models, providing valuable insights into the underlying mechanisms driving disease diagnosis. This comprehensive study contributes significant insights into feature selection techniques and machine learning algorithms for disease diagnosis, benefiting researchers and practitioners in the medical field. Further exploration of feature selection methods and algorithms holds promise for advancing disease diagnosis methodologies, paving the way for more accurate and interpretable diagnostic models.","<method>Boruta</method>, <method>SHAP (SHapley Additive exPlanations)</method>, <method>BorutaShap</method>, <method>LR</method>, <method>SVM</method>, <method>ETC</method>, <method>AdaBoost</method>, <method>RF</method>, <method>LGBM</method>"
2024,https://openalex.org/W4394975332,Medicine,Distilling large language models for matching patients to clinical trials,"Abstract Objective The objective of this study is to systematically examine the efficacy of both proprietary (GPT-3.5, GPT-4) and open-source large language models (LLMs) (LLAMA 7B, 13B, 70B) in the context of matching patients to clinical trials in healthcare. Materials and methods The study employs a multifaceted evaluation framework, incorporating extensive automated and human-centric assessments along with a detailed error analysis for each model, and assesses LLMs’ capabilities in analyzing patient eligibility against clinical trial’s inclusion and exclusion criteria. To improve the adaptability of open-source LLMs, a specialized synthetic dataset was created using GPT-4, facilitating effective fine-tuning under constrained data conditions. Results The findings indicate that open-source LLMs, when fine-tuned on this limited and synthetic dataset, achieve performance parity with their proprietary counterparts, such as GPT-3.5. Discussion This study highlights the recent success of LLMs in the high-stakes domain of healthcare, specifically in patient-trial matching. The research demonstrates the potential of open-source models to match the performance of proprietary models when fine-tuned appropriately, addressing challenges like cost, privacy, and reproducibility concerns associated with closed-source proprietary LLMs. Conclusion The study underscores the opportunity for open-source LLMs in patient-trial matching. To encourage further research and applications in this field, the annotated evaluation dataset and the fine-tuned LLM, Trial-LLAMA, are released for public use.","<method>GPT-3.5</method>, <method>GPT-4</method>, <method>LLAMA 7B</method>, <method>LLAMA 13B</method>, <method>LLAMA 70B</method>, <method>fine-tuning</method>"
2024,https://openalex.org/W4390547705,Medicine,Impact of random oversampling and random undersampling on the performance of prediction models developed using observational health data,"Abstract Background There is currently no consensus on the impact of class imbalance methods on the performance of clinical prediction models. We aimed to empirically investigate the impact of random oversampling and random undersampling, two commonly used class imbalance methods, on the internal and external validation performance of prediction models developed using observational health data. Methods We developed and externally validated prediction models for various outcomes of interest within a target population of people with pharmaceutically treated depression across four large observational health databases. We used three different classifiers (lasso logistic regression, random forest, XGBoost) and varied the target imbalance ratio. We evaluated the impact on model performance in terms of discrimination and calibration. Discrimination was assessed using the area under the receiver operating characteristic curve (AUROC) and calibration was assessed using calibration plots. Results We developed and externally validated a total of 1,566 prediction models. On internal and external validation, random oversampling and random undersampling generally did not result in higher AUROCs. Moreover, we found overestimated risks, although this miscalibration could largely be corrected by recalibrating the models towards the imbalance ratios in the original dataset. Conclusions Overall, we found that random oversampling or random undersampling generally does not improve the internal and external validation performance of prediction models developed in large observational health databases. Based on our findings, we do not recommend applying random oversampling or random undersampling when developing prediction models in large observational health databases.","<method>random oversampling</method>, <method>random undersampling</method>, <method>lasso logistic regression</method>, <method>random forest</method>, <method>XGBoost</method>"
2024,https://openalex.org/W4390579686,Medicine,Auto-detection of the coronavirus disease by using deep convolutional neural networks and X-ray photographs,"Abstract The most widely used method for detecting Coronavirus Disease 2019 (COVID-19) is real-time polymerase chain reaction. However, this method has several drawbacks, including high cost, lengthy turnaround time for results, and the potential for false-negative results due to limited sensitivity. To address these issues, additional technologies such as computed tomography (CT) or X-rays have been employed for diagnosing the disease. Chest X-rays are more commonly used than CT scans due to the widespread availability of X-ray machines, lower ionizing radiation, and lower cost of equipment. COVID-19 presents certain radiological biomarkers that can be observed through chest X-rays, making it necessary for radiologists to manually search for these biomarkers. However, this process is time-consuming and prone to errors. Therefore, there is a critical need to develop an automated system for evaluating chest X-rays. Deep learning techniques can be employed to expedite this process. In this study, a deep learning-based method called Custom Convolutional Neural Network (Custom-CNN) is proposed for identifying COVID-19 infection in chest X-rays. The Custom-CNN model consists of eight weighted layers and utilizes strategies like dropout and batch normalization to enhance performance and reduce overfitting. The proposed approach achieved a classification accuracy of 98.19% and aims to accurately classify COVID-19, normal, and pneumonia samples.","<method>Deep learning</method>, <method>Custom Convolutional Neural Network (Custom-CNN)</method>, <method>dropout</method>, <method>batch normalization</method>"
2024,https://openalex.org/W4390913521,Medicine,A Review of Intraocular Lens Power Calculation Formulas Based on Artificial Intelligence,"Purpose: The proper selection of an intraocular lens power calculation formula is an essential aspect of cataract surgery. This study evaluated the accuracy of artificial intelligence-based formulas. Design: Systematic review. Methods: This review comprises articles evaluating the exactness of artificial intelligence-based formulas published from 2017 to July 2023. The papers were identified by a literature search of various databases (Pubmed/MEDLINE, Google Scholar, Crossref, Cochrane Library, Web of Science, and SciELO) using the terms “IOL formulas”, “FullMonte”, “Ladas”, “Hill-RBF”, “PEARL-DGS”, “Kane”, “Karmona”, “Hoffer QST”, and “Nallasamy”. In total, 25 peer-reviewed articles in English with the maximum sample and the largest number of compared formulas were examined. Results: The scores of the mean absolute error and percentage of patients within ±0.5 D and ±1.0 D were used to estimate the exactness of the formulas. In most studies the Kane formula obtained the smallest mean absolute error and the highest percentage of patients within ±0.5 D and ±1.0 D. Second place was typically achieved by the PEARL DGS formula. The limitations of the studies were also discussed. Conclusions: Kane seems to be the most accurate artificial intelligence-based formula. PEARL DGS also gives very good results. Hoffer QST, Karmona, and Nallasamy are the newest, and need further evaluation.","<method>FullMonte</method>, <method>Ladas</method>, <method>Hill-RBF</method>, <method>PEARL-DGS</method>, <method>Kane</method>, <method>Karmona</method>, <method>Hoffer QST</method>, <method>Nallasamy</method>"
2024,https://openalex.org/W4391202196,Medicine,Screening of miRNAs as prognostic biomarkers and their associated hub targets across Hepatocellular carcinoma using survival-based bioinformatics approach,"The hepatocellular carcinoma (HCC) incident rate is gradually increasing yearly despite all the research and efforts taken by scientific communities and governing bodies. Approximately 90% of all liver cancer cases belong to HCC. Usually, HCC patients approach the treatment in the late stages of this malignancy which becomes the primary cause of high mortality rate. The knowledge about molecular pathogenesis of HCC is limited and needs more attention from researchers to identify the driver genes and miRNAs, which causes to translate this information into clinical practice. Therefore, the key regulators identification of miRNA-mRNA regulatory network is essential to identify HCC-associated genes. We extracted microRNA (miRNA) and messenger RNA (mRNA) expression datasets of normal and tumor HCC patient samples from UCSC Xena followed by identifying differentially expressed genes (DEGs) and differentially expressed miRNAs (DEMs). Univariate and multivariate cox-proportional hazard models were utilized to identify DEMs having significant association with overall survival (OS). Kaplan-Meier (KM) plotter was used to validate the presence of prognostic DEMs. A risk-score model was used to evaluate the effectiveness of KM-plotter validated DEMs combination on risk of samples. Target DEGs of prognostic miRNAs were identified via sources such as miRTargetLink and miRWalk followed by their validation in an external microarray cohort and enrichment analysis. 562 DEGs and 388 DEMs were identified followed by seven prognostic miRNAs (i.e., miR-19a, miR-19b, miR-30d-5p, miR-424-5p, miR-3677-5p, miR-3913-5p, miR-7705) post univariate, multivariate, risk-score model evaluation and KM-plotter analyses. ANLN, MRO, CPEB3 were their targets and were also validated in GSE84005 dataset. The findings of this study decipher that most significant miRNAs and their identified target genes have association with apoptosis, inflammation, cell cycle regulation and cancer-related pathways, which appear to contribute to HCC pathogenesis and therefore, the discovery of new targets.","<method>univariate cox-proportional hazard model</method>, <method>multivariate cox-proportional hazard model</method>, <method>Kaplan-Meier (KM) plotter</method>, <method>risk-score model</method>"
2024,https://openalex.org/W4391480223,Medicine,GAN-based generation of realistic 3D volumetric data: A systematic review and taxonomy,"With the massive proliferation of data-driven algorithms, such as deep learning-based approaches, the availability of high-quality data is of great interest. Volumetric data is very important in medicine, as it ranges from disease diagnoses to therapy monitoring. When the dataset is sufficient, models can be trained to help doctors with these tasks. Unfortunately, there are scenarios where large amounts of data is unavailable. For example, rare diseases and privacy issues can lead to restricted data availability. In non-medical fields, the high cost of obtaining enough high-quality data can also be a concern. A solution to these problems can be the generation of realistic synthetic data using Generative Adversarial Networks (GANs). The existence of these mechanisms is a good asset, especially in healthcare, as the data must be of good quality, realistic, and without privacy issues. Therefore, most of the publications on volumetric GANs are within the medical domain. In this review, we provide a summary of works that generate realistic volumetric synthetic data using GANs. We therefore outline GAN-based methods in these areas with common architectures, loss functions and evaluation metrics, including their advantages and disadvantages. We present a novel taxonomy, evaluations, challenges, and research opportunities to provide a holistic overview of the current state of volumetric GANs.","<method>deep learning-based approaches</method>, <method>Generative Adversarial Networks (GANs)</method>, <method>GAN-based methods</method>"
2024,https://openalex.org/W4392302759,Medicine,Molecular structural modeling and physical characteristics of anti-breast cancer drugs via some novel topological descriptors and regression models,"Research is continuously being pursued to treat cancer patients and prevent the disease by developing new medicines. However, experimental drug design and development is a costly, time-consuming, and challenging process. Alternatively, computational and mathematical techniques play an important role in optimally achieving this goal. Among these mathematical techniques, topological indices (TIs) have many applications in the drugs used for the treatment of breast cancer. TIs can be utilized to forecast the effectiveness of drugs by providing molecular structure information and related properties of the drugs. In addition, these can assist in the design and discovery of new drugs by providing insights into the structure-property/structure-activity relationships. In this article, a Quantitative Structure Property Relationship (QSPR) analysis is carried out using some novel degree-based molecular descriptors and regression models to predict various properties (such as boiling point, melting point, enthalpy, flashpoint, molar refraction, molar volume, and polarizability) of 14 drugs used for the breast cancer treatment. The molecular structures of these drugs are topologically modeled through vertex and edge partitioning techniques of graph theory, and then linear regression models are developed to correlate the computed values with the experimental properties of the drugs to investigate the performance of TIs in predicting these properties. The results confirmed the potential of the considered topological indices as a tool for drug discovery and design in the field of breast cancer treatment.","<method>Quantitative Structure Property Relationship (QSPR) analysis</method>, <method>regression models</method>, <method>linear regression models</method>"
2024,https://openalex.org/W4392351452,Medicine,Response of soil erosion to vegetation and terrace changes in a small watershed on the Loess Plateau over the past 85 years,"Land use on the Chinese Loess Plateau has undergone dramatic changes over the past few decades. The implementation of a series of soil and water conservation measures has significantly altered the soil erosion, transportation, and deposition processes on the Loess Plateau. To effectively address and mitigate soil erosion, it is crucial to accurately quantify the soil loss rate and analyze the contributions of soil and water conservation measures over the past several decades. In this study, the Zhifanggou watershed, located in the hilly area of the Chinese Loess Plateau, is utilized as an illustrative example. Using historical data, remote sensing imagery, and on-site data of soil, vegetation, and soil conservation measures, we assessed the soil loss rates from 1938 to 2022 based on long-term land use changes, utilizing the Chinese Soil Loss Equation (CSLE) model. Furthermore, we employed a quantitative evaluation to assess the impacts of vegetation change and terrace construction on soil erosion. The findings of our study reveal significant transformations in land use. Farmland experienced an initial increase followed by a subsequent decline, while the opposite pattern was observed for forest land. The simulated soil loss rate for the entire watershed exhibited an upward trend, rising from 34.86 t·ha−1·yr−1 in 1938 to 104.11 t·ha−1·yr−1 in 1958, before declining to 56.98 t·ha−1·yr−1 in 1999 and reaching 5.87 t·ha−1·yr−1 in 2022. Attribution analysis showed that vegetation change exerted a dominant influence on recent soil erosion dynamics, accounting for 86.10 % of the total contribution in 2022, while terraces contributed 13.90 %. These findings clarify long-term soil erosion mechanisms and provide guidance for watershed soil and water conservation management.",No methods found.
2024,https://openalex.org/W4392499245,Medicine,Exploring the role of skin temperature in thermal sensation and thermal comfort: A comprehensive review,"The role of skin temperature as a determinant of human thermal sensation and comfort has gained increasing recognition, prompting a need for a systematic review. This review examines the relationship between skin temperature and thermal sensation, synthesizing insights from 172 studies published since 2000. It uniquely focuses on the indispensable roles of local and mean skin temperatures, a perspective not comprehensively explored in previous literature. The review reveals that the most common measurement points for skin temperature are the face and hands, attributed to their higher thermal sensitivity and the practical ease of measurement. It establishes a clear linear relationship between mean skin temperature and user thermal sensation, though affected by the choice of measurement locations and number of points. A notable finding is the varying impact of local skin temperature on overall thermal sensation in changing environments, with local heating less influential than cooling. The review also uncovers significant demographic variations in thermal sensation, strongly influenced by differing skin temperatures across age groups, genders, and climatic regions. For example, elderly populations exhibit a decreased temperature sensitivity, especially towards warmth. Gender differences are also significant, with females experiencing higher skin temperatures in warmer environments and lower in colder ones. Machine learning (ML)-based methods, especially classification tree-based and support vector machine (SVM) techniques, dominate in predicting thermal sensation and comfort, leveraging skin temperature data. While ML methods are prevalent, statistical regression-based approaches offer valuable empirical insights. Thermo-physiological model-based methods provide reliable results by incorporating detailed skin temperature dynamics. The review identifies a gap in understanding how gender, age, and regional differences influence thermal comfort in diverse environments. The study recommends conducting more nuanced experiments to dissect the impact of these factors and proposes the integration of individual demographic variables into ML models to personalize thermal comfort predictions.","<method>classification tree-based</method>, <method>support vector machine (SVM)</method>, <method>statistical regression-based approaches</method>, <method>thermo-physiological model-based methods</method>"
2024,https://openalex.org/W4392516399,Medicine,Artificial intelligence in dermatology: advancements and challenges in skin of color,"Abstract Artificial intelligence (AI) uses algorithms and large language models in computers to simulate human‐like problem‐solving and decision‐making. AI programs have recently acquired widespread popularity in the field of dermatology through the application of online tools in the assessment, diagnosis, and treatment of skin conditions. A literature review was conducted using PubMed and Google Scholar analyzing recent literature (from the last 10 years through October 2023) to evaluate current AI programs in use for dermatologic purposes, identifying challenges in this technology when applied to skin of color (SOC), and proposing future steps to enhance the role of AI in dermatologic practice. Challenges surrounding AI and its application to SOC stem from the underrepresentation of SOC in datasets and issues with image quality and standardization. With these existing issues, current AI programs inevitably do worse at identifying lesions in SOC. Additionally, only 30% of the programs identified in this review had data reported on their use in dermatology, specifically in SOC. Significant development of these applications is required for the accurate depiction of darker skin tone images in datasets. More research is warranted in the future to better understand the efficacy of AI in aiding diagnosis and treatment options for SOC patients.","<method>algorithms</method>, <method>large language models</method>"
2024,https://openalex.org/W4394011823,Medicine,"Artificial intelligence in lung cancer screening: Detection, classification, prediction, and prognosis","Abstract Background The exceptional capabilities of artificial intelligence (AI) in extracting image information and processing complex models have led to its recognition across various medical fields. With the continuous evolution of AI technologies based on deep learning, particularly the advent of convolutional neural networks (CNNs), AI presents an expanded horizon of applications in lung cancer screening, including lung segmentation, nodule detection, false‐positive reduction, nodule classification, and prognosis. Methodology This review initially analyzes the current status of AI technologies. It then explores the applications of AI in lung cancer screening, including lung segmentation, nodule detection, and classification, and assesses the potential of AI in enhancing the sensitivity of nodule detection and reducing false‐positive rates. Finally, it addresses the challenges and future directions of AI in lung cancer screening. Results AI holds substantial prospects in lung cancer screening. It demonstrates significant potential in improving nodule detection sensitivity, reducing false‐positive rates, and classifying nodules, while also showing value in predicting nodule growth and pathological/genetic typing. Conclusions AI offers a promising supportive approach to lung cancer screening, presenting considerable potential in enhancing nodule detection sensitivity, reducing false‐positive rates, and classifying nodules. However, the universality and interpretability of AI results need further enhancement. Future research should focus on the large‐scale validation of new deep learning‐based algorithms and multi‐center studies to improve the efficacy of AI in lung cancer screening.","<method>artificial intelligence (AI)</method>, <method>deep learning</method>, <method>convolutional neural networks (CNNs)</method>, <method>deep learning-based algorithms</method>"
2024,https://openalex.org/W4398141531,Medicine,Enhancing EfficientNetv2 with global and efficient channel attention mechanisms for accurate MRI-Based brain tumor classification,"Abstract The early and accurate diagnosis of brain tumors is critical for effective treatment planning, with Magnetic Resonance Imaging (MRI) serving as a key tool in the non-invasive examination of such conditions. Despite the advancements in Computer-Aided Diagnosis (CADx) systems powered by deep learning, the challenge of accurately classifying brain tumors from MRI scans persists due to the high variability of tumor appearances and the subtlety of early-stage manifestations. This work introduces a novel adaptation of the EfficientNetv2 architecture, enhanced with Global Attention Mechanism (GAM) and Efficient Channel Attention (ECA), aimed at overcoming these hurdles. This enhancement not only amplifies the model’s ability to focus on salient features within complex MRI images but also significantly improves the classification accuracy of brain tumors. Our approach distinguishes itself by meticulously integrating attention mechanisms that systematically enhance feature extraction, thereby achieving superior performance in detecting a broad spectrum of brain tumors. Demonstrated through extensive experiments on a large public dataset, our model achieves an exceptional high-test accuracy of 99.76%, setting a new benchmark in MRI-based brain tumor classification. Moreover, the incorporation of Grad-CAM visualization techniques sheds light on the model’s decision-making process, offering transparent and interpretable insights that are invaluable for clinical assessment. By addressing the limitations inherent in previous models, this study not only advances the field of medical imaging analysis but also highlights the pivotal role of attention mechanisms in enhancing the interpretability and accuracy of deep learning models for brain tumor diagnosis. This research sets the stage for advanced CADx systems, enhancing patient care and treatment outcomes.","<method>EfficientNetv2 architecture</method>, <method>Global Attention Mechanism (GAM)</method>, <method>Efficient Channel Attention (ECA)</method>, <method>Grad-CAM visualization techniques</method>"
2024,https://openalex.org/W4399128365,Medicine,Automated model discovery for human cardiac tissue: Discovering the best model and parameters,"For more than half a century, scientists have developed mathematical models to understand the behavior of the human heart. Today, we have dozens of heart tissue models to choose from, but selecting the best model is limited to expert professionals, prone to user bias, and vulnerable to human error. Here we take the human out of the loop and automate the process of model discovery. Towards this goal, we establish a novel incompressible orthotropic constitutive neural network to simultaneously discover both, model and parameters, that best explain human cardiac tissue. Notably, our network features 32 individual terms, 8 isotropic and 24 anisotropic, and fully autonomously selects the best model, out of more than 4 billion possible combinations of terms. We demonstrate that we can successfully train the network with triaxial shear and biaxial extension tests and systematically sparsify the parameter vector with L1-regularization. Strikingly, we robustly discover a four-term model that features a quadratic term in the second invariant I2, and exponential quadratic terms in the fourth and eighth invariants I4f, I4n, and I8fs. Importantly, our discovered model is interpretable by design and has parameters with well-defined physical units. We show that it outperforms popular existing myocardium models and generalizes well, from homogeneous laboratory tests to heterogeneous whole heart simulations. This is made possible by a new universal material subroutine that directly takes the discovered network weights as input. Automating the process of model discovery has the potential to democratize cardiac modeling, broaden participation in scientific discovery, and accelerate the development of innovative treatments for cardiovascular disease. Our source code, data, and examples are available at https://github.com/LivingMatterLab/CANN.","<method>incompressible orthotropic constitutive neural network</method>, <method>L1-regularization</method>"
2024,https://openalex.org/W4400077472,Medicine,Exploring the factors that influence academic performance in Jordanian higher education institutions,"Since the coronavirus 2019 (COVID-19) pandemic hit the world, many universities have used digital asynchronous learning tools such as Digital Learning Management Systems (DLMS) to continue the educational process. Despite its global usage, only a few studies have investigated its quality in the Jordanian context during the COVID-19 period from a quantitative and qualitative approach perspective. Thus, the current study aims to explore the factors that influence academic performance in Jordanian higher education institutions during the COVID-19 pandemic. A mixed methods research approach was employed to evaluate the quality of the teaching-learning process for Jordanian students in higher education institutions. The triangulated data focused on three core pillars namely if students saw a difference in their grades prior to, during, and after the pandemic, the challenges faced and improvement suggestions. Accordingly, the quantitative approach with an online questionnaire and the qualitative approach with structured interviews were applied to collect the required data from Jordanian students in higher education institutions. The results of the current study revealed that the evaluation of the teaching-learning process quality during the pandemic period affected students' academic performance in different proportions based on their specialization area. In addition, the study results also identified the most important challenges that faced the students during this period and suggested procedures to overcome them and improve the distance learning process. The current study offers empirical evidence on critical success factors underlying digital learning management systems in the COVID-19 era, which can help policymakers in Jordanian universities and the ministry of higher education and scientific research to improve the quality of the teaching-learning process in the Jordanian context.",No methods found.
2024,https://openalex.org/W4391068733,Medicine,Improving diabetes disease patients classification using stacking ensemble method with PIMA and local healthcare data,"Diabetes mellitus, a chronic metabolic disorder, continues to be a major public health issue around the world. It is estimated that one in every two diabetics is undiagnosed. Early diagnosis and management of diabetes can also prevent or delay the onset of complications. With the help of a variety of machine learning and deep learning models, stacking algorithms, and other techniques, our study's goal is to detect diseases early. In this study, we propose two stacking-based models for diabetes disease classification using a combination of the PIMA Indian diabetes dataset, simulated data, and additional data collected from a local healthcare facility. We use both the classical and deep neural network stacking ensemble methods to combine the predictions of multiple classification models and improve classification accuracy and robustness. In the evaluation protocol, we used both the train-test and cross-validation (CV) techniques to validate our proposed model. The highest accuracy is obtained by stacking ensemble with three NN architectures, resulting in an accuracy of 95.50 %, precision of 94 %, recall of 97 %, and f1-score of 96 % using 5-fold CV on simulation study. The stacked accuracy obtained from ML algorithms for the Pima Indian Diabetes dataset is 75.03 % using the train-test split protocol, while the accuracy obtained from the CV protocol is 77.10 % on the stacked model. The range of performance scores that outperformed the CV protocol 2.23 %–12 %. Our proposed method achieves a high accuracy range from 92 % to 95 %, precision, recall, and F1-score ranges from 88 % to 96 % using classical and deep neural network (NN)-based stacking method on the primary dataset. The proposed dataset and ensemble method could be useful in the early detection and treatment of diabetes, as well as in the advancement of machine learning and data analysis techniques in the healthcare industry.","<method>machine learning models</method>, <method>deep learning models</method>, <method>stacking algorithms</method>, <method>stacking-based models</method>, <method>classical stacking ensemble methods</method>, <method>deep neural network stacking ensemble methods</method>, <method>train-test technique</method>, <method>cross-validation (CV) technique</method>"
2024,https://openalex.org/W4391598337,Medicine,Federated Learning for Decentralized Artificial Intelligence in Melanoma Diagnostics,"Importance The development of artificial intelligence (AI)–based melanoma classifiers typically calls for large, centralized datasets, requiring hospitals to give away their patient data, which raises serious privacy concerns. To address this concern, decentralized federated learning has been proposed, where classifier development is distributed across hospitals. Objective To investigate whether a more privacy-preserving federated learning approach can achieve comparable diagnostic performance to a classical centralized (ie, single-model) and ensemble learning approach for AI-based melanoma diagnostics. Design, Setting, and Participants This multicentric, single-arm diagnostic study developed a federated model for melanoma-nevus classification using histopathological whole-slide images prospectively acquired at 6 German university hospitals between April 2021 and February 2023 and benchmarked it using both a holdout and an external test dataset. Data analysis was performed from February to April 2023. Exposures All whole-slide images were retrospectively analyzed by an AI-based classifier without influencing routine clinical care. Main Outcomes and Measures The area under the receiver operating characteristic curve (AUROC) served as the primary end point for evaluating the diagnostic performance. Secondary end points included balanced accuracy, sensitivity, and specificity. Results The study included 1025 whole-slide images of clinically melanoma-suspicious skin lesions from 923 patients, consisting of 388 histopathologically confirmed invasive melanomas and 637 nevi. The median (range) age at diagnosis was 58 (18-95) years for the training set, 57 (18-93) years for the holdout test dataset, and 61 (18-95) years for the external test dataset; the median (range) Breslow thickness was 0.70 (0.10-34.00) mm, 0.70 (0.20-14.40) mm, and 0.80 (0.30-20.00) mm, respectively. The federated approach (0.8579; 95% CI, 0.7693-0.9299) performed significantly worse than the classical centralized approach (0.9024; 95% CI, 0.8379-0.9565) in terms of AUROC on a holdout test dataset (pairwise Wilcoxon signed-rank, P &amp;amp;lt; .001) but performed significantly better (0.9126; 95% CI, 0.8810-0.9412) than the classical centralized approach (0.9045; 95% CI, 0.8701-0.9331) on an external test dataset (pairwise Wilcoxon signed-rank, P &amp;amp;lt; .001). Notably, the federated approach performed significantly worse than the ensemble approach on both the holdout (0.8867; 95% CI, 0.8103-0.9481) and external test dataset (0.9227; 95% CI, 0.8941-0.9479). Conclusions and Relevance The findings of this diagnostic study suggest that federated learning is a viable approach for the binary classification of invasive melanomas and nevi on a clinically representative distributed dataset. Federated learning can improve privacy protection in AI-based melanoma diagnostics while simultaneously promoting collaboration across institutions and countries. Moreover, it may have the potential to be extended to other image classification tasks in digital cancer histopathology and beyond.","<method>federated learning</method>, <method>classical centralized (single-model) approach</method>, <method>ensemble learning approach</method>"
2024,https://openalex.org/W4391655574,Medicine,The stability of cognitive abilities: A meta-analytic review of longitudinal studies.,"Cognitive abilities, including general intelligence and domain-specific abilities such as fluid reasoning, comprehension knowledge, working memory capacity, and processing speed, are regarded as some of the most stable psychological traits, yet there exist no large-scale systematic efforts to document the specific patterns by which their rank-order stability changes over age and time interval, or how their stability differs across abilities, tests, and populations. Determining the conditions under which cognitive abilities exhibit high or low degrees of stability is critical not just to theory development but to applied contexts in which cognitive assessments guide decisions regarding treatment and intervention decisions with lasting consequences for individuals. In order to supplement this important area of research, we present a meta-analysis of longitudinal studies investigating the stability of cognitive abilities. The meta-analysis relied on data from 205 longitudinal studies that involved a total of 87,408 participants, resulting in 1,288 test-retest correlation coefficients among manifest variables. For an age of 20 years and a test-retest interval of 5 years, we found a mean rank-order stability of ρ = .76. The effect of mean sample age on stability was best described by a negative exponential function, with low stability in preschool children, rapid increases in stability in childhood, and consistently high stability from late adolescence to late adulthood. This same functional form continued to best describe age trends in stability after adjusting for test reliability. Stability declined with increasing test-retest interval. This decrease flattened out from an interval of approximately 5 years onward. According to the age and interval moderation models, minimum stability sufficient for individual-level diagnostic decisions (",No methods found.
2024,https://openalex.org/W4391692539,Medicine,Exploration of Interpretability Techniques for Deep COVID-19 Classification Using Chest X-ray Images,"The outbreak of COVID-19 has shocked the entire world with its fairly rapid spread, and has challenged different sectors. One of the most effective ways to limit its spread is the early and accurate diagnosing of infected patients. Medical imaging, such as X-ray and computed tomography (CT), combined with the potential of artificial intelligence (AI), plays an essential role in supporting medical personnel in the diagnosis process. Thus, in this article, five different deep learning models (ResNet18, ResNet34, InceptionV3, InceptionResNetV2, and DenseNet161) and their ensemble, using majority voting, have been used to classify COVID-19, pneumoniæ and healthy subjects using chest X-ray images. Multilabel classification was performed to predict multiple pathologies for each patient, if present. Firstly, the interpretability of each of the networks was thoroughly studied using local interpretability methods—occlusion, saliency, input X gradient, guided backpropagation, integrated gradients, and DeepLIFT—and using a global technique—neuron activation profiles. The mean micro F1 score of the models for COVID-19 classifications ranged from 0.66 to 0.875, and was 0.89 for the ensemble of the network models. The qualitative results showed that the ResNets were the most interpretable models. This research demonstrates the importance of using interpretability methods to compare different models before making a decision regarding the best performing model.","<method>ResNet18</method>, <method>ResNet34</method>, <method>InceptionV3</method>, <method>InceptionResNetV2</method>, <method>DenseNet161</method>, <method>ensemble using majority voting</method>, <method>occlusion</method>, <method>saliency</method>, <method>input X gradient</method>, <method>guided backpropagation</method>, <method>integrated gradients</method>, <method>DeepLIFT</method>, <method>neuron activation profiles</method>"
2024,https://openalex.org/W4392380186,Medicine,Impacts of optimal control strategies on the HBV and COVID-19 co-epidemic spreading dynamics,"Abstract Different cross-sectional and clinical research studies investigated that chronic HBV infected individuals’ co-epidemic with COVID-19 infection will have more complicated liver infection than HBV infected individuals in the absence of COVID-19 infection. The main objective of this study is to investigate the optimal impacts of four time dependent control strategies on the HBV and COVID-19 co-epidemic transmission using compartmental modeling approach. The qualitative analyses of the model investigated the model solutions non-negativity and boundedness, calculated all the models effective reproduction numbers by applying the next generation operator approach, computed all the models disease-free equilibrium point (s) and endemic equilibrium point (s) and proved their local stability, shown the phenomenon of backward bifurcation by applying the Center Manifold criteria. By applied the Pontryagin’s Maximum principle, the study re-formulated and analyzed the co-epidemic model optimal control problem by incorporating four time dependent controlling variables. The study also carried out numerical simulations to verify the model qualitative results and to investigate the optimal impacts of the proposed optimal control strategies. The main finding of the study reveals that implementation of protections, COVID-19 vaccine, and treatment strategies simultaneously is the most effective optimal control strategy to tackle the HBV and COVID-19 co-epidemic spreading in the community.",No methods found.
2024,https://openalex.org/W4393131920,Medicine,Diabetic foot ulcers segmentation challenge report: Benchmark and analysis,"Monitoring the healing progress of diabetic foot ulcers is a challenging process. Accurate segmentation of foot ulcers can help podiatrists to quantitatively measure the size of wound regions to assist prediction of healing status. The main challenge in this field is the lack of publicly available manual delineation, which can be time consuming and laborious. Recently, methods based on deep learning have shown excellent results in automatic segmentation of medical images, however, they require large-scale datasets for training, and there is limited consensus on which methods perform the best. The 2022 Diabetic Foot Ulcers segmentation challenge was held in conjunction with the 2022 International Conference on Medical Image Computing and Computer Assisted Intervention, which sought to address these issues and stimulate progress in this research domain. A training set of 2000 images exhibiting diabetic foot ulcers was released with corresponding segmentation ground truth masks. Of the 72 (approved) requests from 47 countries, 26 teams used this data to develop fully automated systems to predict the true segmentation masks on a test set of 2000 images, with the corresponding ground truth segmentation masks kept private. Predictions from participating teams were scored and ranked according to their average Dice similarity coefficient of the ground truth masks and prediction masks. The winning team achieved a Dice of 0.7287 for diabetic foot ulcer segmentation. This challenge has now entered a live leaderboard stage where it serves as a challenging benchmark for diabetic foot ulcer segmentation.",<method>deep learning</method>
2024,https://openalex.org/W4394620240,Medicine,Human-AI interaction in skin cancer diagnosis: a systematic review and meta-analysis,"Abstract The development of diagnostic tools for skin cancer based on artificial intelligence (AI) is increasing rapidly and will likely soon be widely implemented in clinical use. Even though the performance of these algorithms is promising in theory, there is limited evidence on the impact of AI assistance on human diagnostic decisions. Therefore, the aim of this systematic review and meta-analysis was to study the effect of AI assistance on the accuracy of skin cancer diagnosis. We searched PubMed, Embase, IEE Xplore, Scopus and conference proceedings for articles from 1/1/2017 to 11/8/2022. We included studies comparing the performance of clinicians diagnosing at least one skin cancer with and without deep learning-based AI assistance. Summary estimates of sensitivity and specificity of diagnostic accuracy with versus without AI assistance were computed using a bivariate random effects model. We identified 2983 studies, of which ten were eligible for meta-analysis. For clinicians without AI assistance, pooled sensitivity was 74.8% (95% CI 68.6–80.1) and specificity was 81.5% (95% CI 73.9–87.3). For AI-assisted clinicians, the overall sensitivity was 81.1% (95% CI 74.4–86.5) and specificity was 86.1% (95% CI 79.2–90.9). AI benefitted medical professionals of all experience levels in subgroup analyses, with the largest improvement among non-dermatologists. No publication bias was detected, and sensitivity analysis revealed that the findings were robust. AI in the hands of clinicians has the potential to improve diagnostic accuracy in skin cancer diagnosis. Given that most studies were conducted in experimental settings, we encourage future studies to further investigate these potential benefits in real-life settings.","<method>deep learning-based AI assistance</method>, <method>bivariate random effects model</method>"
2024,https://openalex.org/W4396679651,Medicine,Predictive Modelling of Critical Vital Signs in ICU Patients by Machine Learning: An Early Warning System for Improved Patient Outcomes,"Accurate monitoring of vital signs in an ICU is integral to understanding overall physical well-being for patients. Our research endeavor employed machine learning techniques to construct a predictive classification model utilizing continuous ICU vital sign measurements. The primary aim was to develop an early warning system capable of forecasting whether vital indicators would reach critical values within one hour; our ultimate aim was to enable healthcare professionals, including nurses and doctors, to intervene proactively, preventing emergency situations which could result in organ dysfunction or mortality. Our comprehensive dataset comprises vital sign measurements, lab test results, procedures, and medications from over 50,000 patients collected via rigorous preprocessing procedures like data cleansing, bias correction, feature extraction and selection to produce an insightful dataset with distinguishing attributes. After selecting an algorithmic set that included Decision Trees (DT), Support Vector Machines (SVM), Recurrent Neural Networks (RNN), and Long Short-Term Memory (LSTM), to predict critical vital signs in ICU patients one hour in advance - such as Heart Rate, SpO2, Mean Artery Pressure (MAP), Respiratory Rate (RR), and Systolic Blood Pressure (SBP). Our models included Heart Rate prediction as well as respiratory Rate/RR predictions/SBP estimation models. The results of the study demonstrated the efficacy and accuracy of machine learning methods designed to anticipate imminent changes to vital signs. Utilizing such predictive models, healthcare providers can increase their capacity to address potential complications before they occur, ultimately leading to improved patient outcomes in challenging settings.","<method>Decision Trees (DT)</method>, <method>Support Vector Machines (SVM)</method>, <method>Recurrent Neural Networks (RNN)</method>, <method>Long Short-Term Memory (LSTM)</method>"
2024,https://openalex.org/W4396834505,Medicine,Enhancing cervical cancer detection and robust classification through a fusion of deep learning models,"Abstract Cervical cancer, the second most prevalent cancer affecting women, arises from abnormal cell growth in the cervix, a crucial anatomical structure within the uterus. The significance of early detection cannot be overstated, prompting the use of various screening methods such as Pap smears, colposcopy, and Human Papillomavirus (HPV) testing to identify potential risks and initiate timely intervention. These screening procedures encompass visual inspections, Pap smears, colposcopies, biopsies, and HPV-DNA testing, each demanding the specialized knowledge and skills of experienced physicians and pathologists due to the inherently subjective nature of cancer diagnosis. In response to the imperative for efficient and intelligent screening, this article introduces a groundbreaking methodology that leverages pre-trained deep neural network models, including Alexnet, Resnet-101, Resnet-152, and InceptionV3, for feature extraction. The fine-tuning of these models is accompanied by the integration of diverse machine learning algorithms, with ResNet152 showcasing exceptional performance, achieving an impressive accuracy rate of 98.08%. It is noteworthy that the SIPaKMeD dataset, publicly accessible and utilized in this study, contributes to the transparency and reproducibility of our findings. The proposed hybrid methodology combines aspects of DL and ML for cervical cancer classification. Most intricate and complicated features from images can be extracted through DL. Further various ML algorithms can be implemented on extracted features. This innovative approach not only holds promise for significantly improving cervical cancer detection but also underscores the transformative potential of intelligent automation within the realm of medical diagnostics, paving the way for more accurate and timely interventions.","<method>Alexnet</method>, <method>Resnet-101</method>, <method>Resnet-152</method>, <method>InceptionV3</method>, <method>deep neural network models</method>, <method>fine-tuning</method>, <method>machine learning algorithms</method>, <method>hybrid methodology combining DL and ML</method>"
2024,https://openalex.org/W4399258783,Medicine,A review of uncertainty quantification in medical image analysis: Probabilistic and non-probabilistic methods,"The comprehensive integration of machine learning healthcare models within clinical practice remains suboptimal, notwithstanding the proliferation of high-performing solutions reported in the literature. A predominant factor hindering widespread adoption pertains to an insufficiency of evidence affirming the reliability of the aforementioned models. Recently, uncertainty quantification methods have been proposed as a potential solution to quantify the reliability of machine learning models and thus increase the interpretability and acceptability of the results. In this review, we offer a comprehensive overview of the prevailing methods proposed to quantify the uncertainty inherent in machine learning models developed for various medical image tasks. Contrary to earlier reviews that exclusively focused on probabilistic methods, this review also explores non-probabilistic approaches, thereby furnishing a more holistic survey of research pertaining to uncertainty quantification for machine learning models. Analysis of medical images with the summary and discussion on medical applications and the corresponding uncertainty evaluation protocols are presented, which focus on the specific challenges of uncertainty in medical image analysis. We also highlight some potential future research work at the end. Generally, this review aims to allow researchers from both clinical and technical backgrounds to gain a quick and yet in-depth understanding of the research in uncertainty quantification for medical image analysis machine learning models.","<method>uncertainty quantification methods</method>, <method>probabilistic methods</method>, <method>non-probabilistic approaches</method>"
2024,https://openalex.org/W4399442306,Medicine,Integrative analysis of AI-driven optimization in HIV treatment regimens,"The integration of artificial intelligence (AI) into HIV treatment regimens has revolutionized the approach to personalized care and optimization strategies. This study presents an in-depth analysis of the role of AI in transforming HIV treatment, focusing on its ability to tailor therapy to individual patient needs and enhance treatment outcomes. AI-driven optimization in HIV treatment involves the utilization of advanced algorithms and computational techniques to analyze vast amounts of patient data, including genetic information, viral load measurements, and treatment history. By harnessing the power of machine learning and predictive analytics, AI algorithms can identify patterns and trends in patient data that may not be readily apparent to human clinicians. One of the key benefits of AI-driven optimization is its ability to personalize treatment regimens based on individual patient characteristics and disease progression. By considering factors such as drug resistance profiles, comorbidities, and lifestyle factors, AI algorithms can recommend the most effective and well-tolerated treatment options for each patient, leading to improved adherence and clinical outcomes. Furthermore, AI enables continuous monitoring and adjustment of treatment regimens in real time, allowing healthcare providers to respond rapidly to changes in patient status and evolving viral dynamics. This proactive approach to HIV management can help prevent treatment failure and the development of drug resistance, ultimately leading to better long-term outcomes for patients. Despite its transformative potential, AI-driven optimization in HIV treatment is not without challenges. Ethical considerations, data privacy concerns, and the need for robust validation and regulatory oversight are all important factors that must be addressed to ensure the safe and effective implementation of AI algorithms in clinical practice. In conclusion, the integrative analysis presented in this study underscores the significant impact of AI-driven optimization on the personalization and optimization of HIV treatment regimens. By leveraging AI technologies, healthcare providers can tailor treatment approaches to individual patient needs, leading to improved outcomes and quality of life for people living with HIV. Keywords: Integrative Analysis, AI- Driven, Optimization, HIV Treatment, Regimens.","<method>machine learning</method>, <method>predictive analytics</method>"
2024,https://openalex.org/W4390506438,Medicine,Artificial intelligence for oral squamous cell carcinoma detection based on oral photographs: A comprehensive literature review,"Abstract Introduction Oral squamous cell carcinoma (OSCC) presents a significant global health challenge. The integration of artificial intelligence (AI) and computer vision holds promise for the early detection of OSCC through the analysis of digitized oral photographs. This literature review explores the landscape of AI‐driven OSCC automatic detection, assessing both the performance and limitations of the current state of the art. Materials and Methods An electronic search using several data base was conducted, and a systematic review performed in accordance with PRISMA guidelines (CRD42023441416). Results Several studies have demonstrated remarkable results for this task, consistently achieving sensitivity rates exceeding 85% and accuracy rates surpassing 90%, often encompassing around 1000 images. The review scrutinizes these studies, shedding light on their methodologies, including the use of recent machine learning and pattern recognition approaches coupled with different supervision strategies. However, comparing the results from different papers is challenging due to variations in the datasets used. Discussion Considering these findings, this review underscores the urgent need for more robust and reliable datasets in the field of OSCC detection. Furthermore, it highlights the potential of advanced techniques such as multi‐task learning, attention mechanisms, and ensemble learning as crucial tools in enhancing the accuracy and sensitivity of OSCC detection through oral photographs. Conclusion These insights collectively emphasize the transformative impact of AI‐driven approaches on early OSCC diagnosis, with the potential to significantly improve patient outcomes and healthcare practices.","<method>machine learning</method>, <method>pattern recognition</method>, <method>multi-task learning</method>, <method>attention mechanisms</method>, <method>ensemble learning</method>"
2024,https://openalex.org/W4391096835,Medicine,Diagnostic Performance Comparison between Generative AI and Physicians: A Systematic Review and Meta-Analysis,"Abstract Background The rapid advancement of generative artificial intelligence (AI) has led to the wide dissemination of models with exceptional understanding and generation of human language. Their integration into healthcare has shown potential for improving medical diagnostics, yet a comprehensive diagnostic performance evaluation of generative AI models and the comparison of their diagnostic performance with that of physicians has not been extensively explored. Methods In this systematic review and meta-analysis, a comprehensive search of Medline, Scopus, Web of Science, Cochrane Central, and MedRxiv was conducted for studies published from June 2018 through December 2023, focusing on those that validate generative AI models for diagnostic tasks. The risk of bias was assessed using the Prediction Model Study Risk of Bias Assessment Tool. Meta-regression was performed to summarize the performance of the models and to compare the accuracy of the models with that of physicians. Results The search resulted in 54 studies being included in the meta-analysis. Nine generative AI models were evaluated across 17 medical specialties. The quality assessment indicated a high risk of bias in the majority of studies, primarily due to small sample sizes. The overall accuracy for generative AI models across 54 studies was 56.9% (95% confidence interval [CI]: 51.0–62.7%). The meta-analysis demonstrated that, on average, physicians exceeded the accuracy of the models (difference in accuracy: 14.4% [95% CI: 4.9–23.8%], p-value =0.004). However, both Prometheus (Bing) and GPT-4 showed slightly better performance compared to non-experts (-2.3% [95% CI: -27.0–22.4%], p-value = 0.848 and -0.32% [95% CI: -14.4–13.7%], p-value = 0.962), but slightly underperformed when compared to experts (10.9% [95% CI: -13.1–35.0%], p-value = 0.356 and 12.9% [95% CI: 0.15–25.7%], p-value = 0.048). The sub-analysis revealed significantly improved accuracy in the fields of Gynecology, Pediatrics, Orthopedic surgery, Plastic surgery, and Otolaryngology, while showing reduced accuracy for Neurology, Psychiatry, Rheumatology, and Endocrinology compared to that of General Medicine. No significant heterogeneity was observed based on the risk of bias. Conclusions Generative AI exhibits promising diagnostic capabilities, with accuracy varying significantly by model and medical specialty. Although they have not reached the reliability of expert physicians, the findings suggest that generative AI models have the potential to enhance healthcare delivery and medical education, provided they are integrated with caution and their limitations are well-understood. Key Points Question: What is the diagnostic accuracy of generative AI models and how does this accuracy compare to that of physicians? Findings: This meta-analysis found that generative AI models have a pooled accuracy of 56.9% (95% confidence interval: 51.0–62.7%). The accuracy of expert physicians exceeds that of AI in all specialties, however, some generative AI models are comparable to non-expert physicians. Meaning: The diagnostic performance of generative AI models suggests that they do not match the level of experienced physicians but that they may have potential applications in healthcare delivery and medical education.",<method>generative AI models</method>
2024,https://openalex.org/W4391113105,Medicine,A systematic review of artificial intelligence techniques for oral cancer detection,"Oral cancer is a form of cancer that develops in the tissue of an oral cavity. Detection at an early stage is necessary to prevent the mortality rate in cancer patients. Artificial intelligence (AI) techniques play a significant role in assisting with diagnosing oral cancer. The AI techniques provide better detection accuracy and help automate oral cancer detection. The study shows that AI has a wide range of algorithms and provides outcomes in the most precise manner possible. We provide an overview of different input types and apply an appropriate algorithm to detect oral cancer. We aim to provide an overview of various AI techniques that can be used to automate oral cancer detection and to analyze these techniques to improve the efficiency and accuracy of oral cancer screening. We provide a summary of various methods available for oral cancer detection. We cover different input image formats, their processing, and the need for segmentation and feature extraction. We further include a list of other conventional strategies. We focus on various AI techniques for detecting oral cancer, including deep learning, machine learning, fuzzy computing, data mining, and genetic algorithms, and evaluates their benefits and drawbacks. The larger part of the articles focused on deep learning (37%) methods, followed by machine learning (32%), genetic algorithms (12%), data mining techniques (10%), and fuzzy computing (9%) for oral cancer detection.","<method>deep learning</method>, <method>machine learning</method>, <method>fuzzy computing</method>, <method>data mining</method>, <method>genetic algorithms</method>"
2024,https://openalex.org/W4391164242,Medicine,Socially Aware Synthetic Data Generation for Suicidal Ideation Detection Using Large Language Models,"Suicidal ideation detection is a vital research area that holds great potential for improving mental health support systems. However, the sensitivity surrounding suicide-related data poses challenges in accessing large-scale, annotated datasets necessary for training effective machine learning models. To address this limitation, we introduce an innovative strategy that leverages the capabilities of generative AI models, such as ChatGPT, Flan-T5, and Llama, to create synthetic data for suicidal ideation detection. Our data generation approach is grounded in social factors extracted from psychology literature and aims to ensure coverage of essential information related to suicidal ideation. In our study, we benchmarked against state-of-the-art NLP classification models, specifically, those centered around the BERT family structures. When trained on the real-world dataset, UMD, these conventional models tend to yield F1-scores ranging from 0.75 to 0.87. Our synthetic data-driven method, informed by social factors, offers consistent F1-scores of 0.82 for both models, suggesting that the richness of topics in synthetic data can bridge the performance gap across different model complexities. Most impressively, when we combined a mere 30% of the UMD dataset with our synthetic data, we witnessed a substantial increase in performance, achieving an F1-score of 0.88 on the UMD test set. Such results underscore the cost-effectiveness and potential of our approach in confronting major challenges in the field, such as data scarcity and the quest for diversity in data representation.","<method>generative AI models</method>, <method>ChatGPT</method>, <method>Flan-T5</method>, <method>Llama</method>, <method>NLP classification models</method>, <method>BERT family structures</method>"
2024,https://openalex.org/W4391613588,Medicine,Artificial intelligence framework for heart disease classification from audio signals,"Abstract As cardiovascular disorders are prevalent, there is a growing demand for reliable and precise diagnostic methods within this domain. Audio signal-based heart disease detection is a promising area of research that leverages sound signals generated by the heart to identify and diagnose cardiovascular disorders. Machine learning (ML) and deep learning (DL) techniques are pivotal in classifying and identifying heart disease from audio signals. This study investigates ML and DL techniques to detect heart disease by analyzing noisy sound signals. This study employed two subsets of datasets from the PASCAL CHALLENGE having real heart audios. The research process and visually depict signals using spectrograms and Mel-Frequency Cepstral Coefficients (MFCCs). We employ data augmentation to improve the model’s performance by introducing synthetic noise to the heart sound signals. In addition, a feature ensembler is developed to integrate various audio feature extraction techniques. Several machine learning and deep learning classifiers are utilized for heart disease detection. Among the numerous models studied and previous study findings, the multilayer perceptron model performed best, with an accuracy rate of 95.65%. This study demonstrates the potential of this methodology in accurately detecting heart disease from sound signals. These findings present promising opportunities for enhancing medical diagnosis and patient care.","<method>machine learning (ML)</method>, <method>deep learning (DL)</method>, <method>multilayer perceptron model</method>"
2024,https://openalex.org/W4391820319,Medicine,Investigation on explainable machine learning models to predict chronic kidney diseases,"Chronic kidney disease (CKD) is a major worldwide health problem, affecting a large proportion of the world's population and leading to higher morbidity and death rates. The early stages of CKD sometimes present without visible symptoms, causing patients to be unaware. Early detection and treatments are critical in reducing complications and improving the overall quality of life for people afflicted. In this work, we investigate the use of an explainable artificial intelligence (XAI)-based strategy, leveraging clinical characteristics, to predict CKD. This study collected clinical data from 491 patients, comprising 56 with CKD and 435 without CKD, encompassing clinical, laboratory, and demographic variables. To develop the predictive model, five machine learning (ML) methods, namely logistic regression (LR), random forest (RF), decision tree (DT), Naïve Bayes (NB), and extreme gradient boosting (XGBoost), were employed. The optimal model was selected based on accuracy and area under the curve (AUC). Additionally, the SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) algorithms were utilized to demonstrate the influence of the features on the optimal model. Among the five models developed, the XGBoost model achieved the best performance with an AUC of 0.9689 and an accuracy of 93.29%. The analysis of feature importance revealed that creatinine, glycosylated hemoglobin type A1C (HgbA1C), and age were the three most influential features in the XGBoost model. The SHAP force analysis further illustrated the model's visualization of individualized CKD predictions. For further insights into individual predictions, we also utilized the LIME algorithm. This study presents an interpretable ML-based approach for the early prediction of CKD. The SHAP and LIME methods enhance the interpretability of ML models and help clinicians better understand the rationale behind the predicted outcomes more effectively.","<method>logistic regression (LR)</method>, <method>random forest (RF)</method>, <method>decision tree (DT)</method>, <method>Naïve Bayes (NB)</method>, <method>extreme gradient boosting (XGBoost)</method>, <method>SHAP (SHapley Additive exPlanations)</method>, <method>LIME (Local Interpretable Model-agnostic Explanations)</method>"
2024,https://openalex.org/W4392058879,Medicine,Prostate cancer grading framework based on deep transfer learning and Aquila optimizer,"Abstract Prostate cancer is the one of the most dominant cancer among males. It represents one of the leading cancer death causes worldwide. Due to the current evolution of artificial intelligence in medical imaging, deep learning has been successfully applied in diseases diagnosis. However, most of the recent studies in prostate cancer classification suffers from either low accuracy or lack of data. Therefore, the present work introduces a hybrid framework for early and accurate classification and segmentation of prostate cancer using deep learning. The proposed framework consists of two stages, namely classification stage and segmentation stage. In the classification stage, 8 pretrained convolutional neural networks were fine-tuned using Aquila optimizer and used to classify patients of prostate cancer from normal ones. If the patient is diagnosed with prostate cancer, segmenting the cancerous spot from the overall image using U-Net can help in accurate diagnosis, and here comes the importance of the segmentation stage. The proposed framework is trained on 3 different datasets in order to generalize the framework. The best reported classification accuracies of the proposed framework are 88.91% using MobileNet for the “ISUP Grade-wise Prostate Cancer” dataset and 100% using MobileNet and ResNet152 for the “Transverse Plane Prostate Dataset” dataset with precisions 89.22% and 100%, respectively. U-Net model gives an average segmentation accuracy and AUC of 98.46% and 0.9778, respectively, using the “PANDA: Resized Train Data (512 × 512)” dataset. The results give an indicator of the acceptable performance of the proposed framework.","<method>pretrained convolutional neural networks</method>, <method>Aquila optimizer</method>, <method>MobileNet</method>, <method>ResNet152</method>, <method>U-Net</method>"
2024,https://openalex.org/W4392714393,Medicine,Fostering a Safety Culture in Manufacturing Industry through Safety Behavior: A Structural Equation Modelling Approach,"Creating a robust safety management system is crucial for fostering a culture of safety in the workplace, particularly in industries like manufacturing where improvements are still needed. This study aimed to assess the impact of safety behavior on safety culture within the manufacturing sector. Employing a quantitative approach, questionnaires were distributed to 342 employees in manufacturing firms during data collection. The collected data underwent analysis using Structural Equation Modeling through IBM-SPSS-AMOS 24.0 to test the proposed model. The study findings revealed that components of safety behavior, specifically safety compliance and safety leadership, have a significant influence on safety culture. This implies that prioritizing safety behavior and culture is vital for occupational safety and health, aligning with guidelines set by responsible entities to ensure a secure work environment. The insights gained from this research can be instrumental in highlighting the importance of safety culture, the pivotal role of leadership, the complex nature of safety culture, and the potential for measuring and enhancing it. By understanding these implications, organizations can foster a safety-centric culture that not only protects employees but also enhances overall performance. Additionally, this research contributed to the existing literature by examining an integrated higher-order construct model using the SEM technique, predicting the model by 53 percent. The insights garnered from this study are applicable to various types of firms, emphasizing the integral role of safety culture in any organization.",<method>Structural Equation Modeling</method>
2024,https://openalex.org/W4393222088,Medicine,Methodological insights into ChatGPT’s screening performance in systematic reviews,"Abstract Background The screening process for systematic reviews and meta-analyses in medical research is a labor-intensive and time-consuming task. While machine learning and deep learning have been applied to facilitate this process, these methods often require training data and user annotation. This study aims to assess the efficacy of ChatGPT, a large language model based on the Generative Pretrained Transformers (GPT) architecture, in automating the screening process for systematic reviews in radiology without the need for training data. Methods A prospective simulation study was conducted between May 2nd and 24th, 2023, comparing ChatGPT’s performance in screening abstracts against that of general physicians (GPs). A total of 1198 abstracts across three subfields of radiology were evaluated. Metrics such as sensitivity, specificity, positive and negative predictive values (PPV and NPV), workload saving, and others were employed. Statistical analyses included the Kappa coefficient for inter-rater agreement, ROC curve plotting, AUC calculation, and bootstrapping for p-values and confidence intervals. Results ChatGPT completed the screening process within an hour, while GPs took an average of 7–10 days. The AI model achieved a sensitivity of 95% and an NPV of 99%, slightly outperforming the GPs’ sensitive consensus (i.e., including records if at least one person includes them). It also exhibited remarkably low false negative counts and high workload savings, ranging from 40 to 83%. However, ChatGPT had lower specificity and PPV compared to human raters. The average Kappa agreement between ChatGPT and other raters was 0.27. Conclusions ChatGPT shows promise in automating the article screening phase of systematic reviews, achieving high sensitivity and workload savings. While not entirely replacing human expertise, it could serve as an efficient first-line screening tool, particularly in reducing the burden on human resources. Further studies are needed to fine-tune its capabilities and validate its utility across different medical subfields.","<method>machine learning</method>, <method>deep learning</method>, <method>ChatGPT</method>, <method>Generative Pretrained Transformers (GPT) architecture</method>"
2024,https://openalex.org/W4393991916,Medicine,Advancing Ligand Docking through Deep Learning: Challenges and Prospects in Virtual Screening,"ConspectusMolecular docking, also termed ligand docking (LD), is a pivotal element of structure-based virtual screening (SBVS) used to predict the binding conformations and affinities of protein–ligand complexes. Traditional LD methodologies rely on a search and scoring framework, utilizing heuristic algorithms to explore binding conformations and scoring functions to evaluate binding strengths. However, to meet the efficiency demands of SBVS, these algorithms and functions are often simplified, prioritizing speed over accuracy.The emergence of deep learning (DL) has exerted a profound impact on diverse fields, ranging from natural language processing to computer vision and drug discovery. DeepMind's AlphaFold2 has impressively exhibited its ability to accurately predict protein structures solely from amino acid sequences, highlighting the remarkable potential of DL in conformation prediction. This groundbreaking advancement circumvents the traditional search-scoring frameworks in LD, enhancing both accuracy and processing speed and thereby catalyzing a broader adoption of DL algorithms in binding pose prediction. Nevertheless, a consensus on certain aspects remains elusive.In this Account, we delineate the current status of employing DL to augment LD within the VS paradigm, highlighting our contributions to this domain. Furthermore, we discuss the challenges and future prospects, drawing insights from our scholarly investigations. Initially, we present an overview of VS and LD, followed by an introduction to DL paradigms, which deviate significantly from traditional search-scoring frameworks. Subsequently, we delve into the challenges associated with the development of DL-based LD (DLLD), encompassing evaluation metrics, application scenarios, and physical plausibility of the predicted conformations. In the evaluation of LD algorithms, it is essential to recognize the multifaceted nature of the metrics. While the accuracy of binding pose prediction, often measured by the success rate, is a pivotal aspect, the scoring/screening power and computational speed of these algorithms are equally important given the pivotal role of LD tools in VS. Regarding application scenarios, early methods focused on blind docking, where the binding site is unknown. However, recent studies suggest a shift toward identifying binding sites rather than solely predicting binding poses within these models. In contrast, LD with a known pocket in VS has been shown to be more practical. Physical plausibility poses another significant challenge. Although DLLD models often achieve higher success rates compared to traditional methods, they may generate poses with implausible local structures, such as incorrect bond angles or lengths, which are disadvantageous for postprocessing tasks like visualization. Finally, we discuss the future perspectives for DLLD, emphasizing the need to improve generalization ability, strike a balance between speed and accuracy, account for protein conformation flexibility, and enhance physical plausibility. Additionally, we delve into the comparison between generative and regression algorithms in this context, exploring their respective strengths and potential.","<method>deep learning (DL)</method>, <method>DeepMind's AlphaFold2</method>, <method>DL-based ligand docking (DLLD)</method>, <method>generative algorithms</method>, <method>regression algorithms</method>"
2024,https://openalex.org/W4398169659,Medicine,The Sociodemographic Biases in Machine Learning Algorithms: A Biomedical Informatics Perspective,"Artificial intelligence models represented in machine learning algorithms are promising tools for risk assessment used to guide clinical and other health care decisions. Machine learning algorithms, however, may house biases that propagate stereotypes, inequities, and discrimination that contribute to socioeconomic health care disparities. The biases include those related to some sociodemographic characteristics such as race, ethnicity, gender, age, insurance, and socioeconomic status from the use of erroneous electronic health record data. Additionally, there is concern that training data and algorithmic biases in large language models pose potential drawbacks. These biases affect the lives and livelihoods of a significant percentage of the population in the United States and globally. The social and economic consequences of the associated backlash cannot be underestimated. Here, we outline some of the sociodemographic, training data, and algorithmic biases that undermine sound health care risk assessment and medical decision-making that should be addressed in the health care system. We present a perspective and overview of these biases by gender, race, ethnicity, age, historically marginalized communities, algorithmic bias, biased evaluations, implicit bias, selection/sampling bias, socioeconomic status biases, biased data distributions, cultural biases and insurance status bias, conformation bias, information bias and anchoring biases and make recommendations to improve large language model training data, including de-biasing techniques such as counterfactual role-reversed sentences during knowledge distillation, fine-tuning, prefix attachment at training time, the use of toxicity classifiers, retrieval augmented generation and algorithmic modification to mitigate the biases moving forward.","<method>knowledge distillation</method>, <method>fine-tuning</method>, <method>prefix attachment at training time</method>, <method>toxicity classifiers</method>, <method>retrieval augmented generation</method>, <method>algorithmic modification</method>"
2024,https://openalex.org/W4399244247,Medicine,Investigating influencing factors of learning satisfaction in AI ChatGPT for research: University students perspective,"This study investigates the determinants of ChatGPT adoption among university students and its impact on learning satisfaction. Utilizing the Technology Acceptance Model (TAM) and incorporating insights from interaction learning, collaborative learning, and information quality, a structural equation modeling approach was employed. This research collected valuable responses from 262 students at King Faisal University in Saudi Arabia through the use of self-report questionnaires. The data's reliability and validity were assessed using confirmation factor analysis, followed by path analysis to explore the hypotheses in the proposed model. The results indicate the pivotal roles of interaction learning and collaborative learning in fostering ChatGPT adoption. Social interaction played a significant role, as researchers engaging in conversations and knowledge-sharing expressed increased comfort with ChatGPT. Information quality was found to substantially influence researchers' decisions to continue using ChatGPT, emphasizing the need for ongoing improvement in the accuracy and relevance of content provided. Perceived ease of use and perceived usefulness played intermediary roles in linking ChatGPT engagement to learning satisfaction. User-friendly interfaces and perceived utility were identified as crucial factors affecting overall satisfaction levels. Notably, ChatGPT positively impacted learning motivation, indicating its potential to enhance student engagement and interest in learning. The study's findings have implications for educational practitioners seeking to improve the implementation of AI technologies in university students, emphasizing user-friendly design, collaborative learning, and factors influencing satisfaction. The study concludes with insights into the complex interplay between AI-powered tools, learning objectives, and motivation, highlighting the need for continued research to comprehensively understand these dynamics. This study investigates the determinants of ChatGPT adoption among university students and its impact on learning satisfaction. Utilizing the Technology Acceptance Model (TAM) and incorporating insights from interaction learning, collaborative learning, and information quality, a structural equation modeling approach was employed. This research collected valuable responses from 262 students at King Faisal University in Saudi Arabia through the use of self-report questionnaires. The data's reliability and validity were assessed using confirmation factor analysis, followed by path analysis to explore the hypotheses in the proposed model. The results indicate the pivotal roles of interaction learning and collaborative learning in fostering ChatGPT adoption. Social interaction played a significant role, as researchers engaging in conversations and knowledge-sharing expressed increased comfort with ChatGPT. Information quality was found to substantially influence researchers' decisions to continue using ChatGPT, emphasizing the need for ongoing improvement in the accuracy and relevance of content provided. Perceived ease of use and perceived usefulness played intermediary roles in linking ChatGPT engagement to learning satisfaction. User-friendly interfaces and perceived utility were identified as crucial factors affecting overall satisfaction levels. Notably, ChatGPT positively impacted learning motivation, indicating its potential to enhance student engagement and interest in learning. The study's findings have implications for educational practitioners seeking to improve the implementation of AI technologies in university students, emphasizing user-friendly design, collaborative learning, and factors influencing satisfaction. The study concludes with insights into the complex interplay between AI-powered tools, learning objectives, and motivation, highlighting the need for continued research to comprehensively understand these dynamics.","<method>Technology Acceptance Model (TAM)</method>, <method>structural equation modeling</method>, <method>confirmation factor analysis</method>, <method>path analysis</method>"
2024,https://openalex.org/W4399715357,Medicine,AI-POWERED FRAUD DETECTION IN BANKING: SAFEGUARDING FINANCIAL TRANSACTIONS,"The banking industry's metamorphosis through digitalization has unquestionably revolutionized accessibility and convenience for customers worldwide. However, this paradigm shift has ushered in a new era of challenges, most notably in the realm of cybersecurity. Conventional rule-based fraud detection strategies have struggled to keep pace with the rapid evolution of cyber threats, prompting a surge of interest in more adaptive approaches like unsupervised learning. Furthermore, the COVID-19 pandemic has exacerbated the issue of bank fraud due to the widespread transition to online platforms and the proliferation of charitable funds, which present ripe opportunities for exploitation by cybercriminals. In response to these pressing concerns, this study delves into the realm of machine learning algorithms for the analysis and identification of fraudulent banking transactions. Notably, it contributes scientific novelty by developing models specifically tailored to this purpose and implementing innovative preprocessing techniques to enhance detection accuracy. Utilizing a diverse array of algorithms, including Random Forest, K-Nearest Neighbor (KNN), Naïve Bayes, Decision Trees, and Logistic Regression, the study showcases promising results. In particular, logistic regression and decision tree models exhibit impressive accuracy and Area Under the Curve (AUC) values of approximately 0.98, 0.97 and 0.95, 0.94, respectively. Given the pervasive nature of banking fraud in our digital society, the utilization of artificial intelligence algorithms for fraud detection stands as a critical and timely endeavor, promising enhanced security and trust in the financial ecosystem.","<method>unsupervised learning</method>, <method>Random Forest</method>, <method>K-Nearest Neighbor (KNN)</method>, <method>Naïve Bayes</method>, <method>Decision Trees</method>, <method>Logistic Regression</method>"
2024,https://openalex.org/W4390572162,Medicine,InterDILI: interpretable prediction of drug-induced liver injury through permutation feature importance and attention mechanism,"Abstract Safety is one of the important factors constraining the distribution of clinical drugs on the market. Drug-induced liver injury (DILI) is the leading cause of safety problems produced by drug side effects. Therefore, the DILI risk of approved drugs and potential drug candidates should be assessed. Currently, in vivo and in vitro methods are used to test DILI risk, but both methods are labor-intensive, time-consuming, and expensive. To overcome these problems, many in silico methods for DILI prediction have been suggested. Previous studies have shown that DILI prediction models can be utilized as prescreening tools, and they achieved a good performance. However, there are still limitations in interpreting the prediction results. Therefore, this study focused on interpreting the model prediction to analyze which features could potentially cause DILI. For this, five publicly available datasets were collected to train and test the model. Then, various machine learning methods were applied using substructure and physicochemical descriptors as inputs and the DILI label as the output. The interpretation of feature importance was analyzed by recognizing the following general-to-specific patterns: (i) identifying general important features of the overall DILI predictions, and (ii) highlighting specific molecular substructures which were highly related to the DILI prediction for each compound. The results indicated that the model not only captured the previously known properties to be related to DILI but also proposed a new DILI potential substructural of physicochemical properties. The models for the DILI prediction achieved an area under the receiver operating characteristic (AUROC) of 0.88–0.97 and an area under the Precision-Recall curve (AUPRC) of 0.81–0.95. From this, we hope the proposed models can help identify the potential DILI risk of drug candidates at an early stage and offer valuable insights for drug development.",<machine learning methods>
2024,https://openalex.org/W4391042406,Medicine,"A comparative analysis of machine learning techniques for aboveground biomass estimation: A case study of the Western Ghats, India","Accurate assessment of aboveground biomass (AGB) in tropical forests, particularly within a biodiversity hotspot, is vital for sustainable resource management and the preservation of ecosystems. However, estimating AGB in tropical forests is complex due to the diverse and intricate nature of vegetation, necessitating the integration of data from multiple sources. To tackle this challenge, our study utilized seven machine learning algorithms to analyze various combination of multisource datasets. We developed seven models/scenarios that incorporated Sentinel-1, Sentinel-2 as well as environmental factors such as topography, soil and climate to identify key variables for accurate estimation of AGB. For optimal performance, hyperparameters of the algorithms were fine-tuned through 10-fold cross-validation and their accuracy were assessed using the testing dataset. We found that the integrated model of satellite datasets, topography, climate, and soil variables exhibited the highest accuracy, where ensemble stacking, that combined multiple MLAs, proved to be reliable and best suited for predicting AGB (mean absolute error-3.97 Mg 0.1 ha−1, root mean square error-5.67 Mg 0.1 ha−1, and coefficient of determination - 0.82). Notably, the top predictor variables included Sentinel-2 bands (near infrared and green), soil properties (pH and soil organic carbon), and topography (elevation). The study emphasizes the significance of incorporating environmental variables (specifically topography and soil properties) along with Sentinel datasets to improve the accuracy of AGB estimation. This approach has the potential for broader applications, specifically in regions where vegetation productivity is governed by diverse environmental conditions.","<method>machine learning algorithms</method>, <method>ensemble stacking</method>"
2024,https://openalex.org/W4391166814,Medicine,Identifying top ten predictors of type 2 diabetes through machine learning analysis of UK Biobank data,"Abstract The study aimed to identify the most predictive factors for the development of type 2 diabetes. Using an XGboost classification model, we projected type 2 diabetes incidence over a 10-year horizon. We deliberately minimized the selection of baseline factors to fully exploit the rich dataset from the UK Biobank. The predictive value of features was assessed using shap values, with model performance evaluated via Receiver Operating Characteristic Area Under the Curve, sensitivity, and specificity. Data from the UK Biobank, encompassing a vast population with comprehensive demographic and health data, was employed. The study enrolled 450,000 participants aged 40–69, excluding those with pre-existing diabetes. Among 448,277 participants, 12,148 developed type 2 diabetes within a decade. HbA1c emerged as the foremost predictor, followed by BMI, waist circumference, blood glucose, family history of diabetes, gamma-glutamyl transferase, waist-hip ratio, HDL cholesterol, age, and urate. Our XGboost model achieved a Receiver Operating Characteristic Area Under the Curve of 0.9 for 10-year type 2 diabetes prediction, with a reduced 10-feature model achieving 0.88. Easily measurable biological factors surpassed traditional risk factors like diet, physical activity, and socioeconomic status in predicting type 2 diabetes. Furthermore, high prediction accuracy could be maintained using just the top 10 biological factors, with additional ones offering marginal improvements. These findings underscore the significance of biological markers in type 2 diabetes prediction.","<method>XGboost classification model</method>, <method>shap values</method>"
2024,https://openalex.org/W4391305160,Medicine,Protocol for metadata and image collection at diabetic foot ulcer clinics: enabling research in wound analytics and deep learning,"Abstract Background The escalating impact of diabetes and its complications, including diabetic foot ulcers (DFUs), presents global challenges in quality of life, economics, and resources, affecting around half a billion people. DFU healing is hindered by hyperglycemia-related issues and diverse diabetes-related physiological changes, necessitating ongoing personalized care. Artificial intelligence and clinical research strive to address these challenges by facilitating early detection and efficient treatments despite resource constraints. This study establishes a standardized framework for DFU data collection, introducing a dedicated case report form, a comprehensive dataset named Zivot with patient population clinical feature breakdowns and a baseline for DFU detection using this dataset and a UNet architecture. Results Following this protocol, we created the Zivot dataset consisting of 269 patients with active DFUs, and about 3700 RGB images and corresponding thermal and depth maps for the DFUs. The effectiveness of collecting a consistent and clean dataset was demonstrated using a bounding box prediction deep learning network that was constructed with EfficientNet as the feature extractor and UNet architecture. The network was trained on the Zivot dataset, and the evaluation metrics showed promising values of 0.79 and 0.86 for F1-score and mAP segmentation metrics. Conclusions This work and the Zivot database offer a foundation for further exploration of holistic and multimodal approaches to DFU research.","<method>UNet architecture</method>, <method>bounding box prediction deep learning network</method>, <method>EfficientNet</method>"
2024,https://openalex.org/W4392693790,Medicine,BINDTI: A bi-directional Intention network for drug-target interaction identification based on attention mechanisms,"The identification of drug-target interactions (DTIs) is an essential step in drug discovery. In vitro experimental methods are expensive, laborious, and time-consuming. Deep learning has witnessed promising progress in DTI prediction. However, how to precisely represent drug and protein features is a major challenge for DTI prediction. Here, we developed an end-to-end DTI identification framework called BINDTI based on bi-directional Intention network. First, drug features are encoded with graph convolutional networks based on its 2D molecular graph obtained by its SMILES string. Next, protein features are encoded based on its amino acid sequence through a mixed model called ACmix, which integrates self-attention mechanism and convolution. Third, drug and target features are fused through bi-directional Intention network, which combines Intention and multi-head attention. Finally, unknown drug-target (DT) pairs are classified through multilayer perceptron based on the fused DT features. The results demonstrate that BINDTI greatly outperformed four baseline methods (i.e., CPI-GNN, TransfomerCPI, MolTrans, and IIFDTI) on the BindingDB, BioSNAP, DrugBank, and Human datasets. More importantly, it was more appropriate to predict new DTIs than the four baseline methods on imbalanced datasets. Ablation experimental results elucidated that both bi-directional Intention and ACmix could greatly advance DTI prediction. The fused feature visualization and case studies manifested that the predicted results by BINDTI were basically consistent with the true ones. We anticipate that the proposed BINDTI framework can find new low-cost drug candidates, improve drugs' virtual screening, and further facilitate drug repositioning as well as drug discovery. BINDTI is publicly available at <uri xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">https://github.com/plhhnu/BINDTI</uri> .","<method>graph convolutional networks</method>, <method>self-attention mechanism</method>, <method>convolution</method>, <method>ACmix</method>, <method>bi-directional Intention network</method>, <method>Intention</method>, <method>multi-head attention</method>, <method>multilayer perceptron</method>"
2024,https://openalex.org/W4400006942,Medicine,Prediction of Alzheimer's disease progression within 6 years using speech: A novel approach leveraging language models,"Abstract INTRODUCTION Identification of individuals with mild cognitive impairment (MCI) who are at risk of developing Alzheimer's disease (AD) is crucial for early intervention and selection of clinical trials. METHODS We applied natural language processing techniques along with machine learning methods to develop a method for automated prediction of progression to AD within 6 years using speech. The study design was evaluated on the neuropsychological test interviews of n = 166 participants from the Framingham Heart Study, comprising 90 progressive MCI and 76 stable MCI cases. RESULTS Our best models, which used features generated from speech data, as well as age, sex, and education level, achieved an accuracy of 78.5% and a sensitivity of 81.1% to predict MCI‐to‐AD progression within 6 years. DISCUSSION The proposed method offers a fully automated procedure, providing an opportunity to develop an inexpensive, broadly accessible, and easy‐to‐administer screening tool for MCI‐to‐AD progression prediction, facilitating development of remote assessment. Highlights Voice recordings from neuropsychological exams coupled with basic demographics can lead to strong predictive models of progression to dementia from mild cognitive impairment. The study leveraged AI methods for speech recognition and processed the resulting text using language models. The developed AI‐powered pipeline can lead to fully automated assessment that could enable remote and cost‐effective screening and prognosis for Alzehimer's disease.","<method>natural language processing</method>, <method>machine learning</method>, <method>speech recognition</method>, <method>language models</method>"
2024,https://openalex.org/W4390884647,Medicine,Detecting COVID-19 in chest CT images based on several pre-trained models,"Abstract This paper explores the use of chest CT scans for early detection of COVID-19 and improved patient outcomes. The proposed method employs advanced techniques, including binary cross-entropy, transfer learning, and deep convolutional neural networks, to achieve accurate results. The COVIDx dataset, which contains 104,009 chest CT images from 1,489 patients, is used for a comprehensive analysis of the virus. A sample of 13,413 images from this dataset is categorised into two groups: 7,395 CT scans of individuals with confirmed COVID-19 and 6,018 images of normal cases. The study presents pre-trained transfer learning models such as ResNet (50), VGG (19), VGG (16), and Inception V3 to enhance the DCNN for classifying the input CT images. The binary cross-entropy metric is used to compare COVID-19 cases with normal cases based on predicted probabilities for each class. Stochastic Gradient Descent and Adam optimizers are employed to address overfitting issues. The study shows that the proposed pre-trained transfer learning models achieve accuracies of 99.07%, 98.70%, 98.55%, and 96.23%, respectively, in the validation set using the Adam optimizer. Therefore, the proposed work demonstrates the effectiveness of pre-trained transfer learning models in enhancing the accuracy of DCNNs for image classification. Furthermore, this paper provides valuable insights for the development of more accurate and efficient diagnostic tools for COVID-19.","<method>binary cross-entropy</method>, <method>transfer learning</method>, <method>deep convolutional neural networks</method>, <method>pre-trained transfer learning models</method>, <method>ResNet (50)</method>, <method>VGG (19)</method>, <method>VGG (16)</method>, <method>Inception V3</method>, <method>Stochastic Gradient Descent optimizer</method>, <method>Adam optimizer</method>"
2024,https://openalex.org/W4391892547,Medicine,A Novel Early Detection and Prevention of Coronary Heart Disease Framework Using Hybrid Deep Learning Model and Neural Fuzzy Inference System,"Diabetes is the ""mother of all diseases"" as it affects multiple organs of body of an individual in some way. Its timely detection and management are critically important. Otherwise, the long run, it can cause several complications in a diabetic. Heart disease is one of the major complications of diabetes.This work proposed an Optimal Scrutiny Boosted Graph Convolutional LSTM (O-SBGC-LSTM), SBGC-LSTM enhanced by Eurygaster Optimization Algorithm (EOA) to tune hyperparameters for early prevention and detection of diabetes disease. This work proposed an Optimal Scrutiny Boosted Graph Convolutional LSTM (O-SBGC-LSTM), SBGC-LSTM enhanced by Eurygaster Optimization Algorithm (EOA) to tune hyperparameters for early prevention and detection of diabetes disease. This method not only captures discriminative features in spatial configuration and temporal dynamics but also explore the co-occurrence relationship between spatial and temporal domains. This method also presents a temporal hierarchical architecture to increase temporal receptive fields of top SBGC-LSTM layer, which boosts the ability to learn high-level semantic representation and significantly reduces computation cost. The performance of O-SBGC-LSTM was found overall to be satisfactory, reaching >98% accuracy in most studies. In comparison with classic machine learning approaches, proposed hybrid DL was found to achieve better performance in almost all studies that reported such comparison outcomes. Furthermore, prevention is better than cure. Additionally, employed fuzzy based inference techniques to enhance the prevention procedure using suggestion table.","<method>Optimal Scrutiny Boosted Graph Convolutional LSTM (O-SBGC-LSTM)</method>, <method>Scrutiny Boosted Graph Convolutional LSTM (SBGC-LSTM)</method>, <method>Eurygaster Optimization Algorithm (EOA)</method>, <method>fuzzy based inference techniques</method>"
2024,https://openalex.org/W4392195911,Medicine,<scp>CerviFormer</scp>: A pap smear‐based cervical cancer classification method using cross‐attention and latent transformer,"Abstract Cervical cancer is one of the primary causes of death in women. It should be diagnosed early and treated according to the best medical advice, similar to other diseases, to ensure that its effects are as minimal as possible. Pap smear images are one of the most constructive ways for identifying this type of cancer. This study proposes a cross‐attention‐based Transfomer approach for the reliable classification of cervical cancer in pap smear images. In this study, we propose the CerviFormer‐a model that depends on the Transformers and thereby requires minimal architectural assumptions about the size of the input data. The model uses a cross‐attention technique to repeatedly consolidate the input data into a compact latent Transformer module, which enables it to manage very large‐scale inputs. We evaluated our model on two publicly available pap smear datasets. For 3‐state classification on the Sipakmed data, the model achieved an accuracy of 96.67%. For 2‐state classification on the Herlev data, the model achieved an accuracy of 94.57%. Experimental results on two publicly accessible datasets demonstrate that the proposed method achieves competitive results when compared to contemporary approaches. The proposed method brings forth a comprehensive classification model to detect cervical cancer in pap smear images. This may aid medical professionals in providing better cervical cancer treatment, consequently, enhancing the overall effectiveness of the entire testing process.","<method>cross-attention-based Transformer</method>, <method>Transformers</method>"
2024,https://openalex.org/W4392915169,Medicine,Unified deep learning models for enhanced lung cancer prediction with ResNet-50–101 and EfficientNet-B3 using DICOM images,"Abstract Significant advancements in machine learning algorithms have the potential to aid in the early detection and prevention of cancer, a devastating disease. However, traditional research methods face obstacles, and the amount of cancer-related information is rapidly expanding. The authors have developed a helpful support system using three distinct deep-learning models, ResNet-50, EfficientNet-B3, and ResNet-101, along with transfer learning, to predict lung cancer, thereby contributing to health and reducing the mortality rate associated with this condition. This offer aims to address the issue effectively. Using a dataset of 1,000 DICOM lung cancer images from the LIDC-IDRI repository, each image is classified into four different categories. Although deep learning is still making progress in its ability to analyze and understand cancer data, this research marks a significant step forward in the fight against cancer, promoting better health outcomes and potentially lowering the mortality rate. The Fusion Model, like all other models, achieved 100% precision in classifying Squamous Cells. The Fusion Model and ResNet-50 achieved a precision of 90%, closely followed by EfficientNet-B3 and ResNet-101 with slightly lower precision. To prevent overfitting and improve data collection and planning, the authors implemented a data extension strategy. The relationship between acquiring knowledge and reaching specific scores was also connected to advancing and addressing the issue of imprecise accuracy, ultimately contributing to advancements in health and a reduction in the mortality rate associated with lung cancer.","<method>ResNet-50</method>, <method>EfficientNet-B3</method>, <method>ResNet-101</method>, <method>transfer learning</method>, <method>Fusion Model</method>"
2024,https://openalex.org/W4390490875,Medicine,More Robots are Coming: Large Multimodal Models (ChatGPT) can Solve Visually Diverse Images of Parsons Problems,"Large language models are reshaping computing education. Based on recent research, these models explain code better than students, answer multiple choice questions at or above the class average, and generate code that can pass automated tests in introductory courses. In response to these capabilities, instructors have quickly adjusted their courses and assessment methods to align with shifting learning goals and the increased risk of academic integrity issues. While some scholars have advocated for the integration of visual problems as a safeguard against the capabilities of language models, new multimodal models now have vision and language capabilities that may allow them to analyze and solve visual problems. In this paper, we compare the large multimodal model (LMMs) GPT-4V with Bard, an LLM that uses Google Lens for text recognition. We find that LMMs, which have learned both pixel features (from images) and text features (from prompts) in the same embedding space, performed substantially better than Bard which uses a piecemeal approach. With a specific focus on Parsons problems presented across diverse visual representations, our results show that GPT-4V solved 96.7% these visual problems, struggling minimally with a single Parsons problem. Conversely, Bard performed poorly by only solving 69.2% of problems, struggling with common issues like hallucinations and refusals. These findings suggest that merely transitioning to visual programming problems might not be a panacea to issues of academic integrity in the generative AI era.","<method>large language models (LLMs)</method>, <method>large multimodal models (LMMs)</method>"
2024,https://openalex.org/W4390737642,Medicine,Artificial intelligence for diabetic retinopathy detection: A systematic review,"The incidence of diabetic retinopathy (DR) has increased at a rapid pace in recent years all over the world. Diabetic eye illness is identified as one of the most common reasons for vision loss among people. To properly manage DR, there has been immense research and exploration of state-of-the-art methods using artificial intelligence (AI) enabled models. Specifically, AI-empowered models combine multiple machine learning (ML) and deep learning (DL) based algorithms to improve the performance of the developed system architectures that are commercially utilized for the detection of DR disease. However, these models still exhibit several limitations, such as computational complexity, low accuracy in DR stage detection due to class imbalance, more time consumption, and high maintenance cost. To overcome these limits, a more advanced model is required to accurately predict the DR stage in the initial stages. For example, the identification of DR disease in the initial stage helps the ophthalmologist to make an accurate and safe diagnosis, and thereby, eyesight-related issues may be treated more effectively. This study conducted a systematic literature review (SLR) to provide a detailed discussion of the background of diabetic retinopathy, its major causes, challenges faced by ophthalmologists in DR detection, and possible solutions for identifying DR in the initial stage. Also, the SLR provides an in-depth analysis of the existing state-of-the-art techniques and system models used in DR diagnosis based on AI, ML, and recently developed DL-based approaches. Furthermore, this present survey would be helpful for the research community to receive information on the recent approaches used for DR identification along with their significant challenges and limitations.","<method>artificial intelligence (AI) enabled models</method>, <method>machine learning (ML) based algorithms</method>, <method>deep learning (DL) based algorithms</method>"
2024,https://openalex.org/W4390812034,Medicine,The Utility of AI in Writing a Scientific Review Article on the Impacts of COVID-19 on Musculoskeletal Health,"Abstract Purpose of Review There were two primary purposes to our reviews. First, to provide an update to the scientific community about the impacts of COVID-19 on musculoskeletal health. Second, was to determine the value of using a large language model, ChatGPT 4.0, in the process of writing a scientific review article. To accomplish these objectives, we originally set out to write three review articles on the topic using different methods to produce the initial drafts of the review articles. The first review article was written in the traditional manner by humans, the second was to be written exclusively using ChatGPT (AI-only or AIO), and the third approach was to input the outline and references selected by humans from approach 1 into ChatGPT, using the AI to assist in completing the writing (AI-assisted or AIA). All review articles were extensively fact-checked and edited by all co-authors leading to the final drafts of the manuscripts, which were significantly different from the initial drafts. Recent Findings Unfortunately, during this process, it became clear that approach 2 was not feasible for a very recent topic like COVID-19 as at the time, ChatGPT 4.0 had a cutoff date of September 2021 and all articles published after this date had to be provided to ChatGPT, making approaches 2 and 3 virtually identical. Therefore, only two approaches and two review articles were written (human and AI-assisted). Here we found that the human-only approach took less time to complete than the AI-assisted approach. This was largely due to the number of hours required to fact-check and edit the AI-assisted manuscript. Of note, the AI-assisted approach resulted in inaccurate attributions of references (about 20%) and had a higher similarity index suggesting an increased risk of plagiarism. Summary The main aim of this project was to determine whether the use of AI could improve the process of writing a scientific review article. Based on our experience, with the current state of technology, it would not be advised to solely use AI to write a scientific review article, especially on a recent topic.","<method>large language model, ChatGPT 4.0</method>"
2024,https://openalex.org/W4390959437,Medicine,Machine learning model (RG-DMML) and ensemble algorithm for prediction of students’ retention and graduation in education,"Automated prediction of students' retention and graduation in education using advanced analytical methods such as artificial intelligence (AI), has recently attracted the attention of educators, both in theory and in practice. Whereas invaluable insights and theories for measuring and testing the topic have been proposed, most of the existing methods do not technically highlight the non-trivial factors behind the renowned challenges and attrition. To this effect, by making use of two categories of data collected in a higher education setting about students (i) retention (n = 52262) and (ii) graduation (n = 53639); this study proposes a machine learning model - RG-DMML (retention and graduation data mining and machine learning) and ensemble algorithm for prediction of students' retention and graduation status in education. This was done by training and testing key features that are technically deemed suitable for measuring the constructs (retention and graduation), such as (i) the Average grade of the previous high school, and (ii) the Entry/admission score. The proposed model (RG-DMML) is designed based on the cross industry standard process for data mining (CRISP-DM) methodology, implemented using supervised machine learning technique such as K-Nearest Neighbor (KNN), and validated using the k-fold cross-validation method. The results show that the executed model and algorithm based on the Bagging method and 10-fold cross-validation are efficient and effective for predicting the student's retention and graduation status, with Precision (retention = 0.909, graduation = 0.822), Recall (retention = 1.000, graduation = 0.957), Accuracy (retention = 0.909, graduation = 0.817), F1-Score (retention = 0.952, graduation = 0.885) showing significant high accuracy levels or performance rate, and low Error-rate (retention = 0.090, graduation = 0.182), respectively. In addition, by considering the individual features selected through the Wrapper method in predicting the outputs, the proposed model proved more effective for predicting the students' retention status in comparison to the graduation data. The implications of the models' output and factors that impact the effective prediction or identification of at-risk students, e.g., for timely intervention, counselling, decision-making, and sustainable educational practice are empirically discussed in the study.","<method>machine learning model - RG-DMML (retention and graduation data mining and machine learning)</method>, <method>ensemble algorithm</method>, <method>cross industry standard process for data mining (CRISP-DM) methodology</method>, <method>supervised machine learning technique</method>, <method>K-Nearest Neighbor (KNN)</method>, <method>k-fold cross-validation method</method>, <method>Bagging method</method>, <method>Wrapper method</method>"
2024,https://openalex.org/W4391062514,Medicine,Translating a value-based framework for resilient e-learning impact in post COVID-19 times: Research-based Evidence from Higher Education in Kuwait,"The covid-19 pandemic has changed people's daily lives and behaviors all across the world and has impacted practically every element of human existence. The introduction of remote education systems and the move toward online learning have had some of the most significant effects. The on-site operations of educational institutions, such as schools, colleges, and universities, have had to be suspended in order to stop the virus' spread. In order to effectively disseminate instructional material and guarantee the unbroken progression of students' academic endeavors, educators have been forced to look for novel approaches. The study used the Value-Based Adoption Model (VAM) as a conceptual framework to look into the factors that affected Kuwait's e-learning outcomes in the midst of the covid-19 pandemic. 382 students at Kuwaiti universities and colleges were the source of quantitative data collection. The findings revealed that peer interaction emerged as the most influential factor in shaping outcomes within the educational context of Kuwait, while instructors and course design factors were not significant. Using the VAM, this study investigated the impact of several factors on students' e-learning results during times of crisis. The research expands the existing knowledge base in the field on this subject and suggests developing a well-organized online learning crisis approach. The main contribution of this work is summarized on (i) An integrated framework for the quality of the e-learning experience in universities in post-covid-19 times and (ii) A resilient higher education institutional learning strategy model in post-covid-19 times. The findings of this paper can be generalizable to other Gulf Corporation Council (GCC) countries such as Kingdom of Saudi Arabia, Qatar, United Arab Emirates (UAE), Bahrain and Oman. This is due to the shared cultural traditions and values, along with similar educational systems among these nations.",No methods found.
2024,https://openalex.org/W4391542772,Medicine,Deep learning algorithm-based multimodal MRI radiomics and pathomics data improve prediction of bone metastases in primary prostate cancer,"Abstract Purpose Bone metastasis is a significant contributor to morbidity and mortality in advanced prostate cancer, and early diagnosis is challenging due to its insidious onset. The use of machine learning to obtain prognostic information from pathological images has been highlighted. However, there is a limited understanding of the potential of early prediction of bone metastasis through the feature combination method from various sources. This study presents a method of integrating multimodal data to enhance the feasibility of early diagnosis of bone metastasis in prostate cancer. Methods and materials Overall, 211 patients diagnosed with prostate cancer (PCa) at Gansu Provincial Hospital between January 2017 and February 2023 were included in this study. The patients were randomized (8:2) into a training group ( n = 169) and a validation group ( n = 42). The region of interest (ROI) were segmented from the three magnetic resonance imaging (MRI) sequences (T2WI, DWI, and ADC), and pathological features were extracted from tissue sections (hematoxylin and eosin [H&amp;E] staining, 10 × 20). A deep learning (DL) model using ResNet 50 was employed to extract deep transfer learning (DTL) features. The least absolute shrinkage and selection operator (LASSO) regression method was utilized for feature selection, feature construction, and reducing feature dimensions. Different machine learning classifiers were used to build predictive models. The performance of the models was evaluated using receiver operating characteristic curves. The net clinical benefit was assessed using decision curve analysis (DCA). The goodness of fit was evaluated using calibration curves. A joint model nomogram was eventually developed by combining clinically independent risk factors. Results The best prediction models based on DTL and pathomics features showed area under the curve (AUC) values of 0.89 (95% confidence interval [CI], 0.799–0.989) and 0.85 (95% CI, 0.714–0.989), respectively. The AUC for the best prediction model based on radiomics features and combining radiomics features, DTL features, and pathomics features were 0.86 (95% CI, 0.735–0.979) and 0.93 (95% CI, 0.854–1.000), respectively. Based on DCA and calibration curves, the model demonstrated good net clinical benefit and fit. Conclusion Multimodal radiomics and pathomics serve as valuable predictors of the risk of bone metastases in patients with primary PCa.","<method>deep learning (DL) model using ResNet 50</method>, <method>deep transfer learning (DTL) features</method>, <method>least absolute shrinkage and selection operator (LASSO) regression method</method>, <method>machine learning classifiers</method>"
2024,https://openalex.org/W4391755073,Medicine,Specifying cross-system collaboration strategies for implementation: a multi-site qualitative study with child welfare and behavioral health organizations,"Abstract Background Cross-system interventions that integrate health, behavioral health, and social services can improve client outcomes and expand community impact. Successful implementation of these interventions depends on the extent to which service partners can align frontline services and organizational operations. However, collaboration strategies linking multiple implementation contexts have received limited empirical attention. This study identifies, describes, and specifies multi-level collaboration strategies used during the implementation of Ohio Sobriety Treatment and Reducing Trauma (Ohio START), a cross-system intervention that integrates services across two systems (child welfare and evidence-based behavioral health services) for families that are affected by co-occurring child maltreatment and parental substance use disorders. Methods In phase 1, we used a multi-site qualitative design with 17 counties that implemented Ohio START. Qualitative data were gathered from 104 staff from child welfare agencies, behavioral health treatment organizations, and regional behavioral health boards involved in implementation via 48 small group interviews about collaborative approaches to implementation. To examine cross-system collaboration strategies, qualitative data were analyzed using an iterative template approach and content analysis. In phase 2, a 16-member expert panel met to validate and specify the cross-system collaboration strategies identified in the interviews. The panel was comprised of key child welfare and behavioral health partners and scholars. Results In phase 1, we identified seven cross-system collaboration strategies used for implementation. Three strategies were used to staff the program: (1) contract for expertise, (2) provide joint supervision, and (3) co-locate staff. Two strategies were used to promote service access: (4) referral protocols and (5) expedited access agreements. Two strategies were used to align case plans: (6) shared decision-making meetings, and (7) sharing data. In phase 2, expert panelists specified operational details of the cross-system collaboration strategies, and explained the processes by which strategies were perceived to improve implementation and service system outcomes. Conclusions We identified a range of cross-system collaboration strategies that show promise for improving staffing, service access, and case planning. Leaders, supervisors, and frontline staff used these strategies during all phases of implementation. These findings lay the foundation for future experimental and quasi-experimental studies that test the effectiveness of cross-system collaboration strategies.",No methods found.
2024,https://openalex.org/W4391814741,Medicine,Explainable hybrid vision transformers and convolutional network for multimodal glioma segmentation in brain MRI,"Abstract Accurate localization of gliomas, the most common malignant primary brain cancer, and its different sub-region from multimodal magnetic resonance imaging (MRI) volumes are highly important for interventional procedures. Recently, deep learning models have been applied widely to assist automatic lesion segmentation tasks for neurosurgical interventions. However, these models are often complex and represented as “black box” models which limit their applicability in clinical practice. This article introduces new hybrid vision Transformers and convolutional neural networks for accurate and robust glioma segmentation in Brain MRI scans. Our proposed method, TransXAI, provides surgeon-understandable heatmaps to make the neural networks transparent. TransXAI employs a post-hoc explanation technique that provides visual interpretation after the brain tumor localization is made without any network architecture modifications or accuracy tradeoffs. Our experimental findings showed that TransXAI achieves competitive performance in extracting both local and global contexts in addition to generating explainable saliency maps to help understand the prediction of the deep network. Further, visualization maps are obtained to realize the flow of information in the internal layers of the encoder-decoder network and understand the contribution of MRI modalities in the final prediction. The explainability process could provide medical professionals with additional information about the tumor segmentation results and therefore aid in understanding how the deep learning model is capable of processing MRI data successfully. Thus, it enables the physicians’ trust in such deep learning systems towards applying them clinically. To facilitate TransXAI model development and results reproducibility, we will share the source code and the pre-trained models after acceptance at https://github.com/razeineldin/TransXAI .","<method>vision Transformers</method>, <method>convolutional neural networks</method>, <method>post-hoc explanation technique</method>, <method>encoder-decoder network</method>"
2024,https://openalex.org/W4392231575,Medicine,Cardiologist-level interpretable knowledge-fused deep neural network for automatic arrhythmia diagnosis,"Abstract Background Long-term monitoring of Electrocardiogram (ECG) recordings is crucial to diagnose arrhythmias. Clinicians can find it challenging to diagnose arrhythmias, and this is a particular issue in more remote and underdeveloped areas. The development of digital ECG and AI methods could assist clinicians who need to diagnose arrhythmias outside of the hospital setting. Methods We constructed a large-scale Chinese ECG benchmark dataset using data from 272,753 patients collected from January 2017 to December 2021. The dataset contains ECG recordings from all common arrhythmias present in the Chinese population. Several experienced cardiologists from Shanghai First People’s Hospital labeled the dataset. We then developed a deep learning-based multi-label interpretable diagnostic model from the ECG recordings. We utilized Accuracy, F1 score and AUC-ROC to compare the performance of our model with that of the cardiologists, as well as with six comparison models, using testing and hidden data sets. Results The results show that our approach achieves an F1 score of 83.51%, an average AUC ROC score of 0.977, and 93.74% mean accuracy for 6 common arrhythmias. Results from the hidden dataset demonstrate the performance of our approach exceeds that of cardiologists. Our approach also highlights the diagnostic process. Conclusions Our diagnosis system has superior diagnostic performance over that of clinicians. It also has the potential to help clinicians rapidly identify abnormal regions on ECG recordings, thus improving efficiency and accuracy of clinical ECG diagnosis in China. This approach could therefore potentially improve the productivity of out-of-hospital ECG diagnosis and provides a promising prospect for telemedicine.",<method>deep learning-based multi-label interpretable diagnostic model</method>
2024,https://openalex.org/W4392391366,Medicine,Prediction of Effectiveness and Toxicities of Immune Checkpoint Inhibitors Using Real-World Patient Data,"PURPOSE Although immune checkpoint inhibitors (ICIs) have improved outcomes in certain patients with cancer, they can also cause life-threatening immunotoxicities. Predicting immunotoxicity risks alongside response could provide a personalized risk-benefit profile, inform therapeutic decision making, and improve clinical trial cohort selection. We aimed to build a machine learning (ML) framework using routine electronic health record (EHR) data to predict hepatitis, colitis, pneumonitis, and 1-year overall survival. METHODS Real-world EHR data of more than 2,200 patients treated with ICI through December 31, 2018, were used to develop predictive models. Using a prediction time point of ICI initiation, a 1-year prediction time window was applied to create binary labels for the four outcomes for each patient. Feature engineering involved aggregating laboratory measurements over appropriate time windows (60-365 days). Patients were randomly partitioned into training (80%) and test (20%) sets. Random forest classifiers were developed using a rigorous model development framework. RESULTS The patient cohort had a median age of 63 years and was 61.8% male. Patients predominantly had melanoma (37.8%), lung cancer (27.3%), or genitourinary cancer (16.4%). They were treated with PD-1 (60.4%), PD-L1 (9.0%), and CTLA-4 (19.7%) ICIs. Our models demonstrate reasonably strong performance, with AUCs of 0.739, 0.729, 0.755, and 0.752 for the pneumonitis, hepatitis, colitis, and 1-year overall survival models, respectively. Each model relies on an outcome-specific feature set, though some features are shared among models. CONCLUSION To our knowledge, this is the first ML solution that assesses individual ICI risk-benefit profiles based predominantly on routine structured EHR data. As such, use of our ML solution will not require additional data collection or documentation in the clinic.","<method>machine learning (ML) framework</method>, <method>random forest classifiers</method>"
2024,https://openalex.org/W4392517740,Medicine,"Environmental, social, and governance (ESG) disclosure and firm value: the role of competitive advantage as a mediator","Previous research has examined the relationship between ESG disclosure and firm value, but it has yet to fully explain how the former can increase the latter. Thus, the current study aims to fill the research gap by analyzing the relationship between ESG disclosure and firm value with competitive advantage as a mediating variable. This research was conducted in Indonesia on the ground that Indonesia is a developing country with a great potential for an increased economy even though its ESG implementation is still less optimal. This study employed the purposive sampling method with the criteria that the companies to be included as the sample were non-financial companies listed on the IDX and were consistent in disclosing ESG and publishing their financial reports. The sample of this study comprised 42 non-financial companies in Indonesia within the 2015–2021 period, with a total of 252 observations were conducted. The data were analysed using PLS-SEM with WarpPLS 7.0 to test hypotheses and draw conclusions. The findings showed that ESG disclosure did not affect firm value. However, when competitive advantage was included as a mediating variable in the relationship between the two variables, the results showed a significant positive direction toward firm value. This research contributes to the practical implications and development of signal theory and resource theory, especially in accounting and sustainability disciplines in the context of non-financial companies.",No methods found.
2024,https://openalex.org/W4392694180,Medicine,NSGA-II-DL: Metaheuristic Optimal Feature Selection With Deep Learning Framework for HER2 Classification in Breast Cancer,"Immunohistochemistry (IHC) slides are graded for breast cancer based on visual markers and morphological characteristics of stained membrane regions. The usage of whole slide images (WSIs) from histology in digital pathology algorithms for computer-assisted evaluations has increased recently. Human epidermal growth factor receptor 2 (HER2)-stained microscopic images are challenging, time-consuming, and error-prone to evaluate manually. This is due to different staining, overlapped regions, and huge, non-homogeneous slides. Additionally, the classification of HER2 images by the selection of fundamental features must be used to capture the difficult elements of the images, such as the irregular cell structure and the coloring of the tissue of the cells. To solve the above problems, a transfer learning model-based, trainable metaheuristic method for choosing the best features is suggested in this paper. Moreover, the suggested model is efficient in reducing model complexity and computational costs as well as avoiding overfitting. The four main components of the proposed cascaded design are: (a) converting WSIs to tiled images and enhancing contrast with fast local Laplacian filtering (FlLpF); (b) extracting features with a ResNet50 CNN technique based on transfer learning; (c) selecting the most informative features with the help of a non-dominated sorting genetic algorithm (NSGA-II) optimizer; and (d) using a support vector machine (SVM) to classify HER2 scores. Results from the HER2SC and HER2GAN datasets show that the suggested model is superior to other methods already in use, with 94.4% accuracy, 93.71% precision, 98.07% specificity, 93.83% sensitivity, and a 93.71% F1-score for the HER2SC dataset being achieved.","<method>transfer learning</method>, <method>ResNet50 CNN</method>, <method>non-dominated sorting genetic algorithm (NSGA-II)</method>, <method>support vector machine (SVM)</method>"
2024,https://openalex.org/W4392926377,Medicine,Generalizability of machine learning in predicting antimicrobial resistance in E. coli: a multi-country case study in Africa,"Abstract Background Antimicrobial resistance (AMR) remains a significant global health threat particularly impacting low- and middle-income countries (LMICs). These regions often grapple with limited healthcare resources and access to advanced diagnostic tools. Consequently, there is a pressing need for innovative approaches that can enhance AMR surveillance and management. Machine learning (ML) though underutilized in these settings, presents a promising avenue. This study leverages ML models trained on whole-genome sequencing data from England, where such data is more readily available, to predict AMR in E . coli , targeting key antibiotics such as ciprofloxacin, ampicillin, and cefotaxime. A crucial part of our work involved the validation of these models using an independent dataset from Africa, specifically from Uganda, Nigeria, and Tanzania, to ascertain their applicability and effectiveness in LMICs. Results Model performance varied across antibiotics. The Support Vector Machine excelled in predicting ciprofloxacin resistance (87% accuracy, F1 Score: 0.57), Light Gradient Boosting Machine for cefotaxime (92% accuracy, F1 Score: 0.42), and Gradient Boosting for ampicillin (58% accuracy, F1 Score: 0.66). In validation with data from Africa, Logistic Regression showed high accuracy for ampicillin (94%, F1 Score: 0.97), while Random Forest and Light Gradient Boosting Machine were effective for ciprofloxacin (50% accuracy, F1 Score: 0.56) and cefotaxime (45% accuracy, F1 Score:0.54), respectively. Key mutations associated with AMR were identified for these antibiotics. Conclusion As the threat of AMR continues to rise, the successful application of these models, particularly on genomic datasets from LMICs, signals a promising avenue for improving AMR prediction to support large AMR surveillance programs. This work thus not only expands our current understanding of the genetic underpinnings of AMR but also provides a robust methodological framework that can guide future research and applications in the fight against AMR.","<method>Support Vector Machine</method>, <method>Light Gradient Boosting Machine</method>, <method>Gradient Boosting</method>, <method>Logistic Regression</method>, <method>Random Forest</method>"
2024,https://openalex.org/W4393260925,Medicine,Accurate and interpretable drug-drug interaction prediction enabled by knowledge subgraph learning,"Abstract Background Discovering potential drug-drug interactions (DDIs) is a long-standing challenge in clinical treatments and drug developments. Recently, deep learning techniques have been developed for DDI prediction. However, they generally require a huge number of samples, while known DDIs are rare. Methods In this work, we present KnowDDI, a graph neural network-based method that addresses the above challenge. KnowDDI enhances drug representations by adaptively leveraging rich neighborhood information from large biomedical knowledge graphs. Then, it learns a knowledge subgraph for each drug-pair to interpret the predicted DDI, where each of the edges is associated with a connection strength indicating the importance of a known DDI or resembling strength between a drug-pair whose connection is unknown. Thus, the lack of DDIs is implicitly compensated by the enriched drug representations and propagated drug similarities. Results Here we show the evaluation results of KnowDDI on two benchmark DDI datasets. Results show that KnowDDI obtains the state-of-the-art prediction performance with better interpretability. We also find that KnowDDI suffers less than existing works given a sparser knowledge graph. This indicates that the propagated drug similarities play a more important role in compensating for the lack of DDIs when the drug representations are less enriched. Conclusions KnowDDI nicely combines the efficiency of deep learning techniques and the rich prior knowledge in biomedical knowledge graphs. As an original open-source tool, KnowDDI can help detect possible interactions in a broad range of relevant interaction prediction tasks, such as protein-protein interactions, drug-target interactions and disease-gene interactions, eventually promoting the development of biomedicine and healthcare.","<method>deep learning techniques</method>, <method>graph neural network-based method</method>"
2024,https://openalex.org/W4393276564,Medicine,Artificial intelligence–based assessment of built environment from Google Street View and coronary artery disease prevalence,"Abstract Background and Aims Built environment plays an important role in the development of cardiovascular disease. Tools to evaluate the built environment using machine vision and informatic approaches have been limited. This study aimed to investigate the association between machine vision–based built environment and prevalence of cardiometabolic disease in US cities. Methods This cross-sectional study used features extracted from Google Street View (GSV) images to measure the built environment and link them with prevalence of coronary heart disease (CHD). Convolutional neural networks, linear mixed-effects models, and activation maps were utilized to predict health outcomes and identify feature associations with CHD at the census tract level. The study obtained 0.53 million GSV images covering 789 census tracts in seven US cities (Cleveland, OH; Fremont, CA; Kansas City, MO; Detroit, MI; Bellevue, WA; Brownsville, TX; and Denver, CO). Results Built environment features extracted from GSV using deep learning predicted 63% of the census tract variation in CHD prevalence. The addition of GSV features improved a model that only included census tract-level age, sex, race, income, and education or composite indices of social determinant of health. Activation maps from the features revealed a set of neighbourhood features represented by buildings and roads associated with CHD prevalence. Conclusions In this cross-sectional study, the prevalence of CHD was associated with built environment factors derived from GSV through deep learning analysis, independent of census tract demographics. Machine vision–enabled assessment of the built environment could potentially offer a more precise approach to identify at-risk neighbourhoods, thereby providing an efficient avenue to address and reduce cardiovascular health disparities in urban environments.","<method>convolutional neural networks</method>, <method>linear mixed-effects models</method>, <method>activation maps</method>"
2024,https://openalex.org/W4394581105,Medicine,Machine learning-based detection of acute psychosocial stress from body posture and movements,"Abstract Investigating acute stress responses is crucial to understanding the underlying mechanisms of stress. Current stress assessment methods include self-reports that can be biased and biomarkers that are often based on complex laboratory procedures. A promising additional modality for stress assessment might be the observation of body movements, which are affected by negative emotions and threatening situations. In this paper, we investigated the relationship between acute psychosocial stress induction and body posture and movements. We collected motion data from N = 59 individuals over two studies ( Pilot Study : N = 20, Main Study : N = 39) using inertial measurement unit (IMU)-based motion capture suits. In both studies, individuals underwent the Trier Social Stress Test (TSST) and a stress-free control condition (friendly-TSST; f-TSST) in randomized order. Our results show that acute stress induction leads to a reproducible freezing behavior, characterized by less overall motion as well as more and longer periods of no movement. Based on these data, we trained machine learning pipelines to detect acute stress solely from movement information, achieving an accuracy of $${75.0 \pm 17.7}{\%}$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mrow> <mml:mrow> <mml:mn>75.0</mml:mn> <mml:mo>±</mml:mo> <mml:mn>17.7</mml:mn> </mml:mrow> <mml:mo>%</mml:mo> </mml:mrow> </mml:math> ( Pilot Study ) and $${73.4 \pm 7.7}{\%}$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mrow> <mml:mrow> <mml:mn>73.4</mml:mn> <mml:mo>±</mml:mo> <mml:mn>7.7</mml:mn> </mml:mrow> <mml:mo>%</mml:mo> </mml:mrow> </mml:math> ( Main Study ). This, for the first time, suggests that body posture and movements can be used to detect whether individuals are exposed to acute psychosocial stress. While more studies are needed to further validate our approach, we are convinced that motion information can be a valuable extension to the existing biomarkers and can help to obtain a more holistic picture of the human stress response. Our work is the first to systematically explore the use of full-body body posture and movement to gain novel insights into the human stress response and its effects on the body and mind.",<method>machine learning pipelines</method>
2024,https://openalex.org/W4394693547,Medicine,Urban morphology clustering analysis to identify heat-prone neighbourhoods in cities,"Exposure to heat is a major health concern to urban populations. Cities aim to reduce outdoor thermal stress by adapting the built environment, but the spatial heterogeneity within cities makes it difficult to establish universal mitigation strategies. We present a methodology that identifies the hottest neighbourhoods in a city and links them to underlying patterns in urban form and function, to derive heat mitigation measures for individual neighbourhoods according to their characteristics, mitigation potential, and average surface temperature. The method applies k-means clustering and is applicable to any city using available datasets on surface cover and building morphology, as well as globally available satellite measurements of surface temperatures. Here, we present a heat-mitigation analysis for the city of Zurich. The clustering differentiates seven neighbourhood types, including two types of residential areas, modern neighbourhoods with high-rise buildings, historical districts, and industrial zones. The hottest temperatures are in neighbourhoods with extensive impervious ground cover such as railway tracks and airport parking. Surface temperatures strongly correlate with impervious surface cover and vegetation cover for all neighbourhoods, with building cover only for non-industrial built neighbourhoods, and with sky-view factor for all neighbourhoods except those with large vegetation cover. Historical, modern, and industrial neighbourhoods are particular heat-prone, and increasing vegetation for evaporative cooling is a suggested mitigation strategy for all. Modern and industrial areas could benefit from shading through increase of tree cover, while historical centres may adapt vertical greening as suitable heat mitigation strategy.",<method>k-means clustering</method>
2024,https://openalex.org/W4394767609,Medicine,Cardiac Arrhythmia Classification Using Advanced Deep Learning Techniques on Digitized ECG Datasets,"ECG classification or heartbeat classification is an extremely valuable tool in cardiology. Deep learning-based techniques for the analysis of ECG signals assist human experts in the timely diagnosis of cardiac diseases and help save precious lives. This research aims at digitizing a dataset of images of ECG records into time series signals and then applying deep learning (DL) techniques on the digitized dataset. State-of-the-art DL techniques are proposed for the classification of the ECG signals into different cardiac classes. Multiple DL models, including a convolutional neural network (CNN), a long short-term memory (LSTM) network, and a self-supervised learning (SSL)-based model using autoencoders are explored and compared in this study. The models are trained on the dataset generated from ECG plots of patients from various healthcare institutes in Pakistan. First, the ECG images are digitized, segmenting the lead II heartbeats, and then the digitized signals are passed to the proposed deep learning models for classification. Among the different DL models used in this study, the proposed CNN model achieves the highest accuracy of ∼92%. The proposed model is highly accurate and provides fast inference for real-time and direct monitoring of ECG signals that are captured from the electrodes (sensors) placed on different parts of the body. Using the digitized form of ECG signals instead of images for the classification of cardiac arrhythmia allows cardiologists to utilize DL models directly on ECG signals from an ECG machine for the real-time and accurate monitoring of ECGs.","<method>deep learning (DL) techniques</method>, <method>convolutional neural network (CNN)</method>, <method>long short-term memory (LSTM) network</method>, <method>self-supervised learning (SSL)-based model using autoencoders</method>"
2024,https://openalex.org/W4396832329,Medicine,Rethinking Human-AI Collaboration in Complex Medical Decision Making: A Case Study in Sepsis Diagnosis,"Today's AI systems for medical decision support often succeed on benchmark datasets in research papers but fail in real-world deployment. This work focuses on the decision making of sepsis, an acute life-threatening systematic infection that requires an early diagnosis with high uncertainty from the clinician. Our aim is to explore the design requirements for AI systems that can support clinical experts in making better decisions for the early diagnosis of sepsis. The study begins with a formative study investigating why clinical experts abandon an existing AI-powered Sepsis predictive module in their electrical health record (EHR) system. We argue that a human-centered AI system needs to support human experts in the intermediate stages of a medical decision-making process (e.g., generating hypotheses or gathering data), instead of focusing only on the final decision. Therefore, we build SepsisLab based on a state-of-the-art AI algorithm and extend it to predict the future projection of sepsis development, visualize the prediction uncertainty, and propose actionable suggestions (i.e., which additional laboratory tests can be collected) to reduce such uncertainty. Through heuristic evaluation with six clinicians using our prototype system, we demonstrate that SepsisLab enables a promising human-AI collaboration paradigm for the future of AI-assisted sepsis diagnosis and other high-stakes medical decision making.",<method>state-of-the-art AI algorithm</method>
2024,https://openalex.org/W4400429759,Medicine,Deep learning for lungs cancer detection: a review,"Abstract Although lung cancer has been recognized to be the deadliest type of cancer, a good prognosis and efficient treatment depend on early detection. Medical practitioners’ burden is reduced by deep learning techniques, especially Deep Convolutional Neural Networks (DCNN), which are essential in automating the diagnosis and classification of diseases. In this study, we use a variety of medical imaging modalities, including X-rays, WSI, CT scans, and MRI, to thoroughly investigate the use of deep learning techniques in the field of lung cancer diagnosis and classification. This study conducts a comprehensive Systematic Literature Review (SLR) using deep learning techniques for lung cancer research, providing a comprehensive overview of the methodology, cutting-edge developments, quality assessments, and customized deep learning approaches. It presents data from reputable journals and concentrates on the years 2015–2024. Deep learning techniques solve the difficulty of manually identifying and selecting abstract features from lung cancer images. This study includes a wide range of deep learning methods for classifying lung cancer but focuses especially on the most popular method, the Convolutional Neural Network (CNN). CNN can achieve maximum accuracy because of its multi-layer structure, automatic learning of weights, and capacity to communicate local weights. Various algorithms are shown with performance measures like precision, accuracy, specificity, sensitivity, and AUC; CNN consistently shows the greatest accuracy. The findings highlight the important contributions of DCNN in improving lung cancer detection and classification, making them an invaluable resource for researchers looking to gain a greater knowledge of deep learning’s function in medical applications.","<method>Deep Convolutional Neural Networks (DCNN)</method>, <method>deep learning techniques</method>, <method>Convolutional Neural Network (CNN)</method>"
2024,https://openalex.org/W4405128863,Medicine,Machine Learning in Public Health Forecasting and Monitoring the Zika Virus,"The Zika virus is a severe public health threat all across the world, owing to its spreading mechanism through Aedes mosquitoes and its ability to result in extreme neurological diseases, which include the congenital Zika syndrome and the Guillain-Barré syndrome, amongst others. Conventional monitoring techniques often fail because many asymptomatic cases render early diagnosis challenging. Machine learning (ML) techniques can be seen as a constructive development in addressing this challenge, which entails predicting and tracking the spread of diseases such as Zika through extensive and complex datasets. Data analytic ML systems also enhance early warning systems and situational uplift by using data from social media, climate history, and genetics. This helps reasonably to predict the mosquito population biologically and the environmental factors that favor the spread of the virus for a more practical approach from the public health sector. Over and above, some issues are still pending, especially regarding the quality of data, understanding the models and how to apply such models within the current health systems. These factors must be solved to implement ML successfully in surveillance practice. This review provides an overview of the issue, stating the potential of machine learning applications in the development of public health, whose actions focus on Zika and other diseases transmitted by vectors.","<method>Machine learning (ML) techniques</method>, <method>Data analytic ML systems</method>"
2024,https://openalex.org/W4390483004,Medicine,Population-Specific Glucose Prediction in Diabetes Care With Transformer-Based Deep Learning on the Edge,"Leveraging continuous glucose monitoring (CGM) systems, real-time blood glucose (BG) forecasting is essential for proactive interventions, playing a crucial role in enhancing the management of type 1 diabetes (T1D) and type 2 diabetes (T2D). However, developing a model generalized to a population and subsequently embedding it within a microchip of a wearable device presents significant technical challenges. Furthermore, the domain of BG prediction in T2D remains under-explored in the literature. In light of this, we propose a population-specific BG prediction model, leveraging the capabilities of the temporal fusion Transformer (TFT) to adjust predictions based on personal demographic data. Then the trained model is embedded within a system-on-chip, integral to our low-power and low-cost customized wearable device. This device seamlessly communicates with CGM systems through Bluetooth and provides timely BG predictions using edge computing. When evaluated on two publicly available clinical datasets with a total of 124 participants with T1D or T2D, the embedded TFT model consistently demonstrated superior performance, achieving the lowest prediction errors when compared with a range of machine learning baseline methods. Executing the TFT model on our wearable device requires minimal memory and power consumption, enabling continuous decision support for more than 51 days on a single Li-Poly battery charge. These findings demonstrate the significant potential of the proposed TFT model and wearable device in enhancing the quality of life for people with diabetes and effectively addressing real-world challenges.","<method>temporal fusion Transformer (TFT)</method>, <method>machine learning baseline methods</method>"
2024,https://openalex.org/W4390506124,Medicine,Machine learning models for predicting preeclampsia: a systematic review,"Abstract Background This systematic review provides an overview of machine learning (ML) approaches for predicting preeclampsia. Method This review adhered to the Preferred Reporting Items for Systematic Reviews and Meta-Analyzes (PRISMA) guidelines. We searched the Cochrane Central Register, PubMed, EMBASE, ProQuest, Scopus, and Google Scholar up to February 2023. Search terms were limited to “preeclampsia” AND “artificial intelligence” OR “machine learning” OR “deep learning.” All studies that used ML-based analysis for predicting preeclampsia in pregnant women were considered. Non-English articles and those that are unrelated to the topic were excluded. The PROBAST was used to assess the risk of bias and applicability of each included study. Results The search strategy yielded 128 citations; after duplicates were removed and title and abstract screening was completed, 18 full-text articles were evaluated for eligibility. Four studies were included in this review. Two studies were at low risk of bias, and two had low to moderate risk. All of the study designs included were retrospective cohort studies. Nine distinct models were chosen as ML models from the four studies. Maternal characteristics, medical history, medication intake, obstetrical history, and laboratory and ultrasound findings obtained during prenatal care visits were candidate predictors to train the ML model. Elastic net, stochastic gradient boosting, extreme gradient boosting, and Random forest were among the best models to predict preeclampsia. All four studies used metrics such as the area under the curve, true positive rate, negative positive rate, accuracy, precision, recall, and F1 score. The AUC of ML models varied from 0.860 to 0.973 in four studies. Conclusion The results of studies yielded high prediction performance of ML models for preeclampsia risk from routine early pregnancy information.","<method>Elastic net</method>, <method>stochastic gradient boosting</method>, <method>extreme gradient boosting</method>, <method>Random forest</method>"
2024,https://openalex.org/W4390508699,Medicine,Burnout among public health workers in Canada: a cross-sectional study,"Abstract Background This study presents the prevalence of burnout among the Canadian public health workforce after three years of the COVID-19 pandemic and its association with work-related factors. Methods Data were collected using an online survey distributed through Canadian public health associations and professional networks between November 2022 and January 2023. Burnout was measured using a modified version of the Oldenburg Burnout Inventory (OLBI). Logistic regressions were used to model the relationship between burnout and work-related factors including years of work experience, redeployment to pandemic response, workplace safety and supports, and harassment. Burnout and the intention to leave or retire as a result of the COVID-19 pandemic was explored using multinomial logistic regressions. Results In 2,079 participants who completed the OLBI, the prevalence of burnout was 78.7%. Additionally, 49.1% of participants reported being harassed because of their work during the pandemic. Burnout was positively associated with years of work experience, redeployment to the pandemic response, being harassed during the pandemic, feeling unsafe in the workplace and not being offered workplace supports. Furthermore, burnout was associated with greater odds of intending to leave public health or retire earlier than anticipated. Conclusion The high levels of burnout among our large sample of Canadian public health workers and its association with work-related factors suggest that public health organizations should consider interventions that mitigate burnout and promote recovery.",No methods found.
2024,https://openalex.org/W4390563519,Medicine,Association between dietary antioxidant intakes and chronic respiratory diseases in adults,"BackgroundChronic respiratory diseases (CRDs) pose a significant global health burden. Antioxidant-rich diets have been associated with improved lung health, but the specific relationship with CRDs remains unclear.MethodsThis study examined the relationship between dietary antioxidant intakes and CRDs using data from the 2001–2018 National Health and Nutrition Examination Survey (NHANES). Information on dietary antioxidant intakes, including vitamins A, C, and E, zinc, selenium, and carotenoid, were collected from the 2 24-h recall interviews to calculate composite dietary antioxidant index (CDAI). CRDs were determined based on self-reported physician diagnoses. To examine the relationship between CDAI and CRDs, multivariate logistic regression was used. To study potential non-linear correlations within these associations, restricted cubic spline (RCS) regression was performed.ResultsThe study involved 40 557 individuals. The median CDAI was −0.09 (−2.05, 2.25). We discovered those who were in the fourth quartile of CDAI scores had a 19% lower prevalence than those in the first quartile (OR = 0.81 [0.72–0.91], Ptrend < 0.01) after adjusting for all relevant covariates. The fourth quartile of CDAI was linked with a lower prevalence of emphysema (OR = 0.57 [0.40–0.81], Ptrend < 0.01) and chronic bronchitis (OR = 0.74 [0.62–0.88], Ptrend < 0.01). RCS regression showed that CDAI was non-linearly related to the prevalence of CRDs, with inflection points of 3.20 (P for non-linearity <0.01). The stratified analysis did not identify variables that significantly affected the results.ConclusionHigher dietary antioxidant intakes were related with a lower prevalence of CRDs (particularly emphysema and chronic bronchitis) in general adults.","<method>multivariate logistic regression</method>, <method>restricted cubic spline (RCS) regression</method>"
2024,https://openalex.org/W4390618348,Medicine,Artificial intelligence auxiliary diagnosis and treatment system for breast cancer in developing countries,"BACKGROUND: In many developing countries, a significant number of breast cancer patients are unable to receive timely treatment due to a large population base, high patient numbers, and limited medical resources. OBJECTIVE: This paper proposes a breast cancer assisted diagnosis system based on electronic medical records. The goal of this system is to address the limitations of existing systems, which primarily rely on structured electronic records and may miss crucial information stored in unstructured records. METHODS: The proposed approach is a breast cancer assisted diagnosis system based on electronic medical records. The system utilizes breast cancer enhanced convolutional neural networks with semantic initialization filters (BC-INIT-CNN). It extracts highly relevant tumor markers from unstructured medical records to aid in breast cancer staging diagnosis and effectively utilizes the important information present in unstructured records. RESULTS: The model’s performance is assessed using various evaluation metrics. Such as accuracy, ROC curves, and Precision-Recall curves. Comparative analysis demonstrates that the BC-INIT-CNN model outperforms several existing methods in terms of accuracy and computational efficiency. CONCLUSIONS: The proposed breast cancer assisted diagnosis system based on BC-INIT-CNN showcases the potential to address the challenges faced by developing countries in providing timely treatment to breast cancer patients. By leveraging unstructured medical records and extracting relevant tumor markers, the system enables accurate staging diagnosis and enhances the utilization of valuable information.",<method>breast cancer enhanced convolutional neural networks with semantic initialization filters (BC-INIT-CNN)</method>
2024,https://openalex.org/W4391230466,Medicine,Metabolic syndrome and epigenetic aging: a twin study,"Abstract Background Metabolic syndrome (MetS) is associated with premature aging, but whether this association is driven by genetic or lifestyle factors remains unclear. Methods Two independent discovery cohorts, consisting of twins and unrelated individuals, were examined ( N = 268, aged 23–69 years). The findings were replicated in two cohorts from the same base population. One consisted of unrelated individuals ( N = 1 564), and the other of twins ( N = 293). Participants’ epigenetic age, estimated using blood DNA methylation data, was determined using the epigenetic clocks GrimAge and DunedinPACE. The individual-level linear regression models for investigating the associations of MetS and its components with epigenetic aging were followed by within-twin-pair analyses using fixed-effects regression models to account for genetic factors. Results In individual-level analyses, GrimAge age acceleration was higher among participants with MetS ( N = 56) compared to participants without MetS ( N = 212) (mean 2.078 [95% CI = 0.996,3.160] years vs. −0.549 [−1.053,−0.045] years, between-group p = 3.5E-5). Likewise, the DunedinPACE estimate was higher among the participants with MetS compared to the participants without MetS (1.032 [1.002,1.063] years/calendar year vs. 0.911 [0.896,0.927] years/calendar year, p = 4.8E-11). An adverse profile in terms of specific MetS components was associated with accelerated aging. However, adjustments for lifestyle attenuated these associations; nevertheless, for DunedinPACE, they remained statistically significant. The within-twin-pair analyses suggested that genetics explains these associations fully for GrimAge and partly for DunedinPACE. The replication analyses provided additional evidence that the association between MetS components and accelerated aging is independent of the lifestyle factors considered in this study, however, suggesting that genetics is a significant confounder in this association. Conclusions The results of this study suggests that MetS is associated with accelerated epigenetic aging, independent of physical activity, smoking or alcohol consumption, and that the association may be explained by genetics.",No methods found.
2024,https://openalex.org/W4391450907,Medicine,Publicly available datasets of breast histopathology H&amp;E whole-slide images: A scoping review,"Advancements in digital pathology and computing resources have made a significant impact in the field of computational pathology for breast cancer diagnosis and treatment. However, access to high-quality labeled histopathological images of breast cancer is a big challenge that limits the development of accurate and robust deep learning models. In this scoping review, we identified the publicly available datasets of breast H&E-stained whole-slide images (WSIs) that can be used to develop deep learning algorithms. We systematically searched 9 scientific literature databases and 9 research data repositories and found 17 publicly available datasets containing 10 385 H&E WSIs of breast cancer. Moreover, we reported image metadata and characteristics for each dataset to assist researchers in selecting proper datasets for specific tasks in breast cancer computational pathology. In addition, we compiled 2 lists of breast H&E patches and private datasets as supplementary resources for researchers. Notably, only 28% of the included articles utilized multiple datasets, and only 14% used an external validation set, suggesting that the performance of other developed models may be susceptible to overestimation. The TCGA-BRCA was used in 52% of the selected studies. This dataset has a considerable selection bias that can impact the robustness and generalizability of the trained algorithms. There is also a lack of consistent metadata reporting of breast WSI datasets that can be an issue in developing accurate deep learning models, indicating the necessity of establishing explicit guidelines for documenting breast WSI dataset characteristics and metadata.",No methods found.
2024,https://openalex.org/W4392867820,Medicine,Financial toxicity and its risk factors among patients with cancer in China: A nationwide multisite study,"ObjectiveWe assessed financial toxicity (FT) among Chinese patients with cancer and investigated associated risk factors guided by a multilevel conceptual framework.MethodsEmploying multistage stratified sampling, we selected six tertiary and six secondary hospitals across three economically diverse provinces in China. From February to October 2022, 1,208 patients with cancer participated. FT was measured using the Comprehensive Score for financial Toxicity (COST), with 28 potential risk factors identified at multilevel. Multiple regression analysis was employed for risk factor identification.ResultsFT prevalence was 82.6% (95% confidence interval [CI]: 80.5%, 84.8%), with high FT (COST score≤18.5) observed in 40.9% of participants (95% CI: 38.1%, 43.7%). Significant risk factors included younger age at cancer diagnosis, unmarried status, low annual household income, negative impact of cancer on participants' or family caregiver's work, advanced cancer stage, longer hospital stay for cancer treatment or treatment-related side effects, high perceived stress, poor emotional/informational support, lack of social medical insurance or having urban and rural resident basic medical insurance, lack of commercial medical insurance, tertiary hospital treatment, and inadequate cost discussions with healthcare providers (all p < 0.05).ConclusionsCancer-related FT is prevalent in China, contributing to disparities in cancer care access and health-related outcomes. The risk factors associated with cancer-related FT encompasses multilevel, including patient/family, provider/practice, and payer/policy levels. There is an urgent need for collective efforts by patients, healthcare providers, policymakers, and insurers to safeguard the financial security and well-being of individuals affected by cancer, promoting health equities in the realm of cancer care. We assessed financial toxicity (FT) among Chinese patients with cancer and investigated associated risk factors guided by a multilevel conceptual framework. Employing multistage stratified sampling, we selected six tertiary and six secondary hospitals across three economically diverse provinces in China. From February to October 2022, 1,208 patients with cancer participated. FT was measured using the Comprehensive Score for financial Toxicity (COST), with 28 potential risk factors identified at multilevel. Multiple regression analysis was employed for risk factor identification. FT prevalence was 82.6% (95% confidence interval [CI]: 80.5%, 84.8%), with high FT (COST score≤18.5) observed in 40.9% of participants (95% CI: 38.1%, 43.7%). Significant risk factors included younger age at cancer diagnosis, unmarried status, low annual household income, negative impact of cancer on participants' or family caregiver's work, advanced cancer stage, longer hospital stay for cancer treatment or treatment-related side effects, high perceived stress, poor emotional/informational support, lack of social medical insurance or having urban and rural resident basic medical insurance, lack of commercial medical insurance, tertiary hospital treatment, and inadequate cost discussions with healthcare providers (all p < 0.05). Cancer-related FT is prevalent in China, contributing to disparities in cancer care access and health-related outcomes. The risk factors associated with cancer-related FT encompasses multilevel, including patient/family, provider/practice, and payer/policy levels. There is an urgent need for collective efforts by patients, healthcare providers, policymakers, and insurers to safeguard the financial security and well-being of individuals affected by cancer, promoting health equities in the realm of cancer care.",No methods found.
2024,https://openalex.org/W4392915224,Medicine,"An Extensive Investigation into the Use of Machine Learning Tools and Deep Neural Networks for the Recognition of Skin Cancer: Challenges, Future Directions, and a Comprehensive Review","Skin cancer poses a serious risk to one’s health and can only be effectively treated with early detection. Early identification is critical since skin cancer has a higher fatality rate, and it expands gradually to different areas of the body. The rapid growth of automated diagnosis frameworks has led to the combination of diverse machine learning, deep learning, and computer vision algorithms for detecting clinical samples and atypical skin lesion specimens. Automated methods for recognizing skin cancer that use deep learning techniques are discussed in this article: convolutional neural networks, and, in general, artificial neural networks. The recognition of symmetries is a key point in dealing with the skin cancer image datasets; hence, in developing the appropriate architecture of neural networks, as it can improve the performance and release capacities of the network. The current study emphasizes the need for an automated method to identify skin lesions to reduce the amount of time and effort required for the diagnostic process, as well as the novel aspect of using algorithms based on deep learning for skin lesion detection. The analysis concludes with underlying research directions for the future, which will assist in better addressing the difficulties encountered in human skin cancer recognition. By highlighting the drawbacks and advantages of prior techniques, the authors hope to establish a standard for future analysis in the domain of human skin lesion diagnostics.","<method>convolutional neural networks</method>, <method>artificial neural networks</method>"
2024,https://openalex.org/W4395076002,Medicine,DenRAM: neuromorphic dendritic architecture with RRAM for efficient temporal processing with delays,"An increasing number of studies are highlighting the importance of spatial dendritic branching in pyramidal neurons in the neocortex for supporting non-linear computation through localized synaptic integration. In particular, dendritic branches play a key role in temporal signal processing and feature detection. This is accomplished thanks to coincidence detection (CD) mechanisms enabled by the presence of synaptic delays that align temporally disparate inputs for effective integration. Computational studies on spiking neural networks further highlight the significance of delays for achieving spatio-temporal pattern recognition with pure feed-forward neural networks, without the need of resorting to recurrent architectures. In this work, we present ""DenRAM"", the first realization of a feed-forward spiking neural network with dendritic compartments, implemented using analog electronic circuits integrated into a 130 nm technology node and coupled with Resistive Random Access Memory (RRAM) technology. DenRAM's dendritic circuits use RRAM devices to implement both delays and synaptic weights in the network. By configuring the RRAM devices to reproduce bio-realistic timescales, and by exploiting their heterogeneity we experimentally demonstrate DenRAM's ability to replicate synaptic delay profiles, and to efficiently implement CD for spatio-temporal pattern recognition. To validate the architecture, we conduct comprehensive system-level simulations on two representative temporal benchmarks, demonstrating DenRAM's resilience to analog hardware noise, and its superior accuracy compared to recurrent architectures with an equivalent number of parameters. DenRAM not only brings rich temporal processing capabilities to neuromorphic architectures, but also reduces the memory footprint of edge devices, warrants high accuracy on temporal benchmarks, and represents a significant step-forward in low-power real-time signal processing technologies.",<method>feed-forward spiking neural network</method>
2024,https://openalex.org/W4395084642,Medicine,CerviLearnNet: Advancing cervical cancer diagnosis with reinforcement learning-enhanced convolutional networks,"Women tend to face many problems throughout their lives; cervical cancer is one of the most dangerous diseases that they can face, and it has many negative consequences. Regular screening and treatment of precancerous lesions play a vital role in the fight against cervical cancer. It is becoming increasingly common in medical practice to predict the early stages of serious illnesses, such as heart attacks, kidney failure, and cancer, using machine learning-based techniques. To overcome these obstacles, we propose the use of auxiliary modules and a special residual block, to record contextual interactions between object classes and to support the object reference strategy. Unlike the latest state-of-the-art classification method, we create a new architecture called the Reinforcement Learning Cancer Network, ""RL-CancerNet"", which diagnoses cervical cancer with incredible accuracy. We trained and tested our method on two well-known publicly available datasets, SipaKMeD and Herlev, to assess it and enable comparisons with earlier methods. Cervical cancer images were labeled in this dataset; therefore, they had to be marked manually. Our study shows that, compared to previous approaches for the assignment of classifying cervical cancer as an early cellular change, the proposed approach generates a more reliable and stable image derived from images of datasets of vastly different sizes, indicating that it will be effective for other datasets.","<method>machine learning-based techniques</method>, <method>Reinforcement Learning Cancer Network (RL-CancerNet)</method>"
2024,https://openalex.org/W4396622164,Medicine,Colon and lung cancer classification from multi-modal images using resilient and efficient neural network architectures,"Automatic classification of colon and lung cancer images is crucial for early detection and accurate diagnostics. However, there is room for improvement to enhance accuracy, ensuring better diagnostic precision. This study introduces two novel dense architectures (D1 and D2) and emphasizes their effectiveness in classifying colon and lung cancer from diverse images. It also highlights their resilience, efficiency, and superior performance across multiple datasets. These architectures were tested on various types of datasets, including NCT-CRC-HE-100K (set of 100,000 non-overlapping image patches from hematoxylin and eosin (H&E) stained histological images of human colorectal cancer (CRC) and normal tissue), CRC-VAL-HE-7K (set of 7180 image patches from N=50 patients with colorectal adenocarcinoma, no overlap with patients in NCT-CRC-HE-100K), LC25000 (Lung and Colon Cancer Histopathological Image), and IQ-OTHNCCD (Iraq-Oncology Teaching Hospital/National Center for Cancer Diseases), showcasing their effectiveness in classifying colon and lung cancers from histopathological and Computed Tomography (CT) scan images. This underscores the multi-modal image classification capability of the proposed models. Moreover, the study addresses imbalanced datasets, particularly in CRC-VAL-HE-7K and IQ-OTHNCCD, with a specific focus on model resilience and robustness. To assess overall performance, the study conducted experiments in different scenarios. The D1 model achieved an impressive 99.80% accuracy on the NCT-CRC-HE-100K dataset, with a Jaccard Index (J) of 0.8371, a Matthew's Correlation Coefficient (MCC) of 0.9073, a Cohen's Kappa (Kp) of 0.9057, and a Critical Success Index (CSI) of 0.8213. When subjected to 10-fold cross-validation on LC25000, the D1 model averaged (avg) 99.96% accuracy (avg J, MCC, Kp, and CSI of 0.9993, 0.9987, 0.9853, and 0.9990), surpassing recent reported performances. Furthermore, the ensemble of D1 and D2 reached 93% accuracy (J, MCC, Kp, and CSI of 0.7556, 0.8839, 0.8796, and 0.7140) on the IQ-OTHNCCD dataset, exceeding recent benchmarks and aligning with other reported results. Efficiency evaluations were conducted in various scenarios. For instance, training on only 10% of LC25000 resulted in high accuracy rates of 99.19% (J, MCC, Kp, and CSI of 0.9840, 0.9898, 0.9898, and 0.9837) (D1) and 99.30% (J, MCC, Kp, and CSI of 0.9863, 0.9913, 0.9913, and 0.9861) (D2). In NCT-CRC-HE-100K, D2 achieved an impressive 99.53% accuracy (J, MCC, Kp, and CSI of 0.9906, 0.9946, 0.9946, and 0.9906) with training on only 30% of the dataset and testing on the remaining 70%. When tested on CRC-VAL-HE-7K, D1 and D2 achieved 95% accuracy (J, MCC, Kp, and CSI of 0.8845, 0.9455, 0.9452, and 0.8745) and 96% accuracy (J, MCC, Kp, and CSI of 0.8926, 0.9504, 0.9503, and 0.8798), respectively, outperforming previously reported results and aligning closely with others. Lastly, training D2 on just 10% of NCT-CRC-HE-100K and testing on CRC-VAL-HE-7K resulted in significant outperformance of InceptionV3, Xception, and DenseNet201 benchmarks, achieving an accuracy rate of 82.98% (J, MCC, Kp, and CSI of 0.7227, 0.8095, 0.8081, and 0.6671). Finally, using explainable AI algorithms such as Grad-CAM, Grad-CAM++, Score-CAM, and Faster Score-CAM, along with their emphasized versions, we visualized the features from the last layer of DenseNet201 for histopathological as well as CT-scan image samples. The proposed dense models, with their multi-modality, robustness, and efficiency in cancer image classification, hold the promise of significant advancements in medical diagnostics. They have the potential to revolutionize early cancer detection and improve healthcare accessibility worldwide.","<method>dense architectures (D1 and D2)</method>, <method>ensemble of D1 and D2</method>, <method>InceptionV3</method>, <method>Xception</method>, <method>DenseNet201</method>, <method>Grad-CAM</method>, <method>Grad-CAM++</method>, <method>Score-CAM</method>, <method>Faster Score-CAM</method>"
2024,https://openalex.org/W4399322162,Medicine,Integration of deep learning and habitat radiomics for predicting the response to immunotherapy in NSCLC patients,"Abstract Background The non-invasive biomarkers for predicting immunotherapy response are urgently needed to prevent both premature cessation of treatment and ineffective extension. This study aimed to construct a non-invasive model for predicting immunotherapy response, based on the integration of deep learning and habitat radiomics in patients with advanced non-small cell lung cancer (NSCLC). Methods Independent patient cohorts from three medical centers were enrolled for training ( n = 164) and test ( n = 82). Habitat imaging radiomics features were derived from sub-regions clustered from individual’s tumor by K-means method. The deep learning features were extracted based on 3D ResNet algorithm. Pearson correlation coefficient, T test and least absolute shrinkage and selection operator regression were used to select features. Support vector machine was applied to implement deep learning and habitat radiomics, respectively. Then, a combination model was developed integrating both sources of data. Results The combination model obtained a strong well-performance, achieving area under receiver operating characteristics curve of 0.865 (95% CI 0.772–0.931). The model significantly discerned high and low-risk patients, and exhibited a significant benefit in the clinical use. Conclusion The integration of deep-leaning and habitat radiomics contributed to predicting response to immunotherapy in patients with NSCLC. The developed integration model may be used as potential tool for individual immunotherapy management.","<method>K-means</method>, <method>3D ResNet algorithm</method>, <method>least absolute shrinkage and selection operator regression</method>, <method>Support vector machine</method>"
2024,https://openalex.org/W4399986067,Medicine,Comparing machine learning algorithms to predict vegetation fire detections in Pakistan,"Abstract Vegetation fires have major impacts on the ecosystem and present a significant threat to human life. Vegetation fires consists of forest fires, cropland fires, and other vegetation fires in this study. Currently, there is a limited amount of research on the long-term prediction of vegetation fires in Pakistan. The exact effect of every factor on the frequency of vegetation fires remains unclear when using standard analysis. This research utilized the high proficiency of machine learning algorithms to combine data from several sources, including the MODIS Global Fire Atlas dataset, topographic, climatic conditions, and different vegetation types acquired between 2001 and 2022. We tested many algorithms and ultimately chose four models for formal data processing. Their selection was based on their performance metrics, such as accuracy, computational efficiency, and preliminary test results. The model’s logistic regression, a random forest, a support vector machine, and an eXtreme Gradient Boosting were used to identify and select the nine key factors of forest and cropland fires and, in the case of other vegetation, seven key factors that cause a fire in Pakistan. The findings indicated that the vegetation fire prediction models achieved prediction accuracies ranging from 78.7 to 87.5% for forest fires, 70.4 to 84.0% for cropland fires, and 66.6 to 83.1% for other vegetation. Additionally, the area under the curve (AUC) values ranged from 83.6 to 93.4% in forest fires, 72.6 to 90.6% in cropland fires, and 74.2 to 90.7% in other vegetation. The random forest model had the highest accuracy rate of 87.5% in forest fires, 84.0% in cropland fires, and 83.1% in other vegetation and also the highest AUC value of 93.4% in forest fires, 90.6% in cropland fires, and 90.7% in other vegetation, proving to be the most optimal performance model. The models provided predictive insights into specific conditions and regional susceptibilities to fire occurrences, adding significant value beyond the initial MODIS detection data. The maps generated to analyze Pakistan’s vegetation fire risk showed the geographical distribution of areas with high, moderate, and low vegetation fire risks, highlighting predictive risk assessments rather than historical fire detections.","<method>logistic regression</method>, <method>random forest</method>, <method>support vector machine</method>, <method>eXtreme Gradient Boosting</method>"
2024,https://openalex.org/W4400335482,Medicine,Enhancing ECG-based heart age: impact of acquisition parameters and generalization strategies for varying signal morphologies and corruptions,"Electrocardiogram (ECG) is a non-invasive approach to capture the overall electrical activity produced by the contraction and relaxation of the cardiac muscles. It has been established in the literature that the difference between ECG-derived age and chronological age represents a general measure of cardiovascular health. Elevated ECG-derived age strongly correlates with cardiovascular conditions (e.g., atherosclerotic cardiovascular disease). However, the neural networks for ECG age estimation are yet to be thoroughly evaluated from the perspective of ECG acquisition parameters. Additionally, deep learning systems for ECG analysis encounter challenges in generalizing across diverse ECG morphologies in various ethnic groups and are susceptible to errors with signals that exhibit random or systematic distortions To address these challenges, we perform a comprehensive empirical study to determine the threshold for the sampling rate and duration of ECG signals while considering their impact on the computational cost of the neural networks. To tackle the concern of ECG waveform variability in different populations, we evaluate the feasibility of utilizing pre-trained and fine-tuned networks to estimate ECG age in different ethnic groups. Additionally, we empirically demonstrate that finetuning is an environmentally sustainable way to train neural networks, and it significantly decreases the ECG instances required (by more than <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" id=""IM1""><mml:mn>100</mml:mn><mml:mo>×</mml:mo></mml:math> ) for attaining performance similar to the networks trained from random weight initialization on a complete dataset. Finally, we systematically evaluate augmentation schemes for ECG signals in the context of age estimation and introduce a random cropping scheme that provides best-in-class performance while using shorter-duration ECG signals. The results also show that random cropping enables the networks to perform well with systematic and random ECG signal corruptions.","<method>neural networks</method>, <method>pre-trained networks</method>, <method>fine-tuned networks</method>, <method>finetuning</method>, <method>augmentation schemes</method>, <method>random cropping scheme</method>"
2024,https://openalex.org/W4400896445,Medicine,Recent deep learning-based brain tumor segmentation models using multi-modality magnetic resonance imaging: a prospective survey,"Radiologists encounter significant challenges when segmenting and determining brain tumors in patients because this information assists in treatment planning. The utilization of artificial intelligence (AI), especially deep learning (DL), has emerged as a useful tool in healthcare, aiding radiologists in their diagnostic processes. This empowers radiologists to understand the biology of tumors better and provide personalized care to patients with brain tumors. The segmentation of brain tumors using multi-modal magnetic resonance imaging (MRI) images has received considerable attention. In this survey, we first discuss multi-modal and available magnetic resonance imaging modalities and their properties. Subsequently, we discuss the most recent DL-based models for brain tumor segmentation using multi-modal MRI. We divide this section into three parts based on the architecture: the first is for models that use the backbone of convolutional neural networks (CNN), the second is for vision transformer-based models, and the third is for hybrid models that use both convolutional neural networks and transformer in the architecture. In addition, in-depth statistical analysis is performed of the recent publication, frequently used datasets, and evaluation metrics for segmentation tasks. Finally, open research challenges are identified and suggested promising future directions for brain tumor segmentation to improve diagnostic accuracy and treatment outcomes for patients with brain tumors. This aligns with public health goals to use health technologies for better healthcare delivery and population health management.","<method>deep learning (DL)</method>, <method>convolutional neural networks (CNN)</method>, <method>vision transformer-based models</method>, <method>hybrid models that use both convolutional neural networks and transformer</method>"
2024,https://openalex.org/W4401363688,Medicine,Artificial intelligence applications in the diagnosis and treatment of bacterial infections,"The diagnosis and treatment of bacterial infections in the medical and public health field in the 21st century remain significantly challenging. Artificial Intelligence (AI) has emerged as a powerful new tool in diagnosing and treating bacterial infections. AI is rapidly revolutionizing epidemiological studies of infectious diseases, providing effective early warning, prevention, and control of outbreaks. Machine learning models provide a highly flexible way to simulate and predict the complex mechanisms of pathogen-host interactions, which is crucial for a comprehensive understanding of the nature of diseases. Machine learning-based pathogen identification technology and antimicrobial drug susceptibility testing break through the limitations of traditional methods, significantly shorten the time from sample collection to the determination of result, and greatly improve the speed and accuracy of laboratory testing. In addition, AI technology application in treating bacterial infections, particularly in the research and development of drugs and vaccines, and the application of innovative therapies such as bacteriophage, provides new strategies for improving therapy and curbing bacterial resistance. Although AI has a broad application prospect in diagnosing and treating bacterial infections, significant challenges remain in data quality and quantity, model interpretability, clinical integration, and patient privacy protection. To overcome these challenges and, realize widespread application in clinical practice, interdisciplinary cooperation, technology innovation, and policy support are essential components of the joint efforts required. In summary, with continuous advancements and in-depth application of AI technology, AI will enable doctors to more effectivelyaddress the challenge of bacterial infection, promoting the development of medical practice toward precision, efficiency, and personalization; optimizing the best nursing and treatment plans for patients; and providing strong support for public health safety.","<method>Artificial Intelligence (AI)</method>, <method>Machine learning models</method>, <method>Machine learning-based pathogen identification technology</method>, <method>Machine learning-based antimicrobial drug susceptibility testing</method>"
2024,https://openalex.org/W4403343918,Medicine,A comprehensive investigation of multimodal deep learning fusion strategies for breast cancer classification,"In breast cancer research, diverse data types and formats, such as radiological images, clinical records, histological data, and expression analysis, are employed. Given the intricate nature of natural phenomena, relying on the features of a single modality is seldom sufficient for comprehensive analysis. Therefore, it is possible to guarantee medical relevance and achieve improved clinical outcomes by combining several modalities. The presen study carefully maps and reviews 47 primary articles from six well-known digital libraries that were published between 2018 and 2023 for breast cancer classification based on multimodal deep learning fusion (MDLF) techniques. This systematic literature review encompasses various aspects, including the medical modalities combined, the datasets utilized in these studies, the techniques, models, and architectures used in MDLF and it also discusses the advantages and limitations of each approach. The analysis of selected papers has revealed a compelling trend: the emergence of new modalities and combinations that were previously unexplored in the context of breast cancer classification. This exploration has not only expanded the scope of predictive models but also introduced fresh perspectives for addressing diverse targets, ranging from screening to diagnosis and prognosis. The practical advantages of MDLF are evident in its ability to enhance the predictive capabilities of machine learning models, resulting in improved accuracy across diverse applications. The prevalence of deep learning models underscores their success in autonomously discerning complex patterns, offering a substantial departure from traditional machine learning approaches. Furthermore, the paper explores the challenges and future directions in this field, including the need for larger datasets, the use of ensemble learning methods, and the interpretation of multimodal models.","<method>multimodal deep learning fusion (MDLF)</method>, <method>machine learning models</method>, <method>deep learning models</method>, <method>ensemble learning methods</method>"
2024,https://openalex.org/W4390662926,Medicine,Artificial intelligence performance in detecting lymphoma from medical imaging: a systematic review and meta-analysis,"Abstract Background Accurate diagnosis and early treatment are essential in the fight against lymphatic cancer. The application of artificial intelligence (AI) in the field of medical imaging shows great potential, but the diagnostic accuracy of lymphoma is unclear. This study was done to systematically review and meta-analyse researches concerning the diagnostic performance of AI in detecting lymphoma using medical imaging for the first time. Methods Searches were conducted in Medline, Embase, IEEE and Cochrane up to December 2023. Data extraction and assessment of the included study quality were independently conducted by two investigators. Studies that reported the diagnostic performance of an AI model/s for the early detection of lymphoma using medical imaging were included in the systemic review. We extracted the binary diagnostic accuracy data to obtain the outcomes of interest: sensitivity (SE), specificity (SP), and Area Under the Curve (AUC). The study was registered with the PROSPERO, CRD42022383386. Results Thirty studies were included in the systematic review, sixteen of which were meta-analyzed with a pooled sensitivity of 87% (95%CI 83–91%), specificity of 94% (92–96%), and AUC of 97% (95–98%). Satisfactory diagnostic performance was observed in subgroup analyses based on algorithms types (machine learning versus deep learning, and whether transfer learning was applied), sample size (≤ 200 or &gt; 200), clinicians versus AI models and geographical distribution of institutions (Asia versus non-Asia). Conclusions Even if possible overestimation and further studies with a better standards for application of AI algorithms in lymphoma detection are needed, we suggest the AI may be useful in lymphoma diagnosis.","<method>machine learning</method>, <method>deep learning</method>, <method>transfer learning</method>"
2024,https://openalex.org/W4390796562,Medicine,"If we build it together, will they use it? A mixed-methods study evaluating the implementation of Prep-to-Play PRO: an injury prevention programme for women’s elite Australian Football","Objectives We evaluated the implementation of Prep-to-Play PRO, an injury prevention programme for women’s elite Australian Football League (AFLW). Methods The Reach, Effectiveness, Adoption, Implementation and Maintenance (RE-AIM) of Prep-to-Play PRO were assessed based on the proportion of AFLW players and/or staff who: were aware of the programme (R), believed it may reduce anterior cruciate ligament injury (E), attempted to implement any/all programme components (A), implemented all intended components as practically as possible (I) and intended future programme implementation (M). Quantitative and qualitative data were triangulated to assess 58 RE-AIM items (evidence of yes/no/unsure/no evidence) and the 5 RE-AIM dimensions (fully achieved=evidence of yes on &gt;50% dimension items, partially achieved=50% of items evidence of yes and 50% unsure or 50% mix of unsure and unanswered, or not met=evidence of yes on &lt;50% dimension items). Results Multiple sources including AFLW training observations (n=7 total), post-implementation surveys (141 players, 25 staff), semistructured interviews (19 players, 13 staff) and internal programme records (9 staff) contributed to the RE-AIM assessment. After the 2019 season, 8 of 10 (80%) AFLW clubs fully met all five RE-AIM dimensions. All 10 clubs participating in the AFLW fully achieved the reach (R) dimension. One club partially achieved the implementation (I) dimension, and one club partially achieved the effectiveness (E) and adoption (A) dimensions. Conclusion The Prep-to-Play PRO injury prevention programme for the AFLW achieved high implementation, possibly due to the programme’s deliberately flexible approach coupled with our pragmatic definition of implementation. Engaging key stakeholders at multiple ecological levels (organisation, coaches, athletes) throughout programme development and implementation likely enhanced programme implementation.",No methods found.
2024,https://openalex.org/W4390906064,Medicine,A critical moment in machine learning in medicine: on reproducible and interpretable learning,"Over the past two decades, advances in computational power and data availability combined with increased accessibility to pre-trained models have led to an exponential rise in machine learning (ML) publications. While ML may have the potential to transform healthcare, this sharp increase in ML research output without focus on methodological rigor and standard reporting guidelines has fueled a reproducibility crisis. In addition, the rapidly growing complexity of these models compromises their interpretability, which currently impedes their successful and widespread clinical adoption. In medicine, where failure of such models may have severe implications for patients' health, the high requirements for accuracy, robustness, and interpretability confront ML researchers with a unique set of challenges. In this review, we discuss the semantics of reproducibility and interpretability, as well as related issues and challenges, and outline possible solutions to counteracting the ""black box"". To foster reproducibility, standard reporting guidelines need to be further developed and data or code sharing encouraged. Editors and reviewers may equally play a critical role by establishing high methodological standards and thus preventing the dissemination of low-quality ML publications. To foster interpretable learning, the use of simpler models more suitable for medical data can inform the clinician how results are generated based on input data. Model-agnostic explanation tools, sensitivity analysis, and hidden layer representations constitute further promising approaches to increase interpretability. Balancing model performance and interpretability are important to ensure clinical applicability. We have now reached a critical moment for ML in medicine, where addressing these issues and implementing appropriate solutions will be vital for the future evolution of the field.","<method>simpler models</method>, <method>model-agnostic explanation tools</method>, <method>sensitivity analysis</method>, <method>hidden layer representations</method>"
2024,https://openalex.org/W4391127198,Medicine,Risk predictions of surgical wound complications based on a machine learning algorithm: A systematic review,"Abstract Surgical wounds may arise due to harm inflicted upon soft tissue during surgical intervention, and many complications and injuries may accompany them. These complications can lead to prolonged hospitalization and poorer clinical outcomes. Also, Machine learning (ML) is a Section of artificial intelligence (AI) that has emerged in medical care and is increasingly used for diagnosis, complications, prognosis and recurrence prediction. This study aims to investigate surgical wound risk predictions and management using a ML algorithm by R programming language analysis. The systematic review, following PRISMA guidelines, spanned electronic databases using search terms like ‘machine learning’, ‘surgical’ and ‘wound’. Inclusion criteria covered experimental studies from 1990 to the present on ML's application in surgical wound evaluation. Exclusion criteria included studies lacking full text, focusing on ML in all surgeries, neglecting wound assessment and duplications. Two authors rigorously assessed titles, abstracts and full texts, excluding reviews and guidelines. Ultimately, relevant articles were then analysed. The present study identified nine articles employing ML for surgical wound management. The analysis encompassed various surgical procedures, including Cardiothoracic, Caesarean total abdominal colectomy, Burn plastic surgery, facial plastic surgery, laparotomy, minimal invasive surgery, hernia repair and unspecified surgeries. ML was skillful in evaluating surgical site infections (SSI) in seven studies, while two extended its use to burn‐grade diagnosis and wound classification. Support Vector Machine (SVM) and Convolutional Neural Network (CNN) were the most utilized algorithms. ANN achieved a 96% accuracy in facial plastic surgery wound management. CNN demonstrated commendable accuracies in various surgeries, and SVM exhibited high accuracy in multiple surgeries and burn plastic surgery. In sum, these findings underscore ML's potential for significant improvements in postoperative management and the development of enhanced care techniques, particularly in surgical wound management.","<method>Support Vector Machine (SVM)</method>, <method>Convolutional Neural Network (CNN)</method>, <method>Artificial Neural Network (ANN)</method>"
2024,https://openalex.org/W4391187635,Medicine,Prediction models for postoperative delirium in elderly patients with machine-learning algorithms and SHapley Additive exPlanations,"Abstract Postoperative delirium (POD) is a common and severe complication in elderly patients with hip fractures. Identifying high-risk patients with POD can help improve the outcome of patients with hip fractures. We conducted a retrospective study on elderly patients (≥65 years of age) who underwent orthopedic surgery with hip fracture between January 2014 and August 2019. Conventional logistic regression and five machine-learning algorithms were used to construct prediction models of POD. A nomogram for POD prediction was built with the logistic regression method. The area under the receiver operating characteristic curve (AUC-ROC), accuracy, sensitivity, and precision were calculated to evaluate different models. Feature importance of individuals was interpreted using Shapley Additive Explanations (SHAP). About 797 patients were enrolled in the study, with the incidence of POD at 9.28% (74/797). The age, renal insufficiency, chronic obstructive pulmonary disease (COPD), use of antipsychotics, lactate dehydrogenase (LDH), and C-reactive protein are used to build a nomogram for POD with an AUC of 0.71. The AUCs of five machine-learning models are 0.81 (Random Forest), 0.80 (GBM), 0.68 (AdaBoost), 0.77 (XGBoost), and 0.70 (SVM). The sensitivities of the six models range from 68.8% (logistic regression and SVM) to 91.9% (Random Forest). The precisions of the six machine-learning models range from 18.3% (logistic regression) to 67.8% (SVM). Six prediction models of POD in patients with hip fractures were constructed using logistic regression and five machine-learning algorithms. The application of machine-learning algorithms could provide convenient POD risk stratification to benefit elderly hip fracture patients.","<method>logistic regression</method>, <method>Random Forest</method>, <method>GBM</method>, <method>AdaBoost</method>, <method>XGBoost</method>, <method>SVM</method>"
2024,https://openalex.org/W4391277876,Medicine,Predicting mechanical properties of self-healing concrete with Trichoderma Reesei Fungus using machine learning,"Trichoderma Reesei is a mesophilic and filamentous fungus. It is an anamorph of the fungus Hypocrea jecorina, in addition, T. reesei can secrete large amounts of cellulolytic enzymes and form dextrose PDA (potato dextrose agar) and potato injection. After the preparation of fungi, it is added to the cracked samples. The experimental samples were 150 mm3 cubic compression and 70 mm x 30 mm x 15 mm cracks on the surface of each cube. Different fungi water extracts were used with 0, 50.5, 6.37, and 8.42 liters of water per ml. The results show that the addition of 8.42 (ml) of the mushroom extract with one liter of water has the maximum compressive strength with more than 18.99 MPa for 28 days, 16.7 for 14 days, and 14.5 for 7 days. In this study, linear regression, lasso regression, and rigid regression have been used to predict compressive strength, also the cooperation between mushroom juice per milliliter and compressive strength has been predicted. To find the accuracy, Correlation Coefficient (R2), Mean Absolute Errors (MAE), and Root Mean Square Error have been used. The results of machine learning show that the results of linear regression and rigid regression R2 were more than 0.98. In addition, the relationship compressive strength prediction results showed that R2 for fungi broth with one liter of water was 5.05 mL was more than 0.98. Finally, this study shows that the fungus Trichoderma reesei is an effective agent for curing concrete and improving the compressive strength of concrete.","<method>linear regression</method>, <method>lasso regression</method>, <method>rigid regression</method>"
2024,https://openalex.org/W4391670546,Medicine,Random forest regression for prediction of Covid-19 daily cases and deaths in Turkey,"During pandemic periods, there is an intense flow of patients to hospitals. Depending on the disease, many patients may require hospitalization. In some cases, these patients must be taken to intensive care units and emergency interventions must be performed. However, finding a sufficient number of hospital beds or intensive care units during pandemic periods poses a big problem. In these periods, fast and effective planning is more important than ever. Another problem experienced during pandemic periods is the burial of the dead in case the number of deaths increases. This is also a situation that requires due planning. We can learn some lessons from Covid 19 pandemic and be prepared for the future ones. In this paper, statistical properties of the daily cases and daily deaths in Turkey, which is one of the most affected countries by the pandemic in the World, are studied. It is found that the characteristics are nonstationary. Then, random forest regression is applied to predict Covid-19 daily cases and deaths. In addition, seven other machine learning models, namely bagging, AdaBoost, gradient boosting, XGBoost, decision tree, LSTM and ARIMA regressors are built for comparison. The performance of the models are measured using accuracy, coefficient of variation, root-mean-square score and relative error metrics. When random forest regressors are employed, test data related to daily cases are predicted with an accuracy of 92.30% and with an r2 score of 0.9893. Besides, daily deaths are predicted with an accuracy of 91.39% and with an r2 score of 0.9834. The closest rival in predictions is the bagging regressor. Nevertheless, the results provided by this algoritm changed in different runs and this fact is shown in the study, as well. Comparisons are based on test data. Comparisons with the earlier works are also provided.","<method>random forest regression</method>, <method>bagging</method>, <method>AdaBoost</method>, <method>gradient boosting</method>, <method>XGBoost</method>, <method>decision tree</method>, <method>LSTM</method>, <method>ARIMA regressors</method>"
2024,https://openalex.org/W4391677331,Medicine,An artificial intelligence based abdominal aortic aneurysm prognosis classifier to predict patient outcomes,"Abstract Abdominal aortic aneurysms (AAA) have been rigorously investigated to understand when their clinically-estimated risk of rupture—an event that is the 13th leading cause of death in the US—exceeds the risk associated with repair. Yet the current clinical guideline remains a one-size-fits-all “maximum diameter criterion” whereby AAA exceeding a threshold diameter is thought to make the risk of rupture high enough to warrant intervention. However, between 7 and 23.4% of smaller-sized AAA have been reported to rupture with diameters below the threshold. In this study, we train and assess machine learning models using clinical, biomechanical, and morphological indices from 381 patients to develop an aneurysm prognosis classifier to predict one of three outcomes for a given AAA patient: their AAA will remain stable, their AAA will require repair based as currently indicated from the maximum diameter criterion, or their AAA will rupture. This study represents the largest cohort of AAA patients that utilizes the first available medical image and clinical data to classify patient outcomes. The APC model therefore represents a potential clinical tool to striate specific patient outcomes using machine learning models and patient-specific image-based (biomechanical and morphological) and clinical data as input. Such a tool could greatly assist clinicians in their management decisions for patients with AAA.",<method>machine learning models</method>
2024,https://openalex.org/W4391677493,Medicine,Assessing the Accuracy and Reliability of AI-Generated Responses to Patient Questions Regarding Spine Surgery,"Background: In today’s digital age, patients increasingly rely on online search engines for medical information. The integration of large language models such as GPT-4 into search engines such as Bing raises concerns over the potential transmission of misinformation when patients search for information online regarding spine surgery. Methods: SearchResponse.io, a database that archives People Also Ask (PAA) data from Google, was utilized to determine the most popular patient questions regarding 4 specific spine surgery topics: anterior cervical discectomy and fusion, lumbar fusion, laminectomy, and spinal deformity. Bing’s responses to these questions, along with the cited sources, were recorded for analysis. Two fellowship-trained spine surgeons assessed the accuracy of the answers on a 6-point scale and the completeness of the answers on a 3-point scale. Inaccurate answers were re-queried 2 weeks later. Cited sources were categorized and evaluated against Journal of the American Medical Association (JAMA) benchmark criteria. Interrater reliability was measured with use of the kappa statistic. A linear regression analysis was utilized to explore the relationship between answer accuracy and the type of source, number of sources, and mean JAMA benchmark score. Results: Bing’s responses to 71 PAA questions were analyzed. The average completeness score was 2.03 (standard deviation [SD], 0.36), and the average accuracy score was 4.49 (SD, 1.10). Among the question topics, spinal deformity had the lowest mean completeness score. Re-querying the questions that initially had answers with low accuracy scores resulted in responses with improved accuracy. Among the cited sources, commercial sources were the most prevalent. The JAMA benchmark score across all sources averaged 2.63. Government sources had the highest mean benchmark score (3.30), whereas social media had the lowest (1.75). Conclusions: Bing’s answers were generally accurate and adequately complete, with incorrect responses rectified upon re-querying. The plurality of information was sourced from commercial websites. The type of source, number of sources, and mean JAMA benchmark score were not significantly correlated with answer accuracy. These findings underscore the importance of ongoing evaluation and improvement of large language models to ensure reliable and informative results for patients seeking information regarding spine surgery online amid the integration of these models in the search experience.",No methods found.
2024,https://openalex.org/W4391991419,Medicine,Economic evaluation for medical artificial intelligence: accuracy vs. cost-effectiveness in a diabetic retinopathy screening case,"Abstract Artificial intelligence (AI) models have shown great accuracy in health screening. However, for real-world implementation, high accuracy may not guarantee cost-effectiveness. Improving AI’s sensitivity finds more high-risk patients but may raise medical costs while increasing specificity reduces unnecessary referrals but may weaken detection capability. To evaluate the trade-off between AI model performance and the long-running cost-effectiveness, we conducted a cost-effectiveness analysis in a nationwide diabetic retinopathy (DR) screening program in China, comprising 251,535 participants with diabetes over 30 years. We tested a validated AI model in 1100 different diagnostic performances (presented as sensitivity/specificity pairs) and modeled annual screening scenarios. The status quo was defined as the scenario with the most accurate AI performance. The incremental cost-effectiveness ratio (ICER) was calculated for other scenarios against the status quo as cost-effectiveness metrics. Compared to the status quo (sensitivity/specificity: 93.3%/87.7%), six scenarios were cost-saving and seven were cost-effective. To achieve cost-saving or cost-effective, the AI model should reach a minimum sensitivity of 88.2% and specificity of 80.4%. The most cost-effective AI model exhibited higher sensitivity (96.3%) and lower specificity (80.4%) than the status quo. In settings with higher DR prevalence and willingness-to-pay levels, the AI needed higher sensitivity for optimal cost-effectiveness. Urban regions and younger patient groups also required higher sensitivity in AI-based screening. In real-world DR screening, the most accurate AI model may not be the most cost-effective. Cost-effectiveness should be independently evaluated, which is most likely to be affected by the AI’s sensitivity.",<method>Artificial intelligence (AI) models</method>
2024,https://openalex.org/W4392386754,Medicine,A Comparative Analysis of Machine Learning Algorithms for Breast Cancer Detection and Identification of Key Predictive Features,"Cancer, a disease with numerous subtypes, poses a deadly threat to human life, with the potential for successful clinical treatment heavily reliant on early detection and appropriate treatment planning.The classification of cancer patients into either low or high-risk subgroups is critical.Consequently, various research teams spanning the biomedical and bioinformatics fields have explored the use of Machine Learning (ML) technology in this crucial domain.The impressive capability of ML algorithms to discern significant features in complex datasets underscores their value.In the current study, we propose a framework to detect breast cancer (through benign and malignant categorization) utilizing advanced ML techniques with high accuracy.This framework deploys the Wisconsin Breast Cancer (Diagnostic) dataset.Five supervised ML techniques, namely Decision Tree, Random Forest (RF), Support Vector Machine (SVM), Extreme Gradient Boosting (XGBoost), and Artificial Neural Network (ANN), are trained for classification purposes.Out of 569 samples, 70% are allocated for training while the other 30% for testing.A comprehensive evaluation of ML techniques is performed using an array of metrics: precision, recall, specificity, F1 score, classification accuracy, ROC Curve, training time, and feature utilization.Additionally, feature importance is computed for each classifier.The results reveal that the SVM has the maximum accuracy as 97.66%, with an F1-score of 0.98 for benign and 0.97 for malignant classifications.Conversely, the decision tree registers the minimum performance (94.55%) with an F1-score of 0.95 for benign and 0.91 for malignant classes.Accuracy scores for RF, XGBoost, and ANN stand at 95.32%, 95.91%, and 97.07%, with corresponding F1-scores of 0.96, 0.97, and 0.98 for benign and 0.94, 0.95, and 0.96 for malignant respectively.Interestingly, RF and XGBoost exhibited near-equivalent similarly with respect of accuracy measurements.In the context of the area over the ROC curve, SVM outperformed the other ML classifiers and also reported the shortest training time.Conversely, the ANN reported the longest training time.","<method>Decision Tree</method>, <method>Random Forest (RF)</method>, <method>Support Vector Machine (SVM)</method>, <method>Extreme Gradient Boosting (XGBoost)</method>, <method>Artificial Neural Network (ANN)</method>"
2024,https://openalex.org/W4392471890,Medicine,An interpretable machine learning system for colorectal cancer diagnosis from pathology slides,"Abstract Considering the profound transformation affecting pathology practice, we aimed to develop a scalable artificial intelligence (AI) system to diagnose colorectal cancer from whole-slide images (WSI). For this, we propose a deep learning (DL) system that learns from weak labels, a sampling strategy that reduces the number of training samples by a factor of six without compromising performance, an approach to leverage a small subset of fully annotated samples, and a prototype with explainable predictions, active learning features and parallelisation. Noting some problems in the literature, this study is conducted with one of the largest WSI colorectal samples dataset with approximately 10,500 WSIs. Of these samples, 900 are testing samples. Furthermore, the robustness of the proposed method is assessed with two additional external datasets (TCGA and PAIP) and a dataset of samples collected directly from the proposed prototype. Our proposed method predicts, for the patch-based tiles, a class based on the severity of the dysplasia and uses that information to classify the whole slide. It is trained with an interpretable mixed-supervision scheme to leverage the domain knowledge introduced by pathologists through spatial annotations. The mixed-supervision scheme allowed for an intelligent sampling strategy effectively evaluated in several different scenarios without compromising the performance. On the internal dataset, the method shows an accuracy of 93.44% and a sensitivity between positive (low-grade and high-grade dysplasia) and non-neoplastic samples of 0.996. On the external test samples varied with TCGA being the most challenging dataset with an overall accuracy of 84.91% and a sensitivity of 0.996.","<method>deep learning (DL) system that learns from weak labels</method>, <method>sampling strategy that reduces the number of training samples</method>, <method>approach to leverage a small subset of fully annotated samples</method>, <method>prototype with explainable predictions</method>, <method>active learning features</method>, <method>interpretable mixed-supervision scheme</method>"
2024,https://openalex.org/W4392520481,Medicine,Predicting sepsis in-hospital mortality with machine learning: a multi-center study using clinical and inflammatory biomarkers,"Abstract Background This study aimed to develop and validate an interpretable machine-learning model that utilizes clinical features and inflammatory biomarkers to predict the risk of in-hospital mortality in critically ill patients suffering from sepsis. Methods We enrolled all patients diagnosed with sepsis in the Medical Information Mart for Intensive Care IV (MIMIC-IV, v.2.0), eICU Collaborative Research Care (eICU-CRD 2.0), and the Amsterdam University Medical Centers databases (AmsterdamUMCdb 1.0.2). LASSO regression was employed for feature selection. Seven machine-learning methods were applied to develop prognostic models. The optimal model was chosen based on its accuracy, F1 score and area under curve (AUC) in the validation cohort. Moreover, we utilized the SHapley Additive exPlanations (SHAP) method to elucidate the effects of the features attributed to the model and analyze how individual features affect the model’s output. Finally, Spearman correlation analysis examined the associations among continuous predictor variables. Restricted cubic splines (RCS) explored potential non-linear relationships between continuous risk factors and in-hospital mortality. Results 3535 patients with sepsis were eligible for participation in this study. The median age of the participants was 66 years (IQR, 55–77 years), and 56% were male. After selection, 12 of the 45 clinical parameters collected on the first day after ICU admission remained associated with prognosis and were used to develop machine-learning models. Among seven constructed models, the eXtreme Gradient Boosting (XGBoost) model achieved the best performance, with an AUC of 0.94 and an F1 score of 0.937 in the validation cohort. Feature importance analysis revealed that Age, AST, invasive ventilation treatment, and serum urea nitrogen (BUN) were the top four features of the XGBoost model with the most significant impact. Inflammatory biomarkers may have prognostic value. Furthermore, SHAP force analysis illustrated how the constructed model visualized the prediction of the model. Conclusions This study demonstrated the potential of machine-learning approaches for early prediction of outcomes in patients with sepsis. The SHAP method could improve the interoperability of machine-learning models and help clinicians better understand the reasoning behind the outcome.","<method>LASSO regression</method>, <method>eXtreme Gradient Boosting (XGBoost)</method>, <method>SHapley Additive exPlanations (SHAP)</method>"
2024,https://openalex.org/W4392762829,Medicine,"Nexus between perception, purpose of use, technical challenges and satisfaction for mobile financial services: theory and empirical evidence from Bangladesh","Purpose This study analyzed the relationship between mobile financial services (MFS) usage and customer satisfaction with MFS in Bangladesh, considering perception, purpose of use and technical challenges as the primary factors influencing customer satisfaction with MFS. The aim is to determine the factors most influencing the use of MFS. Design/methodology/approach Data were collected from 400 MFS users through a structured web survey using snowball sampling that is consistent with the nature of MFS users who are difficult to identify or locate. Structural equation modeling (SEM) was used to analyze the data and evaluate the reliability and validity of the measurement model. Findings The results show that customers’ perceptions and satisfaction significantly impact their intention to use MFS. Specifically, customers’ perceptions strongly influence their satisfaction with MFS, and the purpose of use significantly predicts both perception and satisfaction. Technical problems and challenges were found to have no significant impact on satisfaction levels, but other factors were more critical. Furthermore, the integration of innovative technological solutions is crucial for fostering sustainability in MFS, as it enhances reliability and efficiency while minimizing environmental footprints. Research limitations/implications The study was conducted in a single country, relied on self-reported data, and used a cross-sectional design, which limits the ability to draw causal inferences. Future research could explore the factors that influence customer satisfaction with MFS in different countries and regions and incorporate additional variables to provide a more comprehensive understanding of the drivers of customer satisfaction with MFS. Originality/value This study significantly contributes by extending the technology acceptance model (TAM) framework with the innovation resistance theory, offering a nuanced understanding of MFS adoption. The findings challenge conventional wisdom, highlighting the limited impact of technical problems on satisfaction and emphasizing the central role of user perceptions in shaping satisfaction and intention to use.",No methods found.
2024,https://openalex.org/W4392973999,Medicine,"Advancements in Pancreatic Cancer Detection: Integrating Biomarkers, Imaging Technologies, and Machine Learning for Early Diagnosis","Artificial intelligence (AI) has come to play a pivotal role in revolutionizing medical practices, particularly in the field of pancreatic cancer detection and management. As a leading cause of cancer-related deaths, pancreatic cancer warrants innovative approaches due to its typically advanced stage at diagnosis and dismal survival rates. Present detection methods, constrained by limitations in accuracy and efficiency, underscore the necessity for novel solutions. AI-driven methodologies present promising avenues for enhancing early detection and prognosis forecasting. Through the analysis of imaging data, biomarker profiles, and clinical information, AI algorithms excel in discerning subtle abnormalities indicative of pancreatic cancer with remarkable precision. Moreover, machine learning (ML) algorithms facilitate the amalgamation of diverse data sources to optimize patient care. However, despite its huge potential, the implementation of AI in pancreatic cancer detection faces various challenges. Issues such as the scarcity of comprehensive datasets, biases in algorithm development, and concerns regarding data privacy and security necessitate thorough scrutiny. While AI offers immense promise in transforming pancreatic cancer detection and management, ongoing research and collaborative efforts are indispensable in overcoming technical hurdles and ethical dilemmas. This review delves into the evolution of AI, its application in pancreatic cancer detection, and the challenges and ethical considerations inherent in its integration.","<method>AI algorithms</method>, <method>machine learning (ML) algorithms</method>"
2024,https://openalex.org/W4393137838,Medicine,Structural health monitoring on offshore jacket platforms using a novel ensemble deep learning model,"Monitoring health condition of offshore jacket platforms is crucial to prevent unexpected structural damages, where a prevailing challenge involves translating available feature information into structural damage patterns. Although the artificial neural network (ANN) models are popular in addressing this challenge, they often fail to capture the temporal correlations between the feature information and the damage patterns, which reduce their capability for discovering the laws governing the structural damage detection. To bridge this research gap, this study proposes a novel ensemble deep learning model to enhance the temporal feature extraction to improve the damage pattern identification. In this approach, a one-dimensional Convolutional Neural Network (CNN) extracts the spatiotemporal features from the structural vibration measurements. Simultaneously, a SENet attention mechanism is introduced to select the most informatic features. Subsequently, a bidirectional long short-term memory network (BiLSTM) is employed to learn the mapping between the extracted features and the structural damage patterns. Furthermore, the particle swarm optimization (PSO) algorithm is used to optimize the BiLSTM hyperparameters to enhance its stability and reliability. Both simulations and experiments are carried out to collect the vibration responses of the offshore jacket structure in different damage scenarios. The analysis results demonstrate that the proposed method produces remarkable improvement with respect to the accuracy and robustness in identifying the structural damages when compared with the ANNs. The overall detection accuracy of the proposed CNN-BiLSTM-Attention ensemble model is beyond 95%, which provides strong applicability to practical structural health monitoring of offshore platforms.","<method>artificial neural network (ANN)</method>, <method>ensemble deep learning model</method>, <method>one-dimensional Convolutional Neural Network (CNN)</method>, <method>SENet attention mechanism</method>, <method>bidirectional long short-term memory network (BiLSTM)</method>, <method>particle swarm optimization (PSO) algorithm</method>"
2024,https://openalex.org/W4393218816,Medicine,A study on smart home use intention of elderly consumers based on technology acceptance models,"Purpose Smart home devices have great potential to improve the quality of life and independence of older people, positively impacting their health, safety, and comfort. However, Chinese research in this field is still in its early stages. Therefore, more comprehensive and in-depth studies are needed to comprehend the various aspects influencing the acceptance and use of smart homes by older users. Patients and methods This study adopted the Technology Acceptance Model (TAM) and included perceived usefulness, perceived ease of use, usage intention, intergenerational technology support, perceived value, and perceived risk as extension variables to delve deeper into the behavioral intentions of older users in smart home services. The study used a convenience sampling method to randomly distribute 236 questionnaires among older adults over the age of 60 in the school’s community and neighboring urban communities who have experience in smart home use and who can complete human-computer interactions either independently or with the help of others, mainly focusing on the four sections: user characteristics, family situation, experience of use, and usage intention. The study used structural equation modeling (SEM) and factor analysis to analyze the completion of questionnaires. Finally, we conducted a validation analysis of the rationality and scientificity of the model and derived the six dimensions of the model of the influencing factors on the use of smart home products by the elderly and the weight sizes of their corresponding 13 influencing factors. Results The results show that perceived usefulness and perceived ease of use have a positive effect on users’ intention to use smart homes. Perceived ease of use has a positive effect on the perceived usefulness of smart homes. In addition, intergenerational technology support, perceived value, and perceived risk impact users’ perceived usefulness and perceived ease of use of the smart home. Conclusion This research aims to describe the factors influencing older users’ willingness to use smart homes. The findings are not only significant for the elderly in China but also of broad value to other regions and countries facing similar demographic challenges. The development of smart homes not only involves the elderly but is also closely related to all segments of society. The government should increase policy support and guide more social forces to participate in the development of the smart home industry. Service providers and designers should fully understand the demand situation and user experience of target users to develop easy-to-use smart home solutions. At the same time, smart homes, as intelligent products for the elderly, need to focus not only on the basic needs of the elderly such as material life and home safety, but also on the spiritual needs of elderly users. Children or caregivers should always pay attention to the psychological state of the elderly and actively guide them to use smart homes to help them realize their self-worth. We look forward to more research focusing on this area in the future and further exploring the specific issues and solutions involved.","<method>Technology Acceptance Model (TAM)</method>, <method>structural equation modeling (SEM)</method>, <method>factor analysis</method>"
2024,https://openalex.org/W4393281695,Medicine,An ensemble classification approach for cervical cancer prediction using behavioral risk factors,"Cervical cancer is a significant public health concern among females worldwide. Despite being preventable, it remains a leading cause of mortality. Early detection is crucial for successful treatment and improved survival rates. This study proposes an ensemble Machine Learning (ML) classifier for efficient and accurate identification of cervical cancer using medical data. The proposed methodology involves preparing two datasets using effective preprocessing techniques, extracting essential features using the scikit-learn package, and developing an ensemble classifier based on Random Forest, Support Vector Machine, Gaussian Naïve Bayes, and Decision Tree classifier traits. Comparison with other state-of-the-art algorithms using several ML techniques, including support vector machine, decision tree, random forest, Naïve Bayes, logistic regression, CatBoost, and AdaBoost, demonstrates that the proposed ensemble classifier outperforms them significantly, achieving accuracies of 98.06% and 95.45% for Dataset 1 and Dataset 2, respectively. The proposed ensemble classifier outperforms current state-of-the-art algorithms by 1.50% and 6.67% for Dataset 1 and Dataset 2, respectively, highlighting its superior performance compared to existing methods. The study also utilizes a five-fold cross-validation technique to analyze the benefits and drawbacks of the proposed methodology for predicting cervical cancer using medical data. The Receiver Operating Characteristic (ROC) curves with corresponding Area Under the Curve (AUC) values are 0.95 for Dataset 1 and 0.97 for Dataset 2, indicating the overall performance of the classifiers in distinguishing between the classes. Additionally, we employed SHapley Additive exPlanations (SHAP) as an Explainable Artificial Intelligence (XAI) technique to visualize the classifier's performance, providing insights into the important features contributing to cervical cancer identification. The results demonstrate that the proposed ensemble classifier can efficiently and accurately identify cervical cancer and potentially improve cervical cancer diagnosis and treatment.","<method>ensemble Machine Learning (ML) classifier</method>, <method>Random Forest</method>, <method>Support Vector Machine</method>, <method>Gaussian Naïve Bayes</method>, <method>Decision Tree classifier</method>, <method>support vector machine</method>, <method>decision tree</method>, <method>random forest</method>, <method>Naïve Bayes</method>, <method>logistic regression</method>, <method>CatBoost</method>, <method>AdaBoost</method>, <method>five-fold cross-validation</method>, <method>SHapley Additive exPlanations (SHAP)</method>"
2024,https://openalex.org/W4393853620,Medicine,"The economics of deep and machine learning-based algorithms for COVID-19 prediction, detection, and diagnosis shaping the organizational management of hospitals","Research background: Deep and machine learning-based algorithms can assist in COVID-19 image-based medical diagnosis and symptom tracing, optimize intensive care unit admission, and use clinical data to determine patient prioritization and mortality risk, being pivotal in qualitative care provision, reducing medical errors, and increasing patient survival rates, thus diminishing the massive healthcare system burden in relation to severe COVID-19 inpatient stay duration, while increasing operational costs throughout the organizational management of hospitals. Data-driven financial and scenario-based contingency planning, predictive modelling tools, and risk pooling mechanisms should be deployed for additional medical equipment and unforeseen healthcare demand expenses. Purpose of the article: We show that deep and machine learning-based and clinical decision making systems can optimize patient survival likelihood and treatment outcomes with regard to susceptible, infected, and recovered individuals, performing accurate analyses by data modeling based on vital and clinical signs, surveillance data, and infection-related biomarkers, and furthering hospital facility optimization in terms of intensive care unit bed allocation. Methods: The review software systems employed for article screening and quality evaluation were: AMSTAR, AXIS, DistillerSR, Eppi-Reviewer, MMAT, PICO Portal, Rayyan, ROBIS, and SRDR. Findings &amp; value added: Deep and machine learning-based clinical decision support tools can forecast COVID-19 spread, confirmed cases, and infection and mortality rates for data-driven appropriate treatment and resource allocations in effective therapeutic and diagnosis protocol development, by determining suitable measures and regulations and by using symptoms and comorbidities, vital signs, clinical and laboratory data and medical records across intensive care units, impacting the healthcare financing infrastructure. As a result of heightened use of personal protective equipment, hospital pharmacy and medication, outpatient treatment, and medical supplies, revenue loss and financial vulnerability occur, also due to expenses related to hiring additional staff and to critical resource expenditures. Hospital costs for COVID-19 medical care, screening, treatment capacity expansion, and personal protective equipment can lead to further financial losses while affecting COVID-19 frontline hospital workers and patients.","<method>deep learning-based algorithms</method>, <method>machine learning-based algorithms</method>, <method>deep and machine learning-based clinical decision support tools</method>"
2024,https://openalex.org/W4393864192,Medicine,Comprehensive evaluation and performance analysis of machine learning in heart disease prediction,"Heart disease is a leading cause of mortality on a global scale. Accurately predicting cardiovascular disease poses a significant challenge within clinical data analysis. The present study introduces a prediction model that utilizes various combinations of information and employs multiple established classification approaches. The proposed technique combines the genetic algorithm (GA) and the recursive feature elimination method (RFEM) to select relevant features, thus enhancing the model's robustness. Techniques like the under sampling clustering oversampling method (USCOM) address the issue of data imbalance, thereby improving the model's predictive capabilities. The classification challenge employs a multilayer deep convolutional neural network (MLDCNN), trained using the adaptive elephant herd optimization method (AEHOM). The proposed machine learning-based heart disease prediction method (ML-HDPM) demonstrates outstanding performance across various crucial evaluation parameters, as indicated by its comprehensive assessment. During the training process, the ML-HDPM model exhibits a high level of performance, achieving an accuracy rate of 95.5% and a precision rate of 94.8%. The system's sensitivity (recall) performs with a high accuracy rate of 96.2%, while the F-score highlights its well-balanced performance, measuring 91.5%. It is worth noting that the specificity of ML-HDPM is recorded at a remarkable 89.7%. The findings underscore the potential of ML-HDPM to transform the prediction of heart disease and aid healthcare practitioners in providing precise diagnoses, exerting a substantial influence on patient care outcomes.","<method>genetic algorithm (GA)</method>, <method>recursive feature elimination method (RFEM)</method>, <method>under sampling clustering oversampling method (USCOM)</method>, <method>multilayer deep convolutional neural network (MLDCNN)</method>, <method>adaptive elephant herd optimization method (AEHOM)</method>"
2024,https://openalex.org/W4396557972,Medicine,Enhancing breast cancer segmentation and classification: An Ensemble Deep Convolutional Neural Network and U-net approach on ultrasound images,"Breast cancer is a condition where the irregular growth of breast cells occurs uncontrollably, leading to the formation of tumors. It poses a significant threat to women's lives globally, emphasizing the need for enhanced methods of detecting and categorizing the disease. In this work, we propose an Ensemble Deep Convolutional Neural Network (EDCNN) model that exhibits superior accuracy compared to several transfer learning models and the Vision Transformer model. Our EDCNN model integrates the strengths of the MobileNet and Xception models to improve its performance in breast cancer detection and classification. We employ various preprocessing techniques, including image resizing, data normalization, and data augmentation, to prepare the data for analysis. By following these measures, the formatting is optimized, and the model's capacity to make generalizations is improved. We trained and evaluated our proposed EDCNN model using ultrasound images, a widely available modality for breast cancer imaging. The outcomes of our experiments illustrate that the EDCNN model attains an exceptional accuracy of 87.82% on Dataset 1 and 85.69% on Dataset 2, surpassing the performance of several well-known transfer learning models and the Vision Transformer model. Furthermore, an AUC value of 0.91 on Dataset 1 highlights the robustness and effectiveness of our proposed model. Moreover, we highlight the incorporation of the Grad-CAM Explainable Artificial Intelligence (XAI) technique to improve the interpretability and transparency of our proposed model. Additionally, we performed image segmentation using the U-Net segmentation technique on the input ultrasound images. This segmentation process allowed for the identification and isolation of specific regions of interest, facilitating a more comprehensive analysis of breast cancer characteristics. In conclusion, the study presents a creative approach to detecting and categorizing breast cancer, demonstrating the superior performance of the EDCNN model compared to well-established transfer learning models. Through advanced deep learning techniques and image segmentation, this study contributes to improving diagnosis and treatment outcomes in breast cancer.","<method>Ensemble Deep Convolutional Neural Network (EDCNN)</method>, <method>MobileNet</method>, <method>Xception</method>, <method>transfer learning models</method>, <method>Vision Transformer model</method>, <method>Grad-CAM Explainable Artificial Intelligence (XAI) technique</method>, <method>U-Net segmentation technique</method>"
2024,https://openalex.org/W4396894907,Medicine,Employing machine learning for enhanced abdominal fat prediction in cavitation post-treatment,"This study investigates the application of cavitation in non-invasive abdominal fat reduction and body contouring, a topic of considerable interest in the medical and aesthetic fields. We explore the potential of cavitation to alter abdominal fat composition and delve into the optimization of fat prediction models using advanced hyperparameter optimization techniques, Hyperopt and Optuna. Our objective is to enhance the predictive accuracy of abdominal fat dynamics post-cavitation treatment. Employing a robust dataset with abdominal fat measurements and cavitation treatment parameters, we evaluate the efficacy of our approach through regression analysis. The performance of Hyperopt and Optuna regression models is assessed using metrics such as mean squared error, mean absolute error, and R-squared score. Our results reveal that both models exhibit strong predictive capabilities, with R-squared scores reaching 94.12% and 94.11% for post-treatment visceral fat, and 71.15% and 70.48% for post-treatment subcutaneous fat predictions, respectively. Additionally, we investigate feature selection techniques to pinpoint critical predictors within the fat prediction models. Techniques including F-value selection, mutual information, recursive feature elimination with logistic regression and random forests, variance thresholding, and feature importance evaluation are utilized. The analysis identifies key features such as BMI, waist circumference, and pretreatment fat levels as significant predictors of post-treatment fat outcomes. Our findings underscore the effectiveness of hyperparameter optimization in refining fat prediction models and offer valuable insights for the advancement of non-invasive fat reduction methods. This research holds important implications for both the scientific community and clinical practitioners, paving the way for improved treatment strategies in the realm of body contouring.","<method>Hyperopt regression models</method>, <method>Optuna regression models</method>, <method>F-value selection</method>, <method>mutual information</method>, <method>recursive feature elimination with logistic regression</method>, <method>recursive feature elimination with random forests</method>, <method>variance thresholding</method>, <method>feature importance evaluation</method>"
2024,https://openalex.org/W4397044870,Medicine,Advancements in Predictive Microbiology: Integrating New Technologies for Efficient Food Safety Models,"Predictive microbiology is a rapidly evolving field that has gained significant interest over the years due to its diverse application in food safety. Predictive models are widely used in food microbiology to estimate the growth of microorganisms in food products. These models represent the dynamic interactions between intrinsic and extrinsic food factors as mathematical equations and then apply these data to predict shelf life, spoilage, and microbial risk assessment. Due to their ability to predict the microbial risk, these tools are also integrated into hazard analysis critical control point (HACCP) protocols. However, like most new technologies, several limitations have been linked to their use. Predictive models have been found incapable of modeling the intricate microbial interactions in food colonized by different bacteria populations under dynamic environmental conditions. To address this issue, researchers are integrating several new technologies into predictive models to improve efficiency and accuracy. Increasingly, newer technologies such as whole genome sequencing (WGS), metagenomics, artificial intelligence, and machine learning are being rapidly adopted into newer-generation models. This has facilitated the development of devices based on robotics, the Internet of Things, and time-temperature indicators that are being incorporated into food processing both domestically and industrially globally. This study reviewed current research on predictive models, limitations, challenges, and newer technologies being integrated into developing more efficient models. Machine learning algorithms commonly employed in predictive modeling are discussed with emphasis on their application in research and industry and their advantages over traditional models.","<method>artificial intelligence</method>, <method>machine learning</method>, <method>machine learning algorithms</method>"
2024,https://openalex.org/W4399708678,Medicine,Detection of Parkinson disease using multiclass machine learning approach,"Parkinson's Disease (PD) is a prevalent neurological condition characterized by motor and cognitive impairments, typically manifesting around the age of 50 and presenting symptoms such as gait difficulties and speech impairments. Although a cure remains elusive, symptom management through medication is possible. Timely detection is pivotal for effective disease management. In this study, we leverage Machine Learning (ML) and Deep Learning (DL) techniques, specifically K-Nearest Neighbor (KNN) and Feed-forward Neural Network (FNN) models, to differentiate between individuals with PD and healthy individuals based on voice signal characteristics. Our dataset, sourced from the University of California at Irvine (UCI), comprises 195 voice recordings collected from 31 patients. To optimize model performance, we employ various strategies including Synthetic Minority Over-sampling Technique (SMOTE) for addressing class imbalance, Feature Selection to identify the most relevant features, and hyperparameter tuning using RandomizedSearchCV. Our experimentation reveals that the FNN and KSVM models, trained on an 80-20 split of the dataset for training and testing respectively, yield the most promising results. The FNN model achieves an impressive overall accuracy of 99.11%, with 98.78% recall, 99.96% precision, and a 99.23% f1-score. Similarly, the KSVM model demonstrates strong performance with an overall accuracy of 95.89%, recall of 96.88%, precision of 98.71%, and an f1-score of 97.62%. Overall, our study showcases the efficacy of ML and DL techniques in accurately identifying PD from voice signals, underscoring the potential for these approaches to contribute significantly to early diagnosis and intervention strategies for Parkinson's Disease.","<method>K-Nearest Neighbor (KNN)</method>, <method>Feed-forward Neural Network (FNN)</method>, <method>Synthetic Minority Over-sampling Technique (SMOTE)</method>, <method>Feature Selection</method>, <method>RandomizedSearchCV</method>, <method>KSVM</method>"
2024,https://openalex.org/W4400528496,Medicine,Deep learning empowered breast cancer diagnosis: Advancements in detection and classification,"Recent advancements in AI, driven by big data technologies, have reshaped various industries, with a strong focus on data-driven approaches. This has resulted in remarkable progress in fields like computer vision, e-commerce, cybersecurity, and healthcare, primarily fueled by the integration of machine learning and deep learning models. Notably, the intersection of oncology and computer science has given rise to Computer-Aided Diagnosis (CAD) systems, offering vital tools to aid medical professionals in tumor detection, classification, recurrence tracking, and prognosis prediction. Breast cancer, a significant global health concern, is particularly prevalent in Asia due to diverse factors like lifestyle, genetics, environmental exposures, and healthcare accessibility. Early detection through mammography screening is critical, but the accuracy of mammograms can vary due to factors like breast composition and tumor characteristics, leading to potential misdiagnoses. To address this, an innovative CAD system leveraging deep learning and computer vision techniques was introduced. This system enhances breast cancer diagnosis by independently identifying and categorizing breast lesions, segmenting mass lesions, and classifying them based on pathology. Thorough validation using the Curated Breast Imaging Subset of Digital Database for Screening Mammography (CBIS-DDSM) demonstrated the CAD system’s exceptional performance, with a 99% success rate in detecting and classifying breast masses. While the accuracy of detection is 98.5%, when segmenting breast masses into separate groups for examination, the method’s performance was approximately 95.39%. Upon completing all the analysis, the system’s classification phase yielded an overall accuracy of 99.16% for classification. The potential for this integrated framework to outperform current deep learning techniques is proposed, despite potential challenges related to the high number of trainable parameters. Ultimately, this recommended framework offers valuable support to researchers and physicians in breast cancer diagnosis by harnessing cutting-edge AI and image processing technologies, extending recent advances in deep learning to the medical domain.","<method>deep learning</method>, <method>computer vision techniques</method>"
2024,https://openalex.org/W4402323203,Medicine,"Neutralization and Stability of JN.1-derived LB.1, KP.2.3, KP.3 and KP.3.1.1 Subvariants","SUMMARY During the summer of 2024, COVID-19 cases surged globally, driven by variants derived from JN.1 subvariants of SARS-CoV-2 that feature new mutations, particularly in the N-terminal domain (NTD) of the spike protein. In this study, we report on the neutralizing antibody (nAb) escape, infectivity, fusion, and stability of these subvariants—LB.1, KP.2.3, KP.3, and KP.3.1.1. Our findings demonstrate that all of these subvariants are highly evasive of nAbs elicited by the bivalent mRNA vaccine, the XBB.1.5 monovalent mumps virus-based vaccine, or from infections during the BA.2.86/JN.1 wave. This reduction in nAb titers is primarily driven by a single serine deletion (DelS31) in the NTD of the spike, leading to a distinct antigenic profile compared to the parental JN.1 and other variants. We also found that the DelS31 mutation decreases pseudovirus infectivity in CaLu-3 cells, which correlates with impaired cell-cell fusion. Additionally, the spike protein of DelS31 variants appears more conformationally stable, as indicated by reduced S1 shedding both with and without stimulation by soluble ACE2, and increased resistance to elevated temperatures. Molecular modeling suggests that the DelS31 mutation induces a conformational change that stabilizes the NTD and strengthens the NTD-Receptor-Binding Domain (RBD) interaction, thus favoring the down conformation of RBD and reducing accessibility to both the ACE2 receptor and certain nAbs. Additionally, the DelS31 mutation introduces an N-linked glycan modification at N30, which shields the underlying NTD region from antibody recognition. Our data highlight the critical role of NTD mutations in the spike protein for nAb evasion, stability, and viral infectivity, and suggest consideration of updating COVID-19 vaccines with antigens containing DelS31.",No methods found.
2024,https://openalex.org/W4404558078,Medicine,"Nationwide, Couple-Based Genetic Carrier Screening","BackgroundGenomic sequencing technology allows for identification of reproductive couples with an increased chance, as compared with that in the general population, of having a child with an autosomal recessive or X-linked genetic condition.MethodsWe investigated the feasibility, acceptability, and outcomes of a nationwide, couple-based genetic carrier screening program in Australia as part of the Mackenzie's Mission project. Health care providers offered screening to persons before pregnancy or early in pregnancy. The results obtained from testing at least 1281 genes were provided to the reproductive couples. We aimed to ascertain the psychosocial effects on participants, the acceptability of screening to all participants, and the reproductive choices of persons identified as having an increased chance of having a child with a condition for which we screened.ResultsAmong 10,038 reproductive couples enrolled in the study, 9107 (90.7%) completed screening, and 175 (1.9%) were newly identified as having an increased chance of having a child with a genetic condition for which we screened. These conditions involved pathogenic variants in 90 different genes; 74.3% of the conditions were autosomal recessive. Three months after receiving the results, 76.6% of the couples with a newly identified increased chance had used or planned to use reproductive interventions to avoid having an affected child. Those newly identified as having an increased chance had greater anxiety than those with a low chance. The median level of decisional regret was low in all result groups, and 98.9% of participants perceived screening to be acceptable.ConclusionsCouple-based reproductive genetic carrier screening was largely acceptable to participants and was used to inform reproductive decision making. The delivery of screening to a diverse and geographically dispersed population was feasible. (Funded by the Medical Research Future Fund of the Australian government; ClinicalTrials.gov number, NCT04157595.)",No methods found.
2024,https://openalex.org/W4390611263,Medicine,Machine Learning in the Parkinson’s disease smartwatch (PADS) dataset,"Abstract The utilisation of smart devices, such as smartwatches and smartphones, in the field of movement disorders research has gained significant attention. However, the absence of a comprehensive dataset with movement data and clinical annotations, encompassing a wide range of movement disorders including Parkinson’s disease (PD) and its differential diagnoses (DD), presents a significant gap. The availability of such a dataset is crucial for the development of reliable machine learning (ML) models on smart devices, enabling the detection of diseases and monitoring of treatment efficacy in a home-based setting. We conducted a three-year cross-sectional study at a large tertiary care hospital. A multi-modal smartphone app integrated electronic questionnaires and smartwatch measures during an interactive assessment designed by neurologists to provoke subtle changes in movement pathologies. We captured over 5000 clinical assessment steps from 504 participants, including PD, DD, and healthy controls (HC). After age-matching, an integrative ML approach combining classical signal processing and advanced deep learning techniques was implemented and cross-validated. The models achieved an average balanced accuracy of 91.16% in the classification PD vs. HC, while PD vs. DD scored 72.42%. The numbers suggest promising performance while distinguishing similar disorders remains challenging. The extensive annotations, including details on demographics, medical history, symptoms, and movement steps, provide a comprehensive database to ML techniques and encourage further investigations into phenotypical biomarkers related to movement disorders.","<method>classical signal processing</method>, <method>advanced deep learning techniques</method>"
2024,https://openalex.org/W4390707476,Medicine,CT-Based Intratumoral and Peritumoral Radiomics Nomograms for the Preoperative Prediction of Spread Through Air Spaces in Clinical Stage IA Non-small Cell Lung Cancer,"The study aims to investigate the value of intratumoral and peritumoral radiomics and clinical-radiological features for predicting spread through air spaces (STAS) in patients with clinical stage IA non-small cell lung cancer (NSCLC). A total of 336 NSCLC patients from our hospital were randomly divided into the training cohort (n = 236) and the internal validation cohort (n = 100) at a ratio of 7:3, and 69 patients from the other two external hospitals were collected as the external validation cohort. Univariate and multivariate analyses were used to select clinical-radiological features and construct a clinical model. The GTV, PTV5, PTV10, PTV15, PTV20, GPTV5, GPTV10, GPTV15, and GPTV20 models were constructed based on intratumoral and peritumoral (5 mm, 10 mm, 15 mm, 20 mm) radiomics features. Additionally, the radscore of the optimal radiomics model and clinical-radiological predictors were used to construct a combined model and plot a nomogram. Lastly, the ROC curve and AUC value were used to evaluate the diagnostic performance of the model. Tumor density type (OR = 6.738) and distal ribbon sign (OR = 5.141) were independent risk factors for the occurrence of STAS. The GPTV10 model outperformed the other radiomics models, and its AUC values were 0.887, 0.876, and 0.868 in the three cohorts. The AUC values of the combined model constructed based on GPTV10 radscore and clinical-radiological predictors were 0.901, 0.875, and 0.878. DeLong test results revealed that the combined model was superior to the clinical model in the three cohorts. The nomogram based on GPTV10 radscore and clinical-radiological features exhibited high predictive efficiency for STAS status in NSCLC.",<method>radiomics</method>
2024,https://openalex.org/W4390734415,Medicine,Cobdock: an accurate and practical machine learning-based consensus blind docking method,"Abstract Probing the surface of proteins to predict the binding site and binding affinity for a given small molecule is a critical but challenging task in drug discovery. Blind docking addresses this issue by performing docking on binding regions randomly sampled from the entire protein surface. However, compared with local docking, blind docking is less accurate and reliable because the docking space is too largetly sampled. Cavity detection-guided blind docking methods improved the accuracy by using cavity detection (also known as binding site detection) tools to guide the docking procedure. However, it is worth noting that the performance of these methods heavily relies on the quality of the cavity detection tool. This constraint, namely the dependence on a single cavity detection tool, significantly impacts the overall performance of cavity detection-guided methods. To overcome this limitation, we proposed Co nsensus B lind Dock (CoBDock), a novel blind, parallel docking method that uses machine learning algorithms to integrate docking and cavity detection results to improve not only binding site identification but also pose prediction accuracy. Our experiments on several datasets, including PDBBind 2020, ADS, MTi, DUD-E, and CASF-2016, showed that CoBDock has better binding site and binding mode performance than other state-of-the-art cavity detector tools and blind docking methods.",<method>machine learning algorithms</method>
2024,https://openalex.org/W4390817372,Medicine,Brain structure ages—A new biomarker for multi‐disease classification,"Age is an important variable to describe the expected brain's anatomy status across the normal aging trajectory. The deviation from that normative aging trajectory may provide some insights into neurological diseases. In neuroimaging, predicted brain age is widely used to analyze different diseases. However, using only the brain age gap information (i.e., the difference between the chronological age and the estimated age) can be not enough informative for disease classification problems. In this paper, we propose to extend the notion of global brain age by estimating brain structure ages using structural magnetic resonance imaging. To this end, an ensemble of deep learning models is first used to estimate a 3D aging map (i.e., voxel-wise age estimation). Then, a 3D segmentation mask is used to obtain the final brain structure ages. This biomarker can be used in several situations. First, it enables to accurately estimate the brain age for the purpose of anomaly detection at the population level. In this situation, our approach outperforms several state-of-the-art methods. Second, brain structure ages can be used to compute the deviation from the normal aging process of each brain structure. This feature can be used in a multi-disease classification task for an accurate differential diagnosis at the subject level. Finally, the brain structure age deviations of individuals can be visualized, providing some insights about brain abnormality and helping clinicians in real medical contexts.",<method>ensemble of deep learning models</method>
2024,https://openalex.org/W4391097175,Medicine,Toward Improving Breast Cancer Classification Using an Adaptive Voting Ensemble Learning Algorithm,"Over the past decade, breast cancer has been the most common type of cancer in women. Different methods were proposed for breast cancer detection. These methods mainly classify and categorize malignant and Benign tumors. Machine learning is a practical approach for breast cancer classification. Data mining and classification are effective methods to predict and categorize breast cancer. The optimum classification for detecting Breast Cancer (BC) is ensemble-based. The ensemble approach involves using multiple ways to find the best possible solution. This study used the Wisconsin Breast Cancer Diagnostic (WBCD) dataset. We created a voting ensemble classifier that combines four different machine learning models: Extra Trees Classifier (ETC), Light Gradient Boosting Machine (LightGBM), Ridge Classifier (RC), and Linear Discriminant Analysis (LDA). The proposed ELRL-E approach achieved an accuracy of 97.6%, a precision of 96.4%, a recall of 100%, and an F1 score of 98.1%. Various output evaluations are used to evaluate the performance and efficiency of the proposed model and other classifiers. Overall, the recommended strategy performed better. Results are directly compared with the individual classifier and different recognized state-of-the-art classifiers. The primary objective of this study is to identify the most influential ensemble machine learning classifier for breast cancer detection and diagnosis in terms of accuracy and AUC score.","<method>Machine learning</method>, <method>Data mining</method>, <method>classification</method>, <method>ensemble-based classification</method>, <method>voting ensemble classifier</method>, <method>Extra Trees Classifier (ETC)</method>, <method>Light Gradient Boosting Machine (LightGBM)</method>, <method>Ridge Classifier (RC)</method>, <method>Linear Discriminant Analysis (LDA)</method>"
2024,https://openalex.org/W4391111398,Medicine,"Characterization of Walking in Mild Parkinson’s Disease: Reliability, Validity and Discriminant Ability of the Six-Minute Walk Test Instrumented with a Single Inertial Sensor","Although the 6-Minute Walk Test (6MWT) is among the recommended clinical tools to assess gait impairments in individuals with Parkinson’s disease (PD), its standard clinical outcome consists only of the distance walked in 6 min. Integrating a single Inertial Measurement Unit (IMU) could provide additional quantitative and objective information about gait quality complementing standard clinical outcome. This study aims to evaluate the test–retest reliability, validity and discriminant ability of gait parameters obtained by a single IMU during the 6MWT in subjects with mild PD. Twenty-two people with mild PD and ten healthy persons performed the 6MWT wearing an IMU placed on the lower trunk. Features belonging to rhythm and pace, variability, regularity, jerkiness, intensity, dynamic instability and symmetry domains were computed. Test–retest reliability was evaluated through the Intraclass Correlation Coefficient (ICC), while concurrent validity was determined by Spearman’s coefficient. Mann–Whitney U test and the Area Under the receiver operating characteristic Curve (AUC) were then applied to assess the discriminant ability of reliable and valid parameters. Results showed an overall high reliability (ICC ≥ 0.75) and multiple significant correlations with clinical scales in all domains. Several features exhibited significant alterations compared to healthy controls. Our findings suggested that the 6MWT instrumented with a single IMU can provide reliable and valid information about gait features in individuals with PD. This offers objective details about gait quality and the possibility of being integrated into clinical evaluations to better define walking rehabilitation strategies in a quick and easy way.",No methods found.
2024,https://openalex.org/W4391144074,Medicine,"The neutrophil-to-lymphocyte ratio, lymphocyte-to-monocyte ratio, and neutrophil-to-high-density-lipoprotein ratio are correlated with the severity of Parkinson’s disease","Background Inflammation plays a pivotal role in the pathogenesis of Parkinson’s disease (PD). However, the correlation between peripheral inflammatory markers and the severity of PD remains unclear. Methods The following items in plasma were collected for assessment among patients with PD ( n = 303) and healthy controls (HCs; n = 303) were assessed for the neutrophil-to-lymphocyte ratio (NLR), lymphocyte-to-monocyte ratio (LMR) and neutrophil-to-high-density-lipoprotein ratio (NHR) in plasma, and neuropsychological assessments were performed for all patients with PD. Spearman rank or Pearson correlation was used to evaluate the correlation between the NLR, the LMR and the NHR and the severity of PD. Receiver operating characteristic (ROC) curves were used to evaluate the diagnostic performance of the NLR, LMR and NHR for PD. Results The plasma NLR and NHR were substantially higher in patients with PD than in HCs, while the plasma LMR was substantially lower. The plasma NLR was positively correlated with Hoehn and Yahr staging scale (H&amp;amp;Y), Unified Parkinson’s Disease Rating Scale (UPDRS), UPDRS-I, UPDRS-II, and UPDRS-III scores. Conversely, it exhibited a negative relationship with Mini-Mental State Examination (MMSE) and Montreal Cognitive Assessment (MoCA) scores. Furthermore, the plasma NHR was positively correlated with H&amp;amp;Y, UPDRS, UPDRS-I, UPDRS-II and UPDRS-III scores. Moreover, negative associations were established between the plasma LMR and H&amp;amp;Y, UPDRS, UPDRS-I, UPDRS-II, and UPDRS-III scores. Finally, based on the ROC curve analysis, the NLR, LMR and NHR exhibited respectable PD discriminating power. Conclusion Our research indicates that a higher NLR and NHR and a lower LMR may be relevant for assessing the severity of PD and appear to be promising disease-state biomarker candidates.",No methods found.
2024,https://openalex.org/W4391324248,Medicine,"The Prediction of Clinical Mastitis in Dairy Cows Based on Milk Yield, Rumination Time, and Milk Electrical Conductivity Using Machine Learning Algorithms","In commercial dairy farms, mastitis is associated with increased antimicrobial use and associated resistance, which may affect milk production. This study aimed to develop sensor-based prediction models for naturally occurring clinical bovine mastitis using nine machine learning algorithms with data from 447 mastitic and 2146 healthy cows obtained from five commercial farms in Northeast China. The variables were related to daily activity, rumination time, and daily milk yield of cows, as well as milk electrical conductivity. Both Z-standardized and non-standardized datasets pertaining to four specific stages of lactation were used to train and test prediction models. For all four subgroups, the Z-standardized dataset yielded better results than those of the non-standardized one, with the multilayer artificial neural net algorithm showing the best performance. Variables of importance had a similar rank in this algorithm, indicating the consistency of these variables as predictors for bovine mastitis in commercial farms with similar automatic systems. Moreover, the peak milk yield (PMY) of mastitic cows was significantly higher than that of healthy cows (p &lt; 0.005), indicating that high-yielding cattle are more prone to mastitis. Our results show that machine learning algorithms are effective tools for predicting mastitis in dairy cows for immediate intervention and management in commercial farms.","<method>machine learning algorithms</method>, <method>multilayer artificial neural net algorithm</method>"
2024,https://openalex.org/W4391350390,Medicine,"Machine learning in physical activity, sedentary, and sleep behavior research","Abstract The nature of human movement and non-movement behaviors is complex and multifaceted, making their study complicated and challenging. Thanks to the availability of wearable activity monitors, we can now monitor the full spectrum of physical activity, sedentary, and sleep behaviors better than ever before—whether the subjects are elite athletes, children, adults, or individuals with pre-existing medical conditions. The increasing volume of generated data, combined with the inherent complexities of human movement and non-movement behaviors, necessitates the development of new data analysis methods for the research of physical activity, sedentary, and sleep behaviors. The characteristics of machine learning (ML) methods, including their ability to deal with complicated data, make them suitable for such analysis and thus can be an alternative tool to deal with data of this nature. ML can potentially be an excellent tool for solving many traditional problems related to the research of physical activity, sedentary, and sleep behaviors such as activity recognition, posture detection, profile analysis, and correlates research. However, despite this potential, ML has not yet been widely utilized for analyzing and studying these behaviors. In this review, we aim to introduce experts in physical activity, sedentary behavior, and sleep research—individuals who may possess limited familiarity with ML—to the potential applications of these techniques for analyzing their data. We begin by explaining the underlying principles of the ML modeling pipeline, highlighting the challenges and issues that need to be considered when applying ML. We then present the types of ML: supervised and unsupervised learning, and introduce a few ML algorithms frequently used in supervised and unsupervised learning. Finally, we highlight three research areas where ML methodologies have already been used in physical activity, sedentary behavior, and sleep behavior research, emphasizing their successes and challenges. This paper serves as a resource for ML in physical activity, sedentary, and sleep behavior research, offering guidance and resources to facilitate its utilization.","<method>machine learning (ML)</method>, <method>supervised learning</method>, <method>unsupervised learning</method>"
2024,https://openalex.org/W4391593760,Medicine,Enhanced Jaya Optimization Algorithm with Deep Learning Assisted Oral Cancer Diagnosis on IoT Healthcare Systems,"Recently, healthcare systems integrate the power of deep learning (DL) models with the connectivity and data processing capabilities of the Internet of Things (IoT) to enhance the early recognition and diagnosis of disease. Oral cancer diagnosis comprises the detection of cancerous or pre-cancerous abrasions in the oral cavity. Timely identification is essential for successful treatment and enhanced prognosis. Here is an overview of the key aspects of oral cancer diagnosis. One potential benefit of utilizing DL for oral cancer detection is that it analyses huge counts of data fast and accurately, and it could not need clear programming of the rules for recognizing abnormalities. This can create the procedure of detecting oral cancer more effective and efficient. Thus, the study presents an Enhanced Jaya Optimization Algorithm with Deep Learning Based Oral Cancer Classification (EJOADL-OCC) method. The presented EJOADL-OCC method aims to classify and detect the existence of oral cancer accurately and effectively. To accomplish this, the presented EJOADL-OCC method initially exploits median filtering for the noise elimination. Next, the feature vector generation process is performed by the residual network (ResNetv2) model with EJOA as a hyperparameter optimizer. For accurate classification of oral cancer, a continuously restricted Boltzmann machine with a deep belief network (CRBM-DBN) model. The simulated validation of the EJOADL-OCC algorithm is tested by the series of simulations and the outcome demonstrates its supremacy over present DL approaches.","<method>Deep Learning (DL)</method>, <method>Enhanced Jaya Optimization Algorithm (EJOA)</method>, <method>Residual Network (ResNetv2)</method>, <method>Continuously Restricted Boltzmann Machine with Deep Belief Network (CRBM-DBN)</method>"
2024,https://openalex.org/W4391604911,Medicine,Halal cosmetics: a technology-empowered systematic literature review,"Purpose Globally, the halal cosmetics market is experiencing rapid growth and is considered a key economic driver in shaping economy development and growth. However, the extant research on halal cosmetics is fragmented, potentially impeding the field’s advancement when challenged with conflicting viewpoints and limited replications. Therefore, this paper aims to address the knowledge gap by conducting a rigorous and technology-enabled systematic review by leveraging appropriate software to comprehensively evaluate the state of the halal cosmetics literature. Design/methodology/approach A domain-based review using a hybrid approach that incorporates both bibliometric and interpretive analyses are used to comprehensively assess the current progress of halal cosmetics, identify research gaps and suggest potential directions for future research. Findings Through a comprehensive review of 66 articles, this review provides a holistic and comprehensive overview of halal cosmetics that both academic scholars and market practitioners can rely upon in strategizing and positioning for future development of halal cosmetics. The study provides a holistic and comprehensive overview of halal cosmetics that both academic scholars and market practitioners can reply upon in strategizing and positioning for future development of halal cosmetics. Originality/value The fragmented knowledge of extant research on halal cosmetics across various disciplines limits a comprehensive understanding of the field. It is opportune to conduct a comprehensive and systematic review of the field, providing insight into both its current and future progress. In this regard, this review serves as a “one-stop reference” in providing a state-of-the-art understanding of the field, and enables industry practitioners to reveal the full potential and bridge the theory-practice gap in the halal cosmetics industry.",No methods found.
2024,https://openalex.org/W4391643149,Medicine,Development and psychometric validation of a novel scale for measuring ‘psychedelic preparedness’,"Abstract Preparing participants for psychedelic experiences is crucial for ensuring these experiences are safe and, potentially beneficial. However, there is currently no validated measure to assess the extent to which participants are well-prepared for such experiences. Our study aimed to address this gap by developing, validating, and testing the Psychedelic Preparedness Scale (PPS). Using a novel iterative Delphi-focus group methodology (‘DelFo’), followed by qualitative pre-test interviews, we incorporated the perspectives of expert clinicians/researchers and of psychedelic users to generate items for the scale. Psychometric validation of the PPS was carried out in two large online samples of psychedelic users (N = 516; N = 716), and the scale was also administered to a group of participants before and after a 5–7-day psilocybin retreat (N = 46). Exploratory and confirmatory factor analysis identified four factors from the 20-item PPS: Knowledge-Expectations, Intention-Preparation, Psychophysical-Readiness, and Support-Planning. The PPS demonstrated excellent reliability (ω = 0.954) and evidence supporting convergent, divergent and discriminant validity was also obtained. Significant differences between those scoring high and low (on psychedelic preparedness) before the psychedelic experience were found on measures of mental health/wellbeing outcomes assessed after the experience, suggesting that the scale has predictive utility. By prospectively measuring modifiable pre-treatment preparatory behaviours and attitudes using the PPS, it may be possible to determine whether a participant has generated the appropriate mental ‘set’ and is therefore likely to benefit from a psychedelic experience, or at least, less likely to be harmed.",No methods found.
2024,https://openalex.org/W4391730908,Medicine,Shedding light on ai in radiology: A systematic review and taxonomy of eye gaze-driven interpretability in deep learning,"X-ray imaging plays a crucial role in diagnostic medicine. Yet, a significant portion of the global population lacks access to this essential technology due to a shortage of trained radiologists. Eye-tracking data and deep learning models can enhance X-ray analysis by mapping expert focus areas, guiding automated anomaly detection, optimizing workflow efficiency, and bolstering training methods for novice radiologists. However, the literature shows contradictory results regarding the usefulness of eye-tracking data in deep-learning architectures for abnormality detection. We argue that these discrepancies between studies in the literature are due to (a) the way eye-tracking data is (or is not) processed, (b) the types of deep learning architectures chosen, and (c) the type of application that these architectures will have. We conducted a systematic literature review using PRISMA to address these contradicting results. We analyzed 60 studies that incorporated eye-tracking data in a deep-learning approach for different application goals in radiology. We performed a comparative analysis to understand if eye gaze data contains feature maps that can be useful under a deep learning approach and whether they can promote more interpretable predictions. To the best of our knowledge, this is the first survey in the area that performs a thorough investigation of eye gaze data processing techniques and their impacts in different deep learning architectures for applications such as error detection, classification, object detection, expertise level analysis, fatigue estimation and human attention prediction in medical imaging data. Our analysis resulted in two main contributions: (1) taxonomy that first divides the literature by task, enabling us to analyze the value eye movement can bring for each case and build guidelines regarding architectures and gaze processing techniques adequate for each application, and (2) an overall analysis of how eye gaze data can promote explainability in radiology.","<method>deep learning models</method>, <method>deep learning architectures</method>"
2024,https://openalex.org/W4390506881,Medicine,Discovering biomarkers associated and predicting cardiovascular disease with high accuracy using a novel nexus of machine learning techniques for precision medicine,"Abstract Personalized interventions are deemed vital given the intricate characteristics, advancement, inherent genetic composition, and diversity of cardiovascular diseases (CVDs). The appropriate utilization of artificial intelligence (AI) and machine learning (ML) methodologies can yield novel understandings of CVDs, enabling improved personalized treatments through predictive analysis and deep phenotyping. In this study, we proposed and employed a novel approach combining traditional statistics and a nexus of cutting-edge AI/ML techniques to identify significant biomarkers for our predictive engine by analyzing the complete transcriptome of CVD patients. After robust gene expression data pre-processing, we utilized three statistical tests (Pearson correlation, Chi-square test, and ANOVA) to assess the differences in transcriptomic expression and clinical characteristics between healthy individuals and CVD patients. Next, the recursive feature elimination classifier assigned rankings to transcriptomic features based on their relation to the case–control variable. The top ten percent of commonly observed significant biomarkers were evaluated using four unique ML classifiers (Random Forest, Support Vector Machine, Xtreme Gradient Boosting Decision Trees, and k-Nearest Neighbors). After optimizing hyperparameters, the ensembled models, which were implemented using a soft voting classifier, accurately differentiated between patients and healthy individuals. We have uncovered 18 transcriptomic biomarkers that are highly significant in the CVD population that were used to predict disease with up to 96% accuracy. Additionally, we cross-validated our results with clinical records collected from patients in our cohort. The identified biomarkers served as potential indicators for early detection of CVDs. With its successful implementation, our newly developed predictive engine provides a valuable framework for identifying patients with CVDs based on their biomarker profiles.","<method>recursive feature elimination classifier</method>, <method>Random Forest</method>, <method>Support Vector Machine</method>, <method>Xtreme Gradient Boosting Decision Trees</method>, <method>k-Nearest Neighbors</method>, <method>soft voting classifier</method>"
2024,https://openalex.org/W4391135337,Medicine,ACCORD (ACcurate COnsensus Reporting Document): A reporting guideline for consensus methods in biomedicine developed via a modified Delphi,"Background In biomedical research, it is often desirable to seek consensus among individuals who have differing perspectives and experience. This is important when evidence is emerging, inconsistent, limited, or absent. Even when research evidence is abundant, clinical recommendations, policy decisions, and priority-setting may still require agreement from multiple, sometimes ideologically opposed parties. Despite their prominence and influence on key decisions, consensus methods are often poorly reported. Our aim was to develop the first reporting guideline dedicated to and applicable to all consensus methods used in biomedical research regardless of the objective of the consensus process, called ACCORD (ACcurate COnsensus Reporting Document). Methods and findings We followed methodology recommended by the EQUATOR Network for the development of reporting guidelines: a systematic review was followed by a Delphi process and meetings to finalize the ACCORD checklist. The preliminary checklist was drawn from the systematic review of existing literature on the quality of reporting of consensus methods and suggestions from the Steering Committee. A Delphi panel ( n = 72) was recruited with representation from 6 continents and a broad range of experience, including clinical, research, policy, and patient perspectives. The 3 rounds of the Delphi process were completed by 58, 54, and 51 panelists. The preliminary checklist of 56 items was refined to a final checklist of 35 items relating to the article title ( n = 1), introduction ( n = 3), methods ( n = 21), results ( n = 5), discussion ( n = 2), and other information ( n = 3). Conclusions The ACCORD checklist is the first reporting guideline applicable to all consensus-based studies. It will support authors in writing accurate, detailed manuscripts, thereby improving the completeness and transparency of reporting and providing readers with clarity regarding the methods used to reach agreement. Furthermore, the checklist will make the rigor of the consensus methods used to guide the recommendations clear for readers. Reporting consensus studies with greater clarity and transparency may enhance trust in the recommendations made by consensus panels.",No methods found.
2024,https://openalex.org/W4393120004,Medicine,The Conceptual Definition of Sarcopenia: Delphi Consensus from the Global Leadership Initiative in Sarcopenia (GLIS),"Sarcopenia, the age-related loss of muscle mass and strength/function, is an important clinical condition. However, no international consensus on the definition exists. The Global Leadership Initiative in Sarcopenia (GLIS) aimed to address this by establishing the global conceptual definition of sarcopenia. The GLIS steering committee was formed in 2019-21 with representatives from all relevant scientific societies worldwide. During this time, the steering committee developed a set of statements on the topic and invited members from these societies to participate in a two-phase International Delphi Study. Between 2022 and 2023, participants ranked their agreement with a set of statements using an online survey tool (SurveyMonkey). Statements were categorised based on predefined thresholds: strong agreement (>80%), moderate agreement (70-80%) and low agreement (<70%). Statements with strong agreement were accepted, statements with low agreement were rejected and those with moderate agreement were reintroduced until consensus was reached. 107 participants (mean age: 54 ± 12 years [1 missing age], 64% men) from 29 countries across 7 continents/regions completed the Delphi survey. Twenty statements were found to have a strong agreement. These included; 6 statements on 'general aspects of sarcopenia' (strongest agreement: the prevalence of sarcopenia increases with age (98.3%)), 3 statements on 'components of sarcopenia' (muscle mass (89.4%), muscle strength (93.1%) and muscle-specific strength (80.8%) should all be a part of the conceptual definition of sarcopenia)) and 11 statements on 'outcomes of sarcopenia' (strongest agreement: sarcopenia increases the risk of impaired physical performance (97.9%)). A key finding of the Delphi survey was that muscle mass, muscle strength and muscle-specific strength were all accepted as 'components of sarcopenia', whereas impaired physical performance was accepted as an 'outcome' rather than a 'component' of sarcopenia. The GLIS has created the first global conceptual definition of sarcopenia, which will now serve to develop an operational definition for clinical and research settings.",No methods found.
2024,https://openalex.org/W4392851477,Medicine,"Generative AI in healthcare: an implementation science informed translational path on application, integration and governance","Abstract Background Artificial intelligence (AI), particularly generative AI, has emerged as a transformative tool in healthcare, with the potential to revolutionize clinical decision-making and improve health outcomes. Generative AI, capable of generating new data such as text and images, holds promise in enhancing patient care, revolutionizing disease diagnosis and expanding treatment options. However, the utility and impact of generative AI in healthcare remain poorly understood, with concerns around ethical and medico-legal implications, integration into healthcare service delivery and workforce utilisation. Also, there is not a clear pathway to implement and integrate generative AI in healthcare delivery. Methods This article aims to provide a comprehensive overview of the use of generative AI in healthcare, focusing on the utility of the technology in healthcare and its translational application highlighting the need for careful planning, execution and management of expectations in adopting generative AI in clinical medicine. Key considerations include factors such as data privacy, security and the irreplaceable role of clinicians’ expertise. Frameworks like the technology acceptance model (TAM) and the Non-Adoption, Abandonment, Scale-up, Spread and Sustainability (NASSS) model are considered to promote responsible integration. These frameworks allow anticipating and proactively addressing barriers to adoption, facilitating stakeholder participation and responsibly transitioning care systems to harness generative AI’s potential. Results Generative AI has the potential to transform healthcare through automated systems, enhanced clinical decision-making and democratization of expertise with diagnostic support tools providing timely, personalized suggestions. Generative AI applications across billing, diagnosis, treatment and research can also make healthcare delivery more efficient, equitable and effective. However, integration of generative AI necessitates meticulous change management and risk mitigation strategies. Technological capabilities alone cannot shift complex care ecosystems overnight; rather, structured adoption programs grounded in implementation science are imperative. Conclusions It is strongly argued in this article that generative AI can usher in tremendous healthcare progress, if introduced responsibly. Strategic adoption based on implementation science, incremental deployment and balanced messaging around opportunities versus limitations helps promote safe, ethical generative AI integration. Extensive real-world piloting and iteration aligned to clinical priorities should drive development. With conscientious governance centred on human wellbeing over technological novelty, generative AI can enhance accessibility, affordability and quality of care. As these models continue advancing rapidly, ongoing reassessment and transparent communication around their strengths and weaknesses remain vital to restoring trust, realizing positive potential and, most importantly, improving patient outcomes.","<method>generative AI</method>, <method>technology acceptance model (TAM)</method>, <method>Non-Adoption, Abandonment, Scale-up, Spread and Sustainability (NASSS) model</method>, <method>implementation science</method>"
2024,https://openalex.org/W4390919701,Medicine,Chatbots and Large Language Models in Radiology: A Practical Primer for Clinical and Research Applications,"Although chatbots have existed for decades, the emergence of transformer-based large language models (LLMs) has captivated the world through the most recent wave of artificial intelligence chatbots, including ChatGPT. Transformers are a type of neural network architecture that enables better contextual understanding of language and efficient training on massive amounts of unlabeled data, such as unstructured text from the internet. As LLMs have increased in size, their improved performance and emergent abilities have revolutionized natural language processing. Since language is integral to human thought, applications based on LLMs have transformative potential in many industries. In fact, LLM-based chatbots have demonstrated human-level performance on many professional benchmarks, including in radiology. LLMs offer numerous clinical and research applications in radiology, several of which have been explored in the literature with encouraging results. Multimodal LLMs can simultaneously interpret text and images to generate reports, closely mimicking current diagnostic pathways in radiology. Thus, from requisition to report, LLMs have the opportunity to positively impact nearly every step of the radiology journey. Yet, these impressive models are not without limitations. This article reviews the limitations of LLMs and mitigation strategies, as well as potential uses of LLMs, including multimodal models. Also reviewed are existing LLM-based applications that can enhance efficiency in supervised settings.","<method>transformer-based large language models (LLMs)</method>, <method>Transformers</method>, <method>Multimodal LLMs</method>"
2024,https://openalex.org/W4390928581,Medicine,METhodological RadiomICs Score (METRICS): a quality scoring tool for radiomics research endorsed by EuSoMII,"Abstract Purpose To propose a new quality scoring tool, METhodological RadiomICs Score (METRICS), to assess and improve research quality of radiomics studies. Methods We conducted an online modified Delphi study with a group of international experts. It was performed in three consecutive stages: Stage#1, item preparation; Stage#2, panel discussion among EuSoMII Auditing Group members to identify the items to be voted; and Stage#3, four rounds of the modified Delphi exercise by panelists to determine the items eligible for the METRICS and their weights. The consensus threshold was 75%. Based on the median ranks derived from expert panel opinion and their rank-sum based conversion to importance scores, the category and item weights were calculated. Result In total, 59 panelists from 19 countries participated in selection and ranking of the items and categories. Final METRICS tool included 30 items within 9 categories. According to their weights, the categories were in descending order of importance: study design, imaging data, image processing and feature extraction, metrics and comparison, testing, feature processing, preparation for modeling, segmentation, and open science. A web application and a repository were developed to streamline the calculation of the METRICS score and to collect feedback from the radiomics community. Conclusion In this work, we developed a scoring tool for assessing the methodological quality of the radiomics research, with a large international panel and a modified Delphi protocol. With its conditional format to cover methodological variations, it provides a well-constructed framework for the key methodological concepts to assess the quality of radiomic research papers. Critical relevance statement A quality assessment tool, METhodological RadiomICs Score (METRICS), is made available by a large group of international domain experts, with transparent methodology, aiming at evaluating and improving research quality in radiomics and machine learning. Key points • A methodological scoring tool, METRICS, was developed for assessing the quality of radiomics research, with a large international expert panel and a modified Delphi protocol. • The proposed scoring tool presents expert opinion-based importance weights of categories and items with a transparent methodology for the first time. • METRICS accounts for varying use cases, from handcrafted radiomics to entirely deep learning-based pipelines. • A web application has been developed to help with the calculation of the METRICS score ( https://metricsscore.github.io/metrics/METRICS.html ) and a repository created to collect feedback from the radiomics community ( https://github.com/metricsscore/metrics ). Graphical Abstract",No methods found.
2024,https://openalex.org/W4391292768,Medicine,Improving large language models for clinical named entity recognition via prompt engineering,"Abstract Importance The study highlights the potential of large language models, specifically GPT-3.5 and GPT-4, in processing complex clinical data and extracting meaningful information with minimal training data. By developing and refining prompt-based strategies, we can significantly enhance the models’ performance, making them viable tools for clinical NER tasks and possibly reducing the reliance on extensive annotated datasets. Objectives This study quantifies the capabilities of GPT-3.5 and GPT-4 for clinical named entity recognition (NER) tasks and proposes task-specific prompts to improve their performance. Materials and Methods We evaluated these models on 2 clinical NER tasks: (1) to extract medical problems, treatments, and tests from clinical notes in the MTSamples corpus, following the 2010 i2b2 concept extraction shared task, and (2) to identify nervous system disorder-related adverse events from safety reports in the vaccine adverse event reporting system (VAERS). To improve the GPT models' performance, we developed a clinical task-specific prompt framework that includes (1) baseline prompts with task description and format specification, (2) annotation guideline-based prompts, (3) error analysis-based instructions, and (4) annotated samples for few-shot learning. We assessed each prompt's effectiveness and compared the models to BioClinicalBERT. Results Using baseline prompts, GPT-3.5 and GPT-4 achieved relaxed F1 scores of 0.634, 0.804 for MTSamples and 0.301, 0.593 for VAERS. Additional prompt components consistently improved model performance. When all 4 components were used, GPT-3.5 and GPT-4 achieved relaxed F1 socres of 0.794, 0.861 for MTSamples and 0.676, 0.736 for VAERS, demonstrating the effectiveness of our prompt framework. Although these results trail BioClinicalBERT (F1 of 0.901 for the MTSamples dataset and 0.802 for the VAERS), it is very promising considering few training samples are needed. Discussion The study’s findings suggest a promising direction in leveraging LLMs for clinical NER tasks. However, while the performance of GPT models improved with task-specific prompts, there's a need for further development and refinement. LLMs like GPT-4 show potential in achieving close performance to state-of-the-art models like BioClinicalBERT, but they still require careful prompt engineering and understanding of task-specific knowledge. The study also underscores the importance of evaluation schemas that accurately reflect the capabilities and performance of LLMs in clinical settings. Conclusion While direct application of GPT models to clinical NER tasks falls short of optimal performance, our task-specific prompt framework, incorporating medical knowledge and training samples, significantly enhances GPT models' feasibility for potential clinical applications.","<method>GPT-3.5</method>, <method>GPT-4</method>, <method>prompt-based strategies</method>, <method>task-specific prompts</method>, <method>few-shot learning</method>, <method>BioClinicalBERT</method>"
2024,https://openalex.org/W4391221083,Medicine,FDA-Approved Artificial Intelligence and Machine Learning (AI/ML)-Enabled Medical Devices: An Updated Landscape,"As artificial intelligence (AI) has been highly advancing in the last decade, machine learning (ML)-enabled medical devices are increasingly used in healthcare. In this study, we collected publicly available information on AI/ML-enabled medical devices approved by the FDA in the United States, as of the latest update on 19 October 2023. We performed comprehensive analysis of a total of 691 FDA-approved artificial intelligence and machine learning (AI/ML)-enabled medical devices and offer an in-depth analysis of clearance pathways, approval timeline, regulation type, medical specialty, decision type, recall history, etc. We found a significant surge in approvals since 2018, with clear dominance of the radiology specialty in the application of machine learning tools, attributed to the abundant data from routine clinical data. The study also reveals a reliance on the 510(k)-clearance pathway, emphasizing its basis on substantial equivalence and often bypassing the need for new clinical trials. Also, it notes an underrepresentation of pediatric-focused devices and trials, suggesting an opportunity for expansion in this demographic. Moreover, the geographical limitation of clinical trials, primarily within the United States, points to a need for more globally inclusive trials to encompass diverse patient demographics. This analysis not only maps the current landscape of AI/ML-enabled medical devices but also pinpoints trends, potential gaps, and areas for future exploration, clinical trial practices, and regulatory approaches. In conclusion, our analysis sheds light on the current state of FDA-approved AI/ML-enabled medical devices and prevailing trends, contributing to a wider comprehension.",No methods found.
2024,https://openalex.org/W4390587679,Medicine,"A Systematic Review and Meta-Analysis of Artificial Intelligence Tools in Medicine and Healthcare: Applications, Considerations, Limitations, Motivation and Challenges","Artificial intelligence (AI) has emerged as a transformative force in various sectors, including medicine and healthcare. Large language models like ChatGPT showcase AI’s potential by generating human-like text through prompts. ChatGPT’s adaptability holds promise for reshaping medical practices, improving patient care, and enhancing interactions among healthcare professionals, patients, and data. In pandemic management, ChatGPT rapidly disseminates vital information. It serves as a virtual assistant in surgical consultations, aids dental practices, simplifies medical education, and aids in disease diagnosis. A total of 82 papers were categorised into eight major areas, which are G1: treatment and medicine, G2: buildings and equipment, G3: parts of the human body and areas of the disease, G4: patients, G5: citizens, G6: cellular imaging, radiology, pulse and medical images, G7: doctors and nurses, and G8: tools, devices and administration. Balancing AI’s role with human judgment remains a challenge. A systematic literature review using the PRISMA approach explored AI’s transformative potential in healthcare, highlighting ChatGPT’s versatile applications, limitations, motivation, and challenges. In conclusion, ChatGPT’s diverse medical applications demonstrate its potential for innovation, serving as a valuable resource for students, academics, and researchers in healthcare. Additionally, this study serves as a guide, assisting students, academics, and researchers in the field of medicine and healthcare alike.","<method>Large language models</method>, <method>ChatGPT</method>, <method>systematic literature review using the PRISMA approach</method>"
2024,https://openalex.org/W4391469183,Medicine,Observational evidence for primordial black holes: A positivist perspective,"We review numerous arguments for primordial black holes (PBHs) based on observational evidence from a variety of lensing, dynamical, accretion and gravitational-wave effects. This represents a shift from the usual emphasis on PBH constraints and provides what we term a positivist perspective. Microlensing observations of stars and quasars suggest that PBHs of around 1M⊙ could provide much of the dark matter in galactic halos, this being allowed by the Large Magellanic Cloud microlensing observations if the PBHs have an extended mass function. More generally, providing the mass and dark matter fraction of the PBHs is large enough, the associated Poisson fluctuations could generate the first bound objects at a much earlier epoch than in the standard cosmological scenario. This simultaneously explains the recent detection of high-redshift dwarf galaxies, puzzling correlations of the source-subtracted infrared and X-ray cosmic backgrounds, the size and the mass-to-light ratios of ultra-faint-dwarf galaxies, the dynamical heating of the Galactic disc, and the binary coalescences observed by LIGO/Virgo/KAGRA in a mass range not usually associated with stellar remnants. Even if PBHs provide only a small fraction of the dark matter, they could explain various other observational conundra, and sufficiently large ones could seed the supermassive black holes in galactic nuclei or even early galaxies themselves. We argue that PBHs would naturally have formed around the electroweak, quantum chromodynamics and electron–positron annihilation epochs, when the sound-speed inevitably dips. This leads to an extended PBH mass function with a number of distinct bumps, the most prominent one being at around 1M⊙, and this would allow PBHs to explain many of the observations in a unified way.",No methods found.
2024,https://openalex.org/W4392193191,Medicine,Challenges and barriers of using large language models (LLM) such as ChatGPT for diagnostic medicine with a focus on digital pathology – a recent scoping review,"Abstract Background The integration of large language models (LLMs) like ChatGPT in diagnostic medicine, with a focus on digital pathology, has garnered significant attention. However, understanding the challenges and barriers associated with the use of LLMs in this context is crucial for their successful implementation. Methods A scoping review was conducted to explore the challenges and barriers of using LLMs, in diagnostic medicine with a focus on digital pathology. A comprehensive search was conducted using electronic databases, including PubMed and Google Scholar, for relevant articles published within the past four years. The selected articles were critically analyzed to identify and summarize the challenges and barriers reported in the literature. Results The scoping review identified several challenges and barriers associated with the use of LLMs in diagnostic medicine. These included limitations in contextual understanding and interpretability, biases in training data, ethical considerations, impact on healthcare professionals, and regulatory concerns. Contextual understanding and interpretability challenges arise due to the lack of true understanding of medical concepts and lack of these models being explicitly trained on medical records selected by trained professionals, and the black-box nature of LLMs. Biases in training data pose a risk of perpetuating disparities and inaccuracies in diagnoses. Ethical considerations include patient privacy, data security, and responsible AI use. The integration of LLMs may impact healthcare professionals’ autonomy and decision-making abilities. Regulatory concerns surround the need for guidelines and frameworks to ensure safe and ethical implementation. Conclusion The scoping review highlights the challenges and barriers of using LLMs in diagnostic medicine with a focus on digital pathology. Understanding these challenges is essential for addressing the limitations and developing strategies to overcome barriers. It is critical for health professionals to be involved in the selection of data and fine tuning of the models. Further research, validation, and collaboration between AI developers, healthcare professionals, and regulatory bodies are necessary to ensure the responsible and effective integration of LLMs in diagnostic medicine.",<method>large language models (LLMs)</method>
2024,https://openalex.org/W4390940921,Medicine,Solving olympiad geometry without human demonstrations,"Abstract Proving mathematical theorems at the olympiad level represents a notable milestone in human-level automated reasoning 1–4 , owing to their reputed difficulty among the world’s best talents in pre-university mathematics. Current machine-learning approaches, however, are not applicable to most mathematical domains owing to the high cost of translating human proofs into machine-verifiable format. The problem is even worse for geometry because of its unique translation challenges 1,5 , resulting in severe scarcity of training data. We propose AlphaGeometry, a theorem prover for Euclidean plane geometry that sidesteps the need for human demonstrations by synthesizing millions of theorems and proofs across different levels of complexity. AlphaGeometry is a neuro-symbolic system that uses a neural language model, trained from scratch on our large-scale synthetic data, to guide a symbolic deduction engine through infinite branching points in challenging problems. On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist. Notably, AlphaGeometry produces human-readable proofs, solves all geometry problems in the IMO 2000 and 2015 under human expert evaluation and discovers a generalized version of a translated IMO theorem in 2004.","<method>neural language model</method>, <method>neuro-symbolic system</method>"
2024,https://openalex.org/W4390904004,Medicine,The Use of Artificial Intelligence in Writing Scientific Review Articles,"Abstract Purpose of Review With the recent explosion in the use of artificial intelligence (AI) and specifically ChatGPT, we sought to determine whether ChatGPT could be used to assist in writing credible, peer-reviewed, scientific review articles. We also sought to assess, in a scientific study, the advantages and limitations of using ChatGPT for this purpose. To accomplish this, 3 topics of importance in musculoskeletal research were selected: (1) the intersection of Alzheimer’s disease and bone; (2) the neural regulation of fracture healing; and (3) COVID-19 and musculoskeletal health. For each of these topics, 3 approaches to write manuscript drafts were undertaken: (1) human only; (2) ChatGPT only (AI-only); and (3) combination approach of #1 and #2 (AI-assisted). Articles were extensively fact checked and edited to ensure scientific quality, resulting in final manuscripts that were significantly different from the original drafts. Numerous parameters were measured throughout the process to quantitate advantages and disadvantages of approaches. Recent Findings Overall, use of AI decreased the time spent to write the review article, but required more extensive fact checking. With the AI-only approach, up to 70% of the references cited were found to be inaccurate. Interestingly, the AI-assisted approach resulted in the highest similarity indices suggesting a higher likelihood of plagiarism. Finally, although the technology is rapidly changing, at the time of study, ChatGPT 4.0 had a cutoff date of September 2021 rendering identification of recent articles impossible. Therefore, all literature published past the cutoff date was manually provided to ChatGPT, rendering approaches #2 and #3 identical for contemporary citations. As a result, for the COVID-19 and musculoskeletal health topic, approach #2 was abandoned midstream due to the extensive overlap with approach #3. Summary The main objective of this scientific study was to see whether AI could be used in a scientifically appropriate manner to improve the scientific writing process. Indeed, AI reduced the time for writing but had significant inaccuracies. The latter necessitates that AI cannot currently be used alone but could be used with careful oversight by humans to assist in writing scientific review articles.",<method>ChatGPT</method>
2024,https://openalex.org/W4390833194,Medicine,Towards Conversational Diagnostic AI,"At the heart of medicine lies the physician-patient dialogue, where skillful history-taking paves the way for accurate diagnosis, effective management, and enduring trust. Artificial Intelligence (AI) systems capable of diagnostic dialogue could increase accessibility, consistency, and quality of care. However, approximating clinicians' expertise is an outstanding grand challenge. Here, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large Language Model (LLM) based AI system optimized for diagnostic dialogue. AMIE uses a novel self-play based simulated environment with automated feedback mechanisms for scaling learning across diverse disease conditions, specialties, and contexts. We designed a framework for evaluating clinically-meaningful axes of performance including history-taking, diagnostic accuracy, management reasoning, communication skills, and empathy. We compared AMIE's performance to that of primary care physicians (PCPs) in a randomized, double-blind crossover study of text-based consultations with validated patient actors in the style of an Objective Structured Clinical Examination (OSCE). The study included 149 case scenarios from clinical providers in Canada, the UK, and India, 20 PCPs for comparison with AMIE, and evaluations by specialist physicians and patient actors. AMIE demonstrated greater diagnostic accuracy and superior performance on 28 of 32 axes according to specialist physicians and 24 of 26 axes according to patient actors. Our research has several limitations and should be interpreted with appropriate caution. Clinicians were limited to unfamiliar synchronous text-chat which permits large-scale LLM-patient interactions but is not representative of usual clinical practice. While further research is required before AMIE could be translated to real-world settings, the results represent a milestone towards conversational diagnostic AI.","<method>Large Language Model (LLM)</method>, <method>self-play based simulated environment</method>, <method>automated feedback mechanisms</method>"
2024,https://openalex.org/W4394767142,Medicine,"A Breathable, Stretchable, and Self‐Calibrated Multimodal Electronic Skin Based on Hydrogel Microstructures for Wireless Wearables","Abstract Biomimetic electronic skins (e‐skins) are widely used in wearables, smart prosthesis and soft robotics. However, multimodal e‐skins, especially those based on hydrogels, face multiple challenges for practical applications, involving multi‐sensing signal mutual interference, low breathability and stretchability. Here, a breathable and stretchable multimodal e‐skin with a multilayer film microstructure is developed to achieve self‐calibrated sensing of any two of three stimuli: strain, temperature, and humidity, with minimal crosstalk. Hydrogel fibers with different shapes are designed for strain and temperature sensing modules, and the hydrogel film is developed as a humidity sensing module. The multimodal e‐skin exhibits impressive sensing performance, including a low strain detection limit (0.03%), strain linearity (R 2 = 0.990), high‐temperature sensitivity (1.77%/°C), and a wide humidity detection range (33–98% RH). Interestingly, due to the directional anisotropy in strain sensitivity of different shaped fibers, the e‐skin realizes self‐calibrated detection of strain and temperature in different directions. By introducing porous elastomer encapsulation membranes, the breathability and wearing comfort of the e‐skin are attained, while the high stretchability (100% strain) is maintained. Furthermore, a personalized human‐machine interaction system is created by integrating the e‐skin with a wireless circuit to realize real‐time and wireless gesture recognition, physiological signals monitoring, and smart prosthesis.",No methods found.
2024,https://openalex.org/W4391577343,Medicine,The Image Biomarker Standardization Initiative: Standardized Convolutional Filters for Reproducible Radiomics and Enhanced Clinical Insights,"Filters are commonly used to enhance specific structures and patterns in images, such as vessels or peritumoral regions, to enable clinical insights beyond the visible image using radiomics. However, their lack of standardization restricts reproducibility and clinical translation of radiomics decision support tools. In this special report, teams of researchers who developed radiomics software participated in a three-phase study (September 2020 to December 2022) to establish a standardized set of filters. The first two phases focused on finding reference filtered images and reference feature values for commonly used convolutional filters: mean, Laplacian of Gaussian, Laws and Gabor kernels, separable and nonseparable wavelets (including decomposed forms), and Riesz transformations. In the first phase, 15 teams used digital phantoms to establish 33 reference filtered images of 36 filter configurations. In phase 2, 11 teams used a chest CT image to derive reference values for 323 of 396 features computed from filtered images using 22 filter and image processing configurations. Reference filtered images and feature values for Riesz transformations were not established. Reproducibility of standardized convolutional filters was validated on a public data set of multimodal imaging (CT, fluorodeoxyglucose PET, and T1-weighted MRI) in 51 patients with soft-tissue sarcoma. At validation, reproducibility of 486 features computed from filtered images using nine configurations × three imaging modalities was assessed using the lower bounds of 95% CIs of intraclass correlation coefficients. Out of 486 features, 458 were found to be reproducible across nine teams with lower bounds of 95% CIs of intraclass correlation coefficients greater than 0.75. In conclusion, eight filter types were standardized with reference filtered images and reference feature values for verifying and calibrating radiomics software packages. A web-based tool is available for compliance checking. © RSNA, 2024 Supplemental material is available for this article. See also the editorial by Huisman and D'Antonoli in this issue.","<method>mean convolutional filter</method>, <method>Laplacian of Gaussian filter</method>, <method>Laws filter</method>, <method>Gabor kernels</method>, <method>separable wavelets</method>, <method>nonseparable wavelets</method>, <method>Riesz transformations</method>"
2024,https://openalex.org/W4392643126,Medicine,Generative Artificial Intelligence to Transform Inpatient Discharge Summaries to Patient-Friendly Language and Format,"Importance By law, patients have immediate access to discharge notes in their medical records. Technical language and abbreviations make notes difficult to read and understand for a typical patient. Large language models (LLMs [eg, GPT-4]) have the potential to transform these notes into patient-friendly language and format. Objective To determine whether an LLM can transform discharge summaries into a format that is more readable and understandable. Design, Setting, and Participants This cross-sectional study evaluated a sample of the discharge summaries of adult patients discharged from the General Internal Medicine service at NYU (New York University) Langone Health from June 1 to 30, 2023. Patients discharged as deceased were excluded. All discharge summaries were processed by the LLM between July 26 and August 5, 2023. Interventions A secure Health Insurance Portability and Accountability Act–compliant platform, Microsoft Azure OpenAI, was used to transform these discharge summaries into a patient-friendly format between July 26 and August 5, 2023. Main Outcomes and Measures Outcomes included readability as measured by Flesch-Kincaid Grade Level and understandability using Patient Education Materials Assessment Tool (PEMAT) scores. Readability and understandability of the original discharge summaries were compared with the transformed, patient-friendly discharge summaries created through the LLM. As balancing metrics, accuracy and completeness of the patient-friendly version were measured. Results Discharge summaries of 50 patients (31 female [62.0%] and 19 male [38.0%]) were included. The median patient age was 65.5 (IQR, 59.0-77.5) years. Mean (SD) Flesch-Kincaid Grade Level was significantly lower in the patient-friendly discharge summaries (6.2 [0.5] vs 11.0 [1.5]; P &amp;amp;lt; .001). PEMAT understandability scores were significantly higher for patient-friendly discharge summaries (81% vs 13%; P &amp;amp;lt; .001). Two physicians reviewed each patient-friendly discharge summary for accuracy on a 6-point scale, with 54 of 100 reviews (54.0%) giving the best possible rating of 6. Summaries were rated entirely complete in 56 reviews (56.0%). Eighteen reviews noted safety concerns, mostly involving omissions, but also several inaccurate statements (termed hallucinations). Conclusions and Relevance The findings of this cross-sectional study of 50 discharge summaries suggest that LLMs can be used to translate discharge summaries into patient-friendly language and formats that are significantly more readable and understandable than discharge summaries as they appear in electronic health records. However, implementation will require improvements in accuracy, completeness, and safety. Given the safety concerns, initial implementation will require physician review.",<method>Large language models (LLMs)</method>
2024,https://openalex.org/W4391968719,Medicine,Artificial intelligence and IoT driven technologies for environmental pollution monitoring and management,"Detecting hazardous substances in the environment is crucial for protecting human wellbeing and ecosystems. As technology continues to advance, artificial intelligence (AI) has emerged as a promising tool for creating sensors that can effectively detect and analyze these hazardous substances. The increasing advancements in information technology have led to a growing interest in utilizing this technology for environmental pollution detection. AI-driven sensor systems, AI and Internet of Things (IoT) can be efficiently used for environmental monitoring, such as those for detecting air pollutants, water contaminants, and soil toxins. With the increasing concerns about the detrimental impact of legacy and emerging hazardous substances on ecosystems and human health, it is necessary to develop advanced monitoring systems that can efficiently detect, analyze, and respond to potential risks. Therefore, this review aims to explore recent advancements in using AI, sensors and IOTs for environmental pollution monitoring, taking into account the complexities of predicting and tracking pollution changes due to the dynamic nature of the environment. Integrating machine learning (ML) methods has the potential to revolutionize environmental science, but it also poses challenges. Important considerations include balancing model performance and interpretability, understanding ML model requirements, selecting appropriate models, and addressing concerns related to data sharing. Through examining these issues, this study seeks to highlight the latest trends in leveraging AI and IOT for environmental pollution monitoring.","<method>artificial intelligence (AI)</method>, <method>machine learning (ML) methods</method>"
2024,https://openalex.org/W4392754729,Medicine,"Revolutionizing agriculture with artificial intelligence: plant disease detection methods, applications, and their limitations","Accurate and rapid plant disease detection is critical for enhancing long-term agricultural yield. Disease infection poses the most significant challenge in crop production, potentially leading to economic losses. Viruses, fungi, bacteria, and other infectious organisms can affect numerous plant parts, including roots, stems, and leaves. Traditional techniques for plant disease detection are time-consuming, require expertise, and are resource-intensive. Therefore, automated leaf disease diagnosis using artificial intelligence (AI) with Internet of Things (IoT) sensors methodologies are considered for the analysis and detection. This research examines four crop diseases: tomato, chilli, potato, and cucumber. It also highlights the most prevalent diseases and infections in these four types of vegetables, along with their symptoms. This review provides detailed predetermined steps to predict plant diseases using AI. Predetermined steps include image acquisition, preprocessing, segmentation, feature selection, and classification. Machine learning (ML) and deep understanding (DL) detection models are discussed. A comprehensive examination of various existing ML and DL-based studies to detect the disease of the following four crops is discussed, including the datasets used to evaluate these studies. We also provided the list of plant disease detection datasets. Finally, different ML and DL application problems are identified and discussed, along with future research prospects, by combining AI with IoT platforms like smart drones for field-based disease detection and monitoring. This work will help other practitioners in surveying different plant disease detection strategies and the limits of present systems.","<method>image acquisition</method>, <method>preprocessing</method>, <method>segmentation</method>, <method>feature selection</method>, <method>classification</method>, <method>machine learning (ML)</method>, <method>deep learning (DL)</method>"
2024,https://openalex.org/W4392386497,Medicine,Clinical applications of artificial intelligence in robotic surgery,"Abstract Artificial intelligence (AI) is revolutionizing nearly every aspect of modern life. In the medical field, robotic surgery is the sector with some of the most innovative and impactful advancements. In this narrative review, we outline recent contributions of AI to the field of robotic surgery with a particular focus on intraoperative enhancement. AI modeling is allowing surgeons to have advanced intraoperative metrics such as force and tactile measurements, enhanced detection of positive surgical margins, and even allowing for the complete automation of certain steps in surgical procedures. AI is also Query revolutionizing the field of surgical education. AI modeling applied to intraoperative surgical video feeds and instrument kinematics data is allowing for the generation of automated skills assessments. AI also shows promise for the generation and delivery of highly specialized intraoperative surgical feedback for training surgeons. Although the adoption and integration of AI show promise in robotic surgery, it raises important, complex ethical questions. Frameworks for thinking through ethical dilemmas raised by AI are outlined in this review. AI enhancements in robotic surgery is some of the most groundbreaking research happening today, and the studies outlined in this review represent some of the most exciting innovations in recent years.",<method>AI modeling</method>
2024,https://openalex.org/W4394819506,Medicine,Femtosecond laser-textured superhydrophilic coral-like structures spread AgNWs enable strong thermal camouflage and anti-counterfeiting,"Modulating the thermal emission of a material in the infrared (IR) range can be essential for various practical applications such as smart textiles, camouflage, and anti-counterfeiting. Although many different materials or structures have been proposed, the complex manufacturing processes are still hindering their widespread use. Herein, a facile femtosecond laser processing technology and a drop-coating method are introduced to form a patternable low emissivity film. Laser-treated polyimide films resulted in superhydrophilic structured surfaces that are uniformly coated with silver-nanowires (AgNWs) in aqueous solutions for low emissivity surfaces. Furthermore, the emissivity of the samples is as low as ∼0.2 without deterioration over 800 bending-releasing cycles. The as-prepared films also display good thermal camouflage properties, namely, the films reduced the thermal radiation temperature of an object by 35.8 °C when the object temperature was ∼69.1 °C. Additionally, this IR camouflage effect of the AgNWs coated samples shows excellent stability even in harsh environments such as immersion in water, acid, alkali, and salt solution and applied voltage. We also show that information encryption was possible by adjusting the amount of AgNWs. The design of this programmable patterned low emissivity film indicates an idea for the thermal camouflage and anti-counterfeiting technology, which can carry more abundant application scenario and disguise them more complex and sophisticated.",No methods found.
2024,https://openalex.org/W4392791588,Medicine,Can large language models replace humans in systematic reviews? Evaluating <scp>GPT</scp>‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages,"Systematic reviews are vital for guiding practice, research and policy, although they are often slow and labour-intensive. Large language models (LLMs) could speed up and automate systematic reviews, but their performance in such tasks has yet to be comprehensively evaluated against humans, and no study has tested Generative Pre-Trained Transformer (GPT)-4, the biggest LLM so far. This pre-registered study uses a ""human-out-of-the-loop"" approach to evaluate GPT-4's capability in title/abstract screening, full-text review and data extraction across various literature types and languages. Although GPT-4 had accuracy on par with human performance in some tasks, results were skewed by chance agreement and dataset imbalance. Adjusting for these caused performance scores to drop across all stages: for data extraction, performance was moderate, and for screening, it ranged from none in highly balanced literature datasets (~1:1) to moderate in those datasets where the ratio of inclusion to exclusion in studies was imbalanced (~1:3). When screening full-text literature using highly reliable prompts, GPT-4's performance was more robust, reaching ""human-like"" levels. Although our findings indicate that, currently, substantial caution should be exercised if LLMs are being used to conduct systematic reviews, they also offer preliminary evidence that, for certain review tasks delivered under specific conditions, LLMs can rival human performance.","<method>Large language models (LLMs)</method>, <method>Generative Pre-Trained Transformer (GPT)-4</method>"
2024,https://openalex.org/W4390496023,Medicine,Improved Support Vector Machine based on CNN-SVD for vision-threatening diabetic retinopathy detection and classification,"The integration of artificial intelligence (AI) in diagnosing diabetic retinopathy, a major contributor to global vision impairment, is becoming increasingly pronounced. Notably, the detection of vision-threatening diabetic retinopathy (VTDR) has been significantly fortified through automated techniques. Traditionally, the reliance on manual analysis of retinal images, albeit slow and error-prone, constituted the conventional approach. Addressing this, our study introduces a novel methodology that amplifies the robustness and precision of the detection process. This is complemented by the groundbreaking Hierarchical Block Attention (HBA) and HBA-U-Net architecture, which notably propel attention mechanisms in image segmentation. This innovative model refines image processing without imposing excessive computational demands by honing in on individual pixel intricacies, spatial relationships, and channel-specific attention. Building upon this innovation, our proposed method employs a multi-stage strategy encompassing data pre-processing, feature extraction via a hybrid CNN-SVD model, and classification employing an amalgamation of Improved Support Vector Machine-Radial Basis Function (ISVM-RBF), DT, and KNN techniques. Rigorously tested on the IDRiD dataset classified into five severity tiers, the hybrid model yields remarkable performance, achieving a 99.18% accuracy, 98.15% sensitivity, and 100% specificity in VTDR detection, thus surpassing existing methods. These results underscore a more potent avenue for diagnosing and addressing this crucial ocular condition while underscoring AI's transformative potential in medical care, particularly in ophthalmology.","<method>Hierarchical Block Attention (HBA)</method>, <method>HBA-U-Net architecture</method>, <method>hybrid CNN-SVD model</method>, <method>Improved Support Vector Machine-Radial Basis Function (ISVM-RBF)</method>, <method>DT</method>, <method>KNN</method>"
2024,https://openalex.org/W4391109567,Medicine,Recommended implementation of quantitative susceptibility mapping for clinical research in the brain: A consensus of the <scp>ISMRM</scp> electro‐magnetic tissue properties study group,"Abstract This article provides recommendations for implementing QSM for clinical brain research. It is a consensus of the International Society of Magnetic Resonance in Medicine, Electro‐Magnetic Tissue Properties Study Group. While QSM technical development continues to advance rapidly, the current QSM methods have been demonstrated to be repeatable and reproducible for generating quantitative tissue magnetic susceptibility maps in the brain. However, the many QSM approaches available have generated a need in the neuroimaging community for guidelines on implementation. This article outlines considerations and implementation recommendations for QSM data acquisition, processing, analysis, and publication. We recommend that data be acquired using a monopolar 3D multi‐echo gradient echo (GRE) sequence and that phase images be saved and exported in Digital Imaging and Communications in Medicine (DICOM) format and unwrapped using an exact unwrapping approach. Multi‐echo images should be combined before background field removal, and a brain mask created using a brain extraction tool with the incorporation of phase‐quality‐based masking. Background fields within the brain mask should be removed using a technique based on SHARP or PDF, and the optimization approach to dipole inversion should be employed with a sparsity‐based regularization. Susceptibility values should be measured relative to a specified reference, including the common reference region of the whole brain as a region of interest in the analysis. The minimum acquisition and processing details required when reporting QSM results are also provided. These recommendations should facilitate clinical QSM research and promote harmonized data acquisition, analysis, and reporting.",No methods found.
2024,https://openalex.org/W4396571213,Medicine,"Comprehensive review of emerging contaminants: Detection technologies, environmental impact, and management strategies","Emerging contaminants (ECs) are a diverse group of unregulated pollutants increasingly present in the environment. These contaminants, including pharmaceuticals, personal care products, endocrine disruptors, and industrial chemicals, can enter the environment through various pathways and persist, accumulating in the food chain and posing risks to ecosystems and human health. This comprehensive review examines the chemical characteristics, sources, and varieties of ECs. It critically evaluates the current understanding of their environmental and health impacts, highlighting recent advancements and challenges in detection and analysis. The review also assesses existing regulations and policies, identifying shortcomings and proposing potential enhancements. ECs pose significant risks to wildlife and ecosystems by disrupting animal hormones, causing genetic alterations that diminish diversity and resilience, and altering soil nutrient dynamics and the physical environment. Furthermore, ECs present increasing risks to human health, including hormonal disruptions, antibiotic resistance, endocrine disruption, neurological effects, carcinogenic effects, and other long-term impacts. To address these critical issues, the review offers recommendations for future research, emphasizing areas requiring further investigation to comprehend the full implications of these contaminants. It also suggests increased funding and support for research, development of advanced detection technologies, establishment of standardized methods, adoption of precautionary regulations, enhanced public awareness and education, cross-sectoral collaboration, and integration of scientific research into policy-making. By implementing these solutions, we can improve our ability to detect, monitor, and manage ECs, reducing environmental and public health risks.",No methods found.
2024,https://openalex.org/W4391572037,Medicine,Machine Learning Applications in Healthcare: Current Trends and Future Prospects,"The integration of machine learning (ML) in healthcare has witnessed remarkable advancements, transforming the landscape of medical diagnosis, treatment, and overall patient care. This article provides a comprehensive review of the current trends and future prospects of machine learning applications in the healthcare domain.The current landscape is characterized by the utilization of ML algorithms for disease diagnosis and risk prediction, personalized treatment plans, and efficient healthcare resource management. Notable applications include image recognition for radiology and pathology, predictive analytics for disease prognosis, and the development of precision medicine tailored to individual patient profiles.This review explores the evolving role of ML in improving patient outcomes, enhancing clinical decision-making, and optimizing healthcare workflows. It delves into the challenges faced in integrating ML into existing healthcare systems, such as data privacy concerns, interpretability of complex models, and the need for robust validation processes.Additionally, the article discusses future prospects and emerging trends in ML healthcare applications, including the potential for predictive analytics to preemptively identify health issues, the integration of wearable devices and remote monitoring for continuous patient care, and the intersection of ML with genomics for personalized medicine.The overarching goal of this article is to provide healthcare professionals, researchers, and policymakers with insights into the current state of ML applications in healthcare, along with an outlook on the transformative potential that machine learning holds for the future of healthcare delivery and patient outcomes.","<method>machine learning (ML) algorithms</method>, <method>image recognition</method>, <method>predictive analytics</method>"
2024,https://openalex.org/W4399885365,Medicine,Convolutional neural network classification of cancer cytopathology images: taking breast cancer as an example,"Breast cancer is a relatively common cancer among gynecological cancers. Its diagnosis often relies on the pathology of cells in the lesion. The pathological diagnosis of breast cancer not only requires professionals and time, but also sometimes involves subjective judgment. To address the challenges of dependence on pathologists expertise and the time-consuming nature of achieving accurate breast pathological image classification, this paper introduces an approach utilizing convolutional neural networks (CNNs) for the rapid categorization of pathological images, aiming to enhance the efficiency of breast pathological image detection. And the approach enables the rapid and automatic classification of pathological images into benign and malignant groups. The methodology involves utilizing a convolutional neural network (CNN) model leveraging the Inceptionv3 architecture and transfer learning algorithm for extracting features from pathological images. Utilizing a neural network with fully connected layers and employing the SoftMax function for image classification. Additionally, the concept of image partitioning is introduced to handle high-resolution images. To achieve the ultimate classification outcome, the classification probabilities of each image block are aggregated using three algorithms: summation, product, and maximum. Experimental validation was conducted on the BreaKHis public dataset, resulting in accuracy rates surpassing 0.92 across all four magnification coefficients (40X, 100X, 200X, and 400X). It demonstrates that the proposed method effectively enhances the accuracy in classifying pathological images of breast cancer.","<method>convolutional neural networks (CNNs)</method>, <method>Inceptionv3 architecture</method>, <method>transfer learning algorithm</method>, <method>neural network with fully connected layers</method>, <method>SoftMax function for image classification</method>, <method>image partitioning</method>, <method>aggregation algorithms: summation, product, and maximum</method>"
2024,https://openalex.org/W4395050972,Medicine,Optimization of hepatological clinical guidelines interpretation by large language models: a retrieval augmented generation-based framework,"Abstract Large language models (LLMs) can potentially transform healthcare, particularly in providing the right information to the right provider at the right time in the hospital workflow. This study investigates the integration of LLMs into healthcare, specifically focusing on improving clinical decision support systems (CDSSs) through accurate interpretation of medical guidelines for chronic Hepatitis C Virus infection management. Utilizing OpenAI’s GPT-4 Turbo model, we developed a customized LLM framework that incorporates retrieval augmented generation (RAG) and prompt engineering. Our framework involved guideline conversion into the best-structured format that can be efficiently processed by LLMs to provide the most accurate output. An ablation study was conducted to evaluate the impact of different formatting and learning strategies on the LLM’s answer generation accuracy. The baseline GPT-4 Turbo model’s performance was compared against five experimental setups with increasing levels of complexity: inclusion of in-context guidelines, guideline reformatting, and implementation of few-shot learning. Our primary outcome was the qualitative assessment of accuracy based on expert review, while secondary outcomes included the quantitative measurement of similarity of LLM-generated responses to expert-provided answers using text-similarity scores. The results showed a significant improvement in accuracy from 43 to 99% ( p &lt; 0.001), when guidelines were provided as context in a coherent corpus of text and non-text sources were converted into text. In addition, few-shot learning did not seem to improve overall accuracy. The study highlights that structured guideline reformatting and advanced prompt engineering (data quality vs. data quantity) can enhance the efficacy of LLM integrations to CDSSs for guideline delivery.","<method>Large language models (LLMs)</method>, <method>OpenAI’s GPT-4 Turbo model</method>, <method>retrieval augmented generation (RAG)</method>, <method>prompt engineering</method>, <method>few-shot learning</method>"
2024,https://openalex.org/W4396696202,Medicine,The Reporting of a Disproportionality Analysis for Drug Safety Signal Detection Using Individual Case Safety Reports in PharmacoVigilance (READUS-PV): Development and Statement,"Disproportionality analyses using reports of suspected adverse drug reactions are the most commonly used quantitative methods for detecting safety signals in pharmacovigilance. However, their methods and results are generally poorly reported in published articles and existing guidelines do not capture the specific features of disproportionality analyses. We here describe the development of a guideline (REporting of A Disproportionality analysis for drUg Safety signal detection using individual case safety reports in PharmacoVigilance [READUS-PV]) for reporting the results of disproportionality analyses in articles and abstracts. We established a group of 34 international experts from universities, the pharmaceutical industry, and regulatory agencies, with expertise in pharmacovigilance, disproportionality analyses, and assessment of safety signals. We followed a three-step process to develop the checklist: (1) an open-text survey to generate a first list of items; (2) an online Delphi method to select and rephrase the most important items; (3) a final online consensus meeting. Among the panel members, 33 experts responded to round 1 and 30 to round 2 of the Delphi and 25 participated to the consensus meeting. Overall, 60 recommendations for the main body of the manuscript and 13 recommendations for the abstracts were retained by participants after the Delphi method. After merging of some items together and the online consensus meeting, the READUS-PV guidelines comprise a checklist of 32 recommendations, in 14 items, for the reporting of disproportionality analyses in the main body text and four items, comprising 12 recommendations, for abstracts. The READUS-PV guidelines will support authors, editors, peer-reviewers, and users of disproportionality analyses using individual case safety report databases. Adopting these guidelines will lead to more transparent, comprehensive, and accurate reporting and interpretation of disproportionality analyses, facilitating the integration with other sources of evidence.",No methods found.
2024,https://openalex.org/W4393247259,Medicine,Employing deep learning and transfer learning for accurate brain tumor detection,"Abstract Artificial intelligence-powered deep learning methods are being used to diagnose brain tumors with high accuracy, owing to their ability to process large amounts of data. Magnetic resonance imaging stands as the gold standard for brain tumor diagnosis using machine vision, surpassing computed tomography, ultrasound, and X-ray imaging in its effectiveness. Despite this, brain tumor diagnosis remains a challenging endeavour due to the intricate structure of the brain. This study delves into the potential of deep transfer learning architectures to elevate the accuracy of brain tumor diagnosis. Transfer learning is a machine learning technique that allows us to repurpose pre-trained models on new tasks. This can be particularly useful for medical imaging tasks, where labelled data is often scarce. Four distinct transfer learning architectures were assessed in this study: ResNet152, VGG19, DenseNet169, and MobileNetv3. The models were trained and validated on a dataset from benchmark database: Kaggle. Five-fold cross validation was adopted for training and testing. To enhance the balance of the dataset and improve the performance of the models, image enhancement techniques were applied to the data for the four categories: pituitary, normal, meningioma, and glioma. MobileNetv3 achieved the highest accuracy of 99.75%, significantly outperforming other existing methods. This demonstrates the potential of deep transfer learning architectures to revolutionize the field of brain tumor diagnosis.","<method>deep learning</method>, <method>machine vision</method>, <method>deep transfer learning architectures</method>, <method>transfer learning</method>, <method>ResNet152</method>, <method>VGG19</method>, <method>DenseNet169</method>, <method>MobileNetv3</method>, <method>five-fold cross validation</method>"
2024,https://openalex.org/W4391527655,Medicine,"Micro(nano)plastics in the Human Body: Sources, Occurrences, Fates, and Health Risks","The increasing global attention on micro(nano)plastics (MNPs) is a result of their ubiquity in the water, air, soil, and biosphere, exposing humans to MNPs on a daily basis and threatening human health. However, crucial data on MNPs in the human body, including the sources, occurrences, behaviors, and health risks, are limited, which greatly impedes any systematic assessment of their impact on the human body. To further understand the effects of MNPs on the human body, we must identify existing knowledge gaps that need to be immediately addressed and provide potential solutions to these issues. Herein, we examined the current literature on the sources, occurrences, and behaviors of MNPs in the human body as well as their potential health risks. Furthermore, we identified key knowledge gaps that must be resolved to comprehensively assess the effects of MNPs on human health. Additionally, we addressed that the complexity of MNPs and the lack of efficient analytical methods are the main barriers impeding current investigations on MNPs in the human body, necessitating the development of a standard and unified analytical method. Finally, we highlighted the need for interdisciplinary studies from environmental, biological, medical, chemical, computer, and material scientists to fill these knowledge gaps and drive further research. Considering the inevitability and daily occurrence of human exposure to MNPs, more studies are urgently required to enhance our understanding of their potential negative effects on human health.",No methods found.
2024,https://openalex.org/W4398766373,Medicine,PI-QUAL version 2: an update of a standardised scoring system for the assessment of image quality of prostate MRI,"Abstract Multiparametric MRI is the optimal primary investigation when prostate cancer is suspected, and its ability to rule in and rule out clinically significant disease relies on high-quality anatomical and functional images. Avenues for achieving consistent high-quality acquisitions include meticulous patient preparation, scanner setup, optimised pulse sequences, personnel training, and artificial intelligence systems. The impact of these interventions on the final images needs to be quantified. The prostate imaging quality (PI-QUAL) scoring system was the first standardised quantification method that demonstrated the potential for clinical benefit by relating image quality to cancer detection ability by MRI. We present the updated version of PI-QUAL (PI-QUAL v2) which applies to prostate MRI performed with or without intravenous contrast medium using a simplified 3-point scale focused on critical technical and qualitative image parameters. Clinical relevance statement High image quality is crucial for prostate MRI, and the updated version of the PI-QUAL score (PI-QUAL v2) aims to address the limitations of version 1. It is now applicable to both multiparametric MRI and MRI without intravenous contrast medium. Key Points High-quality images are essential for prostate cancer diagnosis and management using MRI . PI-QUAL v2 simplifies image assessment and expands its applicability to prostate MRI without contrast medium . PI-QUAL v2 focuses on critical technical and qualitative image parameters and emphasises T2-WI and DWI .",No methods found.
2024,https://openalex.org/W4390880481,Medicine,Liquid-metal-based three-dimensional microelectrode arrays integrated with implantable ultrathin retinal prosthesis for vision restoration,"Abstract Electronic retinal prostheses for stimulating retinal neurons are promising for vision restoration. However, the rigid electrodes of conventional retinal implants can inflict damage on the soft retina tissue. They also have limited selectivity due to their poor proximity to target cells in the degenerative retina. Here we present a soft artificial retina (thickness, 10 μm) where flexible ultrathin photosensitive transistors are integrated with three-dimensional stimulation electrodes of eutectic gallium–indium alloy. Platinum nanoclusters locally coated only on the tip of these three-dimensional liquid-metal electrodes show advantages in reducing the impedance of the stimulation electrodes. These microelectrodes can enhance the proximity to the target retinal ganglion cells and provide effective charge injections (72.84 mC cm −2 ) to elicit neural responses in the retina. Their low Young’s modulus (234 kPa), owing to their liquid form, can minimize damage to the retina. Furthermore, we used an unsupervised machine learning approach to effectively identify the evoked spikes to grade neural activities within the retinal ganglion cells. Results from in vivo experiments on a retinal degeneration mouse model reveal that the spatiotemporal distribution of neural responses on their retina can be mapped under selective localized illumination areas of light, suggesting the restoration of their vision.",<method>unsupervised machine learning</method>
2024,https://openalex.org/W4390584313,Medicine,A Conceptual Model for Inclusive Technology: Advancing Disability Inclusion through Artificial Intelligence,"Artificial intelligence (AI) has ushered in transformative changes, championing inclusion and accessibility for individuals with disabilities. This article delves into the remarkable AI-driven solutions that have revolutionized their lives across various domains. From assistive technologies such as voice recognition and AI-powered smart glasses catering to diverse needs, to healthcare benefiting from early disease detection algorithms and wearable devices that monitor vital signs and alert caregivers in emergencies, AI has steered in significant enhancements. Moreover, AI-driven prosthetics and exoskeletons have substantially improved mobility for those with limb impairments. The realm of education has not been left untouched, with AI tools creating inclusive learning environments that adapt to individual learning styles, paving the way for academic success among students with disabilities. However, the boundless potential of AI also presents ethical concerns and challenges. Issues like safeguarding data privacy, mitigating algorithmic bias, and bridging the digital divide must be thoughtfully addressed to fully harness AI’s potential in empowering individuals with disabilities. To complement these achievements, a robust conceptual model for AI disability inclusion serves as the theoretical framework, guiding the development of tailored AI solutions. By striking a harmonious balance between innovation and ethics, AI has the power to significantly enhance the overall quality of life for individuals with disabilities across a spectrum of vital areas.","<method>voice recognition</method>, <method>early disease detection algorithms</method>"
2024,https://openalex.org/W4391103530,Medicine,Transformative Breast Cancer Diagnosis using CNNs with Optimized ReduceLROnPlateau and Early Stopping Enhancements,"Abstract Breast cancer stands as a paramount public health concern worldwide, underscoring an imperative necessity within the research sphere for precision-driven and efficacious methodologies facilitating accurate detection. The existing diagnostic approaches in breast cancer often suffer from limitations in accuracy and efficiency, leading to delayed detection and subsequent challenges in personalized treatment planning. The primary focus of this research is to overcome these shortcomings by harnessing the power of advanced deep learning techniques, thereby revolutionizing the precision and reliability of breast cancer classification. This research addresses the critical need for improved breast cancer diagnostics by introducing a novel Convolutional Neural Network (CNN) model integrated with an Early Stopping callback and ReduceLROnPlateau callback. By enhancing the precision and reliability of breast cancer classification, the study aims to overcome the limitations of existing diagnostic methods, ultimately leading to better patient outcomes and reduced mortality rates. The comprehensive methodology includes diverse datasets, meticulous image preprocessing, robust model training, and validation strategies, emphasizing the model's adaptability and reliability in varied clinical contexts. The findings showcase the CNN model's exceptional performance, achieving a 95.2% accuracy rate in distinguishing cancerous and non-cancerous breast tissue in the integrated dataset, thereby demonstrating its potential for enhancing clinical decision-making and fostering the development of AI-driven diagnostic solutions.","<method>Convolutional Neural Network (CNN)</method>, <method>Early Stopping callback</method>, <method>ReduceLROnPlateau callback</method>"
2024,https://openalex.org/W4391528827,Medicine,Deep learning-aided decision support for diagnosis of skin disease across skin tones,"Abstract Although advances in deep learning systems for image-based medical diagnosis demonstrate their potential to augment clinical decision-making, the effectiveness of physician–machine partnerships remains an open question, in part because physicians and algorithms are both susceptible to systematic errors, especially for diagnosis of underrepresented populations. Here we present results from a large-scale digital experiment involving board-certified dermatologists ( n = 389) and primary-care physicians ( n = 459) from 39 countries to evaluate the accuracy of diagnoses submitted by physicians in a store-and-forward teledermatology simulation. In this experiment, physicians were presented with 364 images spanning 46 skin diseases and asked to submit up to four differential diagnoses. Specialists and generalists achieved diagnostic accuracies of 38% and 19%, respectively, but both specialists and generalists were four percentage points less accurate for the diagnosis of images of dark skin as compared to light skin. Fair deep learning system decision support improved the diagnostic accuracy of both specialists and generalists by more than 33%, but exacerbated the gap in the diagnostic accuracy of generalists across skin tones. These results demonstrate that well-designed physician–machine partnerships can enhance the diagnostic accuracy of physicians, illustrating that success in improving overall diagnostic accuracy does not necessarily address bias.",<method>deep learning system decision support</method>
2024,https://openalex.org/W4394762924,Medicine,Exploring the Impact of Artificial Intelligence on Global Health and Enhancing Healthcare in Developing Nations,"Background: Artificial intelligence (AI), which combines computer science with extensive datasets, seeks to mimic human-like intelligence. Subsets of AI are being applied in almost all fields of medicine and surgery. Aim: This review focuses on the applications of AI in healthcare settings in developing countries, designed to underscore its significance by comprehensively outlining the advancements made thus far, the shortcomings encountered in AI applications, the present status of AI integration, persistent challenges, and innovative strategies to surmount them. Methodology: Articles from PubMed, Google Scholar, and Cochrane were searched from 2000 to 2023 with keywords including AI and healthcare, focusing on multiple medical specialties. Results: The increasing role of AI in diagnosis, prognosis prediction, and patient management, as well as hospital management and community healthcare, has made the overall healthcare system more efficient, especially in the high patient load setups and resource-limited areas of developing countries where patient care is often compromised. However, challenges, including low adoption rates and the absence of standardized guidelines, high installation and maintenance costs of equipment, poor transportation and connectivvity issues hinder AI’s full use in healthcare. Conclusion: Despite these challenges, AI holds a promising future in healthcare. Adequate knowledge and expertise of healthcare professionals for the use of AI technology in healthcare is imperative in developing nations.",No methods found.
2024,https://openalex.org/W4396798020,Medicine,"The triple exposure nexus of microplastic particles, plastic-associated chemicals, and environmental pollutants from a human health perspective","The presence of microplastics (MPs) is increasing at a dramatic rate globally, posing risks for exposure and subsequent potential adverse effects on human health. Apart from being physical objects, MP particles contain thousands of plastic-associated chemicals (i.e., monomers, chemical additives, and non-intentionally added substances) captured within the polymer matrix. These chemicals are often migrating from MPs and can be found in various environmental matrices and human food chains; increasing the risks for exposure and health effects. In addition to the physical and chemical attributes of MPs, plastic surfaces effectively bind exogenous chemicals, including environmental pollutants (e.g., heavy metals, persistent organic pollutants). Therefore, MPs can act as vectors of environmental pollution across air, drinking water, and food, further amplifying health risks posed by MP exposure. Critically, fragmentation of plastics in the environment increases the risk for interactions with cells, increases the presence of available surfaces to leach plastic-associated chemicals, and adsorb and transfer environmental pollutants. This review proposes the so-called triple exposure nexus approach to comprehensively map existing knowledge on interconnected health effects of MP particles, plastic-associated chemicals, and environmental pollutants. Based on the available data, there is a large knowledge gap in regard to the interactions and cumulative health effects of the triple exposure nexus. Each component of the triple nexus is known to induce genotoxicity, inflammation, and endocrine disruption, but knowledge about long-term and inter-individual health effects is lacking. Furthermore, MPs are not readily excreted from organisms after ingestion and they have been found accumulated in human blood, cardiac tissue, placenta, etc. Even though the number of studies on MPs-associated health impacts is increasing rapidly, this review underscores that there is a pressing necessity to achieve an integrated assessment of MPs' effects on human health in order to address existing and future knowledge gaps.",No methods found.
2024,https://openalex.org/W4390978664,Medicine,Twenty years of network meta‐analysis: Continuing controversies and recent developments,"Abstract Network meta‐analysis (NMA) is an extension of pairwise meta‐analysis (PMA) which combines evidence from trials on multiple treatments in connected networks. NMA delivers internally consistent estimates of relative treatment efficacy, needed for rational decision making. Over its first 20 years NMA's use has grown exponentially, with applications in both health technology assessment (HTA), primarily re‐imbursement decisions and clinical guideline development, and clinical research publications. This has been a period of transition in meta‐analysis, first from its roots in educational and social psychology, where large heterogeneous datasets could be explored to find effect modifiers, to smaller pairwise meta‐analyses in clinical medicine on average with less than six studies. This has been followed by narrowly‐focused estimation of the effects of specific treatments at specific doses in specific populations in sparse networks, where direct comparisons are unavailable or informed by only one or two studies. NMA is a powerful and well‐established technique but, in spite of the exponential increase in applications, doubts about the reliability and validity of NMA persist. Here we outline the continuing controversies, and review some recent developments. We suggest that heterogeneity should be minimized, as it poses a threat to the reliability of NMA which has not been fully appreciated, perhaps because it has not been seen as a problem in PMA. More research is needed on the extent of heterogeneity and inconsistency in datasets used for decision making, on formal methods for making recommendations based on NMA, and on the further development of multi‐level network meta‐regression.",No methods found.
2024,https://openalex.org/W4391094954,Medicine,Artificial Intelligence in Surgery: The Future is Now,"Background Clinical Artificial intelligence (AI) has reached a critical inflection point. Advances in algorithmic science and increased understanding of operational considerations in AI deployment are opening the door to widespread clinical pathway transformation. For surgery in particular, the application of machine learning algorithms in fields such as computer vision and operative robotics are poised to radically change how we screen, diagnose, risk-stratify, treat and follow-up patients, in both pre- and post-operative stages, and within operating theatres. Summary In this paper, we summarise the current landscape of existing and emerging integrations within complex surgical care pathways. We investigate effective methods for practical use of AI throughout the patient pathway, from early screening and accurate diagnosis to intraoperative robotics, post-operative monitoring and follow-up. Horizon scanning of AI technologies in surgery is used to identify novel innovations that can enhance surgical practice today, with potential for paradigm shifts across core domains of surgical practice in the future. Any AI-driven future must be built on responsible and ethical usage, reinforced by effective oversight of data governance, and of risks to patient safety in deployment. Implementation is additionally bound to considerations of usability and pathway feasibility, and the need for robust healthcare technology assessment and evidence generation. While these factors are traditionally seen as barriers to translating AI into practice, we discuss how holistic implementation practices can create a solid foundation for scaling AI across pathways. Key Messages The next decade will see rapid translation of experimental development into real-world impact. AI will require evolution of work practices, but will also enhance patient safety, enhance surgical quality outcomes, and provide significant value for surgeons and health systems. Surgical practice has always sat on a bedrock of technological innovation. For those that follow this tradition, the future of AI in surgery starts now.",<method>machine learning algorithms</method>
2024,https://openalex.org/W4391177783,Medicine,"Making food systems more resilient to food safety risks by including artificial intelligence, big data, and internet of things into food safety early warning and emerging risk identification tools","Abstract To enhance the resilience of food systems to food safety risks, it is vitally important for national authorities and international organizations to be able to identify emerging food safety risks and to provide early warning signals in a timely manner. This review provides an overview of existing and experimental applications of artificial intelligence (AI), big data, and internet of things as part of early warning and emerging risk identification tools and methods in the food safety domain. There is an ongoing rapid development of systems fed by numerous, real‐time, and diverse data with the aim of early warning and identification of emerging food safety risks. The suitability of big data and AI to support such systems is illustrated by two cases in which climate change drives the emergence of risks, namely, harmful algal blooms affecting seafood and fungal growth and mycotoxin formation in crops. Automation and machine learning are crucial for the development of future real‐time food safety risk early warning systems. Although these developments increase the feasibility and effectiveness of prospective early warning and emerging risk identification tools, their implementation may prove challenging, particularly for low‐ and middle‐income countries due to low connectivity and data availability. It is advocated to overcome these challenges by improving the capability and capacity of national authorities, as well as by enhancing their collaboration with the private sector and international organizations.","<method>artificial intelligence (AI)</method>, <method>machine learning</method>"
2024,https://openalex.org/W4402780379,Medicine,Investigating Spatial Effects through Machine Learning and Leveraging Explainable AI for Child Malnutrition in Pakistan,"While socioeconomic gradients in regional health inequalities are firmly established, the synergistic interactions between socioeconomic deprivation and climate vulnerability within convenient proximity and neighbourhood locations with health disparities remain poorly explored and thus require deep understanding within a regional context. Furthermore, disregarding the importance of spatial spillover effects and nonlinear effects of covariates on childhood stunting are inevitable in dealing with an enduring issue of regional health inequalities. The present study aims to investigate the spatial inequalities in childhood stunting at the district level in Pakistan and validate the importance of spatial lag in predicting childhood stunting. Furthermore, it examines the presence of any nonlinear relationships among the selected independent features with childhood stunting. The study utilized data related to socioeconomic features from MICS 2017–2018 and climatic data from Integrated Contextual Analysis. A multi-model approach was employed to address the research questions, which included Ordinary Least Squares Regression (OLS), various Spatial Models, Machine Learning Algorithms and Explainable Artificial Intelligence methods. Firstly, OLS was used to analyse and test the linear relationships among selected variables. Secondly, Spatial Durbin Error Model (SDEM) was used to detect and capture the impact of spatial spillover on childhood stunting. Third, XGBoost and Random Forest machine learning algorithms were employed to examine and validate the importance of the spatial lag component. Finally, EXAI methods such as SHapley were utilized to identify potential nonlinear relationships. The study found a clear pattern of spatial clustering and geographical disparities in childhood stunting, with multidimensional poverty, high climate vulnerability and early marriage worsening childhood stunting. In contrast, low climate vulnerability, high exposure to mass media and high women’s literacy were found to reduce childhood stunting. The use of machine learning algorithms, specifically XGBoost and Random Forest, highlighted the significant role played by the average value in the neighbourhood in predicting childhood stunting in nearby districts, confirming that the spatial spillover effect is not bounded by geographical boundaries. Furthermore, EXAI methods such as partial dependency plot reveal the existence of a nonlinear relationship between multidimensional poverty and childhood stunting. The study’s findings provide valuable insights into the spatial distribution of childhood stunting in Pakistan, emphasizing the importance of considering spatial effects in predicting childhood stunting. Individual and household-level factors such as exposure to mass media and women’s literacy have shown positive implications for childhood stunting. It further provides a justification for the usage of EXAI methods to draw better insights and propose customised intervention policies accordingly.","<method>Ordinary Least Squares Regression (OLS)</method>, <method>Spatial Durbin Error Model (SDEM)</method>, <method>XGBoost</method>, <method>Random Forest</method>, <method>Explainable Artificial Intelligence (EXAI) methods</method>, <method>SHapley</method>, <method>partial dependency plot</method>"
2024,https://openalex.org/W4394572337,Medicine,Methods of determining optimal cut-point of diagnostic biomarkers with application of clinical data in ROC analysis: an update review,"Abstract Introduction An important application of ROC analysis is the determination of the optimal cut-point for biomarkers in diagnostic studies. This comprehensive review provides a framework of cut-point election for biomarkers in diagnostic medicine. Methods Several methods were proposed for the selection of optional cut-points. The validity and precision of the proposed methods were discussed and the clinical application of the methods was illustrated with a practical example of clinical diagnostic data of C-reactive protein (CRP), erythrocyte sedimentation rate (ESR) and malondialdehyde (MDA) for prediction of inflammatory bowel disease (IBD) patients using the NCSS software. Results Our results in the clinical data suggested that for CRP and MDA, the calculated cut-points of the Youden index, Euclidean index, Product and Union index methods were consistent in predicting IBD patients, while for ESR, only the Euclidean and Product methods yielded similar estimates. However, the diagnostic odds ratio (DOR) method provided more extreme values for the optimal cut-point for all biomarkers analyzed. Conclusion Overall, the four methods including the Youden index, Euclidean index, Product, and IU can produce quite similar optimal cut-points for binormal pairs with the same variance. The cut-point determined with the Youden index may not agree with the other three methods in the case of skewed distributions while DOR does not produce valid informative cut-points. Therefore, more extensive Monte Carlo simulation studies are needed to investigate the conditions of test result distributions that may lead to inconsistent findings in clinical diagnostics.","<method>Youden index</method>, <method>Euclidean index</method>, <method>Product</method>, <method>Union index (IU)</method>, <method>Diagnostic odds ratio (DOR)</method>"
2024,https://openalex.org/W4391376295,Medicine,"Detection of a facemask in real-time using deep learning methods:
  Prevention of Covid 19","A health crisis is raging all over the world with the rapid transmission of the novel-coronavirus disease (Covid-19). Out of the guidelines issued by the World Health Organisation (WHO) to protect us against Covid-19, wearing a facemask is the most effective. Many countries have necessitated the wearing of face masks, but monitoring a large number of people to ensure that they are wearing masks in a crowded place is a challenging task in itself. The novel-coronavirus disease (Covid-19) has already affected our day-to-day life as well as world trade movements. By the end of April 2021, the world has recorded 144,358,956 confirmed cases of novel-coronavirus disease (Covid-19) including 3,066,113 deaths according to the world health organization (WHO). These increasing numbers motivate automated techniques for the detection of a facemask in real-time scenarios for the prevention of Covid-19. We propose a technique using deep learning that works for single and multiple people in a frame recorded via webcam in still or in motion. We have also experimented with our approach in night light. The accuracy of our model is good compared to the other approaches in the literature; ranging from 74% for multiple people in a nightlight to 99% for a single person in daylight.",<method>deep learning</method>
2024,https://openalex.org/W4391963340,Medicine,Ultrahigh-Spatial-Resolution Photon-counting Detector CT Angiography of Coronary Artery Disease for Stenosis Assessment,"Background Coronary CT angiography is a first-line test in coronary artery disease but is limited by severe calcifications. Photon-counting-detector (PCD) CT improves spatial resolution. Purpose To investigate the effect of improved spatial resolution on coronary stenosis assessment and reclassification. Materials and Methods Coronary stenoses were evaluated prospectively in a vessel phantom (in vitro) containing two stenoses (25%, 50%), and retrospectively in patients (in vivo) who underwent ultrahigh-spatial-resolution cardiac PCD CT (from July 2022 to April 2023). Images were reconstructed at standard resolution (section thickness, 0.6 mm; increment, 0.4 mm; Bv44 kernel), high spatial resolution (section thickness, 0.4 mm; increment, 0.2 mm; Bv44 kernel), and ultrahigh spatial resolution (section thickness, 0.2; increment, 0.1 mm; Bv64 kernel). Percentages of diameter stenosis (DS) were compared between reconstructions. In vitro values were compared with the manufacturer specifications of the phantom and patient results were assessed regarding effects on Coronary Artery Disease Reporting and Data System (CAD-RADS) reclassification. Results The in vivo sample included 114 patients (mean age, 68 years ± 9 [SD]; 71 male patients). In vitro percentage DS measurements were more accurate with increasing spatial resolution for both 25% and 50% stenoses (mean bias for standard resolution, high spatial resolution, and ultrahigh spatial resolution, respectively: 10.1%, 8.0%, and 2.3%;",No methods found.
2024,https://openalex.org/W4393119757,Medicine,Unmasking bias in artificial intelligence: a systematic review of bias detection and mitigation strategies in electronic health record-based models,"Abstract Objectives Leveraging artificial intelligence (AI) in conjunction with electronic health records (EHRs) holds transformative potential to improve healthcare. However, addressing bias in AI, which risks worsening healthcare disparities, cannot be overlooked. This study reviews methods to handle various biases in AI models developed using EHR data. Materials and Methods We conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and Meta-analyses guidelines, analyzing articles from PubMed, Web of Science, and IEEE published between January 01, 2010 and December 17, 2023. The review identified key biases, outlined strategies for detecting and mitigating bias throughout the AI model development, and analyzed metrics for bias assessment. Results Of the 450 articles retrieved, 20 met our criteria, revealing 6 major bias types: algorithmic, confounding, implicit, measurement, selection, and temporal. The AI models were primarily developed for predictive tasks, yet none have been deployed in real-world healthcare settings. Five studies concentrated on the detection of implicit and algorithmic biases employing fairness metrics like statistical parity, equal opportunity, and predictive equity. Fifteen studies proposed strategies for mitigating biases, especially targeting implicit and selection biases. These strategies, evaluated through both performance and fairness metrics, predominantly involved data collection and preprocessing techniques like resampling and reweighting. Discussion This review highlights evolving strategies to mitigate bias in EHR-based AI models, emphasizing the urgent need for both standardized and detailed reporting of the methodologies and systematic real-world testing and evaluation. Such measures are essential for gauging models’ practical impact and fostering ethical AI that ensures fairness and equity in healthcare.","<method>resampling</method>, <method>reweighting</method>"
2024,https://openalex.org/W4393941607,Medicine,Large language models as assistance for glaucoma surgical cases: a ChatGPT vs. Google Gemini comparison,"Abstract Purpose The aim of this study was to define the capability of ChatGPT-4 and Google Gemini in analyzing detailed glaucoma case descriptions and suggesting an accurate surgical plan. Methods Retrospective analysis of 60 medical records of surgical glaucoma was divided into “ordinary” ( n = 40) and “challenging” ( n = 20) scenarios. Case descriptions were entered into ChatGPT and Bard’s interfaces with the question “What kind of surgery would you perform?” and repeated three times to analyze the answers’ consistency. After collecting the answers, we assessed the level of agreement with the unified opinion of three glaucoma surgeons. Moreover, we graded the quality of the responses with scores from 1 (poor quality) to 5 (excellent quality), according to the Global Quality Score (GQS) and compared the results. Results ChatGPT surgical choice was consistent with those of glaucoma specialists in 35/60 cases (58%), compared to 19/60 (32%) of Gemini ( p = 0.0001). Gemini was not able to complete the task in 16 cases (27%). Trabeculectomy was the most frequent choice for both chatbots (53% and 50% for ChatGPT and Gemini, respectively). In “challenging” cases, ChatGPT agreed with specialists in 9/20 choices (45%), outperforming Google Gemini performances (4/20, 20%). Overall, GQS scores were 3.5 ± 1.2 and 2.1 ± 1.5 for ChatGPT and Gemini ( p = 0.002). This difference was even more marked if focusing only on “challenging” cases (1.5 ± 1.4 vs. 3.0 ± 1.5, p = 0.001). Conclusion ChatGPT-4 showed a good analysis performance for glaucoma surgical cases, either ordinary or challenging. On the other side, Google Gemini showed strong limitations in this setting, presenting high rates of unprecise or missed answers.","<method>ChatGPT-4</method>, <method>Google Gemini</method>"
2024,https://openalex.org/W4396610906,Medicine,Tribulations and future opportunities for artificial intelligence in precision medicine,"Abstract Upon a diagnosis, the clinical team faces two main questions: what treatment, and at what dose? Clinical trials' results provide the basis for guidance and support for official protocols that clinicians use to base their decisions. However, individuals do not consistently demonstrate the reported response from relevant clinical trials. The decision complexity increases with combination treatments where drugs administered together can interact with each other, which is often the case. Additionally, the individual's response to the treatment varies with the changes in their condition. In practice, the drug and the dose selection depend significantly on the medical protocol and the medical team's experience. As such, the results are inherently varied and often suboptimal. Big data and Artificial Intelligence (AI) approaches have emerged as excellent decision-making tools, but multiple challenges limit their application. AI is a rapidly evolving and dynamic field with the potential to revolutionize various aspects of human life. AI has become increasingly crucial in drug discovery and development. AI enhances decision-making across different disciplines, such as medicinal chemistry, molecular and cell biology, pharmacology, pathology, and clinical practice. In addition to these, AI contributes to patient population selection and stratification. The need for AI in healthcare is evident as it aids in enhancing data accuracy and ensuring the quality care necessary for effective patient treatment. AI is pivotal in improving success rates in clinical practice. The increasing significance of AI in drug discovery, development, and clinical trials is underscored by many scientific publications. Despite the numerous advantages of AI, such as enhancing and advancing Precision Medicine (PM) and remote patient monitoring, unlocking its full potential in healthcare requires addressing fundamental concerns. These concerns include data quality, the lack of well-annotated large datasets, data privacy and safety issues, biases in AI algorithms, legal and ethical challenges, and obstacles related to cost and implementation. Nevertheless, integrating AI in clinical medicine will improve diagnostic accuracy and treatment outcomes, contribute to more efficient healthcare delivery, reduce costs, and facilitate better patient experiences, making healthcare more sustainable. This article reviews AI applications in drug development and clinical practice, making healthcare more sustainable, and highlights concerns and limitations in applying AI.",No methods found.
2024,https://openalex.org/W4391176418,Medicine,Myocardial strain of the left ventricle by speckle tracking echocardiography: From physics to clinical practice,"Abstract Speckle tracking echocardiography (STE) is a reliable imaging technique of recognized clinical value in several settings. This method uses the motion of ultrasound backscatter speckles within echocardiographic images to derive myocardial velocities and deformation parameters, providing crucial insights on several cardiac pathological and physiological processes. Its feasibility, reproducibility, and accuracy have been widely demonstrated, being myocardial strain of the various chambers inserted in diagnostic algorithms and guidelines for various pathologies. The most important parameters are Global longitudinal strain (GLS), Left atrium (LA) reservoir strain, and Global Work Index (GWI): based on large studies the average of the lower limit of normality are −16%, 23%, and 1442 mmHg%, respectively. For GWI, it should be pointed out that myocardial work relies primarily on non‐invasive measurements of blood pressure and segmental strain, both of which exhibit high variability, and thus, this variability constitutes a significant limitation of this parameter. In this review, we describe the principal aspects of the theory behind the use of myocardial strain, from cardiac mechanics to image acquisition techniques, outlining its limitation, and its principal clinical applications: in particular, GLS have a role in determine subclinical myocardial dysfunction (in cardiomyopathies, cardiotoxicity, target organ damage in ambulatory patients with arterial hypertension) and LA strain in determine the risk of AF, specifically in ambulatory patients with arterial hypertension.",No methods found.
2024,https://openalex.org/W4391235397,Medicine,Real-Time Plant Disease Dataset Development and Detection of Plant Disease Using Deep Learning,"Agriculture plays a significant role in meeting food needs and providing food security for the increasingly growing global population, which has increased by 0.88% since 2022. Plant diseases can reduce food production and affect food security. Worldwide crop loss due to plant disease is estimated to be around 14.1%. The lack of proper identification of plant disease at the early stages of infection can result in inappropriate disease control measures. Therefore, the automatic identification and diagnosis of plant diseases are highly recommended. Lack of availability of large amounts of data that are not processed to a large extent is one of the main challenges in plant disease diagnosis. In the current manuscript, we developed datasets for food grains specifically for rice, wheat, and maize to address the identified challenges. The developed datasets consider the common diseases (two bacterial diseases and two fungal diseases of rice, four fungal diseases of maize, and four fungal diseases of wheat) that affect crop yields and cause damage to the whole plant. The datasets developed were applied to eight fine-tuned deep learning models with the same training hyperparameters. The experimental results based on eight fine-tuned deep learning models show that, while recognizing maize leaf diseases, the models Xception and MobileNet performed best with a testing accuracy of 0.9580 and 0.9464 respectively. Similarly, while recognizing the wheat leaf diseases, the models MobileNetV2 and MobileNet performed best with a testing accuracy of 0.9632 and 0.9628 respectively. The Xception and Inception V3 models performed best, with a testing accuracy of 0.9728 and 0.9620, respectively, for recognizing rice leaf diseases. The research also proposes a new convolutional neural network (CNN) model trained from scratch on all three food grain datasets developed. The proposed model performs well and shows a testing accuracy of 0.9704, 0.9706, and 0.9808 respectively on the maize, rice, and wheat datasets.","<method>fine-tuned deep learning models</method>, <method>Xception</method>, <method>MobileNet</method>, <method>MobileNetV2</method>, <method>Inception V3</method>, <method>convolutional neural network (CNN) model trained from scratch</method>"
2024,https://openalex.org/W4396494945,Medicine,Vision–language foundation model for echocardiogram interpretation,"Abstract The development of robust artificial intelligence models for echocardiography has been limited by the availability of annotated clinical data. Here, to address this challenge and improve the performance of cardiac imaging models, we developed EchoCLIP, a vision–language foundation model for echocardiography, that learns the relationship between cardiac ultrasound images and the interpretations of expert cardiologists across a wide range of patients and indications for imaging. After training on 1,032,975 cardiac ultrasound videos and corresponding expert text, EchoCLIP performs well on a diverse range of benchmarks for cardiac image interpretation, despite not having been explicitly trained for individual interpretation tasks. EchoCLIP can assess cardiac function (mean absolute error of 7.1% when predicting left ventricular ejection fraction in an external validation dataset) and identify implanted intracardiac devices (area under the curve (AUC) of 0.84, 0.92 and 0.97 for pacemakers, percutaneous mitral valve repair and artificial aortic valves, respectively). We also developed a long-context variant (EchoCLIP-R) using a custom tokenizer based on common echocardiography concepts. EchoCLIP-R accurately identified unique patients across multiple videos (AUC of 0.86), identified clinical transitions such as heart transplants (AUC of 0.79) and cardiac surgery (AUC 0.77) and enabled robust image-to-text search (mean cross-modal retrieval rank in the top 1% of candidate text reports). These capabilities represent a substantial step toward understanding and applying foundation models in cardiovascular imaging for preliminary interpretation of echocardiographic findings.","<method>vision–language foundation model</method>, <method>EchoCLIP</method>, <method>long-context variant (EchoCLIP-R)</method>, <method>custom tokenizer</method>"
2024,https://openalex.org/W4396723652,Medicine,"Analysis of college students' attitudes toward the use of ChatGPT in their academic activities: effect of intent to use, verification of information and responsible use","Abstract Background In recent years, the use of artificial intelligence (AI) in education has increased worldwide. The launch of the ChatGPT-3 posed great challenges for higher education, given its popularity among university students. The present study aimed to analyze the attitudes of university students toward the use of ChatGPTs in their academic activities. Method This study was oriented toward a quantitative approach and had a nonexperimental design. An online survey was administered to the 499 participants. Results The findings of this study revealed a significant association between various factors and attitudes toward the use of the ChatGPT. The higher beta coefficients for responsible use (β=0.806***), the intention to use frequently (β=0.509***), and acceptance (β=0.441***) suggested that these are the strongest predictors of a positive attitude toward ChatGPT. The presence of positive emotions (β=0.418***) also plays a significant role. Conversely, risk (β=-0.104**) and boredom (β=-0.145**) demonstrate a negative yet less decisive influence. These results provide an enhanced understanding of how students perceive and utilize ChatGPTs, supporting a unified theory of user behavior in educational technology contexts. Conclusion Ease of use, intention to use frequently, acceptance, and intention to verify information influenced the behavioral intention to use ChatGPT responsibly. On the one hand, this study provides suggestions for HEIs to improve their educational curricula to take advantage of the potential benefits of AI and contribute to AI literacy.",No methods found.
2024,https://openalex.org/W4391320803,Medicine,Oral squamous cell carcinoma detection using EfficientNet on histopathological images,"Introduction Oral Squamous Cell Carcinoma (OSCC) poses a significant challenge in oncology due to the absence of precise diagnostic tools, leading to delays in identifying the condition. Current diagnostic methods for OSCC have limitations in accuracy and efficiency, highlighting the need for more reliable approaches. This study aims to explore the discriminative potential of histopathological images of oral epithelium and OSCC. By utilizing a database containing 1224 images from 230 patients, captured at varying magnifications and publicly available, a customized deep learning model based on EfficientNetB3 was developed. The model’s objective was to differentiate between normal epithelium and OSCC tissues by employing advanced techniques such as data augmentation, regularization, and optimization. Methods The research utilized a histopathological imaging database for Oral Cancer analysis, incorporating 1224 images from 230 patients. These images, taken at various magnifications, formed the basis for training a specialized deep learning model built upon the EfficientNetB3 architecture. The model underwent training to distinguish between normal epithelium and OSCC tissues, employing sophisticated methodologies including data augmentation, regularization techniques, and optimization strategies. Results The customized deep learning model achieved significant success, showcasing a remarkable 99% accuracy when tested on the dataset. This high accuracy underscores the model’s efficacy in effectively discerning between normal epithelium and OSCC tissues. Furthermore, the model exhibited impressive precision, recall, and F1-score metrics, reinforcing its potential as a robust diagnostic tool for OSCC. Discussion This research demonstrates the promising potential of employing deep learning models to address the diagnostic challenges associated with OSCC. The model’s ability to achieve a 99% accuracy rate on the test dataset signifies a considerable leap forward in earlier and more accurate detection of OSCC. Leveraging advanced techniques in machine learning, such as data augmentation and optimization, has shown promising results in improving patient outcomes through timely and precise identification of OSCC.","<method>deep learning model based on EfficientNetB3</method>, <method>data augmentation</method>, <method>regularization</method>, <method>optimization</method>"
2024,https://openalex.org/W4391750864,Medicine,Efficacy of virtual reality-based training programs and games on the improvement of cognitive disorders in patients: a systematic review and meta-analysis,"Abstract Introduction Cognitive impairments present challenges for patients, impacting memory, attention, and problem-solving abilities. Virtual reality (VR) offers innovative ways to enhance cognitive function and well-being. This study explores the effects of VR-based training programs and games on improving cognitive disorders. Methods PubMed, Scopus, and Web of Science were systematically searched until May 20, 2023. Two researchers selected and extracted data based on inclusion and exclusion criteria, resolving disagreements through consultation with two other authors. Inclusion criteria required studies of individuals with any cognitive disorder engaged in at least one VR-based training session, reporting cognitive impairment data via scales like the MMSE. Only English-published RCTs were considered, while exclusion criteria included materials not primarily focused on the intersection of VR and cognitive disorders. The risk of bias in the included studies was assessed using the MMAT tool. Publication bias was assessed using funnel plots and Egger’s test. The collected data were utilized to calculate the standardized mean differences (Hedges’s g) between the treatment and control groups. The heterogeneity variance was estimated using the Q test and I2 statistic. The analysis was conducted using Stata version 17.0. Results Ten studies were included in the analysis out of a total of 3,157 retrieved articles. VR had a statistically significant improvement in cognitive impairments among patients (Hedges’s g = 0.42, 95% CI: 0.15, 0.68; p _value = 0.05). games (Hedges’s g = 0.61, 95% CI: 0.30, 0.39; p _value = 0.20) had a more significant impact on cognitive impairment improvement compared to cognitive training programs (Hedges’s g = 0.29, 95% CI: -0.11, 0.69; p _value = 0.24). The type of VR intervention was a significant moderator of the heterogeneity between studies. Conclusion VR-based interventions have demonstrated promise in enhancing cognitive function and addressing cognitive impairment, highlighting their potential as valuable tools in improving care for individuals with cognitive disorders. The findings underscore the relevance of incorporating virtual reality into therapeutic approaches for cognitive disorders.",No methods found.
2024,https://openalex.org/W4399387113,Medicine,ChatGPT prompts for generating multiple-choice questions in medical education and evidence on their validity: a literature review,"Abstract ChatGPT’s role in creating multiple-choice questions (MCQs) is growing but the validity of these artificial-intelligence-generated questions is unclear. This literature review was conducted to address the urgent need for understanding the application of ChatGPT in generating MCQs for medical education. Following the database search and screening of 1920 studies, we found 23 relevant studies. We extracted the prompts for MCQ generation and assessed the validity evidence of MCQs. The findings showed that prompts varied, including referencing specific exam styles and adopting specific personas, which align with recommended prompt engineering tactics. The validity evidence covered various domains, showing mixed accuracy rates, with some studies indicating comparable quality to human-written questions, and others highlighting differences in difficulty and discrimination levels, alongside a significant reduction in question creation time. Despite its efficiency, we highlight the necessity of careful review and suggest a need for further research to optimize the use of ChatGPT in question generation. Main messages Ensure high-quality outputs by utilizing well-designed prompts; medical educators should prioritize the use of detailed, clear ChatGPT prompts when generating MCQs. Avoid using ChatGPT-generated MCQs directly in examinations without thorough review to prevent inaccuracies and ensure relevance. Leverage ChatGPT’s potential to streamline the test development process, enhancing efficiency without compromising quality.",No methods found.
2024,https://openalex.org/W357964437,Medicine,Women's Medical Work in Early Modern France,"For a majority of the French population during the period known as the Renaissance, most medical care would come at the hands of women. Women's medical work, like that of other providers, needs to be situated in specific historical and social contexts. This book adopts a number of methodological approaches which will help to highlight and understand women's medical practices, and may provide new ways to perceive their contribution to the history of medicine more generally. It focuses on women because, as practitioners, they cut across most sectors of medical practice. The book is structured in such a way as to demonstrate how different contexts and communities responded to women's medical work in varied and sometimes contrasting ways. It explores religious understandings of female healing work as lay and religious women. The book presents the study of women's domestic and charitable medical labour, by exploring the impact of print in the context of women as readers and patrons of medical literature, with a focus on the publication of manuals contributing to the domestic care discourse. It examines the role of women in the municipally organised systems of poor relief and child care for foundlings and orphans. The book also follows women's gynaecological and reproductive knowledge, particularly in the contexts of elite and royal court life.",No methods found.
2024,https://openalex.org/W4392017487,Medicine,"Digital twin: Data exploration, architecture, implementation and future","A Digital Twin (DT) is a digital copy or virtual representation of an object, process, service, or system in the real world. It was first introduced to the world by the National Aeronautics and Space Administration (NASA) through its Apollo Mission in the '60s. It can successfully design a virtual object from its physical counterpart. However, the main function of a digital twin system is to provide a bidirectional data flow between the physical and the virtual entity so that it can continuously upgrade the physical counterpart. It is a state-of-the-art iterative method for creating an autonomous system. Data is the brain or building block of any digital twin system. The articles that are found online cover an individual field or two at a time regarding data analysis technology. There are no overall studies found regarding this manner online. The purpose of this study is to provide an overview of the data level in the digital twin system, and it involves the data at various phases. This paper will provide a comparative study among all the fields in which digital twins have been applied in recent years. Digital twin works with a vast amount of data, which needs to be organized, stored, linked, and put together, which is also a motive of our study. Data is essential for building virtual models, making cyber-physical connections, and running intelligent operations. The current development status and the challenges present in the different phases of digital twin data analysis have been discussed. This paper also outlines how DT is used in different fields, like manufacturing, urban planning, agriculture, medicine, robotics, and the military/aviation industry, and shows a data structure based on every sector using recent review papers. Finally, we attempted to give a horizontal comparison based on the features of the data across various fields, to extract the commonalities and uniqueness of the data in different sectors, and to shed light on the challenges at the current level as well as the limitations and future of DT from a data standpoint.",No methods found.
2024,https://openalex.org/W4392450439,Medicine,Dr. Google to Dr. ChatGPT: assessing the content and quality of artificial intelligence-generated medical information on appendicitis,"Abstract Introduction Generative artificial intelligence (AI) chatbots have recently been posited as potential sources of online medical information for patients making medical decisions. Existing online patient-oriented medical information has repeatedly been shown to be of variable quality and difficult readability. Therefore, we sought to evaluate the content and quality of AI-generated medical information on acute appendicitis. Methods A modified DISCERN assessment tool, comprising 16 distinct criteria each scored on a 5-point Likert scale (score range 16–80), was used to assess AI-generated content. Readability was determined using the Flesch Reading Ease (FRE) and Flesch-Kincaid Grade Level (FKGL) scores. Four popular chatbots, ChatGPT-3.5 and ChatGPT-4, Bard, and Claude-2, were prompted to generate medical information about appendicitis. Three investigators independently scored the generated texts blinded to the identity of the AI platforms. Results ChatGPT-3.5, ChatGPT-4, Bard, and Claude-2 had overall mean (SD) quality scores of 60.7 (1.2), 62.0 (1.0), 62.3 (1.2), and 51.3 (2.3), respectively, on a scale of 16–80. Inter-rater reliability was 0.81, 0.75, 0.81, and 0.72, respectively, indicating substantial agreement. Claude-2 demonstrated a significantly lower mean quality score compared to ChatGPT-4 ( p = 0.001), ChatGPT-3.5 ( p = 0.005), and Bard ( p = 0.001). Bard was the only AI platform that listed verifiable sources, while Claude-2 provided fabricated sources. All chatbots except for Claude-2 advised readers to consult a physician if experiencing symptoms. Regarding readability, FKGL and FRE scores of ChatGPT-3.5, ChatGPT-4, Bard, and Claude-2 were 14.6 and 23.8, 11.9 and 33.9, 8.6 and 52.8, 11.0 and 36.6, respectively, indicating difficulty readability at a college reading skill level. Conclusion AI-generated medical information on appendicitis scored favorably upon quality assessment, but most either fabricated sources or did not provide any altogether. Additionally, overall readability far exceeded recommended levels for the public. Generative AI platforms demonstrate measured potential for patient education and engagement about appendicitis.",<method>Generative artificial intelligence (AI) chatbots</method>
2024,https://openalex.org/W4394953197,Medicine,"A scoping review of continuous quality improvement in healthcare system: conceptualization, models and tools, barriers and facilitators, and impact","Abstract Background The growing adoption of continuous quality improvement (CQI) initiatives in healthcare has generated a surge in research interest to gain a deeper understanding of CQI. However, comprehensive evidence regarding the diverse facets of CQI in healthcare has been limited. Our review sought to comprehensively grasp the conceptualization and principles of CQI, explore existing models and tools, analyze barriers and facilitators, and investigate its overall impacts. Methods This qualitative scoping review was conducted using Arksey and O’Malley’s methodological framework. We searched articles in PubMed, Web of Science, Scopus, and EMBASE databases. In addition, we accessed articles from Google Scholar. We used mixed-method analysis, including qualitative content analysis and quantitative descriptive for quantitative findings to summarize findings and PRISMA extension for scoping reviews (PRISMA-ScR) framework to report the overall works. Results A total of 87 articles, which covered 14 CQI models, were included in the review. While 19 tools were used for CQI models and initiatives, Plan-Do-Study/Check-Act cycle was the commonly employed model to understand the CQI implementation process. The main reported purposes of using CQI, as its positive impact, are to improve the structure of the health system (e.g., leadership, health workforce, health technology use, supplies, and costs), enhance healthcare delivery processes and outputs (e.g., care coordination and linkages, satisfaction, accessibility, continuity of care, safety, and efficiency), and improve treatment outcome (reduce morbidity and mortality). The implementation of CQI is not without challenges. There are cultural (i.e., resistance/reluctance to quality-focused culture and fear of blame or punishment), technical, structural (related to organizational structure, processes, and systems), and strategic (inadequate planning and inappropriate goals) related barriers that were commonly reported during the implementation of CQI. Conclusions Implementing CQI initiatives necessitates thoroughly comprehending key principles such as teamwork and timeline. To effectively address challenges, it’s crucial to identify obstacles and implement optimal interventions proactively. Healthcare professionals and leaders need to be mentally equipped and cognizant of the significant role CQI initiatives play in achieving purposes for quality of care.",No methods found.
2024,https://openalex.org/W4390665799,Medicine,Artificial intelligence for skin cancer detection and classification for clinical environment: a systematic review,"Background Skin cancer is one of the most common forms worldwide, with a significant increase in incidence over the last few decades. Early and accurate detection of this type of cancer can result in better prognoses and less invasive treatments for patients. With advances in Artificial Intelligence (AI), tools have emerged that can facilitate diagnosis and classify dermatological images, complementing traditional clinical assessments and being applicable where there is a shortage of specialists. Its adoption requires analysis of efficacy, safety, and ethical considerations, as well as considering the genetic and ethnic diversity of patients. Objective The systematic review aims to examine research on the detection, classification, and assessment of skin cancer images in clinical settings. Methods We conducted a systematic literature search on PubMed, Scopus, Embase, and Web of Science, encompassing studies published until April 4th, 2023. Study selection, data extraction, and critical appraisal were carried out by two independent reviewers. Results were subsequently presented through a narrative synthesis. Results Through the search, 760 studies were identified in four databases, from which only 18 studies were selected, focusing on developing, implementing, and validating systems to detect, diagnose, and classify skin cancer in clinical settings. This review covers descriptive analysis, data scenarios, data processing and techniques, study results and perspectives, and physician diversity, accessibility, and participation. Conclusion The application of artificial intelligence in dermatology has the potential to revolutionize early detection of skin cancer. However, it is imperative to validate and collaborate with healthcare professionals to ensure its clinical effectiveness and safety.",No methods found.
2024,https://openalex.org/W4396636942,Medicine,Artificial intelligence in digital pathology: a systematic review and meta-analysis of diagnostic test accuracy,"Abstract Ensuring diagnostic performance of artificial intelligence (AI) before introduction into clinical practice is essential. Growing numbers of studies using AI for digital pathology have been reported over recent years. The aim of this work is to examine the diagnostic accuracy of AI in digital pathology images for any disease. This systematic review and meta-analysis included diagnostic accuracy studies using any type of AI applied to whole slide images (WSIs) for any disease. The reference standard was diagnosis by histopathological assessment and/or immunohistochemistry. Searches were conducted in PubMed, EMBASE and CENTRAL in June 2022. Risk of bias and concerns of applicability were assessed using the QUADAS-2 tool. Data extraction was conducted by two investigators and meta-analysis was performed using a bivariate random effects model, with additional subgroup analyses also performed. Of 2976 identified studies, 100 were included in the review and 48 in the meta-analysis. Studies were from a range of countries, including over 152,000 whole slide images (WSIs), representing many diseases. These studies reported a mean sensitivity of 96.3% (CI 94.1–97.7) and mean specificity of 93.3% (CI 90.5–95.4). There was heterogeneity in study design and 99% of studies identified for inclusion had at least one area at high or unclear risk of bias or applicability concerns. Details on selection of cases, division of model development and validation data and raw performance data were frequently ambiguous or missing. AI is reported as having high diagnostic accuracy in the reported areas but requires more rigorous evaluation of its performance.",No methods found.
2024,https://openalex.org/W4396729544,Medicine,Reweighting UK Biobank corrects for pervasive selection bias due to volunteering,"Abstract Background Biobanks typically rely on volunteer-based sampling. This results in large samples (power) at the cost of representativeness (bias). The problem of volunteer bias is debated. Here, we (i) show that volunteering biases associations in UK Biobank (UKB) and (ii) estimate inverse probability (IP) weights that correct for volunteer bias in UKB. Methods Drawing on UK Census data, we constructed a subsample representative of UKB’s target population, which consists of all individuals invited to participate. Based on demographic variables shared between the UK Census and UKB, we estimated IP weights (IPWs) for each UKB participant. We compared 21 weighted and unweighted bivariate associations between these demographic variables to assess volunteer bias. Results Volunteer bias in all associations, as naively estimated in UKB, was substantial—in some cases so severe that unweighted estimates had the opposite sign of the association in the target population. For example, older individuals in UKB reported being in better health, in contrast to evidence from the UK Census. Using IPWs in weighted regressions reduced 87% of volunteer bias on average. Volunteer-based sampling reduced the effective sample size of UKB substantially, to 32% of its original size. Conclusions Estimates from large-scale biobanks may be misleading due to volunteer bias. We recommend IP weighting to correct for such bias. To aid in the construction of the next generation of biobanks, we provide suggestions on how to best ensure representativeness in a volunteer-based design. For UKB, IPWs have been made available.",No methods found.
2024,https://openalex.org/W4400993192,Medicine,Damage identification of steel bridge based on data augmentation and adaptive optimization neural network,"With the advancement of deep learning, data-driven structural damage identification (SDI) has shown considerable development. However, collecting vibration signals related to structural damage poses certain challenges, which can undermine the accuracy of the identification results produced by data-driven SDI methods in scenarios where data is scarce. This paper introduces an innovative approach to bridge SDI in a few-shot context by integrating an adaptive simulated annealing particle swarm optimization-convolutional neural network (ASAPSO-CNN) as the foundational framework, augmented by data enhancement techniques. Firstly, three specific types of noise are introduced to augment the source signals used for training. Subsequently, the source signals and augmented signals are recombined to construct a four-dimensional matrix as the input to the CNN, while defining the damage feature vector as the output. Secondly, a CNN is constructed to establish the mapping relationship between the input and output. Then, an adaptive fitness function is proposed that simultaneously considers the accuracy of SDI, model complexity, and training efficiency. The ASAPSO is employed to adaptively optimize the hyperparameters of the CNN. The proposed method is validated on an experimental model of a three-span continuous beam. It is compared with four other data-driven methods, demonstrating good effectiveness and robustness of SDI under cases of scarce data. Finally, the effectiveness of this SDI method is validated in a real-world case of a steel truss bridge.","<method>adaptive simulated annealing particle swarm optimization-convolutional neural network (ASAPSO-CNN)</method>, <method>data enhancement techniques</method>, <method>convolutional neural network (CNN)</method>, <method>adaptive fitness function</method>, <method>adaptive simulated annealing particle swarm optimization (ASAPSO)</method>"
2024,https://openalex.org/W4390588437,Medicine,Enhancing heart disease prediction using a self-attention-based transformer model,"Abstract Cardiovascular diseases (CVDs) continue to be the leading cause of more than 17 million mortalities worldwide. The early detection of heart failure with high accuracy is crucial for clinical trials and therapy. Patients will be categorized into various types of heart disease based on characteristics like blood pressure, cholesterol levels, heart rate, and other characteristics. With the use of an automatic system, we can provide early diagnoses for those who are prone to heart failure by analyzing their characteristics. In this work, we deploy a novel self-attention-based transformer model, that combines self-attention mechanisms and transformer networks to predict CVD risk. The self-attention layers capture contextual information and generate representations that effectively model complex patterns in the data. Self-attention mechanisms provide interpretability by giving each component of the input sequence a certain amount of attention weight. This includes adjusting the input and output layers, incorporating more layers, and modifying the attention processes to collect relevant information. This also makes it possible for physicians to comprehend which features of the data contributed to the model's predictions. The proposed model is tested on the Cleveland dataset, a benchmark dataset of the University of California Irvine (UCI) machine learning (ML) repository. Comparing the proposed model to several baseline approaches, we achieved the highest accuracy of 96.51%. Furthermore, the outcomes of our experiments demonstrate that the prediction rate of our model is higher than that of other cutting-edge approaches used for heart disease prediction.","<method>self-attention-based transformer model</method>, <method>self-attention mechanisms</method>, <method>transformer networks</method>"
2024,https://openalex.org/W4393353042,Medicine,SNC_Net: Skin Cancer Detection by Integrating Handcrafted and Deep Learning-Based Features Using Dermoscopy Images,"The medical sciences are facing a major problem with the auto-detection of disease due to the fast growth in population density. Intelligent systems assist medical professionals in early disease detection and also help to provide consistent treatment that reduces the mortality rate. Skin cancer is considered to be the deadliest and most severe kind of cancer. Medical professionals utilize dermoscopy images to make a manual diagnosis of skin cancer. This method is labor-intensive and time-consuming and demands a considerable level of expertise. Automated detection methods are necessary for the early detection of skin cancer. The occurrence of hair and air bubbles in dermoscopic images affects the diagnosis of skin cancer. This research aims to classify eight different types of skin cancer, namely actinic keratosis (AKs), dermatofibroma (DFa), melanoma (MELa), basal cell carcinoma (BCCa), squamous cell carcinoma (SCCa), melanocytic nevus (MNi), vascular lesion (VASn), and benign keratosis (BKs). In this study, we propose SNC_Net, which integrates features derived from dermoscopic images through deep learning (DL) models and handcrafted (HC) feature extraction methods with the aim of improving the performance of the classifier. A convolutional neural network (CNN) is employed for classification. Dermoscopy images from the publicly accessible ISIC 2019 dataset for skin cancer detection is utilized to train and validate the model. The performance of the proposed model is compared with four baseline models, namely EfficientNetB0 (B1), MobileNetV2 (B2), DenseNet-121 (B3), and ResNet-101 (B4), and six state-of-the-art (SOTA) classifiers. With an accuracy of 97.81%, a precision of 98.31%, a recall of 97.89%, and an F1 score of 98.10%, the proposed model outperformed the SOTA classifiers as well as the four baseline models. Moreover, an Ablation study is also performed on the proposed method to validate its performance. The proposed method therefore assists dermatologists and other medical professionals in early skin cancer detection.","<method>deep learning (DL) models</method>, <method>handcrafted (HC) feature extraction methods</method>, <method>convolutional neural network (CNN)</method>, <method>EfficientNetB0</method>, <method>MobileNetV2</method>, <method>DenseNet-121</method>, <method>ResNet-101</method>"
2024,https://openalex.org/W4400937555,Medicine,The diagnostic and triage accuracy of the GPT-3 artificial intelligence model: an observational study,"BackgroundArtificial intelligence (AI) applications in health care have been effective in many areas of medicine, but they are often trained for a single task using labelled data, making deployment and generalisability challenging. How well a general-purpose AI language model performs diagnosis and triage relative to physicians and laypeople is not well understood.MethodsWe compared the predictive accuracy of Generative Pre-trained Transformer 3 (GPT-3)'s diagnostic and triage ability for 48 validated synthetic case vignettes (<50 words; sixth-grade reading level or below) of both common (eg, viral illness) and severe (eg, heart attack) conditions to a nationally representative sample of 5000 lay people from the USA who could use the internet to find the correct options and 21 practising physicians at Harvard Medical School. There were 12 vignettes for each of four triage categories: emergent, within one day, within 1 week, and self-care. The correct diagnosis and triage category (ie, ground truth) for each vignette was determined by two general internists at Harvard Medical School. For each vignette, human respondents and GPT-3 were prompted to list diagnoses in order of likelihood, and the vignette was marked as correct if the ground-truth diagnosis was in the top three of the listed diagnoses. For triage accuracy, we examined whether the human respondents' and GPT-3's selected triage was exactly correct according to the four triage categories, or matched a dichotomised triage variable (emergent or within 1 day vs within 1 week or self-care). We estimated GPT-3's diagnostic and triage confidence on a given vignette using a modified bootstrap resampling procedure, and examined how well calibrated GPT-3's confidence was by computing calibration curves and Brier scores. We also performed subgroup analysis by case acuity, and an error analysis for triage advice to characterise how its advice might affect patients using this tool to decide if they should seek medical care immediately.FindingsAmong all cases, GPT-3 replied with the correct diagnosis in its top three for 88% (42/48, 95% CI 75–94) of cases, compared with 54% (2700/5000, 53–55) for lay individuals (p<0.0001) and 96% (637/666, 94–97) for physicians (p=0·012). GPT-3 triaged 70% correct (34/48, 57–82) versus 74% (3706/5000, 73–75; p=0.60) for lay individuals and 91% (608/666, 89–93%; p<0.0001) for physicians. As measured by the Brier score, GPT-3 confidence in its top prediction was reasonably well calibrated for diagnosis (Brier score=0·18) and triage (Brier score=0·22). We observed an inverse relationship between case acuity and GPT-3 accuracy (p<0·0001) with a fitted trend line of –8·33% decrease in accuracy for every level of increase in case acuity. For triage error analysis, GPT-3 deprioritised truly emergent cases in seven instances.InterpretationA general-purpose AI language model without any content-specific training could perform diagnosis at levels close to, but below, physicians and better than lay individuals. We found that GPT-3's performance was inferior to physicians for triage, sometimes by a large margin, and its performance was closer to that of lay individuals. Although the diagnostic performance of GPT-3 was comparable to physicians, it was significantly better than a typical person using a search engine.FundingThe National Heart, Lung, and Blood Institute.","<method>Generative Pre-trained Transformer 3 (GPT-3)</method>, <method>modified bootstrap resampling procedure</method>"
2024,https://openalex.org/W4392285688,Medicine,Machine Learning and Digital Biomarkers Can Detect Early Stages of Neurodegenerative Diseases,"Neurodegenerative diseases (NDs) such as Alzheimer’s Disease (AD) and Parkinson’s Disease (PD) are devastating conditions that can develop without noticeable symptoms, causing irreversible damage to neurons before any signs become clinically evident. NDs are a major cause of disability and mortality worldwide. Currently, there are no cures or treatments to halt their progression. Therefore, the development of early detection methods is urgently needed to delay neuronal loss as soon as possible. Despite advancements in Medtech, the early diagnosis of NDs remains a challenge at the intersection of medical, IT, and regulatory fields. Thus, this review explores “digital biomarkers” (tools designed for remote neurocognitive data collection and AI analysis) as a potential solution. The review summarizes that recent studies combining AI with digital biomarkers suggest the possibility of identifying pre-symptomatic indicators of NDs. For instance, research utilizing convolutional neural networks for eye tracking has achieved significant diagnostic accuracies. ROC-AUC scores reached up to 0.88, indicating high model performance in differentiating between PD patients and healthy controls. Similarly, advancements in facial expression analysis through tools have demonstrated significant potential in detecting emotional changes in ND patients, with some models reaching an accuracy of 0.89 and a precision of 0.85. This review follows a structured approach to article selection, starting with a comprehensive database search and culminating in a rigorous quality assessment and meaning for NDs of the different methods. The process is visualized in 10 tables with 54 parameters describing different approaches and their consequences for understanding various mechanisms in ND changes. However, these methods also face challenges related to data accuracy and privacy concerns. To address these issues, this review proposes strategies that emphasize the need for rigorous validation and rapid integration into clinical practice. Such integration could transform ND diagnostics, making early detection tools more cost-effective and globally accessible. In conclusion, this review underscores the urgent need to incorporate validated digital health tools into mainstream medical practice. This integration could indicate a new era in the early diagnosis of neurodegenerative diseases, potentially altering the trajectory of these conditions for millions worldwide. Thus, by highlighting specific and statistically significant findings, this review demonstrates the current progress in this field and the potential impact of these advancements on the global management of NDs.",<method>convolutional neural networks</method>
2024,https://openalex.org/W4400916341,Medicine,Reviewing the current state of virtual reality integration in medical education - a scoping review,"Abstract Background In medical education, new technologies like Virtual Reality (VR) are increasingly integrated to enhance digital learning. Originally used to train surgical procedures, now use cases also cover emergency scenarios and non-technical skills like clinical decision-making. This scoping review aims to provide an overview of VR in medical education, including requirements, advantages, disadvantages, as well as evaluation methods and respective study results to establish a foundation for future VR integration into medical curricula. Methods This review follows the updated JBI methodology for scoping reviews and adheres to the respective PRISMA extension. We included reviews in English or German language from 2012 to March 2022 that examine the use of VR in education for medical and nursing students, registered nurses, and qualified physicians. Data extraction focused on medical specialties, subjects, curricula, technical/didactic requirements, evaluation methods and study outcomes as well as advantages and disadvantages of VR. Results A total of 763 records were identified. After eligibility assessment, 69 studies were included. Nearly half of them were published between 2021 and 2022, predominantly from high-income countries. Most reviews focused on surgical training in laparoscopic and minimally invasive procedures (43.5%) and included studies with qualified physicians as participants (43.5%). Technical, didactic and organisational requirements were highlighted and evaluations covering performance time and quality, skills acquisition and validity, often showed positive outcomes. Accessibility, repeatability, cost-effectiveness, and improved skill development were reported as advantages, while financial challenges, technical limitations, lack of scientific evidence, and potential user discomfort were cited as disadvantages. Discussion Despite a high potential of VR in medical education, there are mandatory requirements for its integration into medical curricula addressing challenges related to finances, technical limitations, and didactic aspects. The reported lack of standardised and validated guidelines for evaluating VR training must be overcome to enable high-quality evidence for VR usage in medical education. Interdisciplinary teams of software developers, AI experts, designers, medical didactics experts and end users are required to design useful VR courses. Technical issues and compromised realism can be mitigated by further technological advancements.",No methods found.
2024,https://openalex.org/W4390607226,Medicine,RanMerFormer: Randomized vision transformer with token merging for brain tumor classification,"Brains are the control center of the nervous system in human bodies, and brain tumor is one of the most deadly diseases. Currently, magnetic resonance imaging (MRI) is the most effective way to brain tumors early detection in clinical diagnoses due to its superior imaging quality for soft tissues. Manual analysis of brain MRI is error-prone which depends on empirical experience and the fatigue state of the radiologists to a large extent. Computer-aided diagnosis (CAD) systems are becoming more and more impactful because they can provide accurate prediction results based on medical images with advanced techniques from computer vision. Therefore, a novel CAD method for brain tumor classification named RanMerFormer is presented in this paper. A pre-trained vision transformer is used as the backbone model. Then, a merging mechanism is proposed to remove the redundant tokens in the vision transformer, which improves computing efficiency substantially. Finally, a randomized vector functional-link serves as the head in the proposed RanMerFormer, which can be trained swiftly. All the simulation results are obtained from two public benchmark datasets, which reveal that the proposed RanMerFormer can achieve state-of-the-art performance for brain tumor classification. The trained RanMerFormer can be applied in real-world scenarios to assist in brain tumor diagnosis.","<method>pre-trained vision transformer</method>, <method>merging mechanism to remove redundant tokens in the vision transformer</method>, <method>randomized vector functional-link</method>"
2024,https://openalex.org/W4391023920,Medicine,A hybrid deep CNN model for brain tumor image multi-classification,"Abstract The current approach to diagnosing and classifying brain tumors relies on the histological evaluation of biopsy samples, which is invasive, time-consuming, and susceptible to manual errors. These limitations underscore the pressing need for a fully automated, deep-learning-based multi-classification system for brain malignancies. This article aims to leverage a deep convolutional neural network (CNN) to enhance early detection and presents three distinct CNN models designed for different types of classification tasks. The first CNN model achieves an impressive detection accuracy of 99.53% for brain tumors. The second CNN model, with an accuracy of 93.81%, proficiently categorizes brain tumors into five distinct types: normal, glioma, meningioma, pituitary, and metastatic. Furthermore, the third CNN model demonstrates an accuracy of 98.56% in accurately classifying brain tumors into their different grades. To ensure optimal performance, a grid search optimization approach is employed to automatically fine-tune all the relevant hyperparameters of the CNN models. The utilization of large, publicly accessible clinical datasets results in robust and reliable classification outcomes. This article conducts a comprehensive comparison of the proposed models against classical models, such as AlexNet, DenseNet121, ResNet-101, VGG-19, and GoogleNet, reaffirming the superiority of the deep CNN-based approach in advancing the field of brain tumor classification and early detection.","<method>deep convolutional neural network (CNN)</method>, <method>grid search optimization</method>, <method>AlexNet</method>, <method>DenseNet121</method>, <method>ResNet-101</method>, <method>VGG-19</method>, <method>GoogleNet</method>"
2024,https://openalex.org/W4391070180,Medicine,AI in medical diagnosis: AI prediction &amp; human judgment,"AI has long been regarded as a panacea for decision-making and many other aspects of knowledge work; as something that will help humans get rid of their shortcomings. We believe that AI can be a useful asset to support decision-makers, but not that it should replace decision-makers. Decision-making uses algorithmic analysis, but it is not solely algorithmic analysis; it also involves other factors, many of which are very human, such as creativity, intuition, emotions, feelings, and value judgments. We have conducted semi-structured open-ended research interviews with 17 dermatologists to understand what they expect from an AI application to deliver to medical diagnosis. We have found four aggregate dimensions along which the thinking of dermatologists can be described: the ways in which our participants chose to interact with AI, responsibility, 'explainability', and the new way of thinking (mindset) needed for working with AI. We believe that our findings will help physicians who might consider using AI in their diagnosis to understand how to use AI beneficially. It will also be useful for AI vendors in improving their understanding of how medics want to use AI in diagnosis. Further research will be needed to examine if our findings have relevance in the wider medical field and beyond.",No methods found.
2024,https://openalex.org/W4391341367,Medicine,Automated Tool Support for Glaucoma Identification With Explainability Using Fundus Images,"Glaucoma is a progressive eye condition that causes irreversible vision loss due to damage to the optic nerve. Recent developments in deep learning and the accessibility of computing resources have provided tool support for automated glaucoma diagnosis. Despite deep learning's advances in disease diagnosis using medical images, generic convolutional neural networks are still not widely used in medical practices due to the limited trustworthiness of these models. Although deep learning-based glaucoma classification has gained popularity in recent years, only a few of them have addressed the explainability and interpretability of the models, which increases confidence in using such applications. This study presents state-of-the-art deep learning techniques to segment and classify fundus images to predict glaucoma conditions and applies visualization techniques to explain the results to ease understandability. Our predictions are based on U-Net with attention mechanisms with ResNet50 for the segmentation process and a modified Inception V3 architecture for the classification. Attention U-Net with modified ResNet50 backbone obtained 99.58% and 98.05% accuracies for optic disc segmentation and optic cup segmentation, respectively for the RIM-ONE dataset. Additionally, we generate heatmaps that highlight the regions that impacted the glaucoma diagnosis using both Gradient-weighted Class Activation Mapping (Grad-CAM) and Grad-CAM++. Our model that classifies the segmented images achieves accuracy, sensitivity, and specificity values of 98.97%, 99.42%, and 95.59%, respectively, with the RIM-ONE dataset. This model can be used as a support tool for automated glaucoma identification using fundus images.","<method>U-Net with attention mechanisms</method>, <method>ResNet50</method>, <method>modified Inception V3 architecture</method>, <method>Attention U-Net with modified ResNet50 backbone</method>, <method>Gradient-weighted Class Activation Mapping (Grad-CAM)</method>, <method>Grad-CAM++</method>"
2024,https://openalex.org/W4391610180,Medicine,"Generative artificial intelligence in drug discovery: basic framework, recent advances, challenges, and opportunities","There are two main ways to discover or design small drug molecules. The first involves fine-tuning existing molecules or commercially successful drugs through quantitative structure-activity relationships and virtual screening. The second approach involves generating new molecules through de novo drug design or inverse quantitative structure-activity relationship. Both methods aim to get a drug molecule with the best pharmacokinetic and pharmacodynamic profiles. However, bringing a new drug to market is an expensive and time-consuming endeavor, with the average cost being estimated at around $2.5 billion. One of the biggest challenges is screening the vast number of potential drug candidates to find one that is both safe and effective. The development of artificial intelligence in recent years has been phenomenal, ushering in a revolution in many fields. The field of pharmaceutical sciences has also significantly benefited from multiple applications of artificial intelligence, especially drug discovery projects. Artificial intelligence models are finding use in molecular property prediction, molecule generation, virtual screening, synthesis planning, repurposing, among others. Lately, generative artificial intelligence has gained popularity across domains for its ability to generate entirely new data, such as images, sentences, audios, videos, novel chemical molecules, etc. Generative artificial intelligence has also delivered promising results in drug discovery and development. This review article delves into the fundamentals and framework of various generative artificial intelligence models in the context of drug discovery via de novo drug design approach. Various basic and advanced models have been discussed, along with their recent applications. The review also explores recent examples and advances in the generative artificial intelligence approach, as well as the challenges and ongoing efforts to fully harness the potential of generative artificial intelligence in generating novel drug molecules in a faster and more affordable manner. Some clinical-level assets generated form generative artificial intelligence have also been discussed in this review to show the ever-increasing application of artificial intelligence in drug discovery through commercial partnerships.","<method>quantitative structure-activity relationships</method>, <method>virtual screening</method>, <method>de novo drug design</method>, <method>inverse quantitative structure-activity relationship</method>, <method>artificial intelligence models</method>, <method>molecular property prediction</method>, <method>molecule generation</method>, <method>virtual screening</method>, <method>synthesis planning</method>, <method>repurposing</method>, <method>generative artificial intelligence</method>, <method>generative artificial intelligence models</method>"
2024,https://openalex.org/W4392703844,Medicine,Collaborative Enhancement of Consistency and Accuracy in US Diagnosis of Thyroid Nodules Using Large Language Models,"Background Large language models (LLMs) hold substantial promise for medical imaging interpretation. However, there is a lack of studies on their feasibility in handling reasoning questions associated with medical diagnosis. Purpose To investigate the viability of leveraging three publicly available LLMs to enhance consistency and diagnostic accuracy in medical imaging based on standardized reporting, with pathology as the reference standard. Materials and Methods US images of thyroid nodules with pathologic results were retrospectively collected from a tertiary referral hospital between July 2022 and December 2022 and used to evaluate malignancy diagnoses generated by three LLMs-OpenAI's ChatGPT 3.5, ChatGPT 4.0, and Google's Bard. Inter- and intra-LLM agreement of diagnosis were evaluated. Then, diagnostic performance, including accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve (AUC), was evaluated and compared for the LLMs and three interactive approaches: human reader combined with LLMs, image-to-text model combined with LLMs, and an end-to-end convolutional neural network model. Results A total of 1161 US images of thyroid nodules (498 benign, 663 malignant) from 725 patients (mean age, 42.2 years ± 14.1 [SD]; 516 women) were evaluated. ChatGPT 4.0 and Bard displayed substantial to almost perfect intra-LLM agreement (κ range, 0.65-0.86 [95% CI: 0.64, 0.86]), while ChatGPT 3.5 showed fair to substantial agreement (κ range, 0.36-0.68 [95% CI: 0.36, 0.68]). ChatGPT 4.0 had an accuracy of 78%-86% (95% CI: 76%, 88%) and sensitivity of 86%-95% (95% CI: 83%, 96%), compared with 74%-86% (95% CI: 71%, 88%) and 74%-91% (95% CI: 71%, 93%), respectively, for Bard. Moreover, with ChatGPT 4.0, the image-to-text-LLM strategy exhibited an AUC (0.83 [95% CI: 0.80, 0.85]) and accuracy (84% [95% CI: 82%, 86%]) comparable to those of the human-LLM interaction strategy with two senior readers and one junior reader and exceeding those of the human-LLM interaction strategy with one junior reader. Conclusion LLMs, particularly integrated with image-to-text approaches, show potential in enhancing diagnostic medical imaging. ChatGPT 4.0 was optimal for consistency and diagnostic accuracy when compared with Bard and ChatGPT 3.5. © RSNA, 2024","<method>OpenAI's ChatGPT 3.5</method>, <method>ChatGPT 4.0</method>, <method>Google's Bard</method>, <method>image-to-text model combined with LLMs</method>, <method>end-to-end convolutional neural network model</method>"
2024,https://openalex.org/W4393021028,Medicine,Foresight—a generative pretrained transformer for modelling of patient timelines using electronic health records: a retrospective modelling study,"BackgroundAn electronic health record (EHR) holds detailed longitudinal information about a patient's health status and general clinical history, a large portion of which is stored as unstructured, free text. Existing approaches to model a patient's trajectory focus mostly on structured data and a subset of single-domain outcomes. This study aims to evaluate the effectiveness of Foresight, a generative transformer in temporal modelling of patient data, integrating both free text and structured formats, to predict a diverse array of future medical outcomes, such as disorders, substances (eg, to do with medicines, allergies, or poisonings), procedures, and findings (eg, relating to observations, judgements, or assessments).MethodsForesight is a novel transformer-based pipeline that uses named entity recognition and linking tools to convert EHR document text into structured, coded concepts, followed by providing probabilistic forecasts for future medical events, such as disorders, substances, procedures, and findings. The Foresight pipeline has four main components: (1) CogStack (data retrieval and preprocessing); (2) the Medical Concept Annotation Toolkit (structuring of the free-text information from EHRs); (3) Foresight Core (deep-learning model for biomedical concept modelling); and (4) the Foresight web application. We processed the entire free-text portion from three different hospital datasets (King's College Hospital [KCH], South London and Maudsley [SLaM], and the US Medical Information Mart for Intensive Care III [MIMIC-III]), resulting in information from 811 336 patients and covering both physical and mental health institutions. We measured the performance of models using custom metrics derived from precision and recall.FindingsForesight achieved a precision@10 (ie, of 10 forecasted candidates, at least one is correct) of 0·68 (SD 0·0027) for the KCH dataset, 0·76 (0·0032) for the SLaM dataset, and 0·88 (0·0018) for the MIMIC-III dataset, for forecasting the next new disorder in a patient timeline. Foresight also achieved a precision@10 value of 0·80 (0·0013) for the KCH dataset, 0·81 (0·0026) for the SLaM dataset, and 0·91 (0·0011) for the MIMIC-III dataset, for forecasting the next new biomedical concept. In addition, Foresight was validated on 34 synthetic patient timelines by five clinicians and achieved a relevancy of 33 (97% [95% CI 91–100]) of 34 for the top forecasted candidate disorder. As a generative model, Foresight can forecast follow-on biomedical concepts for as many steps as required.InterpretationForesight is a general-purpose model for biomedical concept modelling that can be used for real-world risk forecasting, virtual trials, and clinical research to study the progression of disorders, to simulate interventions and counterfactuals, and for educational purposes.FundingNational Health Service Artificial Intelligence Laboratory, National Institute for Health and Care Research Biomedical Research Centre, and Health Data Research UK.","<method>generative transformer</method>, <method>transformer-based pipeline</method>, <method>named entity recognition</method>, <method>deep-learning model</method>"
2024,https://openalex.org/W4398183427,Medicine,Evaluating the accuracy of a state-of-the-art large language model for prediction of admissions from the emergency room,"Abstract Background Artificial intelligence (AI) and large language models (LLMs) can play a critical role in emergency room operations by augmenting decision-making about patient admission. However, there are no studies for LLMs using real-world data and scenarios, in comparison to and being informed by traditional supervised machine learning (ML) models. We evaluated the performance of GPT-4 for predicting patient admissions from emergency department (ED) visits. We compared performance to traditional ML models both naively and when informed by few-shot examples and/or numerical probabilities. Methods We conducted a retrospective study using electronic health records across 7 NYC hospitals. We trained Bio-Clinical-BERT and XGBoost (XGB) models on unstructured and structured data, respectively, and created an ensemble model reflecting ML performance. We then assessed GPT-4 capabilities in many scenarios: through Zero-shot, Few-shot with and without retrieval-augmented generation (RAG), and with and without ML numerical probabilities. Results The Ensemble ML model achieved an area under the receiver operating characteristic curve (AUC) of 0.88, an area under the precision-recall curve (AUPRC) of 0.72 and an accuracy of 82.9%. The naïve GPT-4's performance (0.79 AUC, 0.48 AUPRC, and 77.5% accuracy) showed substantial improvement when given limited, relevant data to learn from (ie, RAG) and underlying ML probabilities (0.87 AUC, 0.71 AUPRC, and 83.1% accuracy). Interestingly, RAG alone boosted performance to near peak levels (0.82 AUC, 0.56 AUPRC, and 81.3% accuracy). Conclusions The naïve LLM had limited performance but showed significant improvement in predicting ED admissions when supplemented with real-world examples to learn from, particularly through RAG, and/or numerical probabilities from traditional ML models. Its peak performance, although slightly lower than the pure ML model, is noteworthy given its potential for providing reasoning behind predictions. Further refinement of LLMs with real-world data is necessary for successful integration as decision-support tools in care settings.","<method>GPT-4</method>, <method>Bio-Clinical-BERT</method>, <method>XGBoost (XGB)</method>, <method>Ensemble model</method>, <method>Zero-shot</method>, <method>Few-shot</method>, <method>Retrieval-augmented generation (RAG)</method>"
2024,https://openalex.org/W4390706643,Medicine,Present and Future Innovations in AI and Cardiac MRI,"Cardiac MRI is used to diagnose and treat patients with a multitude of cardiovascular diseases. Despite the growth of clinical cardiac MRI, complicated image prescriptions and long acquisition protocols limit the specialty and restrain its impact on the practice of medicine. Artificial intelligence (AI)-the ability to mimic human intelligence in learning and performing tasks-will impact nearly all aspects of MRI. Deep learning (DL) primarily uses an artificial neural network to learn a specific task from example data sets. Self-driving scanners are increasingly available, where AI automatically controls cardiac image prescriptions. These scanners offer faster image collection with higher spatial and temporal resolution, eliminating the need for cardiac triggering or breath holding. In the future, fully automated inline image analysis will most likely provide all contour drawings and initial measurements to the reader. Advanced analysis using radiomic or DL features may provide new insights and information not typically extracted in the current analysis workflow. AI may further help integrate these features with clinical, genetic, wearable-device, and ""omics"" data to improve patient outcomes. This article presents an overview of AI and its application in cardiac MRI, including in image acquisition, reconstruction, and processing, and opportunities for more personalized cardiovascular care through extraction of novel imaging markers.","<method>Artificial intelligence (AI)</method>, <method>Deep learning (DL)</method>, <method>artificial neural network</method>, <method>radiomic features</method>, <method>DL features</method>"
2024,https://openalex.org/W4391317367,Medicine,Automated localization of mandibular landmarks in the construction of mandibular median sagittal plane,"Abstract Objective To use deep learning to segment the mandible and identify three-dimensional (3D) anatomical landmarks from cone-beam computed tomography (CBCT) images, the planes constructed from the mandibular midline landmarks were compared and analyzed to find the best mandibular midsagittal plane (MMSP). Methods A total of 400 participants were randomly divided into a training group ( n = 360) and a validation group ( n = 40). Normal individuals were used as the test group ( n = 50). The PointRend deep learning mechanism segmented the mandible from CBCT images and accurately identified 27 anatomic landmarks via PoseNet. 3D coordinates of 5 central landmarks and 2 pairs of side landmarks were obtained for the test group. Every 35 combinations of 3 midline landmarks were screened using the template mapping technique. The asymmetry index (AI) was calculated for each of the 35 mirror planes. The template mapping technique plane was used as the reference plane; the top four planes with the smallest AIs were compared through distance, volume difference, and similarity index to find the plane with the fewest errors. Results The mandible was segmented automatically in 10 ± 1.5 s with a 0.98 Dice similarity coefficient. The mean landmark localization error for the 27 landmarks was 1.04 ± 0.28 mm. MMSP should use the plane made by B (supramentale), Gn (gnathion), and F (mandibular foramen). The average AI grade was 1.6 (min–max: 0.59–3.61). There was no significant difference in distance or volume ( P &gt; 0.05); however, the similarity index was significantly different ( P &lt; 0.01). Conclusion Deep learning can automatically segment the mandible, identify anatomic landmarks, and address medicinal demands in people without mandibular deformities. The most accurate MMSP was the B-Gn-F plane.","<method>PointRend deep learning mechanism</method>, <method>PoseNet</method>"
2024,https://openalex.org/W4391508432,Medicine,Artificial intelligence-driven virtual rehabilitation for people living in the community: A scoping review,"Abstract Virtual Rehabilitation (VRehab) is a promising approach to improving the physical and mental functioning of patients living in the community. The use of VRehab technology results in the generation of multi-modal datasets collected through various devices. This presents opportunities for the development of Artificial Intelligence (AI) techniques in VRehab, namely the measurement, detection, and prediction of various patients’ health outcomes. The objective of this scoping review was to explore the applications and effectiveness of incorporating AI into home-based VRehab programs. PubMed/MEDLINE, Embase, IEEE Xplore, Web of Science databases, and Google Scholar were searched from inception until June 2023 for studies that applied AI for the delivery of VRehab programs to the homes of adult patients. After screening 2172 unique titles and abstracts and 51 full-text studies, 13 studies were included in the review. A variety of AI algorithms were applied to analyze data collected from various sensors and make inferences about patients’ health outcomes, most involving evaluating patients’ exercise quality and providing feedback to patients. The AI algorithms used in the studies were mostly fuzzy rule-based methods, template matching, and deep neural networks. Despite the growing body of literature on the use of AI in VRehab, very few studies have examined its use in patients’ homes. Current research suggests that integrating AI with home-based VRehab can lead to improved rehabilitation outcomes for patients. However, further research is required to fully assess the effectiveness of various forms of AI-driven home-based VRehab, taking into account its unique challenges and using standardized metrics.","<method>fuzzy rule-based methods</method>, <method>template matching</method>, <method>deep neural networks</method>"
2024,https://openalex.org/W4392215277,Medicine,Automated forest inventory: Analysis of high-density airborne LiDAR point clouds with 3D deep learning,"Detailed forest inventories are critical for sustainable and flexible management of forest resources, to conserve various ecosystem services. Modern airborne laser scanners deliver high-density point clouds with great potential for fine-scale forest inventory and analysis, but automatically partitioning those point clouds into meaningful entities like individual trees or tree components remains a challenge. The present study aims to fill this gap and introduces a deep learning framework, termed ForAINet, that is able to perform such a segmentation across diverse forest types and geographic regions. From the segmented data, we then derive relevant biophysical parameters of individual trees as well as stands. The system has been tested on FOR-Instance, a dataset of point clouds that have been acquired in five different countries using surveying drones. The segmentation back-end achieves over 85% F-score for individual trees, respectively over 73% mean IoU across five semantic categories: ground, low vegetation, stems, live branches and dead branches. Building on the segmentation results our pipeline then densely calculates biophysical features of each individual tree (height, crown diameter, crown volume, DBH, and location) and properties per stand (digital terrain model and stand density). Especially crown-related features are in most cases retrieved with high accuracy, whereas the estimates for DBH and location are less reliable, due to the airborne scanning setup.",<method>deep learning framework</method>
2024,https://openalex.org/W4392406184,Medicine,A Lesion-Based Diabetic Retinopathy Detection Through Hybrid Deep Learning Model,"Diabetic retinopathy (DR) can be defined as visual impairment caused by prolonged diabetes affecting the blood vessels in the retina. Globally, it stands as the primary contributor to blindness, impacting approximately 191 million individuals. While prior research has addressed DR classification using retinal fundus images, existing methods often focus on isolated lesion detection, lacking a comprehensive framework for the simultaneous identification of all lesions. Previous studies concentrated on early-stage features like exudates, aneurysms, hemorrhages, and blood vessels, sidelining severe-stage lesions such as cotton wool spots, venous beading, very severe intraretinal microvascular abnormalities (IRMA), diffuse intraretinal hemorrhages, capillary degeneration, highly activated microglia, and retinal pigment epithelium (RPE) damage. In this study, a deep learning approach is proposed to classify DR fundus images by severity levels, utilizing GoogleNet and ResNet models based on adaptive particle swarm optimizer (APSO), for enhanced feature extraction. The extracted features from the hybrid model are further used with different machine learning models like random forest, support vector machine, decision tree, and linear regression models. Experimental results showcased the proposed hybrid framework outperforming advanced approaches with a remarkable 94% accuracy on the benchmark dataset. This method demonstrates potential enhancements in precision, recall, accuracy, and F1 score for different DR severity levels.","<method>deep learning</method>, <method>GoogleNet</method>, <method>ResNet</method>, <method>adaptive particle swarm optimizer (APSO)</method>, <method>random forest</method>, <method>support vector machine</method>, <method>decision tree</method>, <method>linear regression</method>"
2024,https://openalex.org/W4393905353,Medicine,Theranostics and artificial intelligence: new frontiers in personalized medicine,"The field of theranostics is rapidly advancing, driven by the goals of enhancing patient care. Recent breakthroughs in artificial intelligence (AI) and its innovative theranostic applications have marked a critical step forward in nuclear medicine, leading to a significant paradigm shift in precision oncology. For instance, AI-assisted tumor characterization, including automated image interpretation, tumor segmentation, feature identification, and prediction of high-risk lesions, improves diagnostic processes, offering a precise and detailed evaluation. With a comprehensive assessment tailored to an individual's unique clinical profile, AI algorithms promise to enhance patient risk classification, thereby benefiting the alignment of patient needs with the most appropriate treatment plans. By uncovering potential factors unseeable to the human eye, such as intrinsic variations in tumor radiosensitivity or molecular profile, AI software has the potential to revolutionize the prediction of response heterogeneity. For accurate and efficient dosimetry calculations, AI technology offers significant advantages by providing customized phantoms and streamlining complex mathematical algorithms, making personalized dosimetry feasible and accessible in busy clinical settings. AI tools have the potential to be leveraged to predict and mitigate treatment-related adverse events, allowing early interventions. Additionally, generative AI can be utilized to find new targets for developing novel radiopharmaceuticals and facilitate drug discovery. However, while there is immense potential and notable interest in the role of AI in theranostics, these technologies do not lack limitations and challenges. There remains still much to be explored and understood. In this study, we investigate the current applications of AI in theranostics and seek to broaden the horizons for future research and innovation.","<method>AI-assisted tumor characterization</method>, <method>automated image interpretation</method>, <method>tumor segmentation</method>, <method>feature identification</method>, <method>prediction of high-risk lesions</method>, <method>AI algorithms for patient risk classification</method>, <method>AI software for prediction of response heterogeneity</method>, <method>AI technology for dosimetry calculations with customized phantoms</method>, <method>AI tools to predict and mitigate treatment-related adverse events</method>, <method>generative AI for finding new targets and drug discovery</method>"
2024,https://openalex.org/W4395037579,Medicine,Assessing ChatGPT 4.0’s test performance and clinical diagnostic accuracy on USMLE STEP 2 CK and clinical case reports,"Abstract While there is data assessing the test performance of artificial intelligence (AI) chatbots, including the Generative Pre-trained Transformer 4.0 (GPT 4) chatbot (ChatGPT 4.0), there is scarce data on its diagnostic accuracy of clinical cases. We assessed the large language model (LLM), ChatGPT 4.0, on its ability to answer questions from the United States Medical Licensing Exam (USMLE) Step 2, as well as its ability to generate a differential diagnosis based on corresponding clinical vignettes from published case reports. A total of 109 Step 2 Clinical Knowledge (CK) practice questions were inputted into both ChatGPT 3.5 and ChatGPT 4.0, asking ChatGPT to pick the correct answer. Compared to its previous version, ChatGPT 3.5, we found improved accuracy of ChatGPT 4.0 when answering these questions, from 47.7 to 87.2% ( p = 0.035) respectively. Utilizing the topics tested on Step 2 CK questions, we additionally found 63 corresponding published case report vignettes and asked ChatGPT 4.0 to come up with its top three differential diagnosis. ChatGPT 4.0 accurately created a shortlist of differential diagnoses in 74.6% of the 63 case reports (74.6%). We analyzed ChatGPT 4.0’s confidence in its diagnosis by asking it to rank its top three differentials from most to least likely. Out of the 47 correct diagnoses, 33 were the first (70.2%) on the differential diagnosis list, 11 were second (23.4%), and three were third (6.4%). Our study shows the continued iterative improvement in ChatGPT’s ability to answer standardized USMLE questions accurately and provides insights into ChatGPT’s clinical diagnostic accuracy.","<method>Generative Pre-trained Transformer 4.0 (GPT 4) chatbot (ChatGPT 4.0)</method>, <method>ChatGPT 3.5</method>, <method>large language model (LLM)</method>"
2024,https://openalex.org/W4402521185,Medicine,Advanced Ensemble Machine Learning Techniques for Optimizing Diabetes Mellitus Prognostication: A Detailed Examination of Hospital Data,"Diabetes is a chronic disease that affects millions of people worldwide. Early diagnosis and effective management are crucial for reducing its complications. Diabetes is the fourth-highest cause of mortality due to its association with various comorbidities, including heart disease, nerve damage, blood vessel damage, and blindness. The potential of machine learning algorithms in predicting Diabetes and related conditions is significant, and mining diabetes data is an efficient method for extracting new insights.The primary objective of this study is to develop an enhanced ensemble model to predict Diabetes with improved accuracy by leveraging various machine learning algorithms.This study tested several popular machine learning algorithms commonly used in diabetes prediction, including Naive Bayes (NB), Generalized Linear Model (GLM), Logistic Regression (LR), Fast Large Margin (FLM), Deep Learning (DL), Decision Tree (DT), Random Forest (RF), Gradient Boosted Trees (GBT), and Support Vector Machine (SVM). The performance of these algorithms was compared, and two different ensemble techniques—stacking and voting—were used to build a more accurate predictive model.The top three algorithms based on accuracy were Deep Learning, Naive Bayes, and Gradient Boosted Trees. The machine learning algorithms revealed that individuals with Diabetes are significantly affected by the number of chronic conditions they have, as well as their gender and age. The ensemble models, particularly the stacking method, provided higher accuracy than individual algorithms. The stacking ensemble model achieved a slightly better accuracy of 99.94% compared to 99.34% for the voting method.Building an ensemble model significantly increased the accuracy of predicting Diabetes and related conditions. The stacking ensemble model, in particular, demonstrated superior performance, highlighting the importance of combining multiple machine learning approaches to enhance predictive accuracy","<method>Naive Bayes (NB)</method>, <method>Generalized Linear Model (GLM)</method>, <method>Logistic Regression (LR)</method>, <method>Fast Large Margin (FLM)</method>, <method>Deep Learning (DL)</method>, <method>Decision Tree (DT)</method>, <method>Random Forest (RF)</method>, <method>Gradient Boosted Trees (GBT)</method>, <method>Support Vector Machine (SVM)</method>, <method>stacking ensemble</method>, <method>voting ensemble</method>"
2024,https://openalex.org/W4404134492,Medicine,Bias in medical AI: Implications for clinical decision-making,"Biases in medical artificial intelligence (AI) arise and compound throughout the AI lifecycle. These biases can have significant clinical consequences, especially in applications that involve clinical decision-making. Left unaddressed, biased medical AI can lead to substandard clinical decisions and the perpetuation and exacerbation of longstanding healthcare disparities. We discuss potential biases that can arise at different stages in the AI development pipeline and how they can affect AI algorithms and clinical decision-making. Bias can occur in data features and labels, model development and evaluation, deployment, and publication. Insufficient sample sizes for certain patient groups can result in suboptimal performance, algorithm underestimation, and clinically unmeaningful predictions. Missing patient findings can also produce biased model behavior, including capturable but nonrandomly missing data, such as diagnosis codes, and data that is not usually or not easily captured, such as social determinants of health. Expertly annotated labels used to train supervised learning models may reflect implicit cognitive biases or substandard care practices. Overreliance on performance metrics during model development may obscure bias and diminish a model's clinical utility. When applied to data outside the training cohort, model performance can deteriorate from previous validation and can do so differentially across subgroups. How end users interact with deployed solutions can introduce bias. Finally, where models are developed and published, and by whom, impacts the trajectories and priorities of future medical AI development. Solutions to mitigate bias must be implemented with care, which include the collection of large and diverse data sets, statistical debiasing methods, thorough model evaluation, emphasis on model interpretability, and standardized bias reporting and transparency requirements. Prior to real-world implementation in clinical settings, rigorous validation through clinical trials is critical to demonstrate unbiased application. Addressing biases across model development stages is crucial for ensuring all patients benefit equitably from the future of medical AI.","<method>supervised learning</method>, <method>statistical debiasing methods</method>"
2024,https://openalex.org/W4391135342,Medicine,Performance of Generative Pretrained Transformer on the National Medical Licensing Examination in Japan,"The remarkable performance of ChatGPT, launched in November 2022, has significantly impacted the field of natural language processing, inspiring the application of large language models as supportive tools in clinical practice and research worldwide. Although GPT-3.5 recently scored high on the United States Medical Licensing Examination, its performance on medical licensing examinations of other nations, especially non-English speaking nations, has not been sufficiently evaluated. This study assessed GPT’s performance on the National Medical Licensing Examination (NMLE) in Japan and compared it with the actual minimal passing rate for this exam. In particular, the performances of both the GPT-3.5 and GPT-4 models were considered for the comparative analysis. We initially used the GPT models and several prompts for 290 questions without image data from the 116 th NMLE (held in February 2022 in Japan) to maximize the performance for delivering correct answers and explanations of the questions. Thereafter, we tested the performance of the best GPT model (GPT-4) with optimized prompts on a dataset of 262 questions without images from the latest 117 th NMLE (held in February 2023). The best model with the optimized prompts scored 82.7% for the essential questions and 77.2% for the basic and clinical questions, both of which sufficed the minimum passing scoring rates of 80.0% and 74.6%, respectively. After an exploratory analysis of 56 incorrect answers from the model, we identified the three major factors contributing to the generation of the incorrect answers—insufficient medical knowledge, information on Japan-specific medical system and guidelines, and mathematical errors. In conclusion, GPT-4 with our optimized prompts achieved a minimum passing scoring rate in the latest 117 th NMLE in Japan. Beyond its original design of answering examination questions for humans, these artificial intelligence (AI) models can serve as one of the best “sidekicks” for solving problems and addressing the unmet needs in the medical and healthcare fields.","<method>GPT-3.5</method>, <method>GPT-4</method>"
2024,https://openalex.org/W4391707561,Medicine,Efficient Communication in Wireless Sensor Networks Using Optimized Energy Efficient Engroove Leach Clustering Protocol,"The Wireless Sensor Network (WSN) is a network that is constructed in regions that are inaccessible to human beings. The widespread deployment of wireless micro sensors will make it possible to conduct accurate environmental monitoring for a use in both civil and military environments. They make use of these data to monitor and keep track of the physical data of the surrounding environment in order to ensure the sustainability of the area. The data have to be picked up by the sensor, and then sent to the sink node where they may be processed. The nodes of the WSNs are powered by batteries, therefore they eventually run out of power. This energy restriction has an effect on the network life span and environmental sustainability. The objective of this study is to further improve the Engroove Leach (EL) protocol's energy efficiency so that the network can operate for a very long time while consuming the least amount of energy. The lifespan of WSNs is being extended often using clustering and routing strategies. The Meta Inspired Hawks Fragment Optimization (MIHFO) system, which is based on passive clustering, is used in this study to do clustering. The cluster head is chosen based on the nodes' residual energy, distance to neighbors, distance to base station, node degree, and node centrality. Based on distance, residual energy, and node degree, an algorithm known as Heuristic Wing Antfly Optimization (HWAFO) selects the optimum path between the cluster head and Base Station (BS). They examine the number of nodes that are active, their energy consumption, and the number of data packets that the BS receives. The overall experimentation is carried out under the MATLAB environment. From the analysis, it has been discovered that the suggested approach yields noticeably superior outcomes in terms of throughput, packet delivery and drop ratio, and average energy consumption.","<method>Meta Inspired Hawks Fragment Optimization (MIHFO)</method>, <method>Heuristic Wing Antfly Optimization (HWAFO)</method>"
2024,https://openalex.org/W4391752835,Medicine,Immersive virtual reality and augmented reality in anatomy education: A systematic review and meta‐analysis,"Abstract The purpose of this review was to (1) analyze the effectiveness of immersive virtual reality (iVR) and augmented reality (AR) as teaching/learning resources (collectively called XR‐technologies) for gaining anatomy knowledge compared to traditional approaches and (2) gauge students' perceptions of the usefulness of these technologies as learning tools. This meta‐analysis, previously registered in PROSPERO (CRD42023423017), followed PRISMA guidelines. A systematic bibliographical search, without time parameters, was conducted through four databases until June 2023. A meta‐analytic approach investigated knowledge gains and XR's usefulness for learning. Pooled effect sizes were estimated using Cohen's standardized mean difference (SMD) and 95% confidence intervals (95% CI). A single‐group proportional meta‐analysis was conducted to quantify the percentage of students who considered XR devices useful for their learning. Twenty‐seven experimental studies, reporting data from 2199 health sciences students, were included for analysis. XR‐technologies yielded higher knowledge gains than traditional approaches (SMD = 0.40; 95% CI = 0.22 to 0.60), especially when used as supplemental/complementary learning resources (SMD = 0.52; 95% CI = 0.40 to 0.63). Specifically, knowledge performance using XR devices outperformed textbooks and atlases (SMD = 0.32; 95% CI = 0.10 to 0.54) and didactic lectures (SMD = 1.00; 95% CI = 0.57 to 1.42), especially among undergraduate students (SMD = 0.41; 95% CI = 0.20 to 0.62). XR devices were perceived to be more useful for learning than traditional approaches (SMD = 0.54; 95% CI = 0.04 to 1), and 80% of all students who used XR devices reported these devices as useful for learning anatomy. Learners using XR technologies demonstrated increased anatomy knowledge gains and considered these technologies useful for learning anatomy.",No methods found.
2024,https://openalex.org/W4395049366,Medicine,A novel SpaSA based hyper-parameter optimized FCEDN with adaptive CNN classification for skin cancer detection,"Abstract Skin cancer is the most prevalent kind of cancer in people. It is estimated that more than 1 million people get skin cancer every year in the world. The effectiveness of the disease’s therapy is significantly impacted by early identification of this illness. Preprocessing is the initial detecting stage in enhancing the quality of skin images by removing undesired background noise and objects. This study aims is to compile preprocessing techniques for skin cancer imaging that are currently accessible. Researchers looking into automated skin cancer diagnosis might use this article as an excellent place to start. The fully convolutional encoder–decoder network and Sparrow search algorithm (FCEDN-SpaSA) are proposed in this study for the segmentation of dermoscopic images. The individual wolf method and the ensemble ghosting technique are integrated to generate a neighbour-based search strategy in SpaSA for stressing the correct balance between navigation and exploitation. The classification procedure is accomplished by using an adaptive CNN technique to discriminate between normal skin and malignant skin lesions suggestive of disease. Our method provides classification accuracies comparable to commonly used incremental learning techniques while using less energy, storage space, memory access, and training time (only network updates with new training samples, no network sharing). In a simulation, the segmentation performance of the proposed technique on the ISBI 2017, ISIC 2018, and PH2 datasets reached accuracies of 95.28%, 95.89%, 92.70%, and 98.78%, respectively, on the same dataset and assessed the classification performance. It is accurate 91.67% of the time. The efficiency of the suggested strategy is demonstrated through comparisons with cutting-edge methodologies.","<method>fully convolutional encoder–decoder network</method>, <method>Sparrow search algorithm (SpaSA)</method>, <method>individual wolf method</method>, <method>ensemble ghosting technique</method>, <method>adaptive CNN technique</method>, <method>incremental learning techniques</method>"
2024,https://openalex.org/W4396622564,Medicine,Unveiling the shadows: Beyond the hype of AI in education,"Despite the wave of enthusiasm for the role of Artificial Intelligence (AI) in reshaping education, critical voices urge a more tempered approach. This study investigates the less-discussed 'shadows' of AI implementation in educational settings, focusing on potential negatives that may accompany its integration. Through a multi-phased exploration consisting of content analysis and survey research, the study develops and validates a theoretical model that pinpoints several areas of concern. The initial phase, a systematic literature review, yielded 56 relevant studies from which the model was crafted. The subsequent survey with 260 participants from a Saudi Arabian university aimed to validate the model. Findings confirm concerns about human connection, data privacy and security, algorithmic bias, transparency, critical thinking, access equity, ethical issues, teacher development, reliability, and the consequences of AI-generated content. They also highlight correlations between various AI-associated concerns, suggesting intertwined consequences rather than isolated issues. For instance, enhancements in AI transparency could simultaneously support teacher professional development and foster better student outcomes. Furthermore, the study acknowledges the transformative potential of AI but cautions against its unexamined adoption in education. It advocates for comprehensive strategies to maintain human connections, ensure data privacy and security, mitigate biases, enhance system transparency, foster creativity, reduce access disparities, emphasize ethics, prepare teachers, ensure system reliability, and regulate AI-generated content. Such strategies underscore the need for holistic policymaking to leverage AI's benefits while safeguarding against its disadvantages.",No methods found.
2024,https://openalex.org/W4390753190,Medicine,Investigating the impact of motion in the scanner on brain age predictions,"Abstract Brain Age Gap (BAG) is defined as the difference between the brain’s predicted age and the chronological age of an individual. Magnetic resonance imaging (MRI)-based BAG can quantify acceleration of brain aging, and is used to infer brain health as aging and disease interact. Motion in the scanner is a common occurrence that can affect the acquired MRI data and act as a major confound in the derived models. As such, age-related changes in head motion may impact the observed age-related differences. However, the relationship between head motion and BAG as estimated by structural MRI has not been systematically examined. The aim of this study is to assess the impact of motion on voxel-based morphometry (VBM) based BAG. Data were obtained from two sources: i) T1-weighted (T1w) MRIs from the Cambridge Centre for Ageing and Neuroscience (CamCAN) were used to train the brain age prediction model, and ii) T1w MRIs from the Movement-related artifacts (MR-ART) dataset were used to assess the impact of motion on BAG. MR-ART includes one motion-free and two motion-affected (one low and one high) 3D T1w MRIs. We also visually rated the motion levels of the MR-ART MRIs from 0 to 5, with 0 meaning no motion and 5 high motion levels. All images were pre-processed through a standard VBM pipeline. GM density across cortical and subcortical regions were then used to train the brain age prediction model and assess the relationship between BAG and MRI motion. Principal component analysis was used to perform dimension reduction and extract the VBM-based features. BAG was estimated by regressing out the portion of delta age explained by chronological age. Linear mixed-effects models were used to investigate the relationship between BAG and motion session as well as motion severity, including participant IDs as random effects. We repeated the same analysis using cortical thickness based on FreeSurfer 7.4.1 and to compare the results for volumetric versus surface-based measures of brain morphometry. In contrast with the session with no induced motion, predicted delta age was significantly higher for high motion sessions 2.35 years (t = 5.17, p &amp;lt; 0.0001), with marginal effect for low motion sessions 0.95 years (t = 2.11, p = 0.035) for VBM analysis as well as 3.46 years (t = 11.45, p &amp;lt; 0.0001) for high motion and 2.28 years (t = 7.54, p &amp;lt; 0.0001) for low motion based on cortical thickness. In addition, delta age was significantly associated with motion severity as evaluated by visual rating 0.45 years per rating level (t = 4.59, p &amp;lt; 0.0001) for VBM analysis and 0.83 years per motion level (t = 12.89, p &amp;lt; 0.0001) for cortical thickness analysis. Motion in the scanner can significantly impact brain age estimates, and needs to be accounted for as a confound, particularly when studying populations that are known to have higher levels of motion in the scanner. These results have significant implications for brain age studies in aging and neurodegeneration. Based on these findings, we recommend assessment and inclusion of visual motion ratings in such studies. In cases that the visual rating proves prohibitive, we recommend the inclusion of normalized Euler number from FreeSurfer as defined in the manuscript as a covariate in the models.","<method>brain age prediction model</method>, <method>principal component analysis</method>, <method>regression</method>, <method>linear mixed-effects models</method>"
2024,https://openalex.org/W4390987311,Medicine,Reliability of ChatGPT for performing triage task in the emergency department using the Korean Triage and Acuity Scale,"Background Artificial intelligence (AI) technology can enable more efficient decision-making in healthcare settings. There is a growing interest in improving the speed and accuracy of AI systems in providing responses for given tasks in healthcare settings. Objective This study aimed to assess the reliability of ChatGPT in determining emergency department (ED) triage accuracy using the Korean Triage and Acuity Scale (KTAS). Methods Two hundred and two virtual patient cases were built. The gold standard triage classification for each case was established by an experienced ED physician. Three other human raters (ED paramedics) were involved and rated the virtual cases individually. The virtual cases were also rated by two different versions of the chat generative pre-trained transformer (ChatGPT, 3.5 and 4.0). Inter-rater reliability was examined using Fleiss’ kappa and intra-class correlation coefficient (ICC). Results The kappa values for the agreement between the four human raters and ChatGPTs were .523 (version 4.0) and .320 (version 3.5). Of the five levels, the performance was poor when rating patients at levels 1 and 5, as well as case scenarios with additional text descriptions. There were differences in the accuracy of the different versions of GPTs. The ICC between version 3.5 and the gold standard was .520, and that between version 4.0 and the gold standard was .802. Conclusions A substantial level of inter-rater reliability was revealed when GPTs were used as KTAS raters. The current study showed the potential of using GPT in emergency healthcare settings. Considering the shortage of experienced manpower, this AI method may help improve triaging accuracy.","<method>chat generative pre-trained transformer (ChatGPT, 3.5 and 4.0)</method>"
2024,https://openalex.org/W4391145465,Medicine,Assessing generalisability of deep learning-based polyp detection and segmentation methods through a computer vision challenge,"Abstract Polyps are well-known cancer precursors identified by colonoscopy. However, variability in their size, appearance, and location makes the detection of polyps challenging. Moreover, colonoscopy surveillance and removal of polyps are highly operator-dependent procedures and occur in a highly complex organ topology. There exists a high missed detection rate and incomplete removal of colonic polyps. To assist in clinical procedures and reduce missed rates, automated methods for detecting and segmenting polyps using machine learning have been achieved in past years. However, the major drawback in most of these methods is their ability to generalise to out-of-sample unseen datasets from different centres, populations, modalities, and acquisition systems. To test this hypothesis rigorously, we, together with expert gastroenterologists, curated a multi-centre and multi-population dataset acquired from six different colonoscopy systems and challenged the computational expert teams to develop robust automated detection and segmentation methods in a crowd-sourcing Endoscopic computer vision challenge. This work put forward rigorous generalisability tests and assesses the usability of devised deep learning methods in dynamic and actual clinical colonoscopy procedures. We analyse the results of four top performing teams for the detection task and five top performing teams for the segmentation task. Our analyses demonstrate that the top-ranking teams concentrated mainly on accuracy over the real-time performance required for clinical applicability. We further dissect the devised methods and provide an experiment-based hypothesis that reveals the need for improved generalisability to tackle diversity present in multi-centre datasets and routine clinical procedures.","<method>machine learning</method>, <method>deep learning</method>"
2024,https://openalex.org/W4391531696,Medicine,Artificial intelligence in the risk prediction models of cardiovascular disease and development of an independent validation screening tool: a systematic review,"Abstract Background A comprehensive overview of artificial intelligence (AI) for cardiovascular disease (CVD) prediction and a screening tool of AI models (AI-Ms) for independent external validation are lacking. This systematic review aims to identify, describe, and appraise AI-Ms of CVD prediction in the general and special populations and develop a new independent validation score (IVS) for AI-Ms replicability evaluation. Methods PubMed, Web of Science, Embase, and IEEE library were searched up to July 2021. Data extraction and analysis were performed for the populations, distribution, predictors, algorithms, etc. The risk of bias was evaluated with the prediction risk of bias assessment tool (PROBAST). Subsequently, we designed IVS for model replicability evaluation with five steps in five items, including transparency of algorithms, performance of models, feasibility of reproduction, risk of reproduction, and clinical implication, respectively. The review is registered in PROSPERO (No. CRD42021271789). Results In 20,887 screened references, 79 articles (82.5% in 2017–2021) were included, which contained 114 datasets (67 in Europe and North America, but 0 in Africa). We identified 486 AI-Ms, of which the majority were in development ( n = 380), but none of them had undergone independent external validation. A total of 66 idiographic algorithms were found; however, 36.4% were used only once and only 39.4% over three times. A large number of different predictors (range 5–52,000, median 21) and large-span sample size (range 80–3,660,000, median 4466) were observed. All models were at high risk of bias according to PROBAST, primarily due to the incorrect use of statistical methods. IVS analysis confirmed only 10 models as “recommended”; however, 281 and 187 were “not recommended” and “warning,” respectively. Conclusion AI has led the digital revolution in the field of CVD prediction, but is still in the early stage of development as the defects of research design, report, and evaluation systems. The IVS we developed may contribute to independent external validation and the development of this field.",No methods found.
2024,https://openalex.org/W4393201840,Medicine,"Clinical gait analysis using video-based pose estimation: Multiple perspectives, clinical populations, and measuring change","Gait dysfunction is common in many clinical populations and often has a profound and deleterious impact on independence and quality of life. Gait analysis is a foundational component of rehabilitation because it is critical to identify and understand the specific deficits that should be targeted prior to the initiation of treatment. Unfortunately, current state-of-the-art approaches to gait analysis (e.g., marker-based motion capture systems, instrumented gait mats) are largely inaccessible due to prohibitive costs of time, money, and effort required to perform the assessments. Here, we demonstrate the ability to perform quantitative gait analyses in multiple clinical populations using only simple videos recorded using low-cost devices (tablets). We report four primary advances: 1) a novel, versatile workflow that leverages an open-source human pose estimation algorithm (OpenPose) to perform gait analyses using videos recorded from multiple different perspectives (e.g., frontal, sagittal), 2) validation of this workflow in three different populations of participants (adults without gait impairment, persons post-stroke, and persons with Parkinson’s disease) via comparison to ground-truth three-dimensional motion capture, 3) demonstration of the ability to capture clinically relevant, condition-specific gait parameters, and 4) tracking of within-participant changes in gait, as is required to measure progress in rehabilitation and recovery. Importantly, our workflow has been made freely available and does not require prior gait analysis expertise. The ability to perform quantitative gait analyses in nearly any setting using only low-cost devices and computer vision offers significant potential for dramatic improvement in the accessibility of clinical gait analysis across different patient populations.",<method>human pose estimation algorithm (OpenPose)</method>
2024,https://openalex.org/W4393321663,Medicine,"Heavy metal contamination assessment and potential human health risk of water quality of lakes situated in the protected area of Tisa, Romania","Protected areas are significant due to the high value of natural resources they shelter. This study's primary objective is to assess the quality status of the water resources (13 lakes and Tisa River) localized in the protected area of Tisa River on the territory of Romania. A number of 13 lakes and surface water (Tisa River) situated in the protected area through the Natura 2000 ecological network are studied. The chemistry and potential pollution status were analyzed by measuring and analyzing a set of twenty elements and sixteen physico-chemical parameters. The potential impact of anthropogenic activities was settled through the applied analysis and obtained results. A potential human health risk was noticed. Results indicated that waters are rich in Ni and Fe probably due to interaction with groundwater rich in Fe and Ni. Waters are characterized by potential contamination, which if directly or through the food chain consumed could negatively influence the human health. Piper and Gibbs plots indicated that the studied waters are divided into three categories based on water-rock interactions: mixed Ca2+-Na+-HCO3-, CaCO3-, and Na+-HCO3-. Likewise, the applied pollution indices (Heavy metal Pollution Index, HPI and Heavy metal Evaluation Index, HEI) indicated three pollution categories correlated to the As, Ni and Fe amounts. The findings of this research imply that the chemistry of the studied lakes and surface waters is influenced by the geogenic origin and emergence of anthropogenic activities. The significance of this research is related to understanding of mechanisms that influence the water quality, improving and conserving the natural water resources, and correspondingly understanding if any potential human health risks could be identified.",No methods found.
2024,https://openalex.org/W4396570916,Medicine,ELRL-MD: a deep learning approach for myocarditis diagnosis using cardiac magnetic resonance images with ensemble and reinforcement learning integration,"Abstract Objective. Myocarditis poses a significant health risk, often precipitated by viral infections like coronavirus disease, and can lead to fatal cardiac complications. As a less invasive alternative to the standard diagnostic practice of endomyocardial biopsy, which is highly invasive and thus limited to severe cases, cardiac magnetic resonance (CMR) imaging offers a promising solution for detecting myocardial abnormalities. Approach. This study introduces a deep model called ELRL-MD that combines ensemble learning and reinforcement learning (RL) for effective myocarditis diagnosis from CMR images. The model begins with pre-training via the artificial bee colony (ABC) algorithm to enhance the starting point for learning. An array of convolutional neural networks (CNNs) then works in concert to extract and integrate features from CMR images for accurate diagnosis. Leveraging the Z-Alizadeh Sani myocarditis CMR dataset, the model employs RL to navigate the dataset’s imbalance by conceptualizing diagnosis as a decision-making process. Main results. ELRL-DM demonstrates remarkable efficacy, surpassing other deep learning, conventional machine learning, and transfer learning models, achieving an F-measure of 88.2% and a geometric mean of 90.6%. Extensive experimentation helped pinpoint the optimal reward function settings and the perfect count of CNNs. Significance. The study addresses the primary technical challenge of inherent data imbalance in CMR imaging datasets and the risk of models converging on local optima due to suboptimal initial weight settings. Further analysis, leaving out ABC and RL components, confirmed their contributions to the model’s overall performance, underscoring the effectiveness of addressing these critical technical challenges.","<method>ensemble learning</method>, <method>reinforcement learning (RL)</method>, <method>artificial bee colony (ABC) algorithm</method>, <method>convolutional neural networks (CNNs)</method>, <method>deep learning</method>, <method>conventional machine learning</method>, <method>transfer learning</method>"
2024,https://openalex.org/W4396831262,Medicine,GPT-4 Turbo with Vision fails to outperform text-only GPT-4 Turbo in the Japan Diagnostic Radiology Board Examination,"Abstract Purpose To assess the performance of GPT-4 Turbo with Vision (GPT-4TV), OpenAI’s latest multimodal large language model, by comparing its ability to process both text and image inputs with that of the text-only GPT-4 Turbo (GPT-4 T) in the context of the Japan Diagnostic Radiology Board Examination (JDRBE). Materials and methods The dataset comprised questions from JDRBE 2021 and 2023. A total of six board-certified diagnostic radiologists discussed the questions and provided ground-truth answers by consulting relevant literature as necessary. The following questions were excluded: those lacking associated images, those with no unanimous agreement on answers, and those including images rejected by the OpenAI application programming interface. The inputs for GPT-4TV included both text and images, whereas those for GPT-4 T were entirely text. Both models were deployed on the dataset, and their performance was compared using McNemar’s exact test. The radiological credibility of the responses was assessed by two diagnostic radiologists through the assignment of legitimacy scores on a five-point Likert scale. These scores were subsequently used to compare model performance using Wilcoxon's signed-rank test. Results The dataset comprised 139 questions. GPT-4TV correctly answered 62 questions (45%), whereas GPT-4 T correctly answered 57 questions (41%). A statistical analysis found no significant performance difference between the two models (P = 0.44). The GPT-4TV responses received significantly lower legitimacy scores from both radiologists than the GPT-4 T responses. Conclusion No significant enhancement in accuracy was observed when using GPT-4TV with image input compared with that of using text-only GPT-4 T for JDRBE questions.","<method>GPT-4 Turbo with Vision (GPT-4TV)</method>, <method>GPT-4 Turbo (GPT-4 T)</method>"
2024,https://openalex.org/W162420746,Medicine,Mosquito nuisance and control in the UK in 2022 – A questionnaire-based survey of local authorities,"Abstract Mosquitoes are widely distributed across the United Kingdom (UK) and shifts in their distribution, particularly those known for their nuisance-biting behaviour, are expected to lead to an increase in enquiries directed towards Local Authorities (LAs) Environmental Health teams. The Mosquito Nuisance and Control questionnaire aims to gather data on current and recent mosquito nuisance biting and assess local authority capabilities around mosquito control. This questionnaire has been conducted approximately every decade in the UK since 1970. This paper provides the latest results from the Mosquito Nuisance and Control Survey which was sent to all UK LAs in 2022. The questionnaire was distributed to Environmental Health officers within all local authorities across the UK. Participants were given the option to complete the questionnaire online, via email, or by post to ensure comprehensive data collection. The survey focused on assessing the frequency of mosquito nuisance incidents over the last decade, the identification of potential mosquito aquatic habitats, and the implementation and effectiveness of control strategies. The response rate was 62.5% (243/389) of which 3.5% LAs (14/389) reported mosquito nuisance incidents in the past decade, with 8 LAs (~2%) encountering complaints within the last year. Moreover, 89 LAs were able to identify potential mosquito aquatic habitats within their jurisdictions, and four reported activities that required mosquito control measures in the past year. Less than 1% of LAs reported that currently they have the need for dedicated budgets for mosquito control. The reasons for non-responses and inconsistencies in data recording require further investigation, as they may impact data accuracy and hinder effective mosquito management. This survey underscores the need to continue raising awareness about mosquitoes and the potential future need for control. It provides a baseline for preparedness against future mosquito-related risks. By addressing these areas, LAs will be better positioned to take proactive steps to reduce nuisance mosquito populations as they arise and to alleviate the future impact of mosquito-borne diseases and nuisances on their communities. Additionally, LAs can benefit from studying and adopting strategies from EU countries in managing invasive and nuisance biting mosquitoes.",No methods found.
2024,https://openalex.org/W4390531926,Medicine,The role of artificial intelligence in generating original scientific research,"Artificial intelligence (AI) is a revolutionary technology that is finding wide application across numerous sectors. Large language models (LLMs) are an emerging subset technology of AI and have been developed to communicate using human languages. At their core, LLMs are trained with vast amounts of information extracted from the internet, including text and images. Their ability to create human-like, expert text in almost any subject means they are increasingly being used as an aid to presentation, particularly in scientific writing. However, we wondered whether LLMs could go further, generating original scientific research and preparing the results for publication. We tasked GPT-4, an LLM, to write an original pharmaceutics manuscript, on a topic that is itself novel. It was able to conceive a research hypothesis, define an experimental protocol, produce photo-realistic images of printlets, generate believable analytical data from a range of instruments and write a convincing publication-ready manuscript with evidence of critical interpretation. The model achieved all this is less than 1h. Moreover, the generated data were multi-modal in nature, including thermal analyses, vibrational spectroscopy and dissolution testing, demonstrating multi-disciplinary expertise in the LLM. One area in which the model failed, however, was in referencing to the literature. Since the generated experimental results appeared believable though, we suggest that LLMs could certainly play a role in scientific research but with human input, interpretation and data validation. We discuss the potential benefits and current bottlenecks for realising this ambition here.","<method>Large language models (LLMs)</method>, <method>GPT-4</method>"
2024,https://openalex.org/W4391061191,Medicine,LBO-MPAM: Ladybug Beetle Optimization-based multilayer perceptron attention module for segmenting the skin lesion and automatic localization,"In recent years, skin cancer has been the most dangerous disease noticed among people worldwide. Skin cancer should be identified earlier to reduce the rate of mortality. Employing dermoscopic images can identify and categorise skin cancer effectively. But, the visual evaluation is a complex procedure to be done in the dermoscopic image. However, Deep learning (DL) is an efficient method for skin cancer detection; however, segmenting the skin lesion and automatic localisation in an earlier stage is complicated. In this paper, a novel Ladybug Beetle Optimization-Double Attention Based Multilevel 1-D CNN (LBO-DAM 1-D CNN) technique is proposed to detect and classify skin cancer. To improve skin lesion type discriminability, the two types of attention modules are introduced. The Ultra-Lightweight Subspace Attention Module (ULSAM) is utilised for classifying the feature maps into different stages to validate the frequency from different image samples. However, the multilayer perceptron attention module (MLPAM) is determined to provide information regarding skin cancer classification and diminish the noise and unwanted data. To minimise data loss, it is then combined with hierarchical complementarity during classification. Second, a modified MLPAM is used to extract significant feature spaces for network learning, select the most important information, and reduce feature space redundancy. The Ladybug Beetle Optimization (LBO) algorithm provides the optimal classification solution by minimising the loss rate of DAM 1-D CNN architecture. The experimentation is conducted on three different datasets such as ISIC2020, HAM10000, and the melanoma detection dataset. The experimental results revealed that the proposed method is compared with different existing methods such as IMFO-KELM, Mask RCNN, M-SVM, DCNN-9, and TL-CNN with different datasets. These methods attained 94.56, 92.65, 90.56, 88.65, and 95.5 for the ISIC2020 dataset but the proposed method enhanced the classification performance by attaining 97.02. Also, the validation is based on metrics, namely, accuracy, precision, sensitivity, and F1-score of 97.03%, 97.05%, 97.58%, and 97.27% for a total of 500 epochs.","<method>Deep learning (DL)</method>, <method>Ladybug Beetle Optimization-Double Attention Based Multilevel 1-D CNN (LBO-DAM 1-D CNN)</method>, <method>Ultra-Lightweight Subspace Attention Module (ULSAM)</method>, <method>multilayer perceptron attention module (MLPAM)</method>, <method>Ladybug Beetle Optimization (LBO) algorithm</method>, <method>IMFO-KELM</method>, <method>Mask RCNN</method>, <method>M-SVM</method>, <method>DCNN-9</method>, <method>TL-CNN</method>"
2024,https://openalex.org/W4391109191,Medicine,"Developing, Purchasing, Implementing and Monitoring AI Tools in Radiology: Practical Considerations. A Multi-Society Statement from the ACR, CAR, ESR, RANZCR and RSNA","Artificial Intelligence (AI) carries the potential for unprecedented disruption in radiology, with possible positive and negative consequences. The integration of AI in radiology holds the potential to revolutionize healthcare practices by advancing diagnosis, quantification, and management of multiple medical conditions. Nevertheless, the ever-growing availability of AI tools in radiology highlights an increasing need to critically evaluate claims for its utility and to differentiate safe product offerings from potentially harmful, or fundamentally unhelpful ones. This multi-society paper, presenting the views of Radiology Societies in the USA, Canada, Europe, Australia, and New Zealand, defines the potential practical problems and ethical issues surrounding the incorporation of AI into radiological practice. In addition to delineating the main points of concern that developers, regulators, and purchasers of AI tools should consider prior to their introduction into clinical practice, this statement also suggests methods to monitor their stability and safety in clinical use, and their suitability for possible autonomous function. This statement is intended to serve as a useful summary of the practical issues which should be considered by all parties involved in the development of radiology AI resources, and their implementation as clinical tools. This article is simultaneously published in Insights into Imaging (DOI 10.1186/s13244-023-01541-3), Journal of Medical Imaging and Radiation Oncology (DOI 10.1111/1754-9485.13612), Canadian Association of Radiologists Journal (DOI 10.1177/08465371231222229), Journal of the American College of Radiology (DOI 10.1016/j.jacr.2023.12.005), and Radiology: Artificial Intelligence (DOI 10.1148/ryai.230513). Keywords: Artificial Intelligence, Radiology, Automation, Machine Learning Published under a CC BY 4.0 license. ©The Author(s) 2024. Editor's Note: The RSNA Board of Directors has endorsed this article. It has not undergone review or editing by this journal.",No methods found.
2024,https://openalex.org/W4391691781,Medicine,Researchers call for more flexible editorial conduct rather than abruptly adopting only the new MASLD nomenclature,"Global survey of stigma among physicians and patients with nonalcoholic fatty liver diseaseJournal of HepatologyVol. 80Issue 3PreviewPatients with fatty liver disease may experience stigma from the disease or comorbidities. In this cross-sectional study, we aimed to understand stigma among patients with nonalcoholic fatty liver disease (NAFLD)/nonalcoholic steatohepatitis (NASH) and healthcare providers. Full-Text PDF A multisociety Delphi consensus statement on new fatty liver disease nomenclatureJournal of HepatologyVol. 79Issue 6PreviewThe principal limitations of the terms NAFLD and NASH are the reliance on exclusionary confounder terms and the use of potentially stigmatising language. This study set out to determine if content experts and patient advocates were in favour of a change in nomenclature and/or definition. A modified Delphi process was led by three large pan-national liver associations. The consensus was defined a priori as a supermajority (67%) vote. An independent committee of experts external to the nomenclature process made the final recommendation on the acronym and its diagnostic criteria. Full-Text PDF Open AccessDemocracy in ScienceJournal of HepatologyPreviewIt is not standard practice for the Editor-in-Chief to reply to a Letter to the Editor with a dedicated Editorial, but such a response is warranted in the case of the letter from Lonardo A. et al.,1 given its nature and the relevance of its content, which call into question the editorial strategy of the Journal of Hepatology regarding the new nomenclature of ""metabolic fatty liver syndromes"" endorsed by ALEH, AASLD and EASL after a long and difficult consensus process.2 Full-Text PDF Since many of us participated in the multi-society Delphi consensus statement on fatty liver disease nomenclature,[1]Rinella M.E. Lazarus J.V. Ratziu V. et al.A multi-society Delphi consensus statement on new fatty liver disease nomenclature.J Hepatol. 2023; (S0168-8278(23)00418-X)Google Scholar we were interested in a recent article by Younossi and colleagues investigating potential stigma among patients with non-alcoholic fatty liver disease (NAFLD) and among healthcare providers.[2]Younossi Z.M. AlQahtani S.A. Alswat K. et al.Global survey of stigma among physicians and patients with nonalcoholic fatty liver disease.J Hepatol. 2023; (S0168-8278(23)05279-0)Abstract Full Text Full Text PDF Scopus (3) Google Scholar The authors concluded that the perception of NAFLD stigma varied among patients, providers, geographic locations, and sub-specialties.[2]Younossi Z.M. AlQahtani S.A. Alswat K. et al.Global survey of stigma among physicians and patients with nonalcoholic fatty liver disease.J Hepatol. 2023; (S0168-8278(23)05279-0)Abstract Full Text Full Text PDF Scopus (3) Google Scholar Of particular interest, only 8% of patients and 38% of doctors perceived stigma with use of the term.[1]Rinella M.E. Lazarus J.V. Ratziu V. et al.A multi-society Delphi consensus statement on new fatty liver disease nomenclature.J Hepatol. 2023; (S0168-8278(23)00418-X)Google Scholar European patients more commonly (57%) reported being comfortable with either term, with many selecting ""fatty liver disease"". Of note, while the Asia-Pacific region is home to ∼60% of the world's population, only 5% of the study population was from this region.[2]Younossi Z.M. AlQahtani S.A. Alswat K. et al.Global survey of stigma among physicians and patients with nonalcoholic fatty liver disease.J Hepatol. 2023; (S0168-8278(23)05279-0)Abstract Full Text Full Text PDF Scopus (3) Google Scholar Furthermore, only 48% of patients ever disclosed their disease to family members, a proportion which is not different from other diseases.[3]Awareness of family health history as a risk factor for disease --- United States. 2004Google Scholar The 2023 multi-society Delphi consensus statement suggested renaming NAFLD to metabolic dysfunction-associated steatotic liver disease (MASLD), where ""fatty"" was replaced by its synonym ""steatotic"".[2]Younossi Z.M. AlQahtani S.A. Alswat K. et al.Global survey of stigma among physicians and patients with nonalcoholic fatty liver disease.J Hepatol. 2023; (S0168-8278(23)05279-0)Abstract Full Text Full Text PDF Scopus (3) Google Scholar While there was consensus among panelists to change the definition by focusing on metabolic dysfunction and omitting the term ""nonalcoholic"", only 44% of respondents felt that the term"" fatty"" was stigmatizing in the first round. These results, and knowledge of how patients and their healthcare providers perceive stigma when referring to NAFLD, raises the question as to whether patients truly feel stigmatized by the term ""fatty"", particularly when it refers to an organ (the liver), rather than a person. On the contrary, patients might prefer using the term ""fatty"" when describing their liver as it facilitates communication with their healthcare providers on an equal footing. The term ""steatotic"", largely unfamiliar to most patients, could impede a much-needed conversation with their health providers, hindering an understanding of primary causes in many cases, such as overnutrition and/or insufficient physical activity. The notion that ""fatty"" may not be stigmatizing is also supported by liver patients' spokespeople, highlighting that ""in some cultures being fat is regarded as a sign of good health"".[4]Shiha G. Korenjak M. Casanovas T. et al.MAFLD 2022: an ELPA/ALPA/EASO-ECPO joint statement on disease stigma.J Hepatol. 2022; 77: 1717-1719Abstract Full Text Full Text PDF PubMed Scopus (8) Google Scholar Another study reported that this adjective is not invariably stigmatizing.[5]Méndez-Sánchez N. Pal S.C. Fassio E. et al.MAFLD: perceived stigma-a single-center Mexican patient survey.Hepatol Int. 2023; 17: 507-508Crossref PubMed Scopus (3) Google Scholar Another perspective is that patients are unlikely to know that ""steatotic"" is the adjective of steatosis. Importantly, ""steatotic"" cannot be found in any of the three online medical dictionaries nor in old books,[6]Lonardo A. MASLD co-aggregates with HCC in families-names change, fa(c)ts remain.Hepatoma Res. 2023; 9: 50https://doi.org/10.20517/2394-5079.2023.110Crossref Google Scholar implying that persons receiving a diagnosis of MASLD will probably find it difficult to intuitively grasp the essence of the condition and, therefore, largely depend on medical advice for explanations regarding their health status. Moreover, if these persons use 'Google' to search the meaning of ""steatotic"" or ""steatosis"", they will find that it indicates fat accumulation in the liver, a neutral fact that, however, does not fully encapsulate the core concept of the condition. Similarly, physicians frequently experience difficulties in explaining what ""steatotic"" means, other than using the adjective ""fatty"".[7]Clinicians Debate the Usefulness of NAFLD Name Change (medscape.com).Google Scholar Notably, the term ""fatty"" is perfectly acceptable and not considered offensive by many native English speakers. This likely explains why almost one-third of panelists in the Delphi consensus voted against this assertion. In addition, medical terms should communicate clearly to the patient and utilization of the adjective ""steatotic"" goes against efforts to promote better patient engagement and empowerment.[8]Hickmann E. Richter P. Schlieter H. All together now - patient engagement, patient empowerment, and associated terms in personal healthcare.BMC Health Serv Res. 2022; 22: 1116Crossref Scopus (24) Google Scholar Maintaining scientific integrity remains paramount in any ongoing debate surrounding this complex and heterogeneous condition. We assert that additional scientific data need to be accumulated to justify any specific context defined differently: NAFLD, metabolic dysfunction-associated fatty liver disease (MAFLD), and MASLD. The distinction among these nomenclatures, each defined differently, is crucial to further delineate the heterogeneous population and optimize classification for effective management strategies. A large cross-sectional study of individuals who had undergone liver ultrasound and vibration-controlled transient elastography as part of a routine check has shown that the MASLD definition captured more lean patients than the MAFLD definition.[9]Ramírez-Mejía M.M. Jiménez-Gutiérrez C. Eslam M. et al.Breaking new ground: MASLD vs. MAFLD-which holds the key for risk stratification?.Hepatol Int. 2023 Dec 21; (Epub ahead of print. PMID: 38127259)https://doi.org/10.1007/s12072-023-10620-yCrossref Scopus (1) Google Scholar Conversely, individuals with MAFLD exhibited a worse metabolic profile than those with MASLD alone. This study underscores that these nomenclatures and corresponding definitions capture different populations, with MAFLD identifying individuals with a worse metabolic profile while MASLD includes more lean individuals[9]Ramírez-Mejía M.M. Jiménez-Gutiérrez C. Eslam M. et al.Breaking new ground: MASLD vs. MAFLD-which holds the key for risk stratification?.Hepatol Int. 2023 Dec 21; (Epub ahead of print. PMID: 38127259)https://doi.org/10.1007/s12072-023-10620-yCrossref Scopus (1) Google Scholar Hence, the selection of the populations in future research should depend on specific research questions to be investigated. Enforcing the new MASLD nomenclature[10]Malhi H. Brown R.S. Jr. Lim J.K. et al.Precipitous changes in nomenclature and definitions-NAFLD becomes SLD: implications for and expectations of AASLD journals.Hepatology. 2023; 78: 1680-1681Crossref Scopus (0) Google Scholar without considering the context of studies could potentially generate confusion and hinder future research and new insights. Therefore, we propose that journals consider articles independently of the label used by researchers as long as the context is clearly defined. We would appreciate it if the Journal of Hepatology maintained a more flexible approach based on scientific rigor and knowledge-gain when judging future manuscripts. No financial support was received for preparing this manuscript. Stephen Caldwell - acknowledges the following Research Support: Gilead, GenFit, Ipsen, Zydus, Durect, Madrigal, Inventiva, Galectin, Cour, Exact, Ultragenyx, Target, Astra-Zeneca and Royalty Avanos. Robert G. Gish - has performed as Consultant and/or Advisor to (in the last two years): Abacus, Abbott, AbbVie, Albireo, Aligos, Altimunne, Antios, Arrowhead, AstraZeneca, Audentes Therapeutics, Corcept, Dynavax, Effectus, Eiger, Eisai, Enyo, Genentech, Genlantis, Gerson Lehrman Group, Gilead Sciences, GlaxoSmithKline, Helios, HepaTX, HepQuant, Intercept, Janssen, JBS Science, Kinnate Bio, Merck, Precision BioSciences, Pfizer, Seres Therapeutics, Topography Health, Tune Therapeutics, Venatorx, Virion. Current Activity with Scientific or Clinical Advisory Boards: AbbVie, Dynavax, Enyo, Genentech, Genlantis, Gilead, Helios, HepaTX, HepQuant, Intercept, Janssen, Merck, Pfizer, Prodigy. Current Clinical Trials Alliance: Topography Health. Chair Clinical Advisory Board: Prodigy. Advisory Consultant: Diagnostic Companies: Fibronostics, Fujifilm/Wako, Perspectum, Quest, Sonic Incytes. Data Safety Monitoring Board:Arrowhead, CymaBay Therapeutics, Durect, Kezar Life Sciences, Sagimet, Takeda. Consulting Confidentiality Agreements as of 2023: Abacus: 2023-current, Abbvie: 2017-current, Abbott: 2016-current, Access Biologicals: 2016-current, Active Genome Expressed Diagnostics: 2022-current, ADMA Biologics: 2017-current, AEC Partners: 2017-current, Aligos Therapeutics: 2020-current, Arena Pharmaceuticals Inc: 2018-current, Ark Biopharmaceutical Co Ltd: 2022-current, Arrowhead: 2011-current, Arterys Inc: 2018 – current, Alexion: 2018-current, Altimmune: 2020-current, Antios Therapeutics: 2018-current, AprosTx: 2020-current, AstraZeneca: 2023-current, Audentes Therapeutics: 2022-current, Bayer: 2019-current, Bausch/Salix: 2022-current, Chimigen: 2022-current, Cirina: 2017-current, Consumer Health Products Assoc: 2019-current, Corcept: 2023-current, CymaBay Therapeutics Inc: 2020-current, DiaSorin Inc: 2020-current, Dova Pharmaceuticals: 2017-current, DRG Abacus: 2018-current, DURECT Corporation: 2020-current, Dynavax: 2018-current, Echosens: 2020-current, Effectus: 2023-current, Eiger: 2015-current, Eisai: 2018-current, Enyo: 2017-current, Espresso Diagnostics: 2023-current, Exelixis: 2018-current, Fibronostics Inc: 2020-current, Forty-Seven Inc: 2019-current, Fujifilm Wako Diagnostics: 2019-current, Gilead: 2018-current, GlaxoSmithKline: 2022-current, HepQuant: 2018-current, HepaTx: 2017-current, HomShear Therapeutics: 2022-current, IDLogiq: 2020-current, Intellia: 2015-current, Intercept 2010 to current: approved to discuss what is in the public domain, Inotek: 2017-current, Consulting Confidentiality Agreements as of 2022 (continued), Iqvia: 2018-current, Janssen/J&J: 2015-current, JBS: 2022-current, KannaLife: 2019-current, Kezar Life Sciences Inc: 2021-current, Kinnate Bio: 2023-current, LabCorp: 2022-current, Laboratory for Advanced Medicine: 2019-current, Labyrinth Holdings: 2017-current, Life Line Screening: 2020-current, Lilly: 2017-current, Mallinckrodt: 2022-current, MedImmune: 2015-current, Merck: 2017-current, New Enterprise Associates: 2020-current, Ogilvy CommonHealth: 2017-current, Organovo: 2017-current, Orphalan: 2022-current, Patient Connect: 2017-current, Perspectum: 2021-current, Pfizer: 2021-current, Pharmaceutical Research Associates: 2021-current, Precision BioScience: 2023-current, ProdigY Biotech: 2020-current, Prometheus Laboratories: 2017-current, Refuah Solutions: 2020-current, Regulus Therapeutics: 2019-current, Sagimet Inc: 2021-current, Salix: 2019-current, Saol Bermuda Ltd: 2021-current, Seres Therapeutics: 2022-current, Shenzhen HEC Industrial Development: 2021-current, Shionogi Inc: 2017-current, Spring Bank: 2018-current, Syneos: 2022-current, Takeda: 2022-current, Tonghua Anrate Biopharmaceutical: 2021-current, Topography Health: 2020-current, Trimaran: 2017-current, Tune Therapeutics: 2022-current, VBI Vaccine: 2022-current, Venatorx: 2021-current, Viravaxx AG: 2021-current, Virion: 2023-current. These speaker bureau activities focus on HBV, HCV, HDV and liver cancer; specifically, epidemiology, diagnosis, and treatment. In addition, program presentations on vaccination for HBV and management of complications of cirrhosis. Dr. Gish has a speaker's contract to do promotional talks for: AbbVie, AstraZeneca, BMS, Diasorin, Eisai, Genentech, Gilead Sciences Inc., Intercept, Mallinckrodt, VBI Vaccines. Minor stock shareholder (liver space noted only): RiboSciences, CoCrystal. Stock Options: Abacus, Eiger, Genlantis, HepQuant, AngioCrine, HepaTx, JBS Science, Virion. Henning Gronbaek – declares he has received research funding from Intercept, Abbvie, NOVO Nordisk Foundation, Arla, and ADS AIPHIA Development Services AG; advisory board at NOVO Nordisk, Astrazeneca and Ipsen. Norbert Stefan – declares that he has received speaker honoraria from AstraZeneca, Boehringer-Ingelheim, Bristol Myers Squibb, Genkyotex, Intercept Pharma, Lilly, Merck Sharp & Dohme, Novo Nordisk, Pfizer and Sanofi, has received research funding from AstraZeneca, DSM Nutritional Products, F. Hoffmann-La Roche and Sanofi, and has acted as an advisor for AstraZeneca, Genkyotex, GlaxoSmithKline Biologicals, Intercept Pharma, Lilly, Novo Nordisk, Pfizer and Sanofi. Ming-Lung Yu – Research support (grant) from Abbvie, BMS, Gilead, Merck and Roche diagnostics. Consultant of Abbvie, BMS, Gilead, Roche and Roche diagnostics. Speaker of Abbvie, BMS, Eisai, Gilead, Roche and Roche diagnostics. All other authors report no conflicts of interest. Please refer to the accompanying ICMJE disclosure forms for further details. All authors contributed equally to the conceptualization and writing (original draft preparation, review, and editing) of the manuscript. The following are the supplementary data to this article: Download .pdf (1.04 MB) Help with pdf files Multimedia component 1",No methods found.
2024,https://openalex.org/W4391878291,Medicine,Effective lung nodule detection using deep CNN with dual attention mechanisms,"Abstract Novel methods are required to enhance lung cancer detection, which has overtaken other cancer-related causes of death as the major cause of cancer-related mortality. Radiologists have long-standing methods for locating lung nodules in patients with lung cancer, such as computed tomography (CT) scans. Radiologists must manually review a significant amount of CT scan pictures, which makes the process time-consuming and prone to human error. Computer-aided diagnosis (CAD) systems have been created to help radiologists with their evaluations in order to overcome these difficulties. These systems make use of cutting-edge deep learning architectures. These CAD systems are designed to improve lung nodule diagnosis efficiency and accuracy. In this study, a bespoke convolutional neural network (CNN) with a dual attention mechanism was created, which was especially crafted to concentrate on the most important elements in images of lung nodules. The CNN model extracts informative features from the images, while the attention module incorporates both channel attention and spatial attention mechanisms to selectively highlight significant features. After the attention module, global average pooling is applied to summarize the spatial information. To evaluate the performance of the proposed model, extensive experiments were conducted using benchmark dataset of lung nodules. The results of these experiments demonstrated that our model surpasses recent models and achieves state-of-the-art accuracy in lung nodule detection and classification tasks.","<method>convolutional neural network (CNN)</method>, <method>dual attention mechanism</method>, <method>channel attention</method>, <method>spatial attention</method>, <method>global average pooling</method>"
2024,https://openalex.org/W4392004069,Medicine,A precise model for skin cancer diagnosis using hybrid U-Net and improved MobileNet-V3 with hyperparameters optimization,"Abstract Skin cancer is a frequently occurring and possibly deadly disease that necessitates prompt and precise diagnosis in order to ensure efficacious treatment. This paper introduces an innovative approach for accurately identifying skin cancer by utilizing Convolution Neural Network architecture and optimizing hyperparameters. The proposed approach aims to increase the precision and efficacy of skin cancer recognition and consequently enhance patients' experiences. This investigation aims to tackle various significant challenges in skin cancer recognition, encompassing feature extraction, model architecture design, and optimizing hyperparameters. The proposed model utilizes advanced deep-learning methodologies to extract complex features and patterns from skin cancer images. We enhance the learning procedure of deep learning by integrating Standard U-Net and Improved MobileNet-V3 with optimization techniques, allowing the model to differentiate malignant and benign skin cancers. Also substituted the crossed-entropy loss function of the Mobilenet-v3 mathematical framework with a bias loss function to enhance the accuracy. The model's squeeze and excitation component was replaced with the practical channel attention component to achieve parameter reduction. Integrating cross-layer connections among Mobile modules has been proposed to leverage synthetic features effectively. The dilated convolutions were incorporated into the model to enhance the receptive field. The optimization of hyperparameters is of utmost importance in improving the efficiency of deep learning models. To fine-tune the model's hyperparameter, we employ sophisticated optimization methods such as the Bayesian optimization method using pre-trained CNN architecture MobileNet-V3. The proposed model is compared with existing models, i.e., MobileNet, VGG-16, MobileNet-V2, Resnet-152v2 and VGG-19 on the “HAM-10000 Melanoma Skin Cancer dataset"". The empirical findings illustrate that the proposed optimized hybrid MobileNet-V3 model outperforms existing skin cancer detection and segmentation techniques based on high precision of 97.84%, sensitivity of 96.35%, accuracy of 98.86% and specificity of 97.32%. The enhanced performance of this research resulted in timelier and more precise diagnoses, potentially contributing to life-saving outcomes and mitigating healthcare expenditures.","<method>Convolution Neural Network</method>, <method>deep-learning methodologies</method>, <method>Standard U-Net</method>, <method>Improved MobileNet-V3</method>, <method>bias loss function</method>, <method>practical channel attention component</method>, <method>cross-layer connections among Mobile modules</method>, <method>dilated convolutions</method>, <method>Bayesian optimization method</method>, <method>pre-trained CNN architecture MobileNet-V3</method>"
2024,https://openalex.org/W4396802056,Medicine,"""I'm Not Sure, But..."": Examining the Impact of Large Language Models' Uncertainty Expression on User Reliance and Trust","Widely deployed large language models (LLMs) can produce convincing yet incorrect outputs, potentially misleading users who may rely on them as if they were correct. To reduce such overreliance, there have been calls for LLMs to communicate their uncertainty to end users. However, there has been little empirical work examining how users perceive and act upon LLMs' expressions of uncertainty. We explore this question through a large-scale, pre-registered, human-subject experiment (N=404) in which participants answer medical questions with or without access to responses from a fictional LLM-infused search engine. Using both behavioral and self-reported measures, we examine how different natural language expressions of uncertainty impact participants' reliance, trust, and overall task performance. We find that first-person expressions (e.g., ""I'm not sure, but..."") decrease participants' confidence in the system and tendency to agree with the system's answers, while increasing participants' accuracy. An exploratory analysis suggests that this increase can be attributed to reduced (but not fully eliminated) overreliance on incorrect answers. While we observe similar effects for uncertainty expressed from a general perspective (e.g., ""It's not clear, but...""), these effects are weaker and not statistically significant. Our findings suggest that using natural language expressions of uncertainty may be an effective approach for reducing overreliance on LLMs, but that the precise language used matters. This highlights the importance of user testing before deploying LLMs at scale.",No methods found.
2024,https://openalex.org/W4396996682,Medicine,Selection of Germline Genetic Testing Panels in Patients With Cancer: ASCO Guideline,"ASCO Guidelines provide recommendations with comprehensive review and analyses of the relevant literature for each recommendation, following the guideline development process as outlined in the ASCO Guidelines Methodology Manual . ASCO Guidelines follow the ASCO Conflict of Interest Policy for Clinical Practice Guidelines . Clinical Practice Guidelines and other guidance (“Guidance”) provided by ASCO is not a comprehensive or definitive guide to treatment options. It is intended for voluntary use by providers and should be used in conjunction with independent professional judgment. Guidance may not be applicable to all patients, interventions, diseases, or stages of diseases. Guidance is based on review and analysis of relevant literature and is not intended as a statement of the standard of care. ASCO does not endorse third-party drugs, devices, services, or therapies and assumes no responsibility for any harm arising from or related to the use of this information. See complete disclaimer in Appendix 1 and Appendix 2 ( online only) for more. PURPOSE To guide use of multigene panels for germline genetic testing for patients with cancer. METHODS An ASCO Expert Panel convened to develop recommendations on the basis of a systematic review of guidelines, consensus statements, and studies of germline and somatic genetic testing. RESULTS Fifty-two guidelines and consensus statements met eligibility criteria for the primary search; 14 studies were identified for Clinical Question 4. RECOMMENDATIONS Patients should have a family history taken and recorded that includes details of cancers in first- and second-degree relatives and the patient's ethnicity. When more than one gene is relevant based on personal and/or family history, multigene panel testing should be offered. When considering what genes to include in the panel, the minimal panel should include the more strongly recommended genes from Table 1 and may include those less strongly recommended. A broader panel may be ordered when the potential benefits are clearly identified, and the potential harms from uncertain results should be mitigated. Patients who meet criteria for germline genetic testing should be offered germline testing regardless of results from tumor testing. Patients who would not normally be offered germline genetic testing based on personal and/or family history criteria but who have a pathogenic or likely pathogenic variant identified by tumor testing in a gene listed in Table 2 under the outlined circumstances should be offered germline testing. Additional information is available at www.asco.org/molecular-testing-and-biomarkers-guidelines .",No methods found.
2024,https://openalex.org/W4400644938,Medicine,Leveraging artificial intelligence in vaccine development: A narrative review,"Vaccine development stands as a cornerstone of public health efforts, pivotal in curbing infectious diseases and reducing global morbidity and mortality. However, traditional vaccine development methods are often time-consuming, costly, and inefficient. The advent of artificial intelligence (AI) has ushered in a new era in vaccine design, offering unprecedented opportunities to expedite the process. This narrative review explores the role of AI in vaccine development, focusing on antigen selection, epitope prediction, adjuvant identification, and optimization strategies. AI algorithms, including machine learning and deep learning, leverage genomic data, protein structures, and immune system interactions to predict antigenic epitopes, assess immunogenicity, and prioritize antigens for experimentation. Furthermore, AI-driven approaches facilitate the rational design of immunogens and the identification of novel adjuvant candidates with optimal safety and efficacy profiles. Challenges such as data heterogeneity, model interpretability, and regulatory considerations must be addressed to realize the full potential of AI in vaccine development. Integrating emerging technologies, such as single-cell omics and synthetic biology, promises to enhance vaccine design precision and scalability. This review underscores the transformative impact of AI on vaccine development and highlights the need for interdisciplinary collaborations and regulatory harmonization to accelerate the delivery of safe and effective vaccines against infectious diseases.","<method>machine learning</method>, <method>deep learning</method>"
2024,https://openalex.org/W4401615024,Medicine,"Ultra-Flexible, Anti-Freezing, and Adhesive Collagen Fiber-Derived Conductive Organohydrogel E-Skin for Strain, Humidity, Temperature, and Bioelectric Sensing Applications","The development of biomimetic electronic skin (e-skin) has significant value in many fields, including health monitoring, soft robotics, wearable electronic devices, and human-machine interaction. As a potential candidate for e-skin, the application of conductive hydrogel is limited by many factors, such as a complicated fabrication process, insufficient mechanical performance, poor environmental stability, and difficulty in degradation. Here, we adopted a top-down strategy to construct a multifunctional collagen fiber-derived conductive organohydrogel e-skin, in which the collagen fiber scaffold of goatskin was filled with a polyacrylamide network. This organohydrogel displayed excellent fracture stress (2.87 MPa) and fracture strain (542%). It could maintain its multifunctionality even at −20 °C and after long-term storage. Additionally, this organohydrogel demonstrated considerable adhesion and antibacterial properties, allowing it to conform closely to human skin without causing bacterial infection. The e-skin sensors, assembled with this organohydrogel, possessed multiple stimuli-responsive modes to achieve strain, humidity, temperature, and bioelectric responsiveness, allowing for the precise monitoring of body movements, facial expressions, voice communication, and physiological signals. Notably, the discarded e-skin could be effectively degraded under natural environmental conditions. In brief, this study gives new opinions about the development of intelligent multifunctional e-skin and demonstrates a new pathway for the high-value utilization of animal skin.",No methods found.
2024,https://openalex.org/W223178600,Medicine,Occupational Therapy Models for Intervention with Children and Families,"Therapy Models for Intervention with Children and Families explores recent theoretical models that enable occupational therapists to practice and interact with families in a more holistic and occupation-centered manner. This comprehensive and dynamic text offers the latest information on viewing the broader contexts of environment and family in order to meet diverse occupational needs in a range of settings. Edited by Sandee Dunbar, the text presents a variety of case scenarios that feature culturally diverse populations and varying diagnoses of children with occupational needs. With contributions from 11 renowned leaders in occupational therapy, this comprehensive text is designed to increase awareness and understanding of theoretical models and their relationship to current occupational therapy practice with today's children and families. Inside this dynamic text, traditional frames of reference in pediatric practice are explored, including Sensory Integration and Neurodevelopmental Therapy. Some current theoretical models discussed include the Model of Human Occupation, the Person-Environment-Occupation Model, the Ecology of Human Performance Model, and the Occupational Adaptation Model. Incorporated throughout the text is the new Occupational Therapy Practice Framework. Employing a practical approach to this significant aspect of pediatric practice in occupational therapy, Therapy Models for Intervention with Children and Families is an invaluable tool for students at all curriculum levels.",No methods found.
2024,https://openalex.org/W4390604371,Medicine,Evaluation based on Relative Utility and Nonlinear Standardization (ERUNS) Method for Comparing Firm Performance in Energy Sector,"The present paper introduces a new multi-criteria decision making (MCDM) model such as Evaluation based on Relative Utility and Nonlinear Standardization (ERUNS). The proposed method provides a number of unique features such as use of non-linear standardization combined with a new definition for calculating utility values of the alternatives and design flexibility. The new method is applied to compare the firm performances using Economic, Environment, Social and Governance (EESG) framework. Over the last few decades alongside economic gain, the firms have been equally emphasizing on environmental protection, social values and good governance. This paper aims to bring in the economic aspect in terms of market capitalization and enterprise value while uses rating scores for environment, social and governance to put forth an extended performance measurement framework. A set of eight leading firms belonging to the energy sector listed to NSE, India has been considered for the period FY 2022-23 as a case study. The criteria weights are calculated using the Logarithmic Percentage Change-driven Objective Weighting (LOPCOW) method. It is observed that ERUNS provides a stable and reliable ranking result free from the rank reversal issue. This paper shall be of interest to both analysts and practitioners in terms of its robust method and practical performance assessment framework.",No methods found.
2024,https://openalex.org/W4391361574,Medicine,Efficient pneumonia detection using Vision Transformers on chest X-rays,"Abstract Pneumonia is a widespread and acute respiratory infection that impacts people of all ages. Early detection and treatment of pneumonia are essential for avoiding complications and enhancing clinical results. We can reduce mortality, improve healthcare efficiency, and contribute to the global battle against a disease that has plagued humanity for centuries by devising and deploying effective detection methods. Detecting pneumonia is not only a medical necessity but also a humanitarian imperative and a technological frontier. Chest X-rays are a frequently used imaging modality for diagnosing pneumonia. This paper examines in detail a cutting-edge method for detecting pneumonia implemented on the Vision Transformer (ViT) architecture on a public dataset of chest X-rays available on Kaggle. To acquire global context and spatial relationships from chest X-ray images, the proposed framework deploys the ViT model, which integrates self-attention mechanisms and transformer architecture. According to our experimentation with the proposed Vision Transformer-based framework, it achieves a higher accuracy of 97.61%, sensitivity of 95%, and specificity of 98% in detecting pneumonia from chest X-rays. The ViT model is preferable for capturing global context, comprehending spatial relationships, and processing images that have different resolutions. The framework establishes its efficacy as a robust pneumonia detection solution by surpassing convolutional neural network (CNN) based architectures.","<method>Vision Transformer (ViT)</method>, <method>self-attention mechanisms</method>, <method>transformer architecture</method>, <method>convolutional neural network (CNN) based architectures</method>"
2024,https://openalex.org/W4391671371,Medicine,Collaborative threat intelligence: Enhancing IoT security through blockchain and machine learning integration,"Ensuring robust security in the Internet of Things (IoT) landscape is of paramount importance. This research article presents a novel approach to enhance IoT security by leveraging collaborative threat intelligence and integrating blockchain technology with machine learning (ML) models. The iOS application acts as a central control centre, facilitating the reporting and sharing of detected threats. The shared threat data is securely stored on a blockchain network, enabling ML models to access and learn from a diverse range of threat scenarios. The research focuses on implementing Random Forest, Decision Tree classifier, Ensemble, LSTM, and CNN models on the IoT23 dataset within the context of a Collaborative Threat Intelligence Framework for IoT Security. Through an iterative process, the models' accuracy is improved by reducing false negatives through the collaborative threat intelligence system. The article investigates the implementation details, privacy considerations, and the seamless integration of ML-based techniques for continuous model improvement. Experimental evaluations on the IoT23 dataset demonstrate the effectiveness of the proposed system in enhancing IoT security and mitigating potential threats. The research contributes to the advancement of collaborative threat intelligence and blockchain technology in the context of IoT security, paving the way for more secure and reliable IoT deployments.","<method>Random Forest</method>, <method>Decision Tree classifier</method>, <method>Ensemble</method>, <method>LSTM</method>, <method>CNN</method>"
2024,https://openalex.org/W4392520536,Medicine,Antithrombotic Therapy for VTE Disease,"The American College of Chest Physicians (CHEST) Antithrombotic Therapy for Venous Thromboembolism Disease evidence-based guidelines are now updated in a more frequent, focused manner. Guidance statements from the most recent full guidelines and two subsequent updates have not been gathered into a single source. An international panel of experts with experience in prior antithrombotic therapy guideline development reviewed the 2012 CHEST antithrombotic therapy guidelines and its two subsequent updates. All guideline statements and their associated patient, intervention, comparator, and outcome questions were assembled. A modified Delphi process was used to select statements considered relevant to current clinical care. The panel further endorsed minor phrasing changes to match the standard language for guidance statements using the modified Grading of Recommendations, Assessment, Development, and Evaluations (ie, GRADE) format endorsed by the CHEST Guidelines Oversight Committee. The panel appended comments after statements deemed as relevant, including suggesting that statements be updated in future guidelines because of interval evidence. We include 58 guidance statements from prior versions of the antithrombotic therapy guidelines, with updated phrasing as needed to adhere to contemporary nomenclature. Statements were classified as strong or weak recommendations based on high-certainty, moderate-certainty, and low-certainty evidence using GRADE methodology. The panel suggested that five statements are no longer relevant to current practice. As CHEST continues to update guidance statements relevant to antithrombotic therapy for VTE disease, this article serves as a unified collection of currenrtly relevant statements from the preceding three guidelines. Suggestions have been made to update specific statements in future publications.",No methods found.
2024,https://openalex.org/W4394571531,Medicine,Use of artificial intelligence in critical care: opportunities and obstacles,"Abstract Background Perhaps nowhere else in the healthcare system than in the intensive care unit environment are the challenges to create useful models with direct time-critical clinical applications more relevant and the obstacles to achieving those goals more massive. Machine learning-based artificial intelligence (AI) techniques to define states and predict future events are commonplace activities of modern life. However, their penetration into acute care medicine has been slow, stuttering and uneven. Major obstacles to widespread effective application of AI approaches to the real-time care of the critically ill patient exist and need to be addressed. Main body Clinical decision support systems (CDSSs) in acute and critical care environments support clinicians, not replace them at the bedside. As will be discussed in this review, the reasons are many and include the immaturity of AI-based systems to have situational awareness, the fundamental bias in many large databases that do not reflect the target population of patient being treated making fairness an important issue to address and technical barriers to the timely access to valid data and its display in a fashion useful for clinical workflow. The inherent “black-box” nature of many predictive algorithms and CDSS makes trustworthiness and acceptance by the medical community difficult. Logistically, collating and curating in real-time multidimensional data streams of various sources needed to inform the algorithms and ultimately display relevant clinical decisions support format that adapt to individual patient responses and signatures represent the efferent limb of these systems and is often ignored during initial validation efforts. Similarly, legal and commercial barriers to the access to many existing clinical databases limit studies to address fairness and generalizability of predictive models and management tools. Conclusions AI-based CDSS are evolving and are here to stay. It is our obligation to be good shepherds of their use and further development.",No methods found.
2024,https://openalex.org/W4396905964,Medicine,A systematic review and meta-analysis of artificial intelligence versus clinicians for skin cancer diagnosis,"Scientific research of artificial intelligence (AI) in dermatology has increased exponentially. The objective of this study was to perform a systematic review and meta-analysis to evaluate the performance of AI algorithms for skin cancer classification in comparison to clinicians with different levels of expertise. Based on PRISMA guidelines, 3 electronic databases (PubMed, Embase, and Cochrane Library) were screened for relevant articles up to August 2022. The quality of the studies was assessed using QUADAS-2. A meta-analysis of sensitivity and specificity was performed for the accuracy of AI and clinicians. Fifty-three studies were included in the systematic review, and 19 met the inclusion criteria for the meta-analysis. Considering all studies and all subgroups of clinicians, we found a sensitivity (Sn) and specificity (Sp) of 87.0% and 77.1% for AI algorithms, respectively, and a Sn of 79.78% and Sp of 73.6% for all clinicians (overall); differences were statistically significant for both Sn and Sp. The difference between AI performance (Sn 92.5%, Sp 66.5%) vs. generalists (Sn 64.6%, Sp 72.8%), was greater, when compared with expert clinicians. Performance between AI algorithms (Sn 86.3%, Sp 78.4%) vs expert dermatologists (Sn 84.2%, Sp 74.4%) was clinically comparable. Limitations of AI algorithms in clinical practice should be considered, and future studies should focus on real-world settings, and towards AI-assistance.",No methods found.
2024,https://openalex.org/W4399537055,Medicine,Assessing the efficacy of 3D Dual-CycleGAN model for multi-contrast MRI synthesis,"Abstract Background This research presents a novel methodology for synthesizing 3D multi-contrast MRI images utilizing the 3D Dual-CycleGAN architecture. The performance of the model is evaluated on different MRI sequences, including T1-weighted (T1W), T1-weighted contrast-enhanced (T1c), T2-weighted (T2W), and FLAIR sequences. Results Our approach demonstrates proficient learning capabilities in transforming T1W images into target modalities. The proposed framework encompasses a combination of different loss functions including voxel-wise, gradient difference, perceptual, and structural similarity losses. These loss components, along with adversarial and dual cycle-consistency losses, contribute significantly to realistic and accurate syntheses. Evaluation metrics including MAE, PMAE, RMSE, PCC, PSNR, and SSIM are employed to assess the fidelity of synthesized images compared to their ground truth counterparts. Empirical results indicate the effectiveness of the 3D Dual-CycleGAN model in generating T1c images from T1W inputs with minimal average discrepancies (MAE of 2.8 ± 2.61) and strong similarity (SSIM of 0.82 ± 0.28). Furthermore, the synthesis of T2W and FLAIR images yields promising outcomes, demonstrating acceptable average discrepancies (MAE of 3.87 ± 3.32 for T2W and 3.82 ± 3.32 for FLAIR) and reasonable similarities (SSIM of 0.82 ± 0.28 for T2W and 0.80 ± 0.29 for FLAIR) relative to the original images. Conclusions These findings underscore the efficacy of the 3D Dual-CycleGAN model in generating high-fidelity images, with significant implications for diverse applications in the field of medical imaging.",<method>3D Dual-CycleGAN</method>
2024,https://openalex.org/W4400981456,Medicine,Multimodal data integration for oncology in the era of deep neural networks: a review,"Cancer research encompasses data across various scales, modalities, and resolutions, from screening and diagnostic imaging to digitized histopathology slides to various types of molecular data and clinical records. The integration of these diverse data types for personalized cancer care and predictive modeling holds the promise of enhancing the accuracy and reliability of cancer screening, diagnosis, and treatment. Traditional analytical methods, which often focus on isolated or unimodal information, fall short of capturing the complex and heterogeneous nature of cancer data. The advent of deep neural networks has spurred the development of sophisticated multimodal data fusion techniques capable of extracting and synthesizing information from disparate sources. Among these, Graph Neural Networks (GNNs) and Transformers have emerged as powerful tools for multimodal learning, demonstrating significant success. This review presents the foundational principles of multimodal learning including oncology data modalities, taxonomy of multimodal learning, and fusion strategies. We delve into the recent advancements in GNNs and Transformers for the fusion of multimodal data in oncology, spotlighting key studies and their pivotal findings. We discuss the unique challenges of multimodal learning, such as data heterogeneity and integration complexities, alongside the opportunities it presents for a more nuanced and comprehensive understanding of cancer. Finally, we present some of the latest comprehensive multimodal pan-cancer data sources. By surveying the landscape of multimodal data integration in oncology, our goal is to underline the transformative potential of multimodal GNNs and Transformers. Through technological advancements and the methodological innovations presented in this review, we aim to chart a course for future research in this promising field. This review may be the first that highlights the current state of multimodal modeling applications in cancer using GNNs and transformers, presents comprehensive multimodal oncology data sources, and sets the stage for multimodal evolution, encouraging further exploration and development in personalized cancer care.","<method>deep neural networks</method>, <method>Graph Neural Networks (GNNs)</method>, <method>Transformers</method>"
2024,https://openalex.org/W4390793099,Medicine,Generative artificial intelligence in mental health care: potential benefits and current challenges,"The potential of artificial intelligence (AI) in health care is being intensively discussed, given the easy accessibility of programs such as ChatGPT. While it is usually acknowledged that this technology will never replace clinicians, we should be aware of imminent changes around AI supporting: a) routine office work such as billing, b) clinical documentation, c) medical education, and d) routine monitoring of symptoms. These changes will likely happen rapidly. In summer 2023, the largest electronic medical records provider in the US, Epic Systems, announced that it is partnering with OpenAI to integrate ChatGPT technology1. The profound impact that these changes will have on the context and delivery of mental health care warrants attention, but often overlooked is the more fundamental question of changes to the nature of mental health care in terms of improving prevention, diagnosis and treatments. Research on non-clinical samples suggests that AI may augment text-based support programs, but assessments have focused on perceived empathy rather than clinical outcomes. While the former is an important development, it is only the first step towards progressing from feasibility to acceptability and from efficacy to effectiveness. A century of accessible self-help books, more than 60 years of mental health chatbots (Eliza was created in 1959), nearly 30 years of home Internet with access to free online cognitive behavioral therapy and chatrooms, over a decade of smartphone-based mental health apps and text message support programs, and the recent expansion of video-based telehealth, together highlight that access to resources is not a panacea for prevention. The true target for AI preventive programs should not be replicating previous work but rather developing new models able to provide personalized, environmentally and culturally responsive, and scalable support that works effectively for users across all countries and regions. Computer-based diagnosis programs have existed for decades and have not transformed care. Many studies to date suggest that new AI models can diagnose mental health conditions in the context of standardized exam questions or simple case examples2. This is important research, and there is evidence of improvement with new models, but the approach belies the clinical reality of how diagnosis is made or utilized in clinical care. The future of diagnosis in the 21st century can be more inclusive, draw from diverse sources of information, and be outcomes-driven. The true target for AI programs will be to integrate information from clinical exam, patient self-report, digital phenotyping, genetics, neuroimaging, and clinical judgement into novel diagnostic categories that may better reflect the underlying nature of mental illness and offer practical value in guiding effective treatments and cures. Currently, there is a lack of evidence about how AI programs can guide mental health treatment. Impressive studies show that AI can help select psychiatric medications3, but these studies often rely on complete and labelled data sets, which is not the clinical reality, and lack prospective validation. A recent study in oncology points to an emerging challenge: when ChatGPT 3.5 was asked to provide cancer treatment recommendations, the chatbot was most likely to mix incorrect recommendations with correct ones, making errors difficult to detect even for experts4. The true target for AI programs will be in realizing the potential of personalized psychiatry and guiding treatment that will improve outcomes for patients. For AI to support prevention, diagnosis and treatment there are clear next steps. Utilizing a well-established framework for technology evaluation in mental health, these include advances in equity, privacy, evidence, clinical engagement, and interoperability5. Since current datasets used in AI models are trained on non-psychiatric sources, today all major AI chatbots clearly state that their products must not be used for clinical purposes. Even with proper training, risks of AI bias must be carefully explored, given numerous recent examples of clear harm in other medical fields6. A rapid glance at images generated by an AI program when asked to draw ""schizophrenia""7 visualized the extent to which extreme stigma and harmful bias have informed what current AI models conceptualize as mental illness. A second area of focus is privacy, with current AI chatbots unable to protect personal health information. Large language models are trained on data scraped from the Internet which may encompass sensitive personal health information. The European Union is exploring whether OpenAI's ChatGPT complies with the General Data Protection Regulation's requirement that informed consent or strong public health justifications are met to process sensitive information. In the US, privacy issues emerge with the risk that clinicians may input sensitive patient data into chatbots. This problem caused the American Psychiatric Association to release an advisory in summer 2023 noting that clinicians should not enter any patient information into any AI chatbot8. In order to allow integration into health care, authorities will need to determine whether chatbots meet privacy regulations. A third focus is the next generation of evidence, as current studies that suggest the ability of chatbots to perform binary classification of diagnosis (e.g., presence of any depression or none) offer limited practical clinical value. The potential to offer differential diagnosis based on multimodal data sources (e.g., medical records, genetic results, neuroimaging data) remains appealing but as yet untested. Evidence of the true potential for supporting care remains elusive, and the harm caused to the eating disorder community by the public release (and rapid repudiation within one week) of the Tessa chatbot highlights that more robust evidence is necessary than that currently collected9. Like other medical devices, evidence of clinical claims should be supported by high-quality randomized controlled trials that employ digital placebo groups (e.g., a non-therapeutic chatbot). Fourth, a focus on engagement is critical. We already know that engagement with mental health apps has been minimal, and can learn from those experiences. We are aware that engagement is not only a patient challenge, as clinician uptake of this technology is also a widely cited barrier and will require careful attention to implementation frameworks. These consistently highlight that, while innovation is important, there must be a concomitant focus on the recipients (i.e., education and training for both patients and clinicians) as well as on the context of care (e.g., regulation, reimbursement, clinical workflow). The principles of the non-adoption, abandonment, scale-up, spread and sustainability (NASSS) framework remain relevant in AI and offer tangible targets for avoiding failure. Fifth and related, AI models need to be well integrated into the health care system. The era of standalone or self-help programs is rapidly ending, with the realization that such tools may often fragment care, cannot scale, and are rarely sustainable. This requires, in addition to data interoperability, careful designing of how AI interacts with all aspects of the health care system. There is a need for collaboration not only with clinicians but also with patients, family members, administrators, regulators, and of course AI developers. While generative AI technologies continue to evolve, the clinical community today has the opportunity to evolve as well. Clinicians do not need to become experts in generative AI, but a new focus on education about current capabilities, risks and benefits can be a tangible first step towards more informed decision-making around what role these technologies can and should play in care.",No methods found.
2024,https://openalex.org/W4390870882,Medicine,A domain adaptation approach to damage classification with an application to bridge monitoring,"Data-driven machine-learning algorithms generally suffer from a lack of labelled health-state data, mainly those referring to damage conditions. To address such an issue, population-based structural health monitoring seeks to enrich the original dataset by transferring knowledge from a population of monitored structures. Within this context, this paper presents a transfer learning approach, based on domain adaptation, to leverage information from completely-labelled bridge structure data to accurately predict new instances of an unknown target domain. Since intrinsic structural differences may cause distribution shifts, domain adaptation attempts to minimise the distance between the domains and to learn a mapping within a shared feature space. Specifically, the methodology involves the long-term acquisition of natural frequencies from several structural scenarios. Such damage-sensitive features are then aligned via domain adaptation so that a machine-learning algorithm can effectively utilise the labelled source domain data and generalise well to the unlabelled target-domain data. The described procedure is applied to two case studies, including the Z24 and the S101 benchmark bridges and their finite element models, respectively. The results demonstrate the successful exchange of health-state labels to identify the damage class within a population of bridges equipped with SHM systems, showing potential to reduce computational efforts and to deal with scarce or poor data sets in application to bridge network monitoring.","<method>transfer learning</method>, <method>domain adaptation</method>, <method>machine-learning algorithm</method>"
2024,https://openalex.org/W4391317914,Medicine,Exploring data mining and machine learning in gynecologic oncology,"Abstract Gynecologic (GYN) malignancies are gaining new and much-needed attention, perpetually fueling literature. Intra-/inter-tumor heterogeneity and “frightened” global distribution by race, ethnicity, and human development index, are pivotal clues to such ubiquitous interest. To advance “precision medicine” and downplay the heavy burden, data mining (DM) is timely in clinical GYN oncology. No consolidated work has been conducted to examine the depth and breadth of DM applicability as an adjunct to GYN oncology, emphasizing machine learning (ML)-based schemes. This systematic literature review (SLR) synthesizes evidence to fill knowledge gaps, flaws, and limitations. We report this SLR in compliance with Kitchenham and Charters’ guidelines. Defined research questions and PICO crafted a search string across five libraries: PubMed, IEEE Xplore, ScienceDirect, SpringerLink, and Google Scholar—over the past decade. Of the 3499 potential records, 181 primary studies were eligible for in-depth analysis. A spike (60.53%) corollary to cervical neoplasms is denoted onward 2019, predominantly featuring empirical solution proposals drawn from cohorts. Medical records led (23.77%, 53 art.). DM-ML in use is primarily built on neural networks (127 art.), appoint classification (73.19%, 172 art.) and diagnoses (42%, 111 art.), all devoted to assessment. Summarized evidence is sufficient to guide and support the clinical utility of DM schemes in GYN oncology. Gaps persist, inculpating the interoperability of single-institute scrutiny. Cross-cohort generalizability is needed to establish evidence while avoiding outcome reporting bias to locally, site-specific trained models. This SLR is exempt from ethics approval as it entails published articles.","<method>data mining (DM)</method>, <method>machine learning (ML)-based schemes</method>, <method>neural networks</method>"
2024,https://openalex.org/W4391811765,Medicine,Usefulness and Accuracy of Artificial Intelligence Chatbot Responses to Patient Questions for Neurosurgical Procedures,"BACKGROUND AND OBJECTIVES: The Internet has become a primary source of health information, leading patients to seek answers online before consulting health care providers. This study aims to evaluate the implementation of Chat Generative Pre-Trained Transformer (ChatGPT) in neurosurgery by assessing the accuracy and helpfulness of artificial intelligence (AI)–generated responses to common postsurgical questions. METHODS: A list of 60 commonly asked questions regarding neurosurgical procedures was developed. ChatGPT-3.0, ChatGPT-3.5, and ChatGPT-4.0 responses to these questions were recorded and graded by numerous practitioners for accuracy and helpfulness. The understandability and actionability of the answers were assessed using the Patient Education Materials Assessment Tool. Readability analysis was conducted using established scales. RESULTS: A total of 1080 responses were evaluated, equally divided among ChatGPT-3.0, 3.5, and 4.0, each contributing 360 responses. The mean helpfulness score across the 3 subsections was 3.511 ± 0.647 while the accuracy score was 4.165 ± 0.567. The Patient Education Materials Assessment Tool analysis revealed that the AI-generated responses had higher actionability scores than understandability. This indicates that the answers provided practical guidance and recommendations that patients could apply effectively. On the other hand, the mean Flesch Reading Ease score was 33.5, suggesting that the readability level of the responses was relatively complex. The Raygor Readability Estimate scores ranged within the graduate level, with an average score of the 15th grade. CONCLUSION: The artificial intelligence chatbot's responses, although factually accurate, were not rated highly beneficial, with only marginal differences in perceived helpfulness and accuracy between ChatGPT-3.0 and ChatGPT-3.5 versions. Despite this, the responses from ChatGPT-4.0 showed a notable improvement in understandability, indicating enhanced readability over earlier versions.","<method>Chat Generative Pre-Trained Transformer (ChatGPT-3.0)</method>, <method>Chat Generative Pre-Trained Transformer (ChatGPT-3.5)</method>, <method>Chat Generative Pre-Trained Transformer (ChatGPT-4.0)</method>"
2024,https://openalex.org/W4392200867,Medicine,Revolutionizing core muscle analysis in female sexual dysfunction based on machine learning,"Abstract The purpose of this study is to investigate the role of core muscles in female sexual dysfunction (FSD) and develop comprehensive rehabilitation programs to address this issue. We aim to answer the following research questions: what are the roles of core muscles in FSD, and how can machine and deep learning models accurately predict changes in core muscles during FSD? FSD is a common condition that affects women of all ages, characterized by symptoms such as decreased libido, difficulty achieving orgasm, and pain during intercourse. We conducted a comprehensive analysis of changes in core muscles during FSD using machine and deep learning. We evaluated the performance of multiple models, including multi-layer perceptron (MLP), long short-term memory (LSTM), convolutional neural network (CNN), recurrent neural network (RNN), ElasticNetCV, random forest regressor, SVR, and Bagging regressor. The models were evaluated based on mean squared error (MSE), mean absolute error (MAE), and R-squared (R 2 ) score. Our results show that CNN and random forest regressor are the most accurate models for predicting changes in core muscles during FSD. CNN achieved the lowest MSE (0.002) and the highest R 2 score (0.988), while random forest regressor also performed well with an MSE of 0.0021 and an R 2 score of 0.9905. Our study demonstrates that machine and deep learning models can accurately predict changes in core muscles during FSD. The neglected core muscles play a significant role in FSD, highlighting the need for comprehensive rehabilitation programs that address these muscles. By developing these programs, we can improve the quality of life for women with FSD and help them achieve optimal sexual health.","<method>multi-layer perceptron (MLP)</method>, <method>long short-term memory (LSTM)</method>, <method>convolutional neural network (CNN)</method>, <method>recurrent neural network (RNN)</method>, <method>ElasticNetCV</method>, <method>random forest regressor</method>, <method>SVR</method>, <method>Bagging regressor</method>"
2024,https://openalex.org/W4392241969,Medicine,All models are wrong and yours are useless: making clinical prediction models impactful for patients,"All models are wrong and yours are useless: making clinical prediction models impactful for patients Florian MarkowetzCheck for updates Most published clinical prediction models are never used in clinical practice and there is a huge gap between academic research and clinical implementation.Here, I propose ways for academic researchers to be proactive partners in improving clinical practice and to design models in ways that ultimately benefit patients.""All models are wrong, but some are useful"" is an aphorism attributed to the statistician George Box.There is humility in claiming your model is wrong, but there is also bravado in implying your model might be useful.And, honestly, I don't think it is.I think your model is useless.How would I know?I don't even know who you are.Well, it is a bet.A bet I am willing to take because the odds are ridiculously in my favour.I will explain what I mean in the context of clinical prediction models.My points apply to a wide range of preclinical models, both computational and biological, but my own core expertise is with clinical prediction tools.These are computational models from statistics, machine learning or AI that try to predict clinically relevant variables and ultimately aim to help doctors to treat patients better.The papers describing them make claims like ""this model can be used in the clinic""; generally softened with words like ""might"", ""could"", ""potential"", ""promise"", or other techniques to reduce accountability.The Box quote offers a yardstick to measure the success of these models; not by how correctly they describe reality but by how useful they are in helping patients.And in general, almost none of these tools ever help anyone.There is a wealth of systematic reviews in different fields to show how many models have been proposed and how few have even been validated, let alone been adopted in the clinic.For example, 408(!) models for chronic obstructive pulmonary disease were systematically reviewed 1 and as a summary the authors bleakly note ""several methodological pitfalls in their development and a low rate of external validation"".And whatever biomedical area you work in, your experiences will mirror this resultmany novel prediction models, little help for patients.I believe that a model designed to be used for patients is useless unless it is actually used for patients.",No methods found.
2024,https://openalex.org/W4392356867,Medicine,The SAFE procedure: a practical stopping heuristic for active learning-based screening in systematic reviews and meta-analyses,"Abstract Active learning has become an increasingly popular method for screening large amounts of data in systematic reviews and meta-analyses. The active learning process continually improves its predictions on the remaining unlabeled records, with the goal of identifying all relevant records as early as possible. However, determining the optimal point at which to stop the active learning process is a challenge. The cost of additional labeling of records by the reviewer must be balanced against the cost of erroneous exclusions. This paper introduces the SAFE procedure, a practical and conservative set of stopping heuristics that offers a clear guideline for determining when to end the active learning process in screening software like ASReview. The eclectic mix of stopping heuristics helps to minimize the risk of missing relevant papers in the screening process. The proposed stopping heuristic balances the costs of continued screening with the risk of missing relevant records, providing a practical solution for reviewers to make informed decisions on when to stop screening. Although active learning can significantly enhance the quality and efficiency of screening, this method may be more applicable to certain types of datasets and problems. Ultimately, the decision to stop the active learning process depends on careful consideration of the trade-off between the costs of additional record labeling against the potential errors of the current model for the specific dataset and context.",<method>active learning</method>
2024,https://openalex.org/W4392391621,Medicine,Autonomous Artificial Intelligence vs Artificial Intelligence–Assisted Human Optical Diagnosis of Colorectal Polyps: A Randomized Controlled Trial,"Background & AimsArtificial intelligence (AI)–based optical diagnosis systems (CADx) have been developed to allow pathology prediction of colorectal polyps during colonoscopies. However, CADx systems have not yet been validated for autonomous performance. Therefore, we conducted a trial comparing autonomous AI to AI-assisted human (AI-H) optical diagnosis.MethodsWe performed a randomized noninferiority trial of patients undergoing elective colonoscopies at 1 academic institution. Patients were randomized into (1) autonomous AI-based CADx optical diagnosis of diminutive polyps without human input or (2) diagnosis by endoscopists who performed optical diagnosis of diminutive polyps after seeing the real-time CADx diagnosis. The primary outcome was accuracy in optical diagnosis in both arms using pathology as the gold standard. Secondary outcomes included agreement with pathology for surveillance intervals.ResultsA total of 467 patients were randomized (238 patients/158 polyps in the autonomous AI group and 229 patients/179 polyps in the AI-H group). Accuracy for optical diagnosis was 77.2% (95% confidence interval [CI], 69.7–84.7) in the autonomous AI group and 72.1% (95% CI, 65.5–78.6) in the AI-H group (P = .86). For high-confidence diagnoses, accuracy for optical diagnosis was 77.2% (95% CI, 69.7–84.7) in the autonomous AI group and 75.5% (95% CI, 67.9–82.0) in the AI-H group. Autonomous AI had statistically significantly higher agreement with pathology-based surveillance intervals compared to AI-H (91.5% [95% CI, 86.9–96.1] vs 82.1% [95% CI, 76.5–87.7]; P = .016).ConclusionsAutonomous AI-based optical diagnosis exhibits noninferior accuracy to endoscopist-based diagnosis. Both autonomous AI and AI-H exhibited relatively low accuracy for optical diagnosis; however, autonomous AI achieved higher agreement with pathology-based surveillance intervals. (ClinicalTrials.gov, Number NCT05236790) Artificial intelligence (AI)–based optical diagnosis systems (CADx) have been developed to allow pathology prediction of colorectal polyps during colonoscopies. However, CADx systems have not yet been validated for autonomous performance. Therefore, we conducted a trial comparing autonomous AI to AI-assisted human (AI-H) optical diagnosis. We performed a randomized noninferiority trial of patients undergoing elective colonoscopies at 1 academic institution. Patients were randomized into (1) autonomous AI-based CADx optical diagnosis of diminutive polyps without human input or (2) diagnosis by endoscopists who performed optical diagnosis of diminutive polyps after seeing the real-time CADx diagnosis. The primary outcome was accuracy in optical diagnosis in both arms using pathology as the gold standard. Secondary outcomes included agreement with pathology for surveillance intervals. A total of 467 patients were randomized (238 patients/158 polyps in the autonomous AI group and 229 patients/179 polyps in the AI-H group). Accuracy for optical diagnosis was 77.2% (95% confidence interval [CI], 69.7–84.7) in the autonomous AI group and 72.1% (95% CI, 65.5–78.6) in the AI-H group (P = .86). For high-confidence diagnoses, accuracy for optical diagnosis was 77.2% (95% CI, 69.7–84.7) in the autonomous AI group and 75.5% (95% CI, 67.9–82.0) in the AI-H group. Autonomous AI had statistically significantly higher agreement with pathology-based surveillance intervals compared to AI-H (91.5% [95% CI, 86.9–96.1] vs 82.1% [95% CI, 76.5–87.7]; P = .016). Autonomous AI-based optical diagnosis exhibits noninferior accuracy to endoscopist-based diagnosis. Both autonomous AI and AI-H exhibited relatively low accuracy for optical diagnosis; however, autonomous AI achieved higher agreement with pathology-based surveillance intervals. (ClinicalTrials.gov, Number NCT05236790)","<method>autonomous AI-based CADx optical diagnosis</method>, <method>AI-assisted human (AI-H) optical diagnosis</method>"
2024,https://openalex.org/W4392565345,Medicine,Empowering personalized pharmacogenomics with generative AI solutions,"Abstract Objective This study evaluates an AI assistant developed using OpenAI’s GPT-4 for interpreting pharmacogenomic (PGx) testing results, aiming to improve decision-making and knowledge sharing in clinical genetics and to enhance patient care with equitable access. Materials and Methods The AI assistant employs retrieval-augmented generation (RAG), which combines retrieval and generative techniques, by harnessing a knowledge base (KB) that comprises data from the Clinical Pharmacogenetics Implementation Consortium (CPIC). It uses context-aware GPT-4 to generate tailored responses to user queries from this KB, further refined through prompt engineering and guardrails. Results Evaluated against a specialized PGx question catalog, the AI assistant showed high efficacy in addressing user queries. Compared with OpenAI’s ChatGPT 3.5, it demonstrated better performance, especially in provider-specific queries requiring specialized data and citations. Key areas for improvement include enhancing accuracy, relevancy, and representative language in responses. Discussion The integration of context-aware GPT-4 with RAG significantly enhanced the AI assistant’s utility. RAG’s ability to incorporate domain-specific CPIC data, including recent literature, proved beneficial. Challenges persist, such as the need for specialized genetic/PGx models to improve accuracy and relevancy and addressing ethical, regulatory, and safety concerns. Conclusion This study underscores generative AI’s potential for transforming healthcare provider support and patient accessibility to complex pharmacogenomic information. While careful implementation of large language models like GPT-4 is necessary, it is clear that they can substantially improve understanding of pharmacogenomic data. With further development, these tools could augment healthcare expertise, provider productivity, and the delivery of equitable, patient-centered healthcare services.","<method>retrieval-augmented generation (RAG)</method>, <method>context-aware GPT-4</method>, <method>prompt engineering</method>"
2024,https://openalex.org/W4392783658,Medicine,AI applications in musculoskeletal imaging: a narrative review,"This narrative review focuses on clinical applications of artificial intelligence (AI) in musculoskeletal imaging. A range of musculoskeletal disorders are discussed using a clinical-based approach, including trauma, bone age estimation, osteoarthritis, bone and soft-tissue tumors, and orthopedic implant-related pathology. Several AI algorithms have been applied to fracture detection and classification, which are potentially helpful tools for radiologists and clinicians. In bone age assessment, AI methods have been applied to assist radiologists by automatizing workflow, thus reducing workload and inter-observer variability. AI may potentially aid radiologists in identifying and grading abnormal findings of osteoarthritis as well as predicting the onset or progression of this disease. Either alone or combined with radiomics, AI algorithms may potentially improve diagnosis and outcome prediction of bone and soft-tissue tumors. Finally, information regarding appropriate positioning of orthopedic implants and related complications may be obtained using AI algorithms. In conclusion, rather than replacing radiologists, the use of AI should instead help them to optimize workflow, augment diagnostic performance, and keep up with ever-increasing workload.Relevance statement This narrative review provides an overview of AI applications in musculoskeletal imaging. As the number of AI technologies continues to increase, it will be crucial for radiologists to play a role in their selection and application as well as to fully understand their potential value in clinical practice. Key points • AI may potentially assist musculoskeletal radiologists in several interpretative tasks.• AI applications to trauma, age estimation, osteoarthritis, tumors, and orthopedic implants are discussed.• AI should help radiologists to optimize workflow and augment diagnostic performance.","<method>AI algorithms</method>, <method>AI methods</method>, <method>radiomics</method>"
2024,https://openalex.org/W4392820641,Medicine,Analysis of human errors in human-autonomy collaboration in autonomous ships operations through shore control experimental data,"Human-autonomy collaboration plays a pivotal role in the development of Maritime autonomous surface ships (MASS), as Shore control center (SCC) operators may engage in the control loop by directly operating the MASS, or, in the supervisory loop, monitoring the MASS and taking over control when needed. Thus, efficient human performance during takeover control and operation is crucial for the safety of MASS operations. However, since the MASS is still in the early phase of development, the mechanism of human errors is unknown, and the data on human-autonomy collaborative operation is scarce. Human reliability analysis (HRA) aims to assess human errors qualitatively and quantitatively, and is widely used in various complex systems to help safety analysis. This study is dedicated to incorporating advanced HRA methods elements to identify and quantify human errors during taking over control and operation of a MASS in collision avoidance scenarios. It presents virtual experimental results, combined with theoretical human error identification and assessment methods. At first, we apply the Human-System Interaction in Autonomy (H-SIA) method to identify potential human errors; secondly, we identify relevant Performance Shaping Factors (PSFs) including Experience, Boredom, Task complexity, Available time and Pre-warning, and performance measures of the human errors, and implement them in the virtual experiment based on a full-scale autonomous ferry research vessel called milliAmpere2. Finally, we build a Bayesian Network (BN) to present causal and probabilistic relationships between PSFs and human errors through experimental data. The results show that available time has the highest impact on takeover performance of operators, followed by task complexity and pre-warning. Boredom does not present a significant sole impact unless combined with available time. Experience does not show a significant impact on human performance. In addition to the relevance of the human errors analysis to the safe development and operational design of MASS, the developed method benefits other human-autonomy collaborative systems. The developed BN model shows adaptability to assess human error probabilities, and the practical significance of integrating experimental data into the existing HRA methodologies for complex systems.",<method>Bayesian Network (BN)</method>
2024,https://openalex.org/W4396722287,Medicine,Performance of an Open-Source Large Language Model in Extracting Information from Free-Text Radiology Reports,"Purpose To assess the performance of a local open-source large language model (LLM) in various information extraction tasks from real-life emergency brain MRI reports. Materials and Methods All consecutive emergency brain MRI reports written in 2022 from a French quaternary center were retrospectively reviewed. Two radiologists identified MRI scans that were performed in the emergency department for headaches. Four radiologists scored the reports' conclusions as either normal or abnormal. Abnormalities were labeled as either headache-causing or incidental. Vicuna (LMSYS Org), an open-source LLM, performed the same tasks. Vicuna's performance metrics were evaluated using the radiologists' consensus as the reference standard. Results Among the 2398 reports during the study period, radiologists identified 595 that included headaches in the indication (median age of patients, 35 years [IQR, 26-51 years]; 68% [403 of 595] women). A positive finding was reported in 227 of 595 (38%) cases, 136 of which could explain the headache. The LLM had a sensitivity of 98.0% (95% CI: 96.5, 99.0) and specificity of 99.3% (95% CI: 98.8, 99.7) for detecting the presence of headache in the clinical context, a sensitivity of 99.4% (95% CI: 98.3, 99.9) and specificity of 98.6% (95% CI: 92.2, 100.0) for the use of contrast medium injection, a sensitivity of 96.0% (95% CI: 92.5, 98.2) and specificity of 98.9% (95% CI: 97.2, 99.7) for study categorization as either normal or abnormal, and a sensitivity of 88.2% (95% CI: 81.6, 93.1) and specificity of 73% (95% CI: 62, 81) for causal inference between MRI findings and headache. Conclusion An open-source LLM was able to extract information from free-text radiology reports with excellent accuracy without requiring further training.","<method>Vicuna (open-source large language model, LLM)</method>"
2024,https://openalex.org/W4399667220,Medicine,"Triage Performance Across Large Language Models, ChatGPT, and Untrained Doctors in Emergency Medicine: Comparative Study","Background Large language models (LLMs) have demonstrated impressive performances in various medical domains, prompting an exploration of their potential utility within the high-demand setting of emergency department (ED) triage. This study evaluated the triage proficiency of different LLMs and ChatGPT, an LLM-based chatbot, compared to professionally trained ED staff and untrained personnel. We further explored whether LLM responses could guide untrained staff in effective triage. Objective This study aimed to assess the efficacy of LLMs and the associated product ChatGPT in ED triage compared to personnel of varying training status and to investigate if the models’ responses can enhance the triage proficiency of untrained personnel. Methods A total of 124 anonymized case vignettes were triaged by untrained doctors; different versions of currently available LLMs; ChatGPT; and professionally trained raters, who subsequently agreed on a consensus set according to the Manchester Triage System (MTS). The prototypical vignettes were adapted from cases at a tertiary ED in Germany. The main outcome was the level of agreement between raters’ MTS level assignments, measured via quadratic-weighted Cohen κ. The extent of over- and undertriage was also determined. Notably, instances of ChatGPT were prompted using zero-shot approaches without extensive background information on the MTS. The tested LLMs included raw GPT-4, Llama 3 70B, Gemini 1.5, and Mixtral 8x7b. Results GPT-4–based ChatGPT and untrained doctors showed substantial agreement with the consensus triage of professional raters (κ=mean 0.67, SD 0.037 and κ=mean 0.68, SD 0.056, respectively), significantly exceeding the performance of GPT-3.5–based ChatGPT (κ=mean 0.54, SD 0.024; P&lt;.001). When untrained doctors used this LLM for second-opinion triage, there was a slight but statistically insignificant performance increase (κ=mean 0.70, SD 0.047; P=.97). Other tested LLMs performed similar to or worse than GPT-4–based ChatGPT or showed odd triaging behavior with the used parameters. LLMs and ChatGPT models tended toward overtriage, whereas untrained doctors undertriaged. Conclusions While LLMs and the LLM-based product ChatGPT do not yet match professionally trained raters, their best models’ triage proficiency equals that of untrained ED doctors. In its current form, LLMs or ChatGPT thus did not demonstrate gold-standard performance in ED triage and, in the setting of this study, failed to significantly improve untrained doctors’ triage when used as decision support. Notable performance enhancements in newer LLM versions over older ones hint at future improvements with further technological development and specific training.","<method>Large language models (LLMs)</method>, <method>ChatGPT</method>, <method>zero-shot approaches</method>, <method>GPT-4</method>, <method>Llama 3 70B</method>, <method>Gemini 1.5</method>, <method>Mixtral 8x7b</method>, <method>GPT-3.5</method>"
2024,https://openalex.org/W4403545332,Medicine,Evaluating AI and Machine Learning Models in Breast Cancer Detection: A Review of Convolutional Neural Networks (CNN) and Global Research Trends,"Numerous studies have highlighted the significance of artificial intelligence (AI) in breast cancer diagnosis. However, systematic reviews of AI applications in this field often lack cohesion, with each study adopting a unique approach. The aim of this study is to provide a detailed examination of AI's role in breast cancer diagnosis through citation analysis, helping to categorize the key areas that attract academic attention. It also includes a thematic analysis to identify the specific research topics within each category. A total of 30,200 studies related to breast cancer and AI, published between 2015 and 2024, were sourced from databases such as IEEE, Scopus, PubMed, Springer, and Google Scholar. After applying inclusion and exclusion criteria, 32 relevant studies were identified. Most of these studies utilized classification models for breast cancer prediction, with high accuracy being the most commonly reported performance metric. Convolutional Neural Networks (CNN) emerged as the preferred model in many studies. The findings indicate that both the quantity and quality of AI-based algorithms in breast cancer diagnosis are increases in the given years. AI is increasingly seen as a complement to healthcare sector and clinical expertise, with the target of enhancing the accessibility and affordability of quality healthcare worldwide.","<method>classification models</method>, <method>Convolutional Neural Networks (CNN)</method>"
2024,https://openalex.org/W4404054585,Medicine,Targeted protein degradation: advances in drug discovery and clinical practice,"Abstract Targeted protein degradation (TPD) represents a revolutionary therapeutic strategy in disease management, providing a stark contrast to traditional therapeutic approaches like small molecule inhibitors that primarily focus on inhibiting protein function. This advanced technology capitalizes on the cell’s intrinsic proteolytic systems, including the proteasome and lysosomal pathways, to selectively eliminate disease-causing proteins. TPD not only enhances the efficacy of treatments but also expands the scope of protein degradation applications. Despite its considerable potential, TPD faces challenges related to the properties of the drugs and their rational design. This review thoroughly explores the mechanisms and clinical advancements of TPD, from its initial conceptualization to practical implementation, with a particular focus on proteolysis-targeting chimeras and molecular glues. In addition, the review delves into emerging technologies and methodologies aimed at addressing these challenges and enhancing therapeutic efficacy. We also discuss the significant clinical trials and highlight the promising therapeutic outcomes associated with TPD drugs, illustrating their potential to transform the treatment landscape. Furthermore, the review considers the benefits of combining TPD with other therapies to enhance overall treatment effectiveness and overcome drug resistance. The future directions of TPD applications are also explored, presenting an optimistic perspective on further innovations. By offering a comprehensive overview of the current innovations and the challenges faced, this review assesses the transformative potential of TPD in revolutionizing drug development and disease management, setting the stage for a new era in medical therapy.",No methods found.
2024,https://openalex.org/W4390777660,Medicine,Unlocking the Potential of XAI for Improved Alzheimer’s Disease Detection and Classification Using a ViT-GRU Model,"Alzheimer's Disease (AD) is a significant cause of dementia worldwide, and its progression from mild to severe affects an individual's ability to perform daily activities independently. The accurate and early diagnosis of AD is crucial for effective clinical intervention. However, interpreting AD from medical images can be challenging, even for experienced radiologists. Therefore, there is a need for an automatic diagnosis of AD, and researchers have investigated the potential of utilizing Artificial Intelligence (AI) techniques, particularly deep learning models, to address this challenge. This study proposes a framework that combines a Vision Transformer (ViT) and a Gated Recurrent Unit (GRU) to detect AD characteristics from Magnetic Resonance Imaging (MRI) images accurately and reliably. The ViT identifies crucial features from the input image, and the GRU establishes clear correlations between these features. The proposed model overcomes the class imbalance issue in the MRI image dataset and achieves superior accuracy and performance compared to existing methods. The model was trained on the Alzheimer's MRI Preprocessed Dataset obtained from Kaggle, achieving notable accuracies of 99.53% for 4-class and 99.69% for binary classification. It also demonstrated a high accuracy of 99.26% for 3-class on the AD Neuroimaging Initiative (ADNI) Baseline Database. These results were validated through a thorough 10-fold cross-validation process. Furthermore, Explainable AI (XAI) techniques were incorporated to make the model interpretable and explainable. This allows clinicians to understand the model's decision-making process and gain insights into the underlying factors driving the AD diagnosis.","<method>Vision Transformer (ViT)</method>, <method>Gated Recurrent Unit (GRU)</method>, <method>Explainable AI (XAI) techniques</method>"
2024,https://openalex.org/W4391166899,Medicine,"Prediction of atmospheric PM2.5 level by machine learning techniques in Isfahan, Iran","Abstract With increasing levels of air pollution, air quality prediction has attracted more attention. Mathematical models are being developed by researchers to achieve precise predictions. Monitoring and prediction of atmospheric PM 2.5 levels, as a predominant pollutant, is essential in emission mitigation programs. In this study, meteorological datasets from 9 years in Isfahan city, a large metropolis of Iran, were applied to predict the PM 2.5 levels, using four machine learning algorithms including Artificial Neural |Networks (ANNs), K-Nearest-Neighbors (KNN), Support Vector |Machines (SVMs) and ensembles of classification trees Random Forest (RF). The data from 7 air quality monitoring stations located in Isfahan City were taken into consideration. The Confusion Matrix and Cross-Entropy Loss were used to analyze the performance of classification models. Several parameters, including sensitivity, specificity, accuracy, F1 score, precision, and the area under the curve (AUC), are computed to assess model performance. Finally, by introducing the predicted data for 2020 into ArcGIS software and using the IDW (Inverse Distance Weighting) method, interpolation was conducted for the area of Isfahan city and the pollution map was illustrated for each month of the year. The results showed that, based on the accuracy percentage, the ANN model has a better performance (90.1%) in predicting PM 2.5 grades compared to the other models for the applied meteorological dataset, followed by RF (86.1%), SVM (84.6%) and KNN (82.2%) models, respectively. Therefore, ANN modelling provides a feasible procedure for the managerial planning of air pollution control.","<method>Artificial Neural Networks (ANNs)</method>, <method>K-Nearest-Neighbors (KNN)</method>, <method>Support Vector Machines (SVMs)</method>, <method>Random Forest (RF)</method>"
2024,https://openalex.org/W4391810207,Medicine,Machine Learning–Based Prediction of Suicidality in Adolescents With Allergic Rhinitis: Derivation and Validation in 2 Independent Nationwide Cohorts,"Background Given the additional risk of suicide-related behaviors in adolescents with allergic rhinitis (AR), it is important to use the growing field of machine learning (ML) to evaluate this risk. Objective This study aims to evaluate the validity and usefulness of an ML model for predicting suicide risk in patients with AR. Methods We used data from 2 independent survey studies, Korea Youth Risk Behavior Web-based Survey (KYRBS; n=299,468) for the original data set and Korea National Health and Nutrition Examination Survey (KNHANES; n=833) for the external validation data set, to predict suicide risks of AR in adolescents aged 13 to 18 years, with 3.45% (10,341/299,468) and 1.4% (12/833) of the patients attempting suicide in the KYRBS and KNHANES studies, respectively. The outcome of interest was the suicide attempt risks. We selected various ML-based models with hyperparameter tuning in the discovery and performed an area under the receiver operating characteristic curve (AUROC) analysis in the train, test, and external validation data. Results The study data set included 299,468 (KYRBS; original data set) and 833 (KNHANES; external validation data set) patients with AR recruited between 2005 and 2022. The best-performing ML model was the random forest model with a mean AUROC of 84.12% (95% CI 83.98%-84.27%) in the original data set. Applying this result to the external validation data set revealed the best performance among the models, with an AUROC of 89.87% (sensitivity 83.33%, specificity 82.58%, accuracy 82.59%, and balanced accuracy 82.96%). While looking at feature importance, the 5 most important features in predicting suicide attempts in adolescent patients with AR are depression, stress status, academic achievement, age, and alcohol consumption. Conclusions This study emphasizes the potential of ML models in predicting suicide risks in patients with AR, encouraging further application of these models in other conditions to enhance adolescent health and decrease suicide rates.",<method>random forest</method>
2024,https://openalex.org/W4392346526,Medicine,Incorporation of “Artificial Intelligence” for Objective Pain Assessment: A Comprehensive Review,"Pain is a significant health issue, and pain assessment is essential for proper diagnosis, follow-up, and effective management of pain. The conventional methods of pain assessment often suffer from subjectivity and variability. The main issue is to understand better how people experience pain. In recent years, artificial intelligence (AI) has been playing a growing role in improving clinical diagnosis and decision-making. The application of AI offers promising opportunities to improve the accuracy and efficiency of pain assessment. This review article provides an overview of the current state of AI in pain assessment and explores its potential for improving accuracy, efficiency, and personalized care. By examining the existing literature, research gaps, and future directions, this article aims to guide further advancements in the field of pain management. An online database search was conducted via multiple websites to identify the relevant articles. The inclusion criteria were English articles published between January 2014 and January 2024). Articles that were available as full text clinical trials, observational studies, review articles, systemic reviews, and meta-analyses were included in this review. The exclusion criteria were articles that were not in the English language, not available as free full text, those involving pediatric patients, case reports, and editorials. A total of (47) articles were included in this review. In conclusion, the application of AI in pain management could present promising solutions for pain assessment. AI can potentially increase the accuracy, precision, and efficiency of objective pain assessment.",No methods found.
2024,https://openalex.org/W4392817144,Medicine,Accessing care for Long Covid from the perspectives of patients and healthcare practitioners: A qualitative study,"Abstract Background Long Covid is an emerging long‐term condition, with those affected raising concerns about lack of healthcare support. Objective We conducted a qualitative study to identify facilitators and barriers to healthcare access for people with Long Covid, aiming to enhance our understanding of the specific nature of these barriers and how patient experiences may vary. Setting and Participants In the context of the Symptoms, Trajectory, Inequalities and Management: Understanding Long‐COVID to Address and Transform Existing Integrated Care Pathways (STIMULATE‐ICP) Delphi study, a nationally distributed online survey was conducted. Eight patients and eight healthcare practitioners (HCP) were interviewed via telephone or video call. Framework analysis, sensitised by the candidacy theory, was used to identify barriers and facilitators over four levels of access to care. Results Three themes were identified: (i) patients' efforts to navigate emerging pathways for Long Covid, (ii) the patient–HCP interaction and (iii) service resources and structural constraints. Barriers to specialist care included long waiting times, communication gaps across services and a lack of continuity in care. Facilitators included collaborative, patient‐centred approaches, patients' active role in their healthcare and blended approaches for appointments. The perspectives of both patients and HCPs largely aligned. Discussion The candidacy framework was valuable in understanding the experiences of people with Long Covid seeking access to healthcare. Individuals perceived themselves as eligible for care, but they often encountered obstacles in obtaining the expected level of care or, in some cases, did not receive it at all. Our findings are discussed in the context of the candidacy model through multiple processes of identification, negotiation, permeability and appearances at health services. These themes seem to be especially important for the emerging new pathway model and are relevant to both primary and secondary care. Conclusions This study highlights that despite these interviews being conducted two years after the start of the COVID‐19 pandemic, people with Long Covid still struggle to access healthcare, emphasising the ongoing need to provide equitable timely healthcare access for people with Long Covid. Patient or Public Contribution People with Long Covid advised on all stages of this research.",No methods found.
2024,https://openalex.org/W4393033647,Medicine,Optimized Brain Tumor Detection: A Dual-Module Approach for MRI Image Enhancement and Tumor Classification,"Neurological and brain-related cancers are one of the main causes of death worldwide. A commonly used tool in diagnosing these conditions is Magnetic Resonance Imaging (MRI), yet the manual evaluation of MRI images by medical experts presents difficulties due to time constraints and variability. This research introduces a novel, two-module computerized method aimed at increasing the speed and accuracy of brain tumor detection. The first module, termed the Image Enhancement Technique, utilizes a trio of machine learning and imaging strategies—adaptive Wiener filtering, neural networks, and independent component analysis—to normalize images and combat issues such as noise and varying low region contrast. The second module uses Support Vector Machines to validate the output of the first module and perform tumor segmentation and classification. Applied to various types of brain tumors, including meningiomas and pituitary tumors, our method exhibited significant improvements in contrast and classification efficiency. It achieved an average sensitivity and specificity of 0.991, accuracy of 0.989, and a Dice score (DSC) of 0.981. Furthermore, the processing time of our method, averaging at 0.43 seconds, was markedly lower compared to existing methods. These results underscore the superior performance of our approach over current state-of-the-art methods in terms of sensitivity, specificity, precision, and DSC. Future enhancements will seek to increase the robustness of the tumor classification method by employing a standardized approach across a suite of classifiers.","<method>adaptive Wiener filtering</method>, <method>neural networks</method>, <method>independent component analysis</method>, <method>Support Vector Machines</method>"
2024,https://openalex.org/W4393141882,Medicine,Artificial Intelligence (AI) for Early Diagnosis of Retinal Diseases,"Artificial intelligence (AI) has emerged as a transformative tool in the field of ophthalmology, revolutionizing disease diagnosis and management. This paper provides a comprehensive overview of AI applications in various retinal diseases, highlighting its potential to enhance screening efficiency, facilitate early diagnosis, and improve patient outcomes. Herein, we elucidate the fundamental concepts of AI, including machine learning (ML) and deep learning (DL), and their application in ophthalmology, underscoring the significance of AI-driven solutions in addressing the complexity and variability of retinal diseases. Furthermore, we delve into the specific applications of AI in retinal diseases such as diabetic retinopathy (DR), age-related macular degeneration (AMD), Macular Neovascularization, retinopathy of prematurity (ROP), retinal vein occlusion (RVO), hypertensive retinopathy (HR), Retinitis Pigmentosa, Stargardt disease, best vitelliform macular dystrophy, and sickle cell retinopathy. We focus on the current landscape of AI technologies, including various AI models, their performance metrics, and clinical implications. Furthermore, we aim to address challenges and pitfalls associated with the integration of AI in clinical practice, including the “black box phenomenon”, biases in data representation, and limitations in comprehensive patient assessment. In conclusion, this review emphasizes the collaborative role of AI alongside healthcare professionals, advocating for a synergistic approach to healthcare delivery. It highlights the importance of leveraging AI to augment, rather than replace, human expertise, thereby maximizing its potential to revolutionize healthcare delivery, mitigate healthcare disparities, and improve patient outcomes in the evolving landscape of medicine.","<method>artificial intelligence (AI)</method>, <method>machine learning (ML)</method>, <method>deep learning (DL)</method>"
2024,https://openalex.org/W4393204595,Medicine,Sustainability of self-healing polymers: A holistic perspective towards circularity in polymer networks,"Permanent polymer networks present an important sustainability challenge. Irreversible covalent crosslinks impart these materials excellent mechanical properties, thermal and chemical resistance, yet also render them difficult to repair and to recycle. Self-healing mechanisms can extend the lifetime of thermosets and elastomers, improving their durability and making their lifecycle more sustainable. In addition to the lifetime extension, this paper reviews the sustainability of self-healing polymers from a holistic point of view. The entire lifecycle of self-healing polymers is critically assessed with reference to the green chemistry principles and sustainable development. The relation between the self-healing chemistries and the sustainability aspects of each of the phases of the lifecycle are discussed, starting from the feedstocks, monomer functionalisation and polymer synthesis, to processing and manufacturing as well as end-of-life considerations, i.e. recycling or (bio)degradation. The review provides a toolbox for the development of more sustainable thermosets, elastomers and their composites. It is of utmost importance to consider the entire lifecycle of self-healing materials, derived products and – by extension – any material or product. The self-healing ability and often related recyclability should primarily reduce the amount of new materials that are necessary to fulfill societal needs, by extending the lifetime of products and maximizing reprocessing into new products. Increasing healing efficiency and the number of healing cycles improves the overall environmental impact relative to the extended service lifetime. Renewable resources derived from biomass, recycling processes or waste streams should be the first choice to create new self-healing polymers. Finally, biodegradability can be considered as a complementary end-of-life scenario upon accidental loss of self-healing polymer to the environment, provided that the biodegradation does not start under the prospected use conditions of the self-healing polymers and products, but can be postponed until contact with stimuli present in the environment.",No methods found.
2024,https://openalex.org/W4393253634,Medicine,Applied Artificial Intelligence in Healthcare: A Review of Computer Vision Technology Application in Hospital Settings,"Computer vision (CV), a type of artificial intelligence (AI) that uses digital videos or a sequence of images to recognize content, has been used extensively across industries in recent years. However, in the healthcare industry, its applications are limited by factors like privacy, safety, and ethical concerns. Despite this, CV has the potential to improve patient monitoring, and system efficiencies, while reducing workload. In contrast to previous reviews, we focus on the end-user applications of CV. First, we briefly review and categorize CV applications in other industries (job enhancement, surveillance and monitoring, automation, and augmented reality). We then review the developments of CV in the hospital setting, outpatient, and community settings. The recent advances in monitoring delirium, pain and sedation, patient deterioration, mechanical ventilation, mobility, patient safety, surgical applications, quantification of workload in the hospital, and monitoring for patient events outside the hospital are highlighted. To identify opportunities for future applications, we also completed journey mapping at different system levels. Lastly, we discuss the privacy, safety, and ethical considerations associated with CV and outline processes in algorithm development and testing that limit CV expansion in healthcare. This comprehensive review highlights CV applications and ideas for its expanded use in healthcare.",No methods found.
2024,https://openalex.org/W4393405326,Medicine,"Developing Deep LSTMs With Later Temporal Attention for Predicting COVID-19 Severity, Clinical Outcome, and Antibody Level by Screening Serological Indicators Over Time","Objective: The clinical course of COVID-19, as well as the immunological reaction, is notable for its extreme variability. Identifying the main associated factors might help understand the disease progression and physiological status of COVID-19 patients. The dynamic changes of the antibody against Spike protein are crucial for understanding the immune response. This work explores a temporal attention (TA) mechanism of deep learning to predict COVID-19 disease severity, clinical outcomes, and Spike antibody levels by screening serological indicators over time. Methods: We use feature selection techniques to filter feature subsets that are highly correlated with the target. The specific deep Long Short-Term Memory (LSTM) models are employed to capture the dynamic changes of disease severity, clinical outcome, and Spike antibody level. We also propose deep LSTMs with a TA mechanism to emphasize the later blood test records because later records often attract more attention from doctors. Results: Risk factors highly correlated with COVID-19 are revealed. LSTM achieves the highest classification accuracy for disease severity prediction. Temporal Attention Long Short-Term Memory (TA-LSTM) achieves the best performance for clinical outcome prediction. For Spike antibody level prediction, LSTM achieves the best permanence. Conclusion: The experimental results demonstrate the effectiveness of the proposed models. The proposed models can provide a computer-aided medical diagnostics system by simply using time series of serological indicators.","<method>feature selection techniques</method>, <method>deep Long Short-Term Memory (LSTM) models</method>, <method>deep LSTMs with a temporal attention (TA) mechanism</method>"
2024,https://openalex.org/W4395452771,Medicine,Polyphenol‐Copper Derived Self‐Cascade Nanozyme Hydrogel in Boosting Oxygenation and Robust Revascularization for Tissue Regeneration,"Abstract The regeneration of hypoxia‐impaired chronic tissue defects has long been challenging, mainly due to the inefficiency of oxygenation and the limited biological activity of existing oxygen delivery systems in regulating dynamic tissue regeneration process. Herein, a novel polyphenol‐copper coordination strategy to fabricate bioactive superoxide dismutase‐catalase self‐cascade nanozymes (SalB‐CuNCs) is reported, which can serve as an in situ oxygenator and induce angiogenesis simultaneously. The copper‐phenolic hydroxyl coordination structure in SalB‐CuNCs plays a critical role in promoting the enzyme‐like cascade reaction via catechol‐mediated Cu valence state transition and substrate capture mechanism. Furthermore, after incorporating SalB‐CuNCs into a Schiff base hydrogel (COC@SalB‐Cu), the resulting system exhibits outstanding antioxidant and robust oxygenation effect in mitigating the hypoxic microenvironment. Benefiting from the intrinsic angiogenic activity of SalB and copper, COC@SalB‐Cu hydrogel can induce a more complete tube formation by up‐regulating the expression level of vascular endothelial growth factor (VEGF), platelet‐endothelial cell adhesion molecule‐1 (CD31), and endothelial nitric oxide synthase (eNOS). In vivo experiments further demonstrate that the COC@SalB‐Cu hydrogel can significantly restore the oxygen and blood supply, leading to fast tissue regeneration. The present strategy holds enormous promise for the treatment of hypoxia‐related chronic tissue defects and vascular injury in the field of regenerative medicine.",No methods found.
2024,https://openalex.org/W4399534639,Medicine,"A second space age spanning omics, platforms and medicine across orbits","The recent acceleration of commercial, private, and multi-national spaceflight has created an unprecedented level of activity in low Earth orbit (LEO), concomitant with the highest-ever number of crewed missions entering space and preparations for exploration-class (>1 year) missions. Such rapid advancement into space from many new companies, countries, and space-related entities has enabled a""Second Space Age."" This new era is also poised to leverage, for the first time, modern tools and methods of molecular biology and precision medicine, thus enabling precision aerospace medicine for the crews. The applications of these biomedical technologies and algorithms are diverse, encompassing multi-omic, single-cell, and spatial biology tools to investigate human and microbial responses to spaceflight. Additionally, they extend to the development of new imaging techniques, real-time cognitive assessments, physiological monitoring, and personalized risk profiles tailored for astronauts. Furthermore, these technologies enable advancements in pharmacogenomics (PGx), as well as the identification of novel spaceflight biomarkers and the development of corresponding countermeasures. In this review, we highlight some of the recent biomedical research from the National Aeronautics and Space Administration (NASA), Japan Aerospace Exploration Agency (JAXA), European Space Agency (ESA), and other space agencies, and also detail the commercial spaceflight sector's (e.g. SpaceX, Blue Origin, Axiom, Sierra Space) entrance into aerospace medicine and space biology, the first aerospace medicine biobank, and the myriad upcoming missions that will utilize these tools to ensure a permanent human presence beyond LEO, venturing out to other planets and moons.",No methods found.
2024,https://openalex.org/W4401537518,Medicine,A New Brain Network Construction Paradigm for Brain Disorder via Diffusion-Based Graph Contrastive Learning,"Brain network analysis plays an increasingly important role in studying brain function and the exploring of disease mechanisms. However, existing brain network construction tools have some limitations, including dependency on empirical users, weak consistency in repeated experiments and time-consuming processes. In this work, a diffusion-based brain network pipeline, DGCL is designed for end-to-end construction of brain networks. Initially, the brain region-aware module (BRAM) precisely determines the spatial locations of brain regions by the diffusion process, avoiding subjective parameter selection. Subsequently, DGCL employs graph contrastive learning to optimize brain connections by eliminating individual differences in redundant connections unrelated to diseases, thereby enhancing the consistency of brain networks within the same group. Finally, the node-graph contrastive loss and classification loss jointly constrain the learning process of the model to obtain the reconstructed brain network, which is then used to analyze important brain connections. Validation on two datasets, ADNI and ABIDE, demonstrates that DGCL surpasses traditional methods and other deep learning models in predicting disease development stages. Significantly, the proposed model improves the efficiency and generalization of brain network construction. In summary, the proposed DGCL can be served as a universal brain network construction scheme, which can effectively identify important brain connections through generative paradigms and has the potential to provide disease interpretability support for neuroscience research.","<method>diffusion process</method>, <method>graph contrastive learning</method>, <method>node-graph contrastive loss</method>, <method>classification loss</method>"
2024,https://openalex.org/W4386592162,Medicine,"Patient-Specific, Mechanistic Models of Tumor Growth Incorporating Artificial Intelligence and Big Data","Despite the remarkable advances in cancer diagnosis, treatment, and management over the past decade, malignant tumors remain a major public health problem. Further progress in combating cancer may be enabled by personalizing the delivery of therapies according to the predicted response for each individual patient. The design of personalized therapies requires the integration of patient-specific information with an appropriate mathematical model of tumor response. A fundamental barrier to realizing this paradigm is the current lack of a rigorous yet practical mathematical theory of tumor initiation, development, invasion, and response to therapy. We begin this review with an overview of different approaches to modeling tumor growth and treatment, including mechanistic as well as data-driven models based on big data and artificial intelligence. We then present illustrative examples of mathematical models manifesting their utility and discuss the limitations of stand-alone mechanistic and data-driven models. We then discuss the potential of mechanistic models for not only predicting but also optimizing response to therapy on a patient-specific basis. We describe current efforts and future possibilities to integrate mechanistic and data-driven models. We conclude by proposing five fundamental challenges that must be addressed to fully realize personalized care for cancer patients driven by computational models.","<method>artificial intelligence</method>, <method>data-driven models</method>"
2024,https://openalex.org/W4390659128,Medicine,Multi-Class Kidney Abnormalities Detecting Novel System Through Computed Tomography,"Impaired renal function poses a risk across all age groups. Because of the global shortage of nephrologists, the growing public health concern over renal failure, and technological improvements, there is a demand for an AI-driven system capable of autonomously detecting kidney abnormalities. Chronic kidney disease, commonly known as chronic renal failure, is characterized by a progressive decline in kidney function. Renal failure can be caused by a variety of reasons, including cysts, stones, and tumors. Chronic kidney disease may not have apparent symptoms at first, resulting in instances staying untreated until they reach an advanced state. Tumors are dense clumps of tissue that can cause direct injury to glands, spinal cells, and other organs. The presence of a substantial number of solids in the digestive tract causes kidney stone disease, also known as urolithiasis. This study used a deep learning model to detect kidney illnesses to solve the global scarcity of urologists. The project entailed obtaining and annotating a large dataset of 12,446 CT whole abdomen and urogram images, with an emphasis on kidney stones, cysts, and tumors, which are the most common types of renal illness. The dataset was divided into four categories: cyst, tumor, stone, and normal. Data was collected from several hospitals in the Dhaka area. This work presents an innovative and customizable platform for the clinical diagnosis of kidney diseases such as tumors, stones, and cysts. Our YOLOv8 model's enhanced accuracy opens up new possibilities for identifying kidney cysts, stones, and tumors. We used criteria like accuracy, precision, recall, F1 score, and specificity to evaluate its performance. The network attained an accuracy rate of 82.52%, 85.76% precision, 75.28% recall, 75.72% F1 score, and 93.12% specificity.","<method>deep learning model</method>, <method>YOLOv8</method>"
2024,https://openalex.org/W4390670279,Medicine,Awareness and level of digital literacy among students receiving health-based education,"Abstract Background Being digitally literate allows health-based science students to access reliable, up-to-date information efficiently and expands the capacity for continuous learning. Digital literacy facilitates effective communication and collaboration among other healthcare providers. It helps to navigate the ethical implications of using digital technologies and aids the use of digital tools in managing healthcare processes. Our aim in this study is to determine the digital literacy level and awareness of our students receiving health-based education in our university and to pave the way for supporting the current curriculum with courses on digital literacy when necessary. Method Students from Acibadem University who were registered undergraduate education for at least four years of health-based education, School of Medicine, Nutrition and Dietetics, Nursing, Physiotherapy and Rehabilitation, Psychology, Biomedical Engineering, Molecular Biology, and Genetics were included. The questionnaire consisted of 24 queries evaluating digital literacy in 7 fields: software and multimedia, hardware and technical problem solving, network and communication/collaboration, ethics, security, artificial intelligence (A.I.), and interest/knowledge. Two student groups representing all departments were invited for interviews according to the Delphi method. Results The survey was completed by 476 students. Female students had less computer knowledge and previous coding education. Spearman correlation test showed that there were weak positive correlations between the years and the “software and multimedia,” “ethics,” “interest and knowledge” domains, and the average score. The students from Nursing scored lowest in the query after those from the Nutrition and Dietetics department. The highest scores were obtained by Biomedical Engineering students, followed by the School of Medicine. Participants scored the highest in “network” and “A.I.” and lowest in “interest-knowledge” domains. Conclusion It is necessary to define the level of computer skills who start health-based education and shape the curriculum by determining which domains are weak. Creating an educational environment that fosters females’ digital knowledge is recommended. Elective courses across faculties may be offered to enable students to progress and discuss various digital literacy topics. The extent to which students benefit from the digital literacy-supported curriculum may be evaluated. Thus, health-based university students are encouraged to acquire the computer skills required by today’s clinical settings. Registration This study was approved by Acıbadem University and Acıbadem Healthcare Institutions Medical Research Ethics Committee (ATADEK) (11 November 2022, ATADEK registration: 2022-17-138) All methods were carried out in accordance with relevant guidelines and regulations. Informed consent was obtained from the participants.","<method>Delphi method</method>, <method>Spearman correlation test</method>"
2024,https://openalex.org/W4390823043,Medicine,Evaluating the Accuracy of ChatGPT and Google BARD in Fielding Oculoplastic Patient Queries: A Comparative Study on Artificial versus Human Intelligence,"Purpose: This study evaluates and compares the accuracy of responses from 2 artificial intelligence platforms to patients’ oculoplastics-related questions. Methods: Questions directed toward oculoplastic surgeons were collected, rephrased, and input independently into ChatGPT-3.5 and BARD chatbots, using the prompt: “As an oculoplastic surgeon, how can I respond to my patient’s question?.” Responses were independently evaluated by 4 experienced oculoplastic specialists as comprehensive, correct but inadequate, mixed correct and incorrect/outdated data, and completely incorrect. Additionally, the empathy level, length, and automated readability index of the responses were assessed. Results: A total of 112 patient questions underwent evaluation. The rates of comprehensive, correct but inadequate, mixed, and completely incorrect answers for ChatGPT were 71.4%, 12.9%, 10.5%, and 5.1%, respectively, compared with 53.1%, 18.3%, 18.1%, and 10.5%, respectively, for BARD. ChatGPT showed more empathy (48.9%) than BARD (13.2%). All graders found that ChatGPT outperformed BARD in question categories of postoperative healing, medical eye conditions, and medications. Categorizing questions by anatomy, ChatGPT excelled in answering lacrimal questions (83.8%), while BARD performed best in the eyelid group (60.4%). ChatGPT’s answers were longer and potentially more challenging to comprehend than BARD’s. Conclusion: This study emphasizes the promising role of artificial intelligence-powered chatbots in oculoplastic patient education and support. With continued development, these chatbots may potentially assist physicians and offer patients accurate information, ultimately contributing to improved patient care while alleviating surgeon burnout. However, it is crucial to highlight that artificial intelligence may be good at answering questions, but physician oversight remains essential to ensure the highest standard of care and address complex medical cases.","<method>ChatGPT-3.5</method>, <method>BARD chatbots</method>"
2024,https://openalex.org/W4391437034,Medicine,A methodical exploration of imaging modalities from dataset to detection through machine learning paradigms in prominent lung disease diagnosis: a review,"Abstract Background Lung diseases, both infectious and non-infectious, are the most prevalent cause of mortality overall in the world. Medical research has identified pneumonia, lung cancer, and Corona Virus Disease 2019 (COVID-19) as prominent lung diseases prioritized over others. Imaging modalities, including X-rays, computer tomography (CT) scans, magnetic resonance imaging (MRIs), positron emission tomography (PET) scans, and others, are primarily employed in medical assessments because they provide computed data that can be utilized as input datasets for computer-assisted diagnostic systems. Imaging datasets are used to develop and evaluate machine learning (ML) methods to analyze and predict prominent lung diseases. Objective This review analyzes ML paradigms, imaging modalities' utilization, and recent developments for prominent lung diseases. Furthermore, the research also explores various datasets available publically that are being used for prominent lung diseases. Methods The well-known databases of academic studies that have been subjected to peer review, namely ScienceDirect, arXiv, IEEE Xplore, MDPI, and many more, were used for the search of relevant articles. Applied keywords and combinations used to search procedures with primary considerations for review, such as pneumonia, lung cancer, COVID-19, various imaging modalities, ML, convolutional neural networks (CNNs), transfer learning, and ensemble learning. Results This research finding indicates that X-ray datasets are preferred for detecting pneumonia, while CT scan datasets are predominantly favored for detecting lung cancer. Furthermore, in COVID-19 detection, X-ray datasets are prioritized over CT scan datasets. The analysis reveals that X-rays and CT scans have surpassed all other imaging techniques. It has been observed that using CNNs yields a high degree of accuracy and practicability in identifying prominent lung diseases. Transfer learning and ensemble learning are complementary techniques to CNNs to facilitate analysis. Furthermore, accuracy is the most favored metric for assessment.","<method>machine learning (ML)</method>, <method>convolutional neural networks (CNNs)</method>, <method>transfer learning</method>, <method>ensemble learning</method>"
2024,https://openalex.org/W4391437137,Medicine,The effects of educational robotics in STEM education: a multilevel meta-analysis,"Abstract Educational robotics, as emerging technologies, have been widely applied in the field of STEM education to enhance the instructional and learning quality. Although previous research has highlighted potentials of applying educational robotics in STEM education, there is a lack of empirical evidence to investigate and understand the overall effects of using educational robotics in STEM education as well as the critical factors that influence the effects. To fill this gap, this research conducted a multilevel meta-analysis to examine the overall effect size of using educational robotics in STEM education under K-16 education based on 30 effect sizes from 21 studies published between 2010 and 2022. Furthermore, we examined the possible moderator variables of robot-assisted STEM education, including discipline, educational level, instructor support, instructional strategy, interactive type, intervention duration, robotic type, and control group condition. Results showed that educational robotics had the moderate-sized effects on students’ STEM learning compared to the non-robotics condition. Specifically, educational robotics had moderate-sized effects on students’ learning performances and learning attitudes, and insignificant effects on the improvement of computational thinking. Furthermore, we examined the influence of moderator variables in robot-assisted STEM education. Results indicated that the moderator variable of discipline was significantly associated with the effects of educational robotics on STEM learning. Based on the findings, educational and technological implications were provided to guide future research and practice in the application of educational robotics in STEM education.",No methods found.
2024,https://openalex.org/W4391601115,Medicine,"Cybersickness in Virtual Reality: The Role of Individual Differences, Its Effects on Cognitive Functions and Motor Skills, and Intensity Differences during and after Immersion","Background: Given that VR is used in multiple domains, understanding the effects of cybersickness on human cognition and motor skills and the factors contributing to cybersickness is becoming increasing important. This study aimed to explore the predictors of cybersickness and its interplay with cognitive and motor skills. Methods: 30 participants, 20–45 years old, completed the MSSQ and the CSQ-VR, and were immersed in VR. During immersion, they were exposed to a roller coaster ride. Before and after the ride, participants responded to the CSQ-VR and performed VR-based cognitive and psychomotor tasks. After the VR session, participants completed the CSQ-VR again. Results: Motion sickness susceptibility, during adulthood, was the most prominent predictor of cybersickness. Pupil dilation emerged as a significant predictor of cybersickness. Experience with videogaming was a significant predictor of cybersickness and cognitive/motor functions. Cybersickness negatively affected visuospatial working memory and psychomotor skills. Overall the intensity of cybersickness’s nausea and vestibular symptoms significantly decreased after removing the VR headset. Conclusions: In order of importance, motion sickness susceptibility and gaming experience are significant predictors of cybersickness. Pupil dilation appears to be a cybersickness biomarker. Cybersickness affects visuospatial working memory and psychomotor skills. Concerning user experience, cybersickness and its effects on performance should be examined during and not after immersion.",No methods found.
2024,https://openalex.org/W4391687152,Medicine,"Traditional, complementary, and integrative medicine and artificial intelligence: Novel opportunities in healthcare","The convergence of traditional, complementary, and integrative medicine (TCIM) with artificial intelligence (AI) is a promising frontier in healthcare. TCIM is a patient-centric approach that combines conventional medicine with complementary therapies, emphasizing holistic well-being. AI can revolutionize healthcare through data-driven decision-making and personalized treatment plans. This article explores how AI technologies can complement and enhance TCIM, aligning with the shared objectives of researchers from both fields in improving patient outcomes, enhancing care quality, and promoting holistic wellness. This integration of TCIM and AI introduces exciting opportunities but also noteworthy challenges. AI may augment TCIM by assisting in early disease detection, providing personalized treatment plans, predicting health trends, and enhancing patient engagement. Challenges at the intersection of AI and TCIM include data privacy and security, regulatory complexities, maintaining the human touch in patient-provider relationships, and mitigating bias in AI algorithms. Patients' trust, informed consent, and legal accountability are all essential considerations. Future directions in AI-enhanced TCIM include advanced personalized medicine, understanding the efficacy of herbal remedies, and studying patient-provider interactions. Research on bias mitigation, patient acceptance, and trust in AI-driven TCIM healthcare is crucial. In this article, we outlined that the merging of TCIM and AI holds great promise in enhancing healthcare delivery, personalizing treatment plans, preventive care, and patient engagement. Addressing challenges and fostering collaboration between AI experts, TCIM practitioners, and policymakers, however, is vital to harnessing the full potential of this integration.",No methods found.
2024,https://openalex.org/W4392056032,Medicine,A novel fusion framework of deep bottleneck residual convolutional neural network for breast cancer classification from mammogram images,"With over 2.1 million new cases of breast cancer diagnosed annually, the incidence and mortality rate of this disease pose severe global health issues for women. Identifying the disease’s influence is the only practical way to lessen it immediately. Numerous research works have developed automated methods using different medical imaging to identify BC. Still, the precision of each strategy differs based on the available resources, the issue’s nature, and the dataset being used. We proposed a novel deep bottleneck convolutional neural network with a quantum optimization algorithm for breast cancer classification and diagnosis from mammogram images. Two novel deep architectures named three-residual blocks bottleneck and four-residual blocks bottle have been proposed with parallel and single paths. Bayesian Optimization (BO) has been employed to initialize hyperparameter values and train the architectures on the selected dataset. Deep features are extracted from the global average pool layer of both models. After that, a kernel-based canonical correlation analysis and entropy technique is proposed for the extracted deep features fusion. The fused feature set is further refined using an optimization technique named quantum generalized normal distribution optimization. The selected features are finally classified using several neural network classifiers, such as bi-layered and wide-neural networks. The experimental process was conducted on a publicly available mammogram imaging dataset named INbreast, and a maximum accuracy of 96.5% was obtained. Moreover, for the proposed method, the sensitivity rate is 96.45, the precision rate is 96.5, the F1 score value is 96.64, the MCC value is 92.97%, and the Kappa value is 92.97%, respectively. The proposed architectures are further utilized for the diagnosis process of infected regions. In addition, a detailed comparison has been conducted with a few recent techniques showing the proposed framework’s higher accuracy and precision rate.","<method>deep bottleneck convolutional neural network</method>, <method>quantum optimization algorithm</method>, <method>three-residual blocks bottleneck</method>, <method>four-residual blocks bottleneck</method>, <method>Bayesian Optimization (BO)</method>, <method>kernel-based canonical correlation analysis</method>, <method>entropy technique</method>, <method>quantum generalized normal distribution optimization</method>, <method>bi-layered neural network classifier</method>, <method>wide-neural network classifier</method>"
2024,https://openalex.org/W4392139441,Medicine,Artificial intelligence for radiographic imaging detection of caries lesions: a systematic review,"Abstract Background The aim of this systematic review is to evaluate the diagnostic performance of Artificial Intelligence (AI) models designed for the detection of caries lesion (CL). Materials and methods An electronic literature search was conducted on PubMed, Web of Science, SCOPUS, LILACS and Embase databases for retrospective, prospective and cross-sectional studies published until January 2023, using the following keywords: artificial intelligence (AI), machine learning (ML), deep learning (DL), artificial neural networks (ANN), convolutional neural networks (CNN), deep convolutional neural networks (DCNN), radiology, detection, diagnosis and dental caries (DC). The quality assessment was performed using the guidelines of QUADAS-2. Results Twenty articles that met the selection criteria were evaluated. Five studies were performed on periapical radiographs, nine on bitewings, and six on orthopantomography. The number of imaging examinations included ranged from 15 to 2900. Four studies investigated ANN models, fifteen CNN models, and two DCNN models. Twelve were retrospective studies, six cross-sectional and two prospective. The following diagnostic performance was achieved in detecting CL: sensitivity from 0.44 to 0.86, specificity from 0.85 to 0.98, precision from 0.50 to 0.94, PPV (Positive Predictive Value) 0.86, NPV (Negative Predictive Value) 0.95, accuracy from 0.73 to 0.98, area under the curve (AUC) from 0.84 to 0.98, intersection over union of 0.3–0.4 and 0.78, Dice coefficient 0.66 and 0.88, F1-score from 0.64 to 0.92. According to the QUADAS-2 evaluation, most studies exhibited a low risk of bias. Conclusion AI-based models have demonstrated good diagnostic performance, potentially being an important aid in CL detection. Some limitations of these studies are related to the size and heterogeneity of the datasets. Future studies need to rely on comparable, large, and clinically meaningful datasets. Protocol PROSPERO identifier: CRD42023470708","<method>Artificial Neural Networks (ANN)</method>, <method>Convolutional Neural Networks (CNN)</method>, <method>Deep Convolutional Neural Networks (DCNN)</method>"
2024,https://openalex.org/W4393175789,Medicine,"A deep-learning model for intracranial aneurysm detection on CT angiography images in China: a stepwise, multicentre, early-stage clinical validation study","BackgroundArtificial intelligence (AI) models in real-world implementation are scarce. Our study aimed to develop a CT angiography (CTA)-based AI model for intracranial aneurysm detection, assess how it helps clinicians improve diagnostic performance, and validate its application in real-world clinical implementation.MethodsWe developed a deep-learning model using 16 546 head and neck CTA examination images from 14 517 patients at eight Chinese hospitals. Using an adapted, stepwise implementation and evaluation, 120 certified clinicians from 15 geographically different hospitals were recruited. Initially, the AI model was externally validated with images of 900 digital subtraction angiography-verified CTA cases (examinations) and compared with the performance of 24 clinicians who each viewed 300 of these cases (stage 1). Next, as a further external validation a multi-reader multi-case study enrolled 48 clinicians to individually review 298 digital subtraction angiography-verified CTA cases (stage 2). The clinicians reviewed each CTA examination twice (ie, with and without the AI model), separated by a 4-week washout period. Then, a randomised open-label comparison study enrolled 48 clinicians to assess the acceptance and performance of this AI model (stage 3). Finally, the model was prospectively deployed and validated in 1562 real-world clinical CTA cases.FindingsThe AI model in the internal dataset achieved a patient-level diagnostic sensitivity of 0·957 (95% CI 0·939–0·971) and a higher patient-level diagnostic sensitivity than clinicians (0·943 [0·921–0·961] vs 0·658 [0·644–0·672]; p<0·0001) in the external dataset. In the multi-reader multi-case study, the AI-assisted strategy improved clinicians' diagnostic performance both on a per-patient basis (the area under the receiver operating characteristic curves [AUCs]; 0·795 [0·761–0·830] without AI vs 0·878 [0·850–0·906] with AI; p<0·0001) and a per-aneurysm basis (the area under the weighted alternative free-response receiver operating characteristic curves; 0·765 [0·732–0·799] vs 0·865 [0·839–0·891]; p<0·0001). Reading time decreased with the aid of the AI model (87·5 s vs 82·7 s, p<0·0001). In the randomised open-label comparison study, clinicians in the AI-assisted group had a high acceptance of the AI model (92·6% adoption rate), and a higher AUC when compared with the control group (0·858 [95% CI 0·850–0·866] vs 0·789 [0·780–0·799]; p<0·0001). In the prospective study, the AI model had a 0·51% (8/1570) error rate due to poor-quality CTA images and recognition failure. The model had a high negative predictive value of 0·998 (0·994–1·000) and significantly improved the diagnostic performance of clinicians; AUC improved from 0·787 (95% CI 0·766–0·808) to 0·909 (0·894–0·923; p<0·0001) and patient-level sensitivity improved from 0·590 (0·511–0·666) to 0·825 (0·759–0·880; p<0·0001).InterpretationThis AI model demonstrated strong clinical potential for intracranial aneurysm detection with improved clinician diagnostic performance, high acceptance, and practical implementation in real-world clinical cases.FundingNational Natural Science Foundation of China.TranslationFor the Chinese translation of the abstract see Supplementary Materials section.",<method>deep-learning model</method>
2024,https://openalex.org/W4394967854,Medicine,Potential of Large Language Models in Health Care: Delphi Study,"Background A large language model (LLM) is a machine learning model inferred from text data that captures subtle patterns of language use in context. Modern LLMs are based on neural network architectures that incorporate transformer methods. They allow the model to relate words together through attention to multiple words in a text sequence. LLMs have been shown to be highly effective for a range of tasks in natural language processing (NLP), including classification and information extraction tasks and generative applications. Objective The aim of this adapted Delphi study was to collect researchers’ opinions on how LLMs might influence health care and on the strengths, weaknesses, opportunities, and threats of LLM use in health care. Methods We invited researchers in the fields of health informatics, nursing informatics, and medical NLP to share their opinions on LLM use in health care. We started the first round with open questions based on our strengths, weaknesses, opportunities, and threats framework. In the second and third round, the participants scored these items. Results The first, second, and third rounds had 28, 23, and 21 participants, respectively. Almost all participants (26/28, 93% in round 1 and 20/21, 95% in round 3) were affiliated with academic institutions. Agreement was reached on 103 items related to use cases, benefits, risks, reliability, adoption aspects, and the future of LLMs in health care. Participants offered several use cases, including supporting clinical tasks, documentation tasks, and medical research and education, and agreed that LLM-based systems will act as health assistants for patient education. The agreed-upon benefits included increased efficiency in data handling and extraction, improved automation of processes, improved quality of health care services and overall health outcomes, provision of personalized care, accelerated diagnosis and treatment processes, and improved interaction between patients and health care professionals. In total, 5 risks to health care in general were identified: cybersecurity breaches, the potential for patient misinformation, ethical concerns, the likelihood of biased decision-making, and the risk associated with inaccurate communication. Overconfidence in LLM-based systems was recognized as a risk to the medical profession. The 6 agreed-upon privacy risks included the use of unregulated cloud services that compromise data security, exposure of sensitive patient data, breaches of confidentiality, fraudulent use of information, vulnerabilities in data storage and communication, and inappropriate access or use of patient data. Conclusions Future research related to LLMs should not only focus on testing their possibilities for NLP-related tasks but also consider the workflows the models could contribute to and the requirements regarding quality, integration, and regulations needed for successful implementation in practice.","<method>large language model (LLM)</method>, <method>neural network architectures</method>, <method>transformer methods</method>"
2024,https://openalex.org/W4394999034,Medicine,Advances and Challenges in Targeting TGF-β Isoforms for Therapeutic Intervention of Cancer: A Mechanism-Based Perspective,"The TGF-β family is a group of 25 kDa secretory cytokines, in mammals consisting of three dimeric isoforms (TGF-βs 1, 2, and 3), each encoded on a separate gene with unique regulatory elements. Each isoform plays unique, diverse, and pivotal roles in cell growth, survival, immune response, and differentiation. However, many researchers in the TGF-β field often mistakenly assume a uniform functionality among all three isoforms. Although TGF-βs are essential for normal development and many cellular and physiological processes, their dysregulated expression contributes significantly to various diseases. Notably, they drive conditions like fibrosis and tumor metastasis/progression. To counter these pathologies, extensive efforts have been directed towards targeting TGF-βs, resulting in the development of a range of TGF-β inhibitors. Despite some clinical success, these agents have yet to reach their full potential in the treatment of cancers. A significant challenge rests in effectively targeting TGF-βs’ pathological functions while preserving their physiological roles. Many existing approaches collectively target all three isoforms, failing to target just the specific deregulated ones. Additionally, most strategies tackle the entire TGF-β signaling pathway instead of focusing on disease-specific components or preferentially targeting tumors. This review gives a unique historical overview of the TGF-β field often missed in other reviews and provides a current landscape of TGF-β research, emphasizing isoform-specific functions and disease implications. The review then delves into ongoing therapeutic strategies in cancer, stressing the need for more tools that target specific isoforms and disease-related pathway components, advocating mechanism-based and refined approaches to enhance the effectiveness of TGF-β-targeted cancer therapies.",No methods found.
2024,https://openalex.org/W4396753423,Medicine,ChatCAD+: Toward a Universal and Reliable Interactive CAD Using LLMs,"The integration of Computer-Aided Diagnosis (CAD) with Large Language Models (LLMs) presents a promising frontier in clinical applications, notably in automating diagnostic processes akin to those performed by radiologists and providing consultations similar to a virtual family doctor. Despite the promising potential of this integration, current works face at least two limitations: (1) From the perspective of a radiologist, existing studies typically have a restricted scope of applicable imaging domains, failing to meet the diagnostic needs of different patients. Also, the insufficient diagnostic capability of LLMs further undermine the quality and reliability of the generated medical reports. (2) Current LLMs lack the requisite depth in medical expertise, rendering them less effective as virtual family doctors due to the potential unreliability of the advice provided during patient consultations. To address these limitations, we introduce ChatCAD+, to be universal and reliable. Specifically, it is featured by two main modules: (1) Reliable Report Generation and (2) Reliable Interaction. The Reliable Report Generation module is capable of interpreting medical images from diverse domains and generate high-quality medical reports via our proposed hierarchical in-context learning. Concurrently, the interaction module leverages up-to-date information from reputable medical websites to provide reliable medical advice. Together, these designed modules synergize to closely align with the expertise of human medical professionals, offering enhanced consistency and reliability for interpretation and advice. The source code is available at GitHub.",<method>hierarchical in-context learning</method>
2024,https://openalex.org/W4399055889,Medicine,Tomato Leaf Disease Detection using Convolutional Neural Networks,"One of the most important crops that is grown in enormous amounts and has an excellent market value comprises the tomato. They are grown and eaten in large quantities not only in India but also globally. Disease is the primary factor affecting the quantity and quality of this crop’s production. In earlier research, the plant’s leaves solely were taken into account for disease identification; however, in many cases, the illness only affects the fruit, leaving the other plant parts healthy. Using the unaided eye to diagnose a disease can occasionally lead to a prognosis that is off, meaning the wrong pesticide is applied and the plant may get spoiled. The farmers find it challenging to diagnose the disease because specialists are scarce in many of the affected areas. It’s an expensive and time-consuming process, even though professionals are accessible in certain sectors. Early disease detection would lessen the impact on plants and increase agricultural yield. As a result, it is essential to recognise these illnesses accurately and use the appropriate pesticide. These is- sues can be resolved by an automated system. We have developed a system to tackle this problem, which employs a convolutional neural network (CNN) to detect the ailment and recommends a pesticide to aid in its eradication. Since CNN offers its highest level of accuracy, our system incorporates it.",<method>convolutional neural network (CNN)</method>
2024,https://openalex.org/W4400252889,Medicine,"Bias in artificial intelligence for medical imaging: fundamentals, detection, avoidance, mitigation, challenges, ethics, and prospects","Although artificial intelligence (AI) methods hold promise for medical imaging-based prediction tasks, their integration into medical practice may present a double-edged sword due to bias (i.e., systematic errors).AI algorithms have the potential to mitigate cognitive biases in human interpretation, but extensive research has highlighted the tendency of AI systems to internalize biases within their model.This fact, whether intentional or not, may ultimately lead to unintentional consequences in the clinical setting, potentially compromising patient outcomes.This concern is particularly important in medical imaging, where AI has been more progressively and widely embraced than any other medical field.A comprehensive understanding of bias at each stage of the AI pipeline is therefore essential to contribute to developing AI solutions that are not only less biased but also widely applicable.This international collaborative review effort aims to increase awareness within the medical imaging community about the importance of proactively identifying and addressing AI bias to prevent its negative consequences from being realized later.The authors began with the fundamentals of bias by explaining its different definitions and delineating various potential sources.Strategies for detecting and identifying bias were then outlined, followed by a review of techniques for its avoidance and mitigation.Moreover, ethical dimensions, challenges encountered, and prospects were discussed.",No methods found.
2024,https://openalex.org/W4401729065,Medicine,On the pathogenesis of obesity: causal models and missing pieces of the puzzle,"Application of the physical laws of energy and mass conservation at the whole-body level is not necessarily informative about causal mechanisms of weight gain and the development of obesity. The energy balance model (EBM) and the carbohydrate-insulin model (CIM) are two plausible theories, among several others, attempting to explain why obesity develops within an overall common physiological framework of regulation of human energy metabolism. These models have been used to explain the pathogenesis of obesity in individuals as well as the dramatic increases in the prevalence of obesity worldwide over the past half century. Here, we summarize outcomes of a recent workshop in Copenhagen that brought together obesity experts from around the world to discuss causal models of obesity pathogenesis. These discussions helped to operationally define commonly used terms; delineate the structure of each model, particularly focussing on areas of overlap and divergence; challenge ideas about the importance of purported causal factors for weight gain; and brainstorm on the key scientific questions that need to be answered. We hope that more experimental research in nutrition and other related fields, and more testing of the models and their predictions will pave the way and provide more answers about the pathogenesis of obesity than those currently available.",No methods found.
2024,https://openalex.org/W4402559330,Medicine,Generative artificial intelligence in primary care: an online survey of UK general practitioners,"Objectives Following the launch of ChatGPT in November 2022, interest in large language model-powered chatbots has soared with increasing focus on the clinical potential of these tools. We sought to measure general practitioners’ (GPs) current use of this new generation of chatbots to assist with any aspect of clinical practice in the UK. Methods An online survey was distributed to a non-probability sample of GPs registered with the clinician marketing service Doctors.net.uk. The study was launched as a monthly ‘omnibus survey’ which has a predetermined sample size of 1000 participants. Results 531 (53%) respondents were men, 544 (54%) were 46 years or older. 20% (205) reported using generative artificial intelligence (AI) tools in clinical practice; of those who answered affirmatively and were invited to clarify further, 29% (47) reported using these tools to generate documentation after patient appointments and 28% (45) to suggest a differential diagnosis. Discussion Administered a year after ChatGPT was launched, this is the largest survey we know of conducted into doctors’ use of generative AI in clinical practice. Findings suggest that GPs may derive value from these tools, particularly with administrative tasks and to support clinical reasoning. Conclusion Despite a lack of guidance about these tools and unclear work policies, GPs report using generative AI to assist with their job. The medical community will need to find ways to both educate physicians and trainees and guide patients about the safe adoption of these tools.",<method>generative artificial intelligence (AI) tools</method>
2024,https://openalex.org/W4403839497,Medicine,When combinations of humans and AI are useful: A systematic review and meta-analysis,"Abstract Inspired by the increasing use of artificial intelligence (AI) to augment humans, researchers have studied human–AI systems involving different tasks, systems and populations. Despite such a large body of work, we lack a broad conceptual understanding of when combinations of humans and AI are better than either alone. Here we addressed this question by conducting a preregistered systematic review and meta-analysis of 106 experimental studies reporting 370 effect sizes. We searched an interdisciplinary set of databases (the Association for Computing Machinery Digital Library, the Web of Science and the Association for Information Systems eLibrary) for studies published between 1 January 2020 and 30 June 2023. Each study was required to include an original human-participants experiment that evaluated the performance of humans alone, AI alone and human–AI combinations. First, we found that, on average, human–AI combinations performed significantly worse than the best of humans or AI alone (Hedges’ g = −0.23; 95% confidence interval, −0.39 to −0.07). Second, we found performance losses in tasks that involved making decisions and significantly greater gains in tasks that involved creating content. Finally, when humans outperformed AI alone, we found performance gains in the combination, but when AI outperformed humans alone, we found losses. Limitations of the evidence assessed here include possible publication bias and variations in the study designs analysed. Overall, these findings highlight the heterogeneity of the effects of human–AI collaboration and point to promising avenues for improving human–AI systems.",No methods found.
2024,https://openalex.org/W2613971348,Medicine,Learning Environment In Engineering Technology With A High Percentage Of Non Traditional Students,"The paper describes various aspects of university learning environment where a significant percentage of students works full time and has a substantial professional experience.Changes in population of Engineering Technology students at Central Connecticut State University largely reflect needs of the local and national job market as well as general perception of local population on the discipline.The change of non-traditional student population in Engineering Technology at CCSU in the past 10 years is described in relation to the economic health and activities of the area's industry.Reasons behind continuous education of workforce from personal, society and business perspectives are described.Lack of theoretical knowledge and limited availability of time to study, often place the non-traditional students at a disadvantage compared to day-time students.Challenges, as well as learning atmosphere diversification the non-traditional students bring to the classroom are also described.Changes in working students availability for evening and day classes, preliminary analysis of causes of the changes and impact on planning of academic activities are presented.Undertakings by some local companies whose employees attend evening and day classes are described.Some discipline-related technical and non-technical skills are examined in relevance to traditional and non-traditional students.Learning environment with a mixed population is also described from the point of view of benefits to traditional, non-traditional students and faculty.",No methods found.
2024,https://openalex.org/W4391174596,Medicine,Generative Large Language Models for Detection of Speech Recognition Errors in Radiology Reports,"This study evaluated the ability of generative large language models (LLMs) to detect speech recognition errors in radiology reports. A dataset of 3233 CT and MRI reports was assessed by radiologists for speech recognition errors. Errors were categorized as clinically significant or not clinically significant. Performances of five generative LLMs—GPT-3.5-turbo, GPT-4, text-davinci-003, Llama-v2–70B-chat, and Bard—were compared in detecting these errors, using manual error detection as the reference standard. Prompt engineering was used to optimize model performance. GPT-4 demonstrated high accuracy in detecting clinically significant errors (precision, 76.9%; recall, 100%; F1 score, 86.9%) and not clinically significant errors (precision, 93.9%; recall, 94.7%; F1 score, 94.3%). Text-davinci-003 achieved F1 scores of 72% and 46.6% for clinically significant and not clinically significant errors, respectively. GPT-3.5-turbo obtained 59.1% and 32.2% F1 scores, while Llama-v2–70B-chat scored 72.8% and 47.7%. Bard showed the lowest accuracy, with F1 scores of 47.5% and 20.9%. GPT-4 effectively identified challenging errors of nonsense phrases and internally inconsistent statements. Longer reports, resident dictation, and overnight shifts were associated with higher error rates. In conclusion, advanced generative LLMs show potential for automatic detection of speech recognition errors in radiology reports. Keywords: CT, Large Language Model, Machine Learning, MRI, Natural Language Processing, Radiology Reports, Speech, Unsupervised Learning Supplemental material is available for this article. © RSNA, 2024","<method>generative large language models (LLMs)</method>, <method>GPT-3.5-turbo</method>, <method>GPT-4</method>, <method>text-davinci-003</method>, <method>Llama-v2–70B-chat</method>, <method>Bard</method>, <method>prompt engineering</method>"
2024,https://openalex.org/W4391480252,Medicine,Performance of convolutional neural networks for the classification of brain tumors using magnetic resonance imaging,"Brain tumors are a diverse group of neoplasms that are challenging to detect and classify due to their varying characteristics. Deep learning techniques have proven to be effective in tumor classification. However, there is a lack of studies that compare these techniques using a common methodology. This work aims to analyze the performance of convolutional neural networks in the classification of brain tumors. We propose a network consisting of a few convolutional layers, batch normalization, and max-pooling. Then, we explore recent deep architectures, such as VGG, ResNet, EfficientNet, or ConvNeXt. The study relies on two magnetic resonance imaging datasets with over 3000 images of three types of tumors –gliomas, meningiomas, and pituitary tumors–, as well as images without tumors. We determine the optimal hyperparameters of the networks using the training and validation sets. The training and test sets are used to assess the performance of the models from different perspectives, including training from scratch, data augmentation, transfer learning, and fine-tuning. The experiments are performed using the TensorFlow and Keras libraries in Python. We compare the accuracy of the models and analyze their complexity based on the capacity of the networks, their training times, and image throughput. Several networks achieve high accuracy rates on both datasets, with the best model achieving 98.7% accuracy, which is on par with state-of-the-art methods. The average precision for each type of tumor is 94.3% for gliomas, 93.8% for meningiomas, 97.9% for pituitary tumors, and 95.3% for images without tumors. VGG is the largest model with over 171 million parameters, whereas MobileNet and EfficientNetB0 are the smallest ones with 3.2 and 5.9 million parameters, respectively. These two neural networks are also the fastest to train with 23.7 and 25.4 seconds per epoch, respectively. On the other hand, ConvNext is the slowest model with 58.2 seconds per epoch. Our custom model obtained the highest image throughput with 234.37 images per second, followed by MobileNet with 226 images per second. ConvNext obtained the smallest throughput with 97.35 images per second. ResNet, MobileNet, and EfficientNet are the most accurate networks, with MobileNet and EfficientNet demonstrating superior performance in terms of complexity. Most models achieve the best accuracy using transfer learning followed by a fine-tuning step. However, data augmentation does not contribute to increasing the accuracy of the models in general.","<method>convolutional neural networks</method>, <method>VGG</method>, <method>ResNet</method>, <method>EfficientNet</method>, <method>ConvNeXt</method>, <method>training from scratch</method>, <method>data augmentation</method>, <method>transfer learning</method>, <method>fine-tuning</method>"
2024,https://openalex.org/W4391697089,Medicine,A Minimal and Multi-Source Recording Setup for Ankle Joint Kinematics Estimation During Walking Using Only Proximal Information From Lower Limb,"In this study, a minimal setup for the ankle joint kinematics estimation is proposed relying only on proximal information of the lower-limb, i.e. thigh muscles activity and joint kinematics. To this purpose, myoelectric activity of Rectus Femoris (RF), Biceps Femoris (BF), and Vastus Medialis (VM) were recorded by surface electromyography (sEMG) from six healthy subjects during unconstrained walking task. For each subject, the angular kinematics of hip and ankle joints were synchronously recorded with sEMG signal for a total of 288 gait cycles. Two feature sets were extracted from sEMG signals, i.e. time domain (TD) and wavelet (WT) and compared to have a compromise between the reliability and computational capacity, they were used for feeding three regression models, i.e. Artificial Neural Networks, Random Forest, and Least Squares - Support Vector Machine (LS-SVM). BF together with LS-SVM provided the best ankle angle estimation in both TD and WT domains (RMSE < 5.6 deg). The inclusion of Hip joint trajectory significantly enhanced the regression performances of the model (RMSE < 4.5 deg). Results showed the feasibility of estimating the ankle trajectory using only proximal and limited information from the lower limb which would maximize a potential transfemoral amputee user's comfortability while facing the challenge of having a small amount of information thus requiring robust data-driven models. These findings represent a significant step towards the development of a minimal setup useful for the control design of ankle active prosthetics and rehabilitative solutions.","<method>Artificial Neural Networks</method>, <method>Random Forest</method>, <method>Least Squares - Support Vector Machine (LS-SVM)</method>"
2024,https://openalex.org/W4399685394,Engineering,Advanced Modelling of Soil Organic Carbon Content in Coal Mining Areas Using Integrated Spectral Analysis: A Dengcao Coal Mine Case Study,"Effective modelling and integrated spectral analysis approaches can advance modelling precision. To develop an integrated spectral forecast modelling of soil organic carbon (SOC), this research investigated a mining coal in Dengcao Coal Mine Area, Zhengzhou. The study utilizes the Lasso and Ranger algorithms were utilized in spectral band analysis. Four primary models employed during this process include Artificial Neural Network (ANN), Support Vector Machine, Random Forest (RF), and Partial Least Squares Regression (PLSR). The ideal model was chosen. The results showed that, in contrast to when band collection was based on Lasso algorithm modelling, model precision was higher when it was based on the Ranger algorithm. ANN model had an ideal goodness acceptance, and the modelling developed by RF showed the steadiest modelling consequences. Based on the results, a distinct method is proposed in this study for band assortment at the earlier stage of integrated spectral modelling of SOC. The Ranger method can be used to check the spectral particles, and RF or ANN can be chosen to develop the prediction modelling based on different statistics sets, which is appropriate to create the prediction modelling of SOC content in Dengcao Coal Mine Area. This research avails a position for the integrated spectral of Analysis for Advanced Modelling of Soil Organic Carbon Content in Coal Sources alongside a theoretical foundation for innovating portable device for the integrated spectral assessment of SOC content in coal mining habitats. This study might be significant for the changing modelling and monitoring of SOC in mining and environmental areas.","<method>Lasso</method>, <method>Ranger</method>, <method>Artificial Neural Network (ANN)</method>, <method>Support Vector Machine</method>, <method>Random Forest (RF)</method>, <method>Partial Least Squares Regression (PLSR)</method>"
2024,https://openalex.org/W4392754525,Engineering,"Applying Lean Principles to Eliminate Project Waste, Maximize Value, Cut Superfluous Steps, Reduce Rework and Focus on Customer Centricity","With increasingly complex demands, dynamic environments, and rapid technology changes defining business landscapes, project management methodologies are prime for disruption. Traditional project models – characterized by rigid, sequential stage-gates and siloed functional groups – struggle with wastefulness, reactive mindsets, and misalignment to customer purpose. As such, practitioners are turning to lean philosophies pioneered in manufacturing but applicable across sectors. This paper examines deploying lean principles to project environments to eliminate activities that do not directly add customer value (waste), amplify learning, decide slowly but deliver quickly, empower teams, and continuously improve. Core facets of lean covered include Value Stream Mapping to visualize workflow and identify waste; Kanban systems to limit work-in-progress, facilitate pull-based work authorization, and surface bottlenecks; Root Cause Analysis to get to the heart of problems; 5S activities for well-organized, mistake-proof workstations; and Kaizen events for rapid iterations. As illustrated through integrated case studies, these tools and mindsets facilitate lean’s primary aim within projects – determining what matters for the end-customer and optimizing all activities to directly serve this purpose. Tactics covered include defining value from the beneficiary standpoint early on, focusing on enhancing workflow from end-to-end rather than functional silos, and instilling learning and improvement mechanisms at all levels – from C-suite leadership to ground floor production. The paper closes by delineating a gradual implementation methodology – factoring in change management challenges – as well as outlining skills development required for staff to transition toward cross-functional, accountable, lean-oriented teams. While specifics vary across industries and project types, overarching insights suggest the principles represent the future of project delivery – cutting waste-related costs by upwards of 30%, accelerating timelines by over a third, minimizing scrap/rework to near-zero levels, and serving customer goals substantially better. For leaders to position projects and organizations for increasing marketplace complexity, leanness is pivotal.",No methods found.
2024,https://openalex.org/W4399657851,Engineering,Fusion of finite element and machine learning methods to predict rock shear strength parameters,"Abstract The trial-and-error method for calibrating rock mechanics parameters has the disadvantages of complexity, being time-consuming, and difficulty in ensuring accuracy. Harnessing the repeatability and scalability intrinsic to numerical simulation calculations and amalgamating them with the data-driven attributes of machine learning methods, this study uses the finite element analysis software RS2 to establish 252 sets of sandstone sample data. The recursive feature elimination and cross-validation method is employed for feature selection. The shear strength parameters of sandstone are predicted using machine learning models optimized by the particle swarm optimization (PSO) algorithm, including the backpropagation neural network, Bayesian ridge regression, support vector regression (SVR), and light gradient boosting machine. The predicted value of cohesion is proposed as the input feature to predict the friction angle. The results indicate that the optimal input characteristics for predicting cohesion are elastic modulus, Poisson's ratio, peak stress, and peak strain, while the optimal input characteristics for predicting friction angle are peak stress and cohesion. The PSO-SVR model demonstrates the best performance. The maximum error between the predicted values of cohesion and friction angle and the calculated results of RSData program are 3.5% and 4.31%, respectively. The finite element calculation is in good agreement with the stress–strain curve obtained in the laboratory. The sensitivity analysis indicates that SVR's prediction performance for cohesion and friction angle tends to be stable when the sample size is &amp;gt;25. These results offer a valuable reference for accurately predicting rock mechanics parameters.","<method>recursive feature elimination</method>, <method>cross-validation</method>, <method>particle swarm optimization (PSO)</method>, <method>backpropagation neural network</method>, <method>Bayesian ridge regression</method>, <method>support vector regression (SVR)</method>, <method>light gradient boosting machine</method>"
2024,https://openalex.org/W4392872715,Engineering,GLC_FCS30D: the first global 30 m land-cover dynamics monitoring product with a fine classification system for the period from 1985 to 2022 generated using dense-time-series Landsat imagery and the continuous change-detection method,"Abstract. Land-cover change has been identified as an important cause or driving force of global climate change and is a significant research topic. Over the past few decades, global land-cover mapping has progressed; however, long-time-series global land-cover-change monitoring data are still sparse, especially those at 30 m resolution. In this study, we describe GLC_FCS30D, a novel global 30 m land-cover dynamics monitoring dataset containing 35 land-cover subcategories and covering the period 1985–2022 in 26 time steps (maps were updated every 5 years before 2000 and annually after 2000). GLC_FCS30D has been developed using continuous change detection and all available Landsat imagery based on the Google Earth Engine platform. Specifically, we first take advantage of the continuous change-detection model and the full time series of Landsat observations to capture the time points of changed pixels and identify the temporally stable areas. Then, we apply a spatiotemporal refinement method to derive the globally distributed and high-confidence training samples from these temporally stable areas. Next, local adaptive classification models are used to update the land-cover information for the changed pixels, and a temporal-consistency optimization algorithm is adopted to improve their temporal stability and suppress some false changes. Further, the GLC_FCS30D product is validated using 84 526 globally distributed validation samples from 2020. It achieves an overall accuracy of 80.88 % (±0.27 %) for the basic classification system (10 major land-cover types) and 73.04 % (±0.30 %) for the LCCS (Land Cover Classification System) level-1 validation system (17 LCCS land-cover types). Meanwhile, two third-party time-series datasets used for validation from the United States and Europe Union are also collected for analyzing accuracy variations, and the results show that GLC_FCS30D offers significant stability in terms of variation across the accuracy time series and achieves mean accuracies of 79.50 % (±0.50 %) and 81.91 % (±0.09 %) over the two regions. Lastly, we draw conclusions about the global land-cover-change information from the GLC_FCS30D dataset; namely, that forest and cropland variations have dominated global land-cover change over past 37 years, the net loss of forests reached about 2.5 million km2, and the net gain in cropland area is approximately 1.3 million km2. Therefore, the novel dataset GLC_FCS30D is an accurate land-cover-dynamics time-series monitoring product that benefits from its diverse classification system, high spatial resolution, and long time span (1985–2022); thus, it will effectively support global climate change research and promote sustainable development analysis. The GLC_FCS30D dataset is available via https://doi.org/10.5281/zenodo.8239305 (Liu et al., 2023).","<method>continuous change-detection model</method>, <method>spatiotemporal refinement method</method>, <method>local adaptive classification models</method>, <method>temporal-consistency optimization algorithm</method>"
2024,https://openalex.org/W4399450035,Engineering,Power Hungry Processing: Watts Driving the Cost of AI Deployment?,"Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of ""generality"" comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and 'general-purpose' models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.","<method>finetuned models</method>, <method>multi-purpose generative architectures</method>"
2024,https://openalex.org/W4401070841,Engineering,Transformer-Based Visual Segmentation: A Survey,"Visual segmentation seeks to partition images, video frames, or point clouds into multiple segments or groups. This technique has numerous real-world applications, such as autonomous driving, image editing, robot sensing, and medical analysis. Over the past decade, deep learning-based methods have made remarkable strides in this area. Recently, transformers, a type of neural network based on self-attention originally designed for natural language processing, have considerably surpassed previous convolutional or recurrent approaches in various vision processing tasks. Specifically, vision transformers offer robust, unified, and even simpler solutions for various segmentation tasks. This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements. We first review the background, encompassing problem definitions, datasets, and prior convolutional methods. Next, we summarize a meta-architecture that unifies all recent transformer-based approaches. Based on this meta-architecture, we examine various method designs, including modifications to the meta-architecture and associated applications. We also present several specific subfields, including 3D point cloud segmentation, foundation model tuning, domain-aware segmentation, efficient segmentation, and medical segmentation. Additionally, we compile and re-evaluate the reviewed methods on several well-established datasets. Finally, we identify open challenges in this field and propose directions for future research. The project page can be found at <uri xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">https://github.com/lxtGH/Awesome-Segmentation-With-Transformer</uri> .","<method>deep learning-based methods</method>, <method>transformers</method>, <method>convolutional approaches</method>, <method>recurrent approaches</method>, <method>vision transformers</method>"
2024,https://openalex.org/W4390837884,Engineering,The deep learning applications in IoT-based bio- and medical informatics: a systematic literature review,"Abstract Nowadays, machine learning (ML) has attained a high level of achievement in many contexts. Considering the significance of ML in medical and bioinformatics owing to its accuracy, many investigators discussed multiple solutions for developing the function of medical and bioinformatics challenges using deep learning (DL) techniques. The importance of DL in Internet of Things (IoT)-based bio- and medical informatics lies in its ability to analyze and interpret large amounts of complex and diverse data in real time, providing insights that can improve healthcare outcomes and increase efficiency in the healthcare industry. Several applications of DL in IoT-based bio- and medical informatics include diagnosis, treatment recommendation, clinical decision support, image analysis, wearable monitoring, and drug discovery. The review aims to comprehensively evaluate and synthesize the existing body of the literature on applying deep learning in the intersection of the IoT with bio- and medical informatics. In this paper, we categorized the most cutting-edge DL solutions for medical and bioinformatics issues into five categories based on the DL technique utilized: convolutional neural network , recurrent neural network , generative adversarial network , multilayer perception , and hybrid methods. A systematic literature review was applied to study each one in terms of effective properties, like the main idea, benefits, drawbacks, methods, simulation environment, and datasets. After that, cutting-edge research on DL approaches and applications for bioinformatics concerns was emphasized. In addition, several challenges that contributed to DL implementation for medical and bioinformatics have been addressed, which are predicted to motivate more studies to develop medical and bioinformatics research progressively. According to the findings, most articles are evaluated using features like accuracy, sensitivity, specificity, F -score, latency, adaptability, and scalability.","<method>convolutional neural network</method>, <method>recurrent neural network</method>, <method>generative adversarial network</method>, <method>multilayer perception</method>, <method>hybrid methods</method>"
2024,https://openalex.org/W4391018556,Engineering,Battery safety: Machine learning-based prognostics,"Lithium-ion batteries play a pivotal role in a wide range of applications, from electronic devices to large-scale electrified transportation systems and grid-scale energy storage. Nevertheless, they are vulnerable to both progressive aging and unexpected failures, which can result in catastrophic events such as explosions or fires. Given their expanding global presence, the safety of these batteries and potential hazards from serious malfunctions are now major public concerns. Over the past decade, scholars and industry experts are intensively exploring methods to monitor battery safety, spanning from materials to cell, pack and system levels and across various spectral, spatial, and temporal scopes. In this Review, we start by summarizing the mechanisms and nature of battery failures. Following this, we explore the intricacies in predicting battery system evolution and delve into the specialized knowledge essential for data-driven, machine learning models. We offer an exhaustive review spotlighting the latest strides in battery fault diagnosis and failure prognosis via an array of machine learning approaches. Our discussion encompasses: (1) supervised and reinforcement learning integrated with battery models, apt for predicting faults/failures and probing into failure causes and safety protocols at the cell level; (2) unsupervised, semi-supervised, and self-supervised learning, advantageous for harnessing vast data sets from battery modules/packs; (3) few-shot learning tailored for gleaning insights from scarce examples, alongside physics-informed machine learning to bolster model generalization and optimize training in data-scarce settings. We conclude by casting light on the prospective horizons of comprehensive, real-world battery prognostics and management.","<method>supervised learning</method>, <method>reinforcement learning</method>, <method>unsupervised learning</method>, <method>semi-supervised learning</method>, <method>self-supervised learning</method>, <method>few-shot learning</method>, <method>physics-informed machine learning</method>"
2024,https://openalex.org/W4391804837,Engineering,Towards sustainable power generation: Recent advancements in floating photovoltaic technologies,"Floating solar photovoltaic systems are rapidly gaining traction due to their potential for higher energy yield and efficiency compared to conventional land-based solar photovoltaic systems. Recent studies indicate that this technology generates 0.6% to 4.4% more energy and exhibits efficiency improvements ranging from 0.1% to 4.45% over its land-based counterpart. Numerous studies conducted on evaluating this innovative technology have been reported, providing various insights required for further development. Unfortunately, these important pieces of information have been scattered: a comprehensive compilation of these findings is currently unavailable, resulting in a significant gap in knowledge and information. Thus, the main objective of this work is to provide a comprehensive insight into this new technology, various research and developments that have been reported and potential future development. The critical review indicates that advancements in this technology shall focus on improved floating structure design, robust instrumentation, wireless monitoring, and sensing capabilities. Moreover, novel technological solutions such as tracking systems, bi-facial solar panels, satellite-based array optimization, programming algorithms for grid integration, and artificial intelligence shall be further explored. Additionally, it was found that the integration of floating photovoltaic in marine environments and hydropower reservoirs holds significant promise for transforming global energy production. Despite these advancements, several hurdles remain, including safety concerns, risks associated with electricity–water interactions, standardization issues, national policy considerations, and potential increases in surrounding ground temperatures. It is vital to address the remaining challenges and leverage technological innovations to realize the full potential of floating photovoltaics in the transition towards sustainable energy production.",<method>artificial intelligence</method>
2024,https://openalex.org/W4394935921,Engineering,Machine learning-based predictive model for thermal comfort and energy optimization in smart buildings,"In the current context of energy transition and increasing climate change, optimizing building performance has become a critical objective. Efficient energy use and occupant comfort are paramount considerations in building design and operation. To address these challenges, this study introduces a predictive model leveraging Machine Learning (ML) algorithms. The model aims to predict thermal comfort levels and optimize energy consumption in Heating, Ventilation, and Air Conditioning (HVAC) systems. Four distinct ML algorithms Support Vector Machine (SVM), Artificial Neural Network (ANN), Random Forest (RF), and EXtreme Gradient Boosting (XGBOOST) are employed for this purpose. Data for the model is collected using a network of Raspberry Pi boards equipped with multiple sensors. Performance evaluation of the ML algorithms is conducted using statistical error metrics, including, Root Mean Square Error (RMSE), Mean Square Error (MSE), Mean Absolute Error (MAE), and coefficient of determination (R2). Results reveal that the RF and XGBOOST algorithms exhibit superior performance, achieving accuracies of 96.7% and 9.64% respectively. In contrast, the SVM algorithm demonstrates inferior performance with a R2 of 81.1%. These findings underscore the predictive capability of the RF and XGBOOST model in forecasting Predicted Mean Vote (PMV) values. The proposed model holds promise for enhancing occupant thermal comfort in buildings while simultaneously optimizing energy consumption in HVAC systems. Further research could explore the practical applications of these findings in building design and operation.","<method>Support Vector Machine (SVM)</method>, <method>Artificial Neural Network (ANN)</method>, <method>Random Forest (RF)</method>, <method>EXtreme Gradient Boosting (XGBOOST)</method>"
2024,https://openalex.org/W4400020165,Engineering,"Big data, machine learning, and digital twin assisted additive manufacturing: A review","Additive manufacturing (AM) has undergone significant development over the past decades, resulting in vast amounts of data that carry valuable information. Numerous research studies have been conducted to extract insights from AM data and utilize it for optimizing various aspects such as the manufacturing process, supply chain, and real-time monitoring. Data integration into proposed digital twin frameworks and the application of machine learning techniques is expected to play pivotal roles in advancing AM in the future. In this paper, we provide an overview of machine learning and digital twin-assisted AM. On one hand, we discuss the research domain and highlight the machine-learning methods utilized in this field, including material analysis, design optimization, process parameter optimization, defect detection and monitoring, and sustainability. On the other hand, we examine the status of digital twin-assisted AM from the current research status to the technical approach and offer insights into future developments and perspectives in this area. This review paper aims to examine present research and development in the convergence of big data, machine learning, and digital twin-assisted AM. Although there are numerous review papers on machine learning for additive manufacturing and others on digital twins for AM, no existing paper has considered how these concepts are intrinsically connected and interrelated. Our paper is the first to integrate the three concepts big data, machine learning, and digital twins and propose a cohesive framework for how they can work together to improve the efficiency, accuracy, and sustainability of AM processes. By exploring latest advancements and applications within these domains, our objective is to emphasize the potential advantages and future possibilities associated with integration of these technologies in AM.",<method>machine learning</method>
2024,https://openalex.org/W4390494339,Engineering,"A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions","Automating the monitoring of industrial processes has the potential to enhance efficiency and optimize quality by promptly detecting abnormal events and thus facilitating timely interventions. Deep learning, with its capacity to discern non-trivial patterns within large datasets, plays a pivotal role in this process. Standard deep learning methods are suitable to solve a specific task given a specific type of data. During training, deep learning demands large volumes of labeled data. However, due to the dynamic nature of the industrial processes and environment, it is impractical to acquire large-scale labeled data for standard deep learning training for every slightly different case anew. Deep transfer learning offers a solution to this problem. By leveraging knowledge from related tasks and accounting for variations in data distributions, the transfer learning framework solves new tasks with little or even no additional labeled data. The approach bypasses the need to retrain a model from scratch for every new setup and dramatically reduces the labeled data requirement. This survey first provides an in-depth review of deep transfer learning, examining the problem settings of transfer learning and classifying the prevailing deep transfer learning methods. Moreover, we delve into applications of deep transfer learning in the context of a broad spectrum of time series anomaly detection tasks prevalent in primary industrial domains, e.g., manufacturing process monitoring, predictive maintenance, energy management, and infrastructure facility monitoring. We discuss the challenges and limitations of deep transfer learning in industrial contexts and conclude the survey with practical directions and actionable suggestions to address the need to leverage diverse time series data for anomaly detection in an increasingly dynamic production environment.","<method>deep learning</method>, <method>standard deep learning methods</method>, <method>deep transfer learning</method>, <method>transfer learning framework</method>"
2024,https://openalex.org/W4391822043,Engineering,Role of information processing and digital supply chain in supply chain resilience through supply chain risk management,"Purpose Supply chain (SC) management is more challenging than ever. Significantly, the pandemic has provoked global and economic destruction that appeared in the manufacturing industry as a “black swan.” Therefore, the purpose of this study was to examine the role of information processing and digital supply chain in supply chain resilience through supply chain risk management. Design/methodology/approach This study examines SC risk management and resilience from an information processing theory perspective. The authors used data collected from 251 SC professionals in the manufacturing industry, and the authors used a quantitative method to analyze the data. The data was analyzed using partial least squares-structural equation modeling. To confirm the higher-order measurement model, the authors used SmartPLS version 4 software. Findings This study found that information processing capability (disruptive orientation and visibility in high-order) and digital SC significantly and positively affect SC risk management and resilience. Similarly, SC risk management positively mediates the relationship between information processing capability and digital SC. However, information processing capability was found to have a more substantial effect on SC risk management than the digital SC. Research limitations/implications This study has both academic and practical contributions. It contributed to existing information processing theory, and manufacturing firms can improve their performance by proactively responding to SC disruptions by recognizing the pivotal role of study variables in risk management for a resilient SC. Originality/value The conceptual model of this study is based on information processing theory, which asserts that synchronizing information processing capabilities and digital SCs allows a firm to deal with unplanned events. SC disruption orientation and visibility are considered risk controllers as they allow the firms to be more proactive. An integrated model of conceptualizing the disruption orientation, visibility (higher-order) and digital SC with information processing theory makes this research novel.",No methods found.
2024,https://openalex.org/W4399326707,Engineering,Enhancing precision agriculture: A comprehensive review of machine learning and AI vision applications in all-terrain vehicle for farm automation,"The automation of all-terrain vehicles (ATVs) through the integration of advanced technologies such as machine learning (ML) and artificial intelligence (AI) vision has significantly changed precision agriculture. This paper aims to analyse and develop trends to provide comprehensive knowledge of the current state of ATV-based precision agriculture and the future possibilities of ML and AI. A bibliometric analysis was conducted through network diagram with keywords taken from previous publications in the domain. This review comprehensively analyses the potential of machine learning and artificial intelligence in transforming farming operations through the automation of tasks and the deployment of all-terrain vehicles. The research extensively analyses how machine learning methods have influenced several aspects of agricultural activities, such as planting, harvesting, spraying, weeding, crop monitoring, and others. AI vision systems are being researched for their ability to enhance precise and prompt decision-making in ATV-driven agricultural automation. These technologies have been thoroughly tested to show how they can improve crop yield, reducing overall investment, and make farming more efficient. Examples include machine learning-based seeding accuracy, AI-enabled crop health monitoring, and the use of AI vision for accurate pesticide application. The assessment examines challenges such as data privacy problems and scalability constraints, along with potential advancements and future prospects in the field. This will assist researchers and practitioners in making well-informed judgments regarding farming practices that are efficient, sustainable, and technologically robust.","<method>machine learning</method>, <method>artificial intelligence vision</method>, <method>machine learning methods</method>, <method>AI vision systems</method>, <method>machine learning-based seeding accuracy</method>, <method>AI-enabled crop health monitoring</method>, <method>AI vision for accurate pesticide application</method>"
2024,https://openalex.org/W4391332961,Engineering,A novel framework for developing environmentally sustainable and cost-effective ultra-high-performance concrete (UHPC) using advanced machine learning and multi-objective optimization techniques,"This study aims to propose a novel framework for strength prediction and multi-objective optimization (MOO) of economical and environmentally sustainable ultra-high-performance concrete (UHPC) which aids in intelligent, sustainable, and resilient construction. Different tree- and boosting ensemble-based machine learning (ML) models are integrated to form an accurate and reliable prediction model for the uniaxial compressive strength of UHPC. The optimized models are integrated into a super learner model, resulting in a robust predictive model that is used as one of the objective functions in the MOO problem. A total of 19 objective functions are considered, including cost, uniaxial compressive strength, and 17 environmental impact categories that comprehensively evaluate the environmental sustainability of the UHPC mix. The resulting impacts from the mid-point indicators were calculated using the Eco-invent v3.7 Life Cycle Inventory database. The results showed that the super learner model accurately predicted the uniaxial compressive strength of UHPC. The MOO resulted in Pareto fronts, demonstrating the trade-off among the uniaxial compressive strength, cost, and environmental sustainability of the mix and a broad range of solutions that can be obtained for the 19 objectives. The study provides a useful tool for designers and decision-makers to select the optimal UHPC mixture that meets specific project requirements. Finally, for the practical application of the ML predictive model and MOO algorithm for UHPC, a graphical user interface-based software tool, FAI-OSUSCONCRET, was developed. This software tool offers fast, accurate, and intelligent predictions and multi-objective optimizations tailored to specific project requirements, thus resulting in a UHPC mixture that perfectly meets project needs.","<method>tree-based ensemble machine learning models</method>, <method>boosting ensemble-based machine learning models</method>, <method>super learner model</method>, <method>multi-objective optimization (MOO)</method>"
2024,https://openalex.org/W4396919436,Engineering,Mechanical (static and dynamic) characterization and thermal stability of hybrid green composites for engineering applications,"The development of sustainability in industry has made it imperative to utilize waste and accessible natural resources properly. Our goal is to transform the naturally occurring resources, tamarind seed powder (TSP), eelgrass (EG), and adamant creeper (AC), into goods that are beneficial to society. In this study, different weight proportions of alkali-treated AC, EG, and TSP were combined to make hybrid natural fiber (NF) composite materials using the hand layup process. The material strength of the alkali treated (15% AC, 20% EG, and 15% TSP) composite sample was attained 211 MPa, 278 MPa, 21.7 J/m2, and 84.6 for tensile, flexural, impact, and hardness in that sequence. The storage modulus, loss modulus, and mechanical loss factor were measured and plotted against temperature in a dynamic mechanical analysis (DMA) experiment. All composites had higher storage and loss moduli and a lower mechanical loss factor, indicating more elastic properties. The findings demonstrated that adding fiber-filled TSP fillers tended to make the epoxy matrix more viscoelastic stiffness. As the weight percentage of fibers in the composite increases, discernible changes in the structural damping capacity have also been noticed. The thermal characteristics of composites were inspected by thermogravimetric analysis (TGA), and the TGA outcomes substantiated that the thermal constancy of a 20% EG-reinforced composite was good. The mechanical, dynamic, and thermal qualities of this composite material will yield significant benefits for the industrial sector.",No methods found.
2024,https://openalex.org/W4391168980,Engineering,Utilizing Hybrid Machine Learning and Soft Computing Techniques for Landslide Susceptibility Mapping in a Drainage Basin,"The hydrological system of thebasin of Lake Urmia is complex, deriving its supply from a network comprising 13 perennial rivers, along withnumerous small springs and direct precipitation onto the lake’s surface. Among these contributors, approximately half of the inflow is attributed to the Zarrineh River and the Simineh River. Remarkably, Lake Urmia lacks a natural outlet, with its water loss occurring solely through evaporation processes. This study employed a comprehensive methodology integrating ground surveys, remote sensing analyses, and meticulous documentation of historical landslides within the basin as primary information sources. Through this investigative approach, we preciselyidentified and geolocated a total of 512 historical landslide occurrences across the Urmia Lake drainage basin, leveraging GPS technology for precision. Thisarticle introduces a suite of hybrid machine learning predictive models, such as support-vector machine (SVM), random forest (RF), decision trees (DT), logistic regression (LR), fuzzy logic (FL), and the technique for order of preference by similarity to the ideal solution (TOPSIS). These models were strategically deployed to assess landslide susceptibility within the region. The outcomes of the landslide susceptibility assessment reveal that the main high susceptible zones for landslide occurrence are concentrated in the northwestern, northern, northeastern, and some southern and southeastern areas of the region. Moreover, when considering the implementation of predictions using different algorithms, it became evident that SVM exhibited superior performance regardingboth accuracy (0.89) and precision (0.89), followed by RF, with and accuracy of 0.83 and a precision of 0.83. However, it is noteworthy that TOPSIS yielded the lowest accuracy value among the algorithms assessed.","<method>support-vector machine (SVM)</method>, <method>random forest (RF)</method>, <method>decision trees (DT)</method>, <method>logistic regression (LR)</method>, <method>fuzzy logic (FL)</method>, <method>technique for order of preference by similarity to the ideal solution (TOPSIS)</method>"
2024,https://openalex.org/W4391558404,Engineering,Exploring the Potential of ChatGPT in Automated Code Refinement: An Empirical Study,"Code review is an essential activity for ensuring the quality and maintainability of software projects. However, it is a time-consuming and often error-prone task that can significantly impact the development process. Recently, ChatGPT, a cutting-edge language model, has demonstrated impressive performance in various natural language processing tasks, suggesting its potential to automate code review processes. However, it is still unclear how well ChatGPT performs in code review tasks. To fill this gap, in this paper, we conduct the first empirical study to understand the capabilities of ChatGPT in code review tasks, specifically focusing on automated code refinement based on given code reviews. To conduct the study, we select the existing benchmark CodeReview and construct a new code review dataset with high quality. We use CodeReviewer, a state-of-the-art code review tool, as a baseline for comparison with ChatGPT. Our results show that ChatGPT outperforms CodeReviewer in code refinement tasks. Specifically, our results show that ChatGPT achieves higher EM and BLEU scores of 22.78 and 76.44 respectively, while the state-of-the-art method achieves only 15.50 and 62.88 on a high-quality code review dataset. We further identify the root causes for ChatGPT's underperformance and propose several strategies to mitigate these challenges. Our study provides insights into the potential of ChatGPT in automating the code review process, and highlights the potential research directions.","<method>ChatGPT</method>, <method>CodeReviewer</method>"
2024,https://openalex.org/W4391831565,Engineering,Comparison of Random Forest and XGBoost Classifiers Using Integrated Optical and SAR Features for Mapping Urban Impervious Surface,"The integration of optical and SAR datasets through ensemble machine learning models shows promising results in urban remote sensing applications. The integration of multi-sensor datasets enhances the accuracy of information extraction. This research presents a comparison of two ensemble machine learning classifiers (random forest and extreme gradient boost (XGBoost)) classifiers using an integration of optical and SAR features and simple layer stacking (SLS) techniques. Therefore, Sentinel-1 (SAR) and Landsat 8 (optical) datasets were used with SAR textures and enhanced modified indices to extract features for the year 2023. The classification process utilized two machine learning algorithms, random forest and XGBoost, for urban impervious surface extraction. The study focused on three significant East Asian cities with diverse urban dynamics: Jakarta, Manila, and Seoul. This research proposed a novel index called the Normalized Blue Water Index (NBWI), which distinguishes water from other features and was utilized as an optical feature. Results showed an overall accuracy of 81% for UIS classification using XGBoost and 77% with RF while classifying land use land cover into four major classes (water, vegetation, bare soil, and urban impervious). However, the proposed framework with the XGBoost classifier outperformed the RF algorithm and Dynamic World (DW) data product and comparatively showed higher classification accuracy. Still, all three results show poor separability with bare soil class compared to ground truth data. XGBoost outperformed random forest and Dynamic World in classification accuracy, highlighting its potential use in urban remote sensing applications.","<method>ensemble machine learning models</method>, <method>random forest</method>, <method>extreme gradient boost (XGBoost)</method>, <method>simple layer stacking (SLS)</method>"
2024,https://openalex.org/W4390486945,Engineering,4400 TEU cargo ship dynamic analysis by Gaidai reliability method,"Abstract Modern cargo vessel transport constitutes an important part of global economy; hence it is of paramount importance to develop novel, more efficient reliability methods for cargo ships, especially if onboard recorded data is available. Classic reliability methods, dealing with timeseries, do not have the advantage of dealing efficiently with system high dimensionality and cross-correlation between different dimensions. This study validates novel structural reliability method suitable for multi-dimensional structural systems versus a well-established bivariate statistical method. An example of this reliability study was a chosen container ship subjected to large deck panel stresses during sailing. Risk of losing containers, due to extreme motions is the primary concern for ship cargo transport. Due to non-stationarity and complicated nonlinearities of both waves and ship motions, it is challenging to model such a phenomenon. In the case of extreme motions, the role of nonlinearities dramatically increases, activating effects of second and higher order. Moreover, laboratory tests may also be questioned. Therefore, data measured on actual ships during their voyages in harsh weather provides a unique insight into statistics of ship motions. This study aimed at benchmarking and validation of the state-of-the-art method, which enables extraction of the necessary information about the extreme system dynamics from onboard measured time histories. The method proposed in this study opens up broad possibilities of predicting simply, yet efficiently potential failure or structural damage risks for the nonlinear multi-dimensional cargo vessel dynamic systems as a whole. Note that advocated novel reliability method can be used for a wide range of complex engineering systems, thus not limited to cargo ship only.",No methods found.
2024,https://openalex.org/W4390511794,Engineering,Firefly algorithm based WSN-IoT security enhancement with machine learning for intrusion detection,"Abstract A Wireless Sensor Network (WSN) aided by the Internet of Things (IoT) is a collaborative system of WSN systems and IoT networks are work to exchange, gather, and handle data. The primary objective of this collaboration is to enhance data analysis and automation to facilitate improved decision-making. Securing IoT with the assistance of WSN necessitates the implementation of protective measures to confirm the safety and reliability of the interconnected WSN and IoT components. This research significantly advances the current state of the art in IoT and WSN security by synergistically harnessing the potential of machine learning and the Firefly Algorithm. The contributions of this work are twofold: firstly, the proposed FA-ML technique exhibits an exceptional capability to enhance intrusion detection accuracy within the WSN-IoT landscape. Secondly, the amalgamation of the Firefly Algorithm and machine learning introduces a novel dimension to the domain of security-oriented optimization techniques. The implications of this research resonate across various sectors, ranging from critical infrastructure protection to industrial automation and beyond, where safeguarding the integrity of interconnected systems are of paramount importance. The amalgamation of cutting-edge machine learning and bio-inspired algorithms marks a pivotal step forward in crafting robust and intelligent security measures for the evolving landscape of IoT-driven technologies. For intrusion detection in the WSN-IoT, the FA-ML method employs a support vector machine (SVM) machine model for classification with parameter tuning accomplished using a Grey Wolf Optimizer (GWO) algorithm. The experimental evaluation is simulated using NSL-KDD Dataset, revealing the remarkable enhancement of the FA-ML technique, achieving a maximum accuracy of 99.34%. In comparison, the KNN-PSO and XGBoost models achieved lower accuracies of 96.42% and 95.36%, respectively. The findings validate the potential of the FA-ML technique as an active security solution for WSN-IoT systems, harnessing the power of machine learning and the Firefly Algorithm to bolster intrusion detection capabilities.","<method>Firefly Algorithm</method>, <method>machine learning</method>, <method>FA-ML technique</method>, <method>support vector machine (SVM)</method>, <method>Grey Wolf Optimizer (GWO)</method>, <method>KNN-PSO</method>, <method>XGBoost</method>"
2024,https://openalex.org/W4391512775,Engineering,Peak and ultimate stress-strain model of confined ultra-high-performance concrete (UHPC) using hybrid machine learning model with conditional tabular generative adversarial network,"Ultra-high-performance concrete (UHPC) has gained prominence owing to its exceptional physical and mechanical properties and improved sustainability, making it ideal for large-scale structural applications. While numerous analytical studies have focused on predicting the stress-strain response of unconfined UHPC, there remains a lack of a reliable model for predicting the stress-strain response of confined UHPC, which poses challenges to efficient design and broader adoption, particularly in seismically active regions. To bridge this gap, the present study introduces a framework that implements machine learning (ML) models augmented by a state-of-the-art conditional tabular generative adversarial network (CTGAN) and Optuna, which a next-generation optimization framework, to accurately predict the peak and ultimate axial stress-strain responses of UHPC confined with either normal-strength steel or high-strength steel. The Optuna-optimized CTGAN is employed to address the issue of limited data by generating synthetic datasets of hypothetical confined UHPC specimens. A comprehensive database of confined UHPC stress-strain responses was compiled from existing literature and used to condition the CTGAN. The augmented database is then leveraged to develop a hybrid ML model that integrates extreme gradient boosting, gradient boosting machine, support vector regression, and K-nearest neighbors for predicting peak and ultimate stress-strain responses of confined UHPC. The predictive accuracy of the proposed hybrid ML model is evaluated and compared with a diverse set of ML models of varying complexity, and the results demonstrate its superior performance in predicting the peak and ultimate stress-strain response of confined UHPC. Furthermore, a graphical user interface of the proposed model is developed to facilitate its practical implementation and provide a rapid, autonomous, and accurate prediction of the stress-strain response of confined UHPC at both peak and ultimate states.","<method>conditional tabular generative adversarial network (CTGAN)</method>, <method>Optuna</method>, <method>extreme gradient boosting</method>, <method>gradient boosting machine</method>, <method>support vector regression</method>, <method>K-nearest neighbors</method>"
2024,https://openalex.org/W4393001808,Engineering,Multi-Source and Multi-modal Deep Network Embedding for Cross-Network Node Classification,"In recent years, to address the issue of networked data sparsity in node classification tasks, cross-network node classification (CNNC) leverages the richer information from a source network to enhance the performance of node classification in the target network, which typically has sparser information. However, in real-world applications, labeled nodes may be collected from multiple sources with multiple modalities (e.g., text, vision, and video). Naive application of single-source and single-modal CNNC methods may result in sub-optimal solutions. To this end, in this article, we propose a model called Multi-source and Multi-modal Cross-network Deep Network Embedding (M 2 CDNE) for cross-network node classification. In M 2 CDNE, we propose a deep multi-modal network embedding approach that combines the extracted deep multi-modal features to make the node vector representations network invariant. In addition, we apply dynamic adversarial adaptation to assess the significance of marginal and conditional probability distributions between each source and target network to make node vector representations label discriminative. Furthermore, we devise to classify nodes in the target network through the related source classifier and aggregate different predictions utilizing respective network weights, corresponding to the discrepancy between each source and target network. Extensive experiments performed on real-world datasets demonstrate that the proposed M 2 CDNE significantly outperforms the state-of-the-art approaches.","<method>cross-network node classification (CNNC)</method>, <method>Multi-source and Multi-modal Cross-network Deep Network Embedding (M 2 CDNE)</method>, <method>deep multi-modal network embedding</method>, <method>dynamic adversarial adaptation</method>"
2024,https://openalex.org/W4390533101,Engineering,A vehicular network based intelligent transport system for smart cities using machine learning algorithms,"Abstract Smart cities and the Internet of Things have enabled the integration of communicating devices for efficient decision-making. Notably, traffic congestion is one major problem faced by daily commuters in urban cities. In developed countries, specialized sensors are deployed to gather traffic information to predict traffic patterns. Any traffic updates are shared with the commuters via the Internet. Such solutions become impracticable when physical infrastructure and Internet connectivity are either non-existent or very limited. In case of developing countries, no roadside units are available and Internet connectivity is still an issue in remote areas. Internet traffic analysis is a thriving field of study due to the myriad ways in which it may be put to practical use. In the intelligent Internet-of-Vehicles (IOVs), traffic congestion can be predicted and identified using cutting-edge technologies. Using tree-based decision-tree, random-forest, extra-tree, and XGBoost machine learning (ML) strategies, this research proposes an intelligent-transport-system for the IOVs-based vehicular network traffic in a smart city set-up. The suggested system uses ensemble learning and averages the selection of crucial features to give high detection accuracy at minimal computational costs, as demonstrated by the simulation results. For IOV-based vehicular network traffic, the tree-based ML approaches with feature-selection (FS) outperformed those without FS. When contrasted to the lowest KNN accuracy of 96.6% and the highest SVM accuracy of 98.01%, the Stacking approach demonstrates superior accuracy as 99.05%.","<method>decision-tree</method>, <method>random-forest</method>, <method>extra-tree</method>, <method>XGBoost</method>, <method>ensemble learning</method>, <method>feature-selection (FS)</method>, <method>KNN</method>, <method>SVM</method>, <method>Stacking</method>"
2024,https://openalex.org/W4390572374,Engineering,Energy harvester reliability study by Gaidai reliability method,"Abstract This study validates a novel structural reliability method, particularly suitable for high‐dimensional green energy harvesting device dynamic systems, versus a well‐established bivariate statistical method, known to accurately predict two‐dimensional system extreme response contours. Classic reliability methods dealing with time series do not always have an advantage of dealing easily with dynamic system high dimensionality, along with complex cross‐correlations among different system components. Energy harvesters constitute an important part of modern offshore green energy engineering; hence, proper experimental study along with safety and reliability analysis are of practical design and engineering importance. To study the performance of galloping energy harvesters, a series of laboratory wind tunnel tests have been conducted, selecting different wind speeds. This study illustrates the usage of the advocated novel reliability method, by analyzing bivariate statistics of experimental galloping energy harvester's dynamics. The bivariate statistics was extracted from available experimental results, more specifically for the device's voltage‐force dataset. Advantage of the proposed methodology being that relatively short experimental data record may still yield meaningful design results, provided proper statistical methods have been applied. Safety and reliability are important engineering concerns for all kinds of green energy devices. In the case of measured device's structural response, an accurate prediction of system failure or damage probability is possible, as illustrated in this study. Distinctive advantage of advocated novel semi‐analytical reliability methodology being the fact that it can tackle dynamic systems with practically unlimited number of dimensions (or components), along with complex nonlinear cross‐correlations between different system key components.",No methods found.
2024,https://openalex.org/W4390817508,Engineering,Conventional to Deep Ensemble Methods for Hyperspectral Image Classification: A Comprehensive Survey,"Hyperspectral image classification has become a hot research topic. HSI has been widely used in a wide range of real-world application areas due to the in-depth spectral information stored within each pixel. Noticeably, the detailed features - i.e., a nonlinear correlation between the obtained spectral data and the correlating HSI data object, generate efficient classification results that are complex for traditional techniques. Deep Learning (DL) has recently been validated as an influential feature extractor that efficiently identifies the nonlinear issues that have arisen in various computer vision challenges. This motivates using DL for Hyperspectral Image Classification (HSIC), which shows promising results. This survey provides a brief description of DL for HSIC and compares cutting-edge methodologies in the field. We will first summarize the key challenges for HSIC, and then we will discuss the superiority of DL and DL-ensemble in addressing these issues. In this article, we divide the state-of-the-art DL methodologies and DL with ensemble into spectral features, spatial features, and combined spatial-spectral features in order to comprehensively and critically evaluate the progress (future research directions as well) of such methodologies for HSIC. Furthermore, we will take into account that DL involves a substantial percentage of labeled training images, whereas obtaining such a number for HSI is time and cost-consuming. As a result, this survey describes some methodologies for improving the classification performance of DL techniques, which can serve as future recommendations.","<method>Deep Learning (DL)</method>, <method>DL-ensemble</method>"
2024,https://openalex.org/W4391318991,Engineering,Optimization of 3D printed parameters for socket prosthetic manufacturing using the taguchi method and response surface methodology,"The most prevalent 3D printing technology on the market currently is Fused deposition modeling (FDM). The main objective of this study is to analyze the effect of the FDM process parameters on the printing time and socket weight of the printed socket prosthetic process as a component of the transtibial prosthesis. Optimization and modeling of the 3D printing prosthetic socket process were carried out using the Taguchi and RSM methods. The Taguchi method was used to evaluate the influence of various factors (socket thickness, layer height, infill density, print speed, and nozzle temperature) on the amount of printing time and socket weight. Furthermore, the factors that have a significant effect are selected and modeled using the RSM method to obtain the highest percentage of printing time and socket weight. The applied material is polylactic acid (PLA) filament. Five printing process parameters were socket thickness (3 mm, 4 mm, 5 mm), layer height (0.1 mm, 0.15 mm, 0.3 mm), infill density (80 %, 90 %, 100 %), print speed (70 mm/s, 80 mm/s, 90 mm/s) and nozzle temperature (190 °C, 200 °C, 210 °C). The proposed quadratic models for reducing both printing time and socket weight were in good accordance with the actual experimental data. The coefficients of determination (R2) for test data were 0.9743 and 0.9993 for printing time and socket weight, respectively.","<method>Taguchi method</method>, <method>RSM method</method>"
2024,https://openalex.org/W4391301691,Engineering,AI-Driven Digital Twin Model for Reliable Lithium-Ion Battery Discharge Capacity Predictions,"The present study proposes a novel method for predicting the discharge capabilities of lithium-ion (Li-ion) batteries using a digital twin model in practice. By combining cutting-edge machine learning techniques, such as AdaBoost and long short-term memory (LSTM) network, with a semiempirical mathematical structure, the digital twin (DT)—a virtual representation that mimics the behavior of actual batteries in real time is constructed. Various metaheuristic optimization methods, such as antlion, grey wolf optimization (GWO), and improved grey wolf optimization (IGWO), are used to adjust hyperparameters in order to optimize the models. As indicators of performance, mean absolute error (MAE) and root-mean-square error (RMSE) are applied to the models after they have undergone extensive training and ten-fold cross-validation. The models are rigorously trained and cross-validated using the NASA battery aging dataset, a widely accepted benchmark dataset for battery research. The IGWO-AdaBoost digital twin model emerges as the standout performer, achieving exceptional accuracy in predicting the discharge capacity. This model demonstrates the lowest mean absolute error (MAE) of 0.01, showcasing its superior precision in estimating discharge capabilities. Additionally, the root mean square error (RMSE) for the IGWO-AdaBoost DT model is also the lowest at 0.01. The findings of this study offer insightful information about the potential utilization of the digital twin model to accurately predict the discharge capacity of batteries.","<method>AdaBoost</method>, <method>long short-term memory (LSTM) network</method>, <method>antlion optimization</method>, <method>grey wolf optimization (GWO)</method>, <method>improved grey wolf optimization (IGWO)</method>"
2024,https://openalex.org/W4390667445,Engineering,Automated data processing and feature engineering for deep learning and big data applications: A survey,"Modern approach to artificial intelligence (AI) aims to design algorithms that learn directly from data. This approach has achieved impressive results and has contributed significantly to the progress of AI, particularly in the sphere of supervised deep learning. It has also simplified the design of machine learning systems as the learning process is highly automated. However, not all data processing tasks in conventional deep learning pipelines have been automated. In most cases data has to be manually collected, preprocessed and further extended through data augmentation before they can be effective for training. Recently, special techniques for automating these tasks have emerged. The automation of data processing tasks is driven by the need to utilize large volumes of complex, heterogeneous data for machine learning and big data applications. Today, end-to-end automated data processing systems based on automated machine learning (AutoML) techniques are capable of taking raw data and transforming them into useful features for Big Data tasks by automating all intermediate processing stages. In this work, we present a thorough review of approaches for automating data processing tasks in deep learning pipelines, including automated data preprocessing– e.g., data cleaning, labeling, missing data imputation, and categorical data encoding–as well as data augmentation (including synthetic data generation using generative AI methods) and feature engineering–specifically, automated feature extraction, feature construction and feature selection. In addition to automating specific data processing tasks, we discuss the use of AutoML methods and tools to simultaneously optimize all stages of the machine learning pipeline.","<method>supervised deep learning</method>, <method>data augmentation</method>, <method>automated machine learning (AutoML)</method>, <method>automated data preprocessing</method>, <method>data cleaning</method>, <method>labeling</method>, <method>missing data imputation</method>, <method>categorical data encoding</method>, <method>synthetic data generation using generative AI methods</method>, <method>automated feature extraction</method>, <method>feature construction</method>, <method>feature selection</method>"
2024,https://openalex.org/W4391479301,Engineering,Comparative Assessment of Two Global Sensitivity Approaches Considering Model and Parameter Uncertainty,"Abstract Global Sensitivity Analysis (GSA) is key to assisting appraisal of the behavior of hydrological systems through model diagnosis considering multiple sources of uncertainty. Uncertainty sources typically comprise incomplete knowledge in (a) conceptual and mathematical formulation of models and (b) parameters embedded in the models. In this context, there is the need for detailed investigations aimed at a robust quantification of the importance of model and parameter uncertainties in a rigorous multi‐model context. This study aims at evaluating and comparing two modern multi‐model GSA methodologies. These are the first GSA approaches embedding both model and parameter uncertainty sources and encompass the variance‐based framework based on Sobol indices (as derived by Dai &amp; Ye, 2015, https://doi.org/10.1016/j.jhydrol.2015.06.034 ) and the moment‐based approach upon which the formulation of the multi‐model AMA indices (as derived by Dell'Oca et al., 2020, https://doi.org/10.1029/2019wr025754 ) is based. We provide an assessment of various aspects of sensitivity upon considering a joint analysis of these two approaches in a multi‐model context. Our work relies on well‐established scenarios that comprise (a) a synthetic setting related to reactive transport across a groundwater system and (b) an experimentally‐based study considering heavy metal sorption onto a soil. Our study documents that the joint use of these GSA approaches can provide different while complementary information to assess mutual consistency of approaches and to enrich the information content provided by GSA under model and parameter uncertainty. While being related to groundwater settings, our results can be considered as reference for future GSA studies coping with model and parameter uncertainty.",No methods found.
2024,https://openalex.org/W4392980686,Engineering,Data-driven evolution of water quality models: An in-depth investigation of innovative outlier detection approaches-A case study of Irish Water Quality Index (IEWQI) model,"Recently, there has been a significant advancement in the water quality index (WQI) models utilizing data-driven approaches, especially those integrating machine learning and artificial intelligence (ML/AI) technology. Although, several recent studies have revealed that the data-driven model has produced inconsistent results due to the data outliers, which significantly impact model reliability and accuracy. The present study was carried out to assess the impact of data outliers on a recently developed Irish Water Quality Index (IEWQI) model, which relies on data-driven techniques. To the author's best knowledge, there has been no systematic framework for evaluating the influence of data outliers on such models. For the purposes of assessing the outlier impact of the data outliers on the water quality (WQ) model, this was the first initiative in research to introduce a comprehensive approach that combines machine learning with advanced statistical techniques. The proposed framework was implemented in Cork Harbour, Ireland, to evaluate the IEWQI model's sensitivity to outliers in input indicators to assess the water quality. In order to detect the data outlier, the study utilized two widely used ML techniques, including Isolation Forest (IF) and Kernel Density Estimation (KDE) within the dataset, for predicting WQ with and without these outliers. For validating the ML results, the study used five commonly used statistical measures. The performance metric (R2) indicates that the model performance improved slightly (R2 increased from 0.92 to 0.95) in predicting WQ after removing the data outlier from the input. But the IEWQI scores revealed that there were no statistically significant differences among the actual values, predictions with outliers, and predictions without outliers, with a 95% confidence interval at p < 0.05. The results of model uncertainty also revealed that the model contributed <1% uncertainty to the final assessment results for using both datasets (with and without outliers). In addition, all statistical measures indicated that the ML techniques provided reliable results that can be utilized for detecting outliers and their impacts on the IEWQI model. The findings of the research reveal that although the data outliers had no significant impact on the IEWQI model architecture, they had moderate impacts on the rating schemes' of the model. This finding indicated that detecting the data outliers could improve the accuracy of the IEWQI model in rating WQ as well as be helpful in mitigating the model eclipsing problem. In addition, the results of the research provide evidence of how the data outliers influenced the data-driven model in predicting WQ and reliability, particularly since the study confirmed that the IEWQI model's could be effective for accurately rating WQ despite the presence of the data outliers in the input. It could occur due to the spatio-temporal variability inherent in WQ indicators. However, the research assesses the influence of data input outliers on the IEWQI model and underscores important areas for future investigation. These areas include expanding temporal analysis using multi-year data, examining spatial outlier patterns, and evaluating detection methods. Moreover, it is essential to explore the real-world impacts of revised rating categories, involve stakeholders in outlier management, and fine-tune model parameters. Analysing model performance across varying temporal and spatial resolutions and incorporating additional environmental data can significantly enhance the accuracy of WQ assessment. Consequently, this study offers valuable insights to strengthen the IEWQI model's robustness and provides avenues for enhancing its utility in broader WQ assessment applications. Moreover, the study successfully adopted the framework for evaluating how data input outliers affect the data-driven model, such as the IEWQI model. The current study has been carried out in Cork Harbour for only a single year of WQ data. The framework should be tested across various domains for evaluating the response of the IEWQI model's in terms of the spatio-temporal resolution of the domain. Nevertheless, the study recommended that future research should be conducted to adjust or revise the IEWQI model's rating schemes and investigate the practical effects of data outliers on updated rating categories. However, the study provides potential recommendations for enhancing the IEWQI model's adaptability and reveals its effectiveness in expanding its applicability in more general WQ assessment scenarios.","<method>Isolation Forest (IF)</method>, <method>Kernel Density Estimation (KDE)</method>"
2024,https://openalex.org/W4391226403,Engineering,Machine learning for multi-dimensional performance optimization and predictive modelling of nanopowder-mixed electric discharge machining (EDM),"Abstract Aluminium 6061 (Al6061) is a widely used material for various industrial applications due to low density and high strength. Nevertheless, the conventional machining operations are not the best choice for the machining purposes. Therefore, amongst all the non-conventional machining operations, electric discharge machining (EDM) is opted to carry out the research due to its wide ability to cut the materials. But the high electrode wear rate (EWR) and high dimensional inaccuracy or overcut (OC) of EDM limit its usage. Consequently, nanopowder is added to the dielectric medium to address the abovementioned issues. Nanopowder mixed EDM (NPMEDM) process is a complex process in terms of performance predictability for different materials. Similarly, the interactions between the process parameters such as peak current ( I p ), spark voltage ( S v ), pulse on time ( P on ) and powder concentration ( C p ) in dielectric enhance the parametric sensitivity. In addition, the cryogenic treatment (CT) of electrodes makes the process complex limiting conventional simulation approaches for modelling inter-relationships. An alternative approach requires experimental exploration and systematic investigation to model EWR and overcutting problems of EDM. Thus, artificial neural networks (ANNs) are used for predictive modelling of the process which are integrated with multi-objective genetic algorithm (MOGA) for parametric optimization. The approach uses experimental data based on response surface methodology (RSM) design of experiments. Moreover, the process physics is thoroughly discussed with parametric effect analysis supported with evidence of microscopic images, scanning electron microscopy (SEM) and 3D surface topographic images. Based on multi-dimensional optimization results, the NT brass electrode showed an improvement of 65.02% in EWR and 59.73% in OC using deionized water. However, CT brass electrode showed 78.41% reduction in EWR and 67.79% improved dimensional accuracy in deionized water. In addition to that, CT brass electrode gave 27.69% less EWR and 81.40% improved OC in deionized water compared to kerosene oil.","<method>artificial neural networks (ANNs)</method>, <method>multi-objective genetic algorithm (MOGA)</method>, <method>response surface methodology (RSM)</method>"
2024,https://openalex.org/W4394015596,Engineering,Predicting the mechanical properties of plastic concrete: An optimization method by using genetic programming and ensemble learners,"This study presents a comparative analysis of individual and ensemble learning algorithms (ELAs) to predict the compressive strength (CS) and flexural strength (FS) of plastic concrete. Multilayer perceptron neuron network (MLPNN), Support vector machine (SVM), random forest (RF), and decision tree (DT) were used as base learners, which were then combined with bagging and Adaboost methods to improve the predictive performance. In addition, gene expression programming (GEP) was used to develop computational equations that can be used to predict the CS and FS of plastic concrete. An extensive database containing 357 and 125 data points was obtained from the literature, and the eight most impactful ingredients were used in the model's development. The accuracy of all models was assessed using several statistical measures, including an error matrix, Akaike information criterion (AIC), K-fold cross-validation, and other external validation equations. Furthermore, sensitivity and SHAP analysis were performed to evaluate input variables' relative significance and impact on the anticipated CS and FS. Based on statistical measures and other validation criteria, GEP outpaces all other individual models, whereas, in ELAs, the SVR ensemble with Adaboost and RF modified with the Bagging technique demonstrated superior performance. SHapley Additive exPlanations (SHAP) and sensitivity analysis reveal that plastic, cement, water, and the age of the specimens have the highest influence, while superplasticizer has the lowest impact, which is consistent with experimental studies. Moreover, GUI and GEP-based simple mathematical correlation can enhance the practical scope of this study and be an effective tool for the pre-mix design of plastic concrete.","<method>Multilayer perceptron neuron network (MLPNN)</method>, <method>Support vector machine (SVM)</method>, <method>random forest (RF)</method>, <method>decision tree (DT)</method>, <method>bagging</method>, <method>Adaboost</method>, <method>gene expression programming (GEP)</method>, <method>SVR ensemble with Adaboost</method>, <method>RF modified with the Bagging technique</method>"
2024,https://openalex.org/W4391855187,Engineering,Machine learning-assisted in-situ adaptive strategies for the control of defects and anomalies in metal additive manufacturing,"In metal additive manufacturing (AM), the material microstructure and part geometry are formed incrementally. Consequently, the resulting part could be defect- and anomaly-free if sufficient care is taken to deposit each layer under optimal process conditions. Conventional closed-loop control (CLC) engineering solutions which sought to achieve this were deterministic and rule-based, thus resulting in limited success in the stochastic environment experienced in the highly dynamic AM process. On the other hand, emerging machine learning (ML) based strategies are better suited to providing the robustness, scope, flexibility, and scalability required for process control in an uncertain environment. Offline ML models that help optimise AM process parameters before a build begins and online ML models that efficiently processed in-situ sensory data to detect and diagnose flaws in real-time (or near-real-time) have been developed. However, ML models that enable a process to take evasive or corrective actions in relation to flaws via on the fly decision-making are only emerging. These models must possess prognostic capabilities to provide context-sensitive recommendations for in-situ process control based on real-time diagnostics. In this article, we pinpoint the shortcomings in traditional CLC strategies, and provide a framework for defect and anomaly control through ML-assisted CLC in AM. We discuss flaws in terms of their causes, in-situ detectability, and controllability, and examine their management under three scenarios: avoidance, mitigation, and repair. Then, we summarise the research into ML models developed for offline optimisation and in-situ diagnosis before initiating a detailed conversation on the implementation of ML-assisted in-situ process control. We found that researchers favoured reinforcement learning approaches or inverse ML models for making rapid, situation-aware control decisions. We also observed that, to-date, the defects addressed were those that may be quantified relatively easily autonomously, and that mitigation (rather than avoidance or repair) was the aim of ML-assisted in-situ control strategies. Additionally, we highlight the various technologies that must seamlessly combine to advance the field of autonomous in-situ control so that it becomes a reality in industrial settings. Finally, we raise awareness of seldom discussed, yet highly pertinent, topics relevant to adaptive control. Our work closes a significant gap in the current AM literature by broaching wide-ranging discussions on matters relevant to in-situ adaptive control in AM.","<method>machine learning (ML) based strategies</method>, <method>offline ML models</method>, <method>online ML models</method>, <method>reinforcement learning approaches</method>, <method>inverse ML models</method>"
2024,https://openalex.org/W4392640075,Engineering,Performance assessment of machine learning algorithms for mapping of land use/land cover using remote sensing data,"The rapid increase in population accelerates the rate of change of Land use/Land cover (LULC) in various parts of the world. This phenomenon caused a huge strain for natural resources. Hence, continues monitoring of LULC changes gained a significant importance for management of natural resources and assessing the climate change impacts. Recently, application of machine learning algorithms on RS (remote sensing) data for rapid and accurate mapping of LULC gained significant importance due to growing need of LULC estimation for ecosystem services, natural resource management and environmental management. Hence, it is crucial to access and compare the performance of different machine learning classifiers for accurate mapping of LULC. The primary objective of this study was to compare the performance of CART (Classification and Regression Tree), RF (Random Forest) and SVM (Support Vector Machine) for LULC estimation by processing RS data on Google Earth Engine (GEE). In total four classes of LULC (Water Bodies, Vegetation Cover, Urban Land and Barren Land) for city of Lahore were extracted using satellite images from Landsat-7, Landsat-8 and Landsat-9 for years 2008, 2015 and 2022, respectively. According to results, RF is the best performing classifier with maximum overall accuracy of 95.2% and highest Kappa coefficient value of 0.87, SVM achieved maximum accuracy of 89.8% with highest Kappa of 0.84 and CART showed maximum overall accuracy of 89.7% with Kappa value of 0.79. Results from this study can give assistance for decision makers, planners and RS experts to choose a suitable machine learning algorithm for LULC classification in an unplanned urbanized city like Lahore.","<method>Classification and Regression Tree (CART)</method>, <method>Random Forest (RF)</method>, <method>Support Vector Machine (SVM)</method>"
2024,https://openalex.org/W4390754233,Engineering,Groundwater Quality Assessment and Irrigation Water Quality Index Prediction Using Machine Learning Algorithms,"The evaluation of groundwater quality is crucial for irrigation purposes; however, due to financial constraints in developing countries, such evaluations suffer from insufficient sampling frequency, hindering comprehensive assessments. Therefore, associated with machine learning approaches and the irrigation water quality index (IWQI), this research aims to evaluate the groundwater quality in Naama, a region in southwest Algeria. Hydrochemical parameters (cations, anions, pH, and EC), qualitative indices (SAR,RSC,Na%,MH,and PI), as well as geospatial representations were used to determine the groundwater’s suitability for irrigation in the study area. In addition, efficient machine learning approaches for forecasting IWQI utilizing Extreme Gradient Boosting (XGBoost), Support vector regression (SVR), and K-Nearest Neighbours (KNN) models were implemented. In this research, 166 groundwater samples were used to calculate the irrigation index. The results showed that 42.18% of them were of excellent quality, 34.34% were of very good quality, 6.63% were good quality, 9.64% were satisfactory, and 4.21% were considered unsuitable for irrigation. On the other hand, results indicate that XGBoost excels in accuracy and stability, with a low RMSE (of 2.8272 and a high R of 0.9834. SVR with only four inputs (Ca2+, Mg2+, Na+, and K) demonstrates a notable predictive capability with a low RMSE of 2.6925 and a high R of 0.98738, while KNN showcases robust performance. The distinctions between these models have important implications for making informed decisions in agricultural water management and resource allocation within the region.","<method>Extreme Gradient Boosting (XGBoost)</method>, <method>Support Vector Regression (SVR)</method>, <method>K-Nearest Neighbours (KNN)</method>"
2024,https://openalex.org/W4391248672,Engineering,Pedestrian 3D Shape Understanding for Person Re-Identification via Multi-View Learning,"Recent development in computing power has resulted in performance improvements on holistic(none-occluded) person Re-Identification (ReID) tasks. Nevertheless, the precision of the recent research will diminish when a pedestrian is obstructed by obstacles. Within the realm of 2D space, the loss of information from obstructed objects continues to pose significant challenges in the context of person ReID. Person is a 3D non-grid object, and thus semantic representation learning in only 2D space limits the understanding of occluded person. In the present work, we propose a network based on 3D multi-view learning, allowing it to acquire geometric and shape details of an occluded pedestrian from 3D space. Simultaneously, it capitalizes on advancements in 2D-based networks to extract semantic representations from 3D multi-views. Specifically, the surface random selection strategy is proposed to convert images of 2D RGB into 3D multi-views. Using this strategy, we build four extensive 3D multi-view data collections for person ReID. After that, Pedestrian 3D Shape Understanding for Person Re-Identification via Multi-View Learning(MV-3DSReID), is proposed for identifying the person by learning person geometry and structure representation from the groups of multi-view images. In comparison to alternative data formats (e.g., 2D RGB, 3D point cloud), multi-view images complement each other's detailed features of the 3D object by adjusting rendering viewpoints, thus facilitating a more comprehensive understanding of the person for both holistic and occluded ReID situations. Experiments on occluded and holistic ReID tasks demonstrate performance levels comparable to state-of-the-art methods, validating the effectiveness of our proposed approach in tackling challenges related to occlusion. The code is available at https://github.com/hangjiaqi1/MV-TransReID.","<method>3D multi-view learning</method>, <method>2D-based networks</method>, <method>surface random selection strategy</method>, <method>Pedestrian 3D Shape Understanding for Person Re-Identification via Multi-View Learning (MV-3DSReID)</method>"
2024,https://openalex.org/W4391973098,Engineering,The use of machine learning techniques to investigate the properties of metakaolin-based geopolymer concrete,"The construction industry significantly contributes to global greenhouse gas emissions, highlighting the imperative for developing environmentally friendly construction materials. Geopolymers, particularly those utilizing metakaolin (MK), have emerged as a promising green alternative to conventional concrete. However, the acquisition of MK-based geopolymer concrete with optimal mechanical properties poses challenges due to numerous influential factors, disagreement over various findings, and the lack of a reliable predictive model. This study aimed to address this gap by employing a wide range of machine learning methods, namely gradient boosting machine, random forest, decision tree, artificial neural network, and support vector machine. Different optimization and regularization techniques were used to comprehensively understand the factors affecting the compressive strength of MK-based geopolymer concrete, including mixture design, chemical characteristics of the initial binder and activators, and different curing regimes. The results demonstrated the exceptional performance of the gradient boosting machine in predicting the compressive strength of MK-based geopolymer concrete, achieving a coefficient of determination of 0.983 and a mean absolute error of 1.615 MPa. Additionally, the study employed partial dependence plots, feature importance analysis, and SHapley Additive exPlanations (SHAP) to elucidate the proposed models. The coarse-to-fine aggregate ratio, H2O/Na2O molar ratio, extra water content, and sodium hydroxide concentration were identified as the most critical parameters affecting the compressive strength of MK-based geopolymer concrete. This research contributes to advancing the development of sustainable construction materials, streamlining experimental tasks, minimizing the need for labor and materials, improving time efficiency, and providing valuable insights for optimizing the design of MK-based geopolymer concrete.","<method>gradient boosting machine</method>, <method>random forest</method>, <method>decision tree</method>, <method>artificial neural network</method>, <method>support vector machine</method>"
2024,https://openalex.org/W4392529708,Engineering,A machine learning-based framework for clustering residential electricity load profiles to enhance demand response programs,"Load shapes derived from smart meter data are frequently employed to analyze daily energy consumption patterns, particularly in the context of applications like Demand Response (DR). Nevertheless, one of the most important challenges to this endeavor lies in identifying the most suitable consumer clusters with similar consumption behaviors. In this paper, we present a novel machine learning based framework in order to achieve optimal load profiling through a real case study, utilizing data from almost 5000 households in London. Four widely used clustering algorithms are applied specifically K-means, K-medoids, Hierarchical Agglomerative Clustering and Density-based Spatial Clustering. An empirical analysis as well as multiple evaluation metrics are leveraged to assess those algorithms. Following that, we redefine the problem as a probabilistic classification one, with the classifier emulating the behavior of a clustering algorithm, leveraging Explainable AI (xAI) to enhance the interpretability of our solution. According to the clustering algorithm analysis the optimal number of clusters for this case is seven. Despite that, our methodology shows that two of the clusters, almost 10% of the dataset, exhibit significant internal dissimilarity. As a result, these clusters have been excluded from consideration for DR programs. The scalability and versatility of our solution makes it an ideal choice for power utility companies aiming to segment their users for creating more targeted DR programs.","<method>K-means</method>, <method>K-medoids</method>, <method>Hierarchical Agglomerative Clustering</method>, <method>Density-based Spatial Clustering</method>, <method>probabilistic classification</method>, <method>Explainable AI (xAI)</method>"
2024,https://openalex.org/W4399035606,Engineering,Multivariate Gaidai hazard assessment method in combination with deconvolution scheme to predict extreme wave heights,"Current study advocates novel Gaidai hazards assessment methodology that may be utilized for excessive wave-heights spatiotemporal risk analysis, thus advancing climate change studies. Gaidai hazards assessment methodology being particularly suitable for multivariate dynamic environmental ocean systems, that have been MC (i.e., Monte Carlo) numerically simulated, either physically measured across a representative time period, resulting in synchronous quasi-ergodic timeseries. Offshore waves affect reliable production and operational safety of offshore and marine structures. Current study presents two reliability methods: first, state-of-the-art spatiotemporal reliability methodology, designed for multi-dimensional dynamic systems, to be presented in the current study; second, novel deconvolution extrapolation technique to follow. Primary target being accurate environmental ocean system's hazard hazards assessment. Classic risk assessment methods, dealing with measured timeseries may not always possess advantages of dealing efficiently with the environmental ocean system's high dimensionality, along with nonlinear cross-correlation patterns between various environmental ocean system's components. In-situ significant wave-height measured dataset, measured in different offshore areas, to be analyzed in the current study by means of application of advocated reliability methodology. Offshore waves representing complex highly-nonlinear, cross-correlated environmental dynamic system. Global climate change being also an important factor, affecting offshore wave-heights. Primary purpose of the current study had been to benchmark novel hazards assessment methodology, while utilizing efficiently underlying raw measured dataset. Methods put forth in the current study may be utilized for hazard hazards assessments for a large variety of nonlinear high dimensional environmental ocean systems.","<method>Monte Carlo (MC) simulation</method>, <method>spatiotemporal reliability methodology</method>, <method>deconvolution extrapolation technique</method>"
2024,https://openalex.org/W4400937555,Engineering,The diagnostic and triage accuracy of the GPT-3 artificial intelligence model: an observational study,"BackgroundArtificial intelligence (AI) applications in health care have been effective in many areas of medicine, but they are often trained for a single task using labelled data, making deployment and generalisability challenging. How well a general-purpose AI language model performs diagnosis and triage relative to physicians and laypeople is not well understood.MethodsWe compared the predictive accuracy of Generative Pre-trained Transformer 3 (GPT-3)'s diagnostic and triage ability for 48 validated synthetic case vignettes (<50 words; sixth-grade reading level or below) of both common (eg, viral illness) and severe (eg, heart attack) conditions to a nationally representative sample of 5000 lay people from the USA who could use the internet to find the correct options and 21 practising physicians at Harvard Medical School. There were 12 vignettes for each of four triage categories: emergent, within one day, within 1 week, and self-care. The correct diagnosis and triage category (ie, ground truth) for each vignette was determined by two general internists at Harvard Medical School. For each vignette, human respondents and GPT-3 were prompted to list diagnoses in order of likelihood, and the vignette was marked as correct if the ground-truth diagnosis was in the top three of the listed diagnoses. For triage accuracy, we examined whether the human respondents' and GPT-3's selected triage was exactly correct according to the four triage categories, or matched a dichotomised triage variable (emergent or within 1 day vs within 1 week or self-care). We estimated GPT-3's diagnostic and triage confidence on a given vignette using a modified bootstrap resampling procedure, and examined how well calibrated GPT-3's confidence was by computing calibration curves and Brier scores. We also performed subgroup analysis by case acuity, and an error analysis for triage advice to characterise how its advice might affect patients using this tool to decide if they should seek medical care immediately.FindingsAmong all cases, GPT-3 replied with the correct diagnosis in its top three for 88% (42/48, 95% CI 75–94) of cases, compared with 54% (2700/5000, 53–55) for lay individuals (p<0.0001) and 96% (637/666, 94–97) for physicians (p=0·012). GPT-3 triaged 70% correct (34/48, 57–82) versus 74% (3706/5000, 73–75; p=0.60) for lay individuals and 91% (608/666, 89–93%; p<0.0001) for physicians. As measured by the Brier score, GPT-3 confidence in its top prediction was reasonably well calibrated for diagnosis (Brier score=0·18) and triage (Brier score=0·22). We observed an inverse relationship between case acuity and GPT-3 accuracy (p<0·0001) with a fitted trend line of –8·33% decrease in accuracy for every level of increase in case acuity. For triage error analysis, GPT-3 deprioritised truly emergent cases in seven instances.InterpretationA general-purpose AI language model without any content-specific training could perform diagnosis at levels close to, but below, physicians and better than lay individuals. We found that GPT-3's performance was inferior to physicians for triage, sometimes by a large margin, and its performance was closer to that of lay individuals. Although the diagnostic performance of GPT-3 was comparable to physicians, it was significantly better than a typical person using a search engine.FundingThe National Heart, Lung, and Blood Institute.","<method>Generative Pre-trained Transformer 3 (GPT-3)</method>, <method>modified bootstrap resampling procedure</method>"
2024,https://openalex.org/W4390738871,Engineering,Internet of things sensors and support vector machine integrated intelligent irrigation system for agriculture industry,"Abstract Because there is more demand for freshwater around the world and the world’s population is growing at the same time, there is a severe lack of freshwater resources in the central part of the planet. The world’s current population of 7.2 billion people is expected to grow to over 9 billion by the year 2050. The vast majority of freshwater is used for things like cooking, cleaning, and farming. Most industrialised countries are in desperate need of smart irrigation systems, which are now a must-have because of how quickly technology is improving. In article presents IoT based Sensor integrated intelligent irrigation system for agriculture industry. IoT based humidity and soil sensors are used to collect soil related data. This data is stored in a centralized cloud. Features are selected by CFS algorithm. This will help in discarding irrelevant data. Clustering of data is performed by K means algorithm. This will help in keeping similar data together. Then classification model is build using the SVM, Random Forest and Naïve Bayes algorithm. Model is trained, validated and tested using the acquired data. Historical soil and humidity related data is also used in training the model. K-means SVM hybrid classifier is achieving better results for classification, prediction of water demand and saving fresh water by intelligent irrigation. K-means SVM hybrid classifier has achieved accuracy rate of 98.5 percent. Specificity, recall and precision of K-means SVM hybrid classifier is also higher than random forest and naïve bayes classifier.","<method>CFS algorithm</method>, <method>K means algorithm</method>, <method>SVM</method>, <method>Random Forest</method>, <method>Naïve Bayes algorithm</method>, <method>K-means SVM hybrid classifier</method>"
2024,https://openalex.org/W4391796054,Engineering,Optical remote sensing of crop biophysical and biochemical parameters: An overview of advances in sensor technologies and machine learning algorithms for precision agriculture,"This paper provides an overview of the recent developments in remote sensing technology and machine learning algorithms for estimating important biophysical and biochemical parameters for precision farming. The objectives are (i) to provide an overview of recent advances in remotely sensed retrieval of biophysical and biochemical parameters brought by the developments in sensor technologies and robust machine learning algorithms and (ii) to identify the sources of uncertainty in retrieving biophysical and biochemical parameters and implications for precision agriculture. The review revealed that developments in crop biophysical and biochemical parameters retrieval techniques were mainly driven by announcements and the availability of new sensors. Two ground-breaking events can be identified, i.e., the availability of Sentinel-2 and the SuperDove constellation. The two provide high temporal-high spatial resolution data relevant for site-specific management and super-spectral configuration, enabling retrieval of crop growth and health parameters. The free availability of Sentinel-2 triggered the testing of its spectral configurations and upscaling of retrieval approaches using simulated data from field spectrometers and airborne hyperspectral sensors. SuperDoves will likely reduce the cost of very high-resolution data while providing unprecedented capabilities for detailed, accurate and frequent characterisation of field variability. Studies showed that the red-edge bands and hybrid models coupling Radiative Transfer Model (RTM) and machine learning regression algorithms (MLRA) are promising for operational and accurate monitoring of stress-related crop parameters to aid time-sensitive agronomic decisions. However, such models were tested in Mediterranean climates and performed poorly in African semi-arid areas and China's temperate continental semi-humid monsoon climates. Therefore, locally-calibrated RTM models incorporating crop-type maps and other spatio-temporal constraints may reduce uncertainties when adapted to data-scarce regions. Generally, permanent experimental sites and a lack of systematic calibration data on various crops are some limiting factors to using remote sensing technologies for PA in Sub-Saharan Africa. Other complexities arise from farm configurations, such as small field sizes and mixed cropping practices. Therefore, future studies should develop generic, scalable and transferable models, especially within under-studied areas.",<method>machine learning regression algorithms (MLRA)</method>
2024,https://openalex.org/W4393167823,Engineering,Risk analysis and assessment of water resource carrying capacity based on weighted gray model with improved entropy weighting method in the central plains region of China,"The issue of global water shortage is a serious concern. The scientific evaluation of water resource carrying capacity (WRCC) serves as the foundation for implementing measures to protect water resources. In addition, most of the studies are based on the analysis and research of regional WRCC from the aspects of water quantity and water quality. There are few studies on the four aspects of water resources endowment conditions, society, economy and ecological environment, which is difficult to scientifically and accurately reflect the analysis and evaluation of regional WRCC by the four systems. Therefore, it is necessary to conduct a deeper discussion and Analysis on this topic. This study presents a WRCC index system and corresponding ranking criteria based on 20 influencing factors from four aspects: water resources endowment (WRE), economy, society, and ecological environment. In addition, by combining the improved entropy weighting method (EWM) with gray correlation analysis, the weighted gray technique for order preference by similarity to an ideal solution (TOPSIS) model is proposed for analyzing and assessing WRCC risk. Finally, the WRCC of the study area from 2012 to 2021 is comprehensively evaluated in the central plains region of China (CPROC) as an example. The results show that the comprehensive evaluation obtained a multi-year average value of 0.2935, and the water resources shortage in the CPROC is generally in grade III status. The comprehensive average value of Beijing is 0.345, and the comprehensive average value of Henan is 0.397. The overall degree of water resources shortage is in the state of grade V shortage, Shaanxi is in the state of grade IV shortage, and the degree of water resources in Tianjin and Shanxi is relatively good. This study provides corresponding scientific basis and methodological guidance for the sustainable utilization of water resources and healthy socio-economic performance in the CPROC.","<method>improved entropy weighting method (EWM)</method>, <method>gray correlation analysis</method>, <method>weighted gray technique for order preference by similarity to an ideal solution (TOPSIS) model</method>"
2024,https://openalex.org/W4393210635,Engineering,Energy and economic analysis of building integrated photovoltaic thermal system: Seasonal dynamic modeling assisted with machine learning-aided method and multi-objective genetic optimization,"Building integrated photovoltaic thermal (BIPV/T) systems offer a highly effective means of generating clean energy for both electricity and heating purposes in residential buildings. Hence, this article introduces a new BIPV/T system to optimally minimize the energy consumption of a household residential building. The meticulous design of the proposed BIPV/T system is accomplished through MATLAB/Simulink® dynamic modeling. Performance analysis for the BIPV/T system is performed under different seasonal conditions with in-depth techno-economic analyses to estimate the expected enhancement in the thermal, electrical, and economic performance of the system. Moreover, a sensitivity analysis is conducted to explore the impact of various factors on the energetic and economic performances of the proposed BIPV/T system. More so, the two-layer feed-forward back-propagation artificial neural network modeling is developed to accurately predict the hourly solar radiation and ambient temperature for the BIPV/T. Additionally, a multi-objective optimization using the NSGA-II method is also conducted for the minimization of the total BIPV/T plant area and maximization of the total efficiency and net thermal power of the system as well as to estimate the optimized operating conditions for input variables across different seasons within the provided ranges. The sensitivity analysis revealed that higher solar flux levels lead to increased electric output power of the BIPV/T plant, but total efficiency decreases due to higher thermal losses. Moreover, the proposed NSGA-II shows a feasible method to attain a maximum net thermal power and optimal total efficiency of 5320 W and 63% with a minimal total plant area of 32.89 m2 that attained a very low deviation index from the ideal solution. The levelised cost of electricity is obtained as 0.10 $/kWh under the optimal conditions. Thus, these findings offer valuable insights into the potential of BIPV/T systems as a sustainable and efficient energy solution for residential applications.","<method>two-layer feed-forward back-propagation artificial neural network</method>, <method>NSGA-II</method>"
2024,https://openalex.org/W4390607226,Engineering,RanMerFormer: Randomized vision transformer with token merging for brain tumor classification,"Brains are the control center of the nervous system in human bodies, and brain tumor is one of the most deadly diseases. Currently, magnetic resonance imaging (MRI) is the most effective way to brain tumors early detection in clinical diagnoses due to its superior imaging quality for soft tissues. Manual analysis of brain MRI is error-prone which depends on empirical experience and the fatigue state of the radiologists to a large extent. Computer-aided diagnosis (CAD) systems are becoming more and more impactful because they can provide accurate prediction results based on medical images with advanced techniques from computer vision. Therefore, a novel CAD method for brain tumor classification named RanMerFormer is presented in this paper. A pre-trained vision transformer is used as the backbone model. Then, a merging mechanism is proposed to remove the redundant tokens in the vision transformer, which improves computing efficiency substantially. Finally, a randomized vector functional-link serves as the head in the proposed RanMerFormer, which can be trained swiftly. All the simulation results are obtained from two public benchmark datasets, which reveal that the proposed RanMerFormer can achieve state-of-the-art performance for brain tumor classification. The trained RanMerFormer can be applied in real-world scenarios to assist in brain tumor diagnosis.","<method>pre-trained vision transformer</method>, <method>merging mechanism to remove redundant tokens in the vision transformer</method>, <method>randomized vector functional-link</method>"
2024,https://openalex.org/W4392855331,Engineering,The Making of the “Good Bad” Job: How Algorithmic Management Manufactures Consent Through Constant and Confined Choices,"This research explores how a new relation of production—the shift from human managers to algorithmic managers on digital platforms—manufactures workplace consent. While most research has argued that the task standardization and surveillance that accompany algorithmic management will give rise to the quintessential “bad job” (Kalleberg, Reskin, and Hudson, 2000; Kalleberg, 2011), I find that, surprisingly, many workers report liking and finding choice while working under algorithmic management. Drawing on a seven-year qualitative study of the largest sector in the gig economy, the ride-hailing industry, I describe how workers navigate being managed by an algorithm. I begin by showing how algorithms segment the work at multiple sites of human–algorithm interactions and how this configuration of the work process allows for more-frequent and narrow choice. I find that workers use two sets of tactics. In engagement tactics, individuals generally follow the algorithmic nudges and do not try to get around the system; in deviance tactics, individuals manipulate their input into the algorithmic management system. While the behaviors associated with these tactics are practical opposites, they both elicit consent, or active, enthusiastic participation by workers to align their efforts with managerial interests, and both contribute to workers seeing themselves as skillful agents. However, this choice-based consent can mask the more-structurally problematic elements of the work, contributing to the growing popularity of what I call the “good bad” job.",No methods found.
2024,https://openalex.org/W4394627421,Engineering,Multiobjective Scheduling of Energy-Efficient Stochastic Hybrid Open Shop With Brain Storm Optimization and Simulation Evaluation,"Recently, energy conservation in manufacturing industry, particular in energy-intensive industries, receives much attention in order to meet the environmental protection and sustainable development needs. Optimal job scheduling is of great importance in reducing unnecessary energy consumption. To this end, both energy and time-related criteria need to be taken into consideration to achieve an efficient and sustainable production process. Generally, it is difficult to obtain the accurate processing time of jobs in advance due to various uncertainties in open shop scheduling problems arising from manufacturing and service systems. This work formulates a stochastic multiobjective hybrid open shop scheduling problem that consists of open shop and parallel-machine models. First, a multiobjective chance-constrained program is established to minimize total tardiness and energy consumption while meeting makespan requirements. Second, we newly develop a multiobjective framework integrating a brain storm optimizer and a simulation system to solve this problem. We combine population evolution to enhance exploration and external archive evolution to strengthen exploitation into the brain storm optimizer to seek for promising solutions. A simulation system is accordingly designed by using stochastic simulation and discrete-event simulation to assess the searched solutions. Finally, by conducting experiments and comparing the proposed method with several existing algorithms and an exact solver, our results confirm that it significantly outperforms its peers in tackling the considered problem.","<method>brain storm optimizer</method>, <method>stochastic simulation</method>, <method>discrete-event simulation</method>"
2024,https://openalex.org/W4399303474,Engineering,Improving Forest Above-Ground Biomass Estimation by Integrating Individual Machine Learning Models,"The accurate estimation of forest above-ground biomass (AGB) is crucial for sustainable forest management and tracking the carbon cycle of forest ecosystem. Machine learning algorithms have been proven to have great potential in forest AGB estimation with remote sensing data. Though many studies have demonstrated that a single machine learning model can produce highly accurate estimations of forest AGB in many situations, efforts are still required to explore the possible improvement in forest AGB estimation for a specific scenario under study. This study aims to investigate the performance of novel ensemble machine learning methods for forest AGB estimation and analyzes whether these methods are affected by forest types, independent variables, and spatial autocorrelation. Four well-known machine learning models (CatBoost, LightGBM, random forest (RF), and XGBoost) were compared for forest AGB estimation in the study using eight scenarios devised on the basis of two study regions, two variable types, and two validation strategies. Subsequently, a hybrid model combining the strengths of these individual models was proposed for forest AGB estimation. The findings indicated that no individual model outperforms the others in all scenarios. The RF model demonstrates superior performance in scenarios 5, 6, and 7, while the CatBoost model shows the best performance in the remaining scenarios. Moreover, the proposed hybrid model consistently has the best performance in all scenarios in spite of some uncertainties. The ensemble strategy developed in this study for the hybrid model substantially improves estimation accuracy and exhibits greater stability, effectively addressing the challenge of model selection encountered in the forest AGB forecasting process.","<method>CatBoost</method>, <method>LightGBM</method>, <method>random forest (RF)</method>, <method>XGBoost</method>, <method>ensemble machine learning methods</method>, <method>hybrid model</method>"
2024,https://openalex.org/W3136357470,Engineering,Optimization algorithms as robust feedback controllers,"Mathematical optimization is one of the cornerstones of modern engineering research and practice. Yet, throughout all application domains, mathematical optimization is, for the most part, considered to be a numerical discipline. Optimization problems are formulated to be solved numerically with specific algorithms running on microprocessors. An emerging alternative is to view optimization algorithms as dynamical systems. Besides being insightful in itself, this perspective liberates optimization methods from specific numerical and algorithmic aspects and opens up new possibilities to endow complex real-world systems with sophisticated self-optimizing behavior. Towards this goal, it is necessary to understand how numerical optimization algorithms can be converted into feedback controllers to enable robust ""closed-loop optimization"". In this article, we focus on recent control designs under the name of ""feedback-based optimization"" which implement optimization algorithms directly in closed loop with physical systems. In addition to a brief overview of selected continuous-time dynamical systems for optimization, our particular emphasis in this survey lies on closed-loop stability as well as the robust enforcement of physical and operational constraints in closed-loop implementations. To bypass accessing partial model information of physical systems, we further elaborate on fully data-driven and model-free operations. We highlight an emerging application in autonomous reserve dispatch in power systems, where the theory has transitioned to practice by now. We also provide short expository reviews of pioneering applications in communication networks and electricity grids, as well as related research streams, including extremum seeking and pertinent methods from model predictive and process control, to facilitate high-level comparisons with the main topic of this survey.",No methods found.
2024,https://openalex.org/W4391018616,Engineering,Short-term power load forecasting based on AC-BiLSTM model,"The practice of ultra-short-term power load forecasting serves as a critical strategy for enabling rapid response and real-time dispatch in power systems. By improving the accuracy of load forecasting, both the safety of power systems and the efficiency of electricity usage can be significantly enhanced. Addressing the challenges posed by the non-linear and temporal characteristics of grid load data, this study introduces a novel ultra-short-term power load forecasting model, integrating Convolutional Neural Networks (CNN), Bidirectional Long Short-Term Memory networks (BiLSTM), and an Attention mechanism, referred to as the AC-BiLSTM model. This innovative approach harnesses the power of CNN and BiLSTM to extract spatio-temporal features of load data, while the Attention mechanism allocates optimal weights to the hidden states of the BiLSTM model, thereby amplifying crucial historical load sequence data and minimizing information loss. The final output of the model is then determined through a fully connected layer. To validate the efficacy of this approach, an empirical study was conducted using real load data from a specific region. The results, obtained from two contrasting experimental scenarios, demonstrate a significant enhancement in forecasting accuracy. This finding underscores the potential of the AC-BiLSTM model as a reliable tool for both strategic planning and maintaining operational stability in power systems.","<method>Convolutional Neural Networks (CNN)</method>, <method>Bidirectional Long Short-Term Memory networks (BiLSTM)</method>, <method>Attention mechanism</method>"
2024,https://openalex.org/W4391178461,Engineering,Assessment of surrogate models for flood inundation: The physics-guided LSG model vs. state-of-the-art machine learning models,"Hydrodynamic models can accurately simulate flood inundation but are limited by their high computational demand that scales non-linearly with model complexity, resolution, and domain size. Therefore, it is often not feasible to use high-resolution hydrodynamic models for real-time flood predictions or when a large number of predictions are needed for probabilistic flood design. Computationally efficient surrogate models have been developed to address this issue. The recently developed Low-fidelity, Spatial analysis, and Gaussian Process Learning (LSG) model has shown strong performance in both computational efficiency and simulation accuracy. The LSG model is a physics-guided surrogate model that simulates flood inundation by first using an extremely coarse and simplified (i.e. low-fidelity) hydrodynamic model to provide an initial estimate of flood inundation. Then, the low-fidelity estimate is upskilled via Empirical Orthogonal Functions (EOF) analysis and Sparse Gaussian Process models to provide accurate high-resolution predictions. Despite the promising results achieved thus far, the LSG model has not been benchmarked against other surrogate models. Such a comparison is needed to fully understand the value of the LSG model and to provide guidance for future research efforts in flood inundation simulation. This study compares the LSG model to four state-of-the-art surrogate flood inundation models. The surrogate models are assessed for their ability to simulate the temporal and spatial evolution of flood inundation for events both within and beyond the range used for model training. The models are evaluated for three distinct case studies in Australia and the United Kingdom. The LSG model is found to be superior in accuracy for both flood extent and water depth, including when applied to flood events outside the range of training data used, while achieving high computational efficiency. In addition, the low-fidelity model is found to play a crucial role in achieving the overall superior performance of the LSG model.","<method>Gaussian Process Learning</method>, <method>Empirical Orthogonal Functions (EOF) analysis</method>, <method>Sparse Gaussian Process models</method>"
2024,https://openalex.org/W4391592188,Engineering,Prompt Engineering or Fine-Tuning? A Case Study on Phishing Detection with Large Language Models,"Large Language Models (LLMs) are reshaping the landscape of Machine Learning (ML) application development. The emergence of versatile LLMs capable of undertaking a wide array of tasks has reduced the necessity for intensive human involvement in training and maintaining ML models. Despite these advancements, a pivotal question emerges: can these generalized models negate the need for task-specific models? This study addresses this question by comparing the effectiveness of LLMs in detecting phishing URLs when utilized with prompt-engineering techniques versus when fine-tuned. Notably, we explore multiple prompt-engineering strategies for phishing URL detection and apply them to two chat models, GPT-3.5-turbo and Claude 2. In this context, the maximum result achieved was an F1-score of 92.74% by using a test set of 1000 samples. Following this, we fine-tune a range of base LLMs, including GPT-2, Bloom, Baby LLaMA, and DistilGPT-2—all primarily developed for text generation—exclusively for phishing URL detection. The fine-tuning approach culminated in a peak performance, achieving an F1-score of 97.29% and an AUC of 99.56% on the same test set, thereby outperforming existing state-of-the-art methods. These results highlight that while LLMs harnessed through prompt engineering can expedite application development processes, achieving a decent performance, they are not as effective as dedicated, task-specific LLMs.","<method>prompt-engineering techniques</method>, <method>fine-tuning</method>"
2024,https://openalex.org/W4391708456,Engineering,Delineation of groundwater potential zonation using geoinformatics and AHP techniques with remote sensing data,"Among all other valuable natural resources, groundwater is crucial for global economic growth and food security. This study aimed to delineate groundwater potential zones (GWPZ) in the Gidabo watershed of the Main Ethiopian Rift. The demand for groundwater supplies for various applications has risen recently in the watershed due to rapid population upsurge. An integrated Geographical Information System, Remote Sensing, and Analytical Hierarchy Process (AHP) has been utilized. Eight groundwater regulating factors, including rainfall, elevation, drainage density, soil types, lineament density, slope, lithology, and land use/land cover, have been taken in the analysis. To assign suitable weights to each factor, AHP was employed, as each element contributes differently to groundwater occurrence. The weighted overlay analysis (WOA) technique was then used in the ArcGIS environment to integrate all thematic layers and generate a GWPZ map. The delineated GWPZ in the watershed was classified into five categories. The poor GWPZ covered 18.7 %, the low GWPZ covered 33.8 %, the moderate GWPZ covered 23.4 %, the high GWPZ covered 18.1 %, and the very high GWPZ covered 5.8 % of the area. Well and spring data were used to validate the model, and the ROC (Receiver Operating Characteristic) curve method was applied. The results showed good accuracy of 76.8 %. The result of this research can be valuable for planning and managing groundwater resources in the Gidabo watershed.","<method>Analytical Hierarchy Process (AHP)</method>, <method>Receiver Operating Characteristic (ROC) curve method</method>"
2024,https://openalex.org/W4391720077,Engineering,Artificial intelligence-based evaluation of the factors affecting the sales of an iron and steel company,"It is important to predict the sales of an iron and steel company and to identify the variables that influence these sales for future planning. The aim in this study was to identify and model the key factors that influence the sales volume of an iron and steel company using artificial neural networks (ANNs). We attempted to obtain an integrated result from the performance/sales levels of 5 models, to use the ANN approach with hybrid algorithms, and also to present an exemplary application in the base metals industry, where there is a limited number of studies. This study contributes to the literature as the first application of artificial intelligence methods in the iron and steel industry. The ANN models incorporated 6 macroeconomic variables and price-to-sales data and their results were evaluated. An ordinary least squares regression model was also used to facilitate the comparison of results, while gray relational analysis (GRA) was used to draw a comprehensive conclusion based on the ANN results. The results showed that the variables USD/TL exchange rate, product prices, and interest rates, in descending order, had the highest degree of influence in determining the sales of the iron and steel company. Furthermore, these variables are crucial for forecasting future sales and strategic planning. The study showed that the ANN outperformed classical regression models in terms of prediction accuracy. In the model applications conducted for 5 different product groups, it was observed that 3 models (models 2, 3, and 4), including model 4, which sold a higher volume of products than the total of the other products, had an overall performance above 80%. In addition, GRA was found to be a valuable tool for synthesizing insights from different ANN models based on their respective performance levels.","<method>artificial neural networks (ANNs)</method>, <method>ordinary least squares regression model</method>, <method>gray relational analysis (GRA)</method>"
2024,https://openalex.org/W4391997375,Engineering,A machine learning approach to predict the efficiency of corrosion inhibition by natural product-based organic inhibitors,"Abstract This paper presents a quantitative structure–property relationship (QSPR)-based machine learning (ML) framework designed for predicting corrosion inhibition efficiency (CIE) values in natural organic inhibitor compounds. The modeling dataset comprises 50 natural organic compounds, with 11 quantum chemical properties (QCP) serving as input features, and the target variable being the corrosion inhibition efficiency (CIE) value. To enhance the predictive accuracy of the ML model, the kernel density estimation (KDE) function is employed to generate virtual samples during the training process, with the overarching goal of refining the precision of the ML model. Three distinct models, namely random forest (RF), gradient boosting (GB), and k-nearest neighbor (KNN), are tested in the study. The results demonstrate a noteworthy enhancement in the prediction performance of the models, attributable to the incorporation of virtual samples that effectively improve the correlation between input features and target values. Consequently, the accuracy of the predicted CIE values is significantly augmented, aligning more closely with the actual CIE values. Performance improvements were evident across all models after the incorporation of virtual samples. The GB, RF, and KNN models exhibited increments in R 2 values from 0.557 to 0.996, 0.522 to 0.999, and 0.415 to 0.994, respectively, concomitant with the introduction of 500 virtual samples. Additionally, each model demonstrated a notable reduction in RMSE values, transitioning from 1.41 to 0.19, 1.27 to 0.10, and 1.22 to 0.16, respectively. While the GB model initially outperformed others before the addition of virtual samples, the performance of the model exhibited fluctuation as the number of virtual samples varied. This behavior suggests that the KDE function provides a certain level of resilience against model variations. The proposed approach contributes to the effective design and exploration of corrosion inhibitor candidates, offering a reliable and accurate predictive tool that bridges the gap between theoretical studies and experimental synthesis.","<method>random forest (RF)</method>, <method>gradient boosting (GB)</method>, <method>k-nearest neighbor (KNN)</method>"
2024,https://openalex.org/W4392208039,Engineering,Vehicle as a Service (VaaS): Leverage Vehicles to Build Service Networks and Capabilities for Smart Cities,"Smart cities demand resources for rich immersive sensing, ubiquitous communications, powerful computing, large storage, and high intelligence (SCCSI) to support various kinds of applications, such as public safety, connected and autonomous driving, smart and connected health, and smart living. At the same time, it is widely recognized that vehicles, such as connected and autonomous vehicles, equipped with significantly powerful SCCSI capabilities, will become ubiquitous in future smart cities. By observing the convergence of these two trends, this article advocates the use of vehicles to build a cost-effective service network, based on the Vehicle as a Service (VaaS) paradigm, where vehicles empowered with SCCSI capability form a web of mobile servers and communicators to provide SCCSI services in smart cities. Towards this goal, this article first examines the potential use cases in smart cities and possible upgrades required for the transition from traditional vehicular ad hoc networks (VANETs) to VaaS. Then, the system architecture and use cases of the VaaS paradigm are comprehensively discussed. At last, the open problems of this paradigm and future research directions, including architectural design, service provisioning, incentive design, and security & privacy, are identified. It is expected that this paper paves the way towards developing a cost-effective and sustainable approach for smart cities.",No methods found.
2024,https://openalex.org/W4392356648,Engineering,Cost-sensitive learning for imbalanced medical data: a review,"Abstract Integrating Machine Learning (ML) in medicine has unlocked many opportunities to harness complex medical data, enhancing patient outcomes and advancing the field. However, the inherent imbalanced distribution of medical data poses a significant challenge, resulting in biased ML models that perform poorly on minority classes. Mitigating the impact of class imbalance has prompted researchers to explore various strategies, wherein Cost-Sensitive Learning (CSL) arises as a promising approach to improve the accuracy and reliability of ML models. This paper presents the first review of CSL for imbalanced medical data. A comprehensive exploration of the existing literature encompassed papers published from January 2010 to December 2022 and sourced from five major digital libraries. A total of 173 papers were selected, analysed, and classified based on key criteria, including publication years, channels and sources, research types, empirical types, medical sub-fields, medical tasks, CSL approaches, strengths and weaknesses of CSL, frequently used datasets and data types, evaluation metrics, and development tools. The results indicate a noteworthy publication rise, particularly since 2020, and a strong preference for CSL direct approaches. Data type analysis unveiled diverse modalities, with medical images prevailing. The underutilisation of cost-related metrics and the prevalence of Python as the primary programming tool are highlighted. The strengths and weaknesses analysis covered three aspects: CSL strategy, CSL approaches, and relevant works. This study serves as a valuable resource for researchers seeking to explore the current state of research, identify strengths and gaps in the existing literature and advance CSL’s application for imbalanced medical data.","<method>Machine Learning (ML)</method>, <method>Cost-Sensitive Learning (CSL)</method>"
2024,https://openalex.org/W4392925664,Engineering,Regulating the Electronic Synergy of Asymmetric Atomic Fe Sites with Adjacent Defects for Boosting Activity and Durability toward Oxygen Reduction,"Abstract The oxygen reduction reaction (ORR) plays a fundamental role in sustainable energy technologies. However, the creation of non‐precious metal electrocatalysts with high ORR activity and durability under all pH conditions is of great significance but remains challenging. Herein, the aim is to overcome this challenge by creating a Fe single atom catalyst on a 2D defect‐containing nitrogen‐doped carbon support (Fe 1 /DNC) via a microenvironment engineering strategy. Microkinetic modeling reveals that FeN 4 (OH) moieties are the real active sites under reaction conditions. Due to the synergistic promotion effect of denser accessible FeN 4 (OH) moieties and defect‐induced electronic properties, Fe 1 /DNC catalyst achieves extraordinary ORR activity under alkaline, acidic, and neutral conditions, with half‐wave potentials of 0.95, 0.82, and 0.70 V, respectively. Moreover, a negligible performance decay is observed with this Fe catalyst in stability and methanol tolerance tests. Zn‐air battery employing Fe 1 /DNC delivers remarkable peak power density and long‐term operational durability. Theoretical analysis provides compelling evidence that the defects adjacent to FeN 4 (OH) moieties can endow an inductive effect to reshape electronic properties to balance the OOH* formation and OH* reduction. This work offers insight into the regulation of asymmetric coordination structure and electronic properties of metal sites for boosting electrocatalytic activity and stability.",No methods found.
2024,https://openalex.org/W4394598286,Engineering,Performance analysis and optimization of thermal barrier coated piston diesel engine fuelled with biodiesel using RSM,"The current research investigates diesel and simarouba biodiesel blends (10%, 20%, & 30% by volume) in conventional and Low Heat Rejection (LHR) diesel engines, each rated at 4.4 kW. While optimization techniques like Response Surface Method and Taguchi have been extensively studied, the impact of LHR and optimization on LHR engine performance and emissions is rarely explored. Converting the conventional engine to LHR involved applying 300 μm of stabilized zirconia to the piston crown to enhance combustion efficiency. Performance and emissions were analyzed at rated injection pressure (200 bar) and timing (23° before top dead center - btdc). Experiments were continued on LHR engine by varying injection timings (advancing - 26°btdc and retarding - 20°btdc). Advanced injection timing showed significant improvement in performance of Low heat rejection engine. MINITAB statistical tool is used to optimize engine performance using Response Surface Method. The 20% blend showed improved performance in both engines. The optimum values for Low heat rejection engine responses are 26.8%, 0.32 kg/kW-h, 0.018%, 59.59 ppm, and 1419.03 ppm for brake thermal efficiency, brake specific fuel consumption, carbon monoxide, and unburnt hydrocarbons, respectively. Confirmation experiments aligned well with model predictions, indicating the potential of LHR engines to enhance thermal efficiency and reduce emissions.","<method>Response Surface Method</method>, <method>Taguchi</method>"
2024,https://openalex.org/W4401593044,Engineering,Overcoming the Limits of Cross-Sensitivity: Pattern Recognition Methods for Chemiresistive Gas Sensor Array,"Abstract As information acquisition terminals for artificial olfaction, chemiresistive gas sensors are often troubled by their cross-sensitivity, and reducing their cross-response to ambient gases has always been a difficult and important point in the gas sensing area. Pattern recognition based on sensor array is the most conspicuous way to overcome the cross-sensitivity of gas sensors. It is crucial to choose an appropriate pattern recognition method for enhancing data analysis, reducing errors and improving system reliability, obtaining better classification or gas concentration prediction results. In this review, we analyze the sensing mechanism of cross-sensitivity for chemiresistive gas sensors. We further examine the types, working principles, characteristics, and applicable gas detection range of pattern recognition algorithms utilized in gas-sensing arrays. Additionally, we report, summarize, and evaluate the outstanding and novel advancements in pattern recognition methods for gas identification. At the same time, this work showcases the recent advancements in utilizing these methods for gas identification, particularly within three crucial domains: ensuring food safety, monitoring the environment, and aiding in medical diagnosis. In conclusion, this study anticipates future research prospects by considering the existing landscape and challenges. It is hoped that this work will make a positive contribution towards mitigating cross-sensitivity in gas-sensitive devices and offer valuable insights for algorithm selection in gas recognition applications.",<method>pattern recognition</method>
2024,https://openalex.org/W4390483800,Engineering,Comprehensive Risk Analysis and Decision-Making Model for Hydroelectricity Energy Investments,"The risks of hydroelectricity energy investments should be managed effectively to increase the performance of these projects. Thus, more significant risks should be identified to take effective measures for risk management without experiencing high costs. Accordingly, the purpose of this study is to define critical risks in hydroelectricity energy investment projects by making a priority analysis. Within this scope, a new decision-making model is created. In the first stage, five different risks are examined by considering Spherical fuzzy Entropy. Moreover, the second stage consists of ranking emerging seven countries with the help of Spherical fuzzy multi-attribute ideal-real comparative assessment (MAIRCA). The main contribution of this study is that more important risks of hydroelectricity energy investments can be identified by the help of the priority analysis. This situation provides an opportunity to implement effective strategies to increase these investments without having high costs. Additionally, considering Spherical fuzzy sets has a positive impact on the appropriateness of the results. Since these numbers use a wider data range, the effectiveness of the analysis results can increase. It is determined that the most important risk is environmental risk with the highest weight value of 0.2478. Financial risks and personnel risks are other significant factors that affect the performance of the hydroelectricity energy investments. Furthermore, as a result of ranking the alternatives, it is seen that China is the most suitable country for hydroelectric energy investments. India and Mexico are other successful countries in this respect. However, Turkey and Indonesia have lower performance for this situation.","<method>Spherical fuzzy Entropy</method>, <method>Spherical fuzzy multi-attribute ideal-real comparative assessment (MAIRCA)</method>"
2024,https://openalex.org/W4390501772,Engineering,Remote sensing based forest cover classification using machine learning,"Abstract Pakistan falls significantly below the recommended forest coverage level of 20 to 30 percent of total area, with less than 6 percent of its land under forest cover. This deficiency is primarily attributed to illicit deforestation for wood and charcoal, coupled with a failure to embrace advanced techniques for forest estimation, monitoring, and supervision. Remote sensing techniques leveraging Sentinel-2 satellite images were employed. Both single-layer stacked images and temporal layer stacked images from various dates were utilized for forest classification. The application of an artificial neural network (ANN) supervised classification algorithm yielded notable results. Using a single-layer stacked image from Sentinel-2, an impressive 91.37% training overall accuracy and 0.865 kappa coefficient were achieved, along with 93.77% testing overall accuracy and a 0.902 kappa coefficient. Furthermore, the temporal layer stacked image approach demonstrated even better results. This method yielded 98.07% overall training accuracy, 97.75% overall testing accuracy, and kappa coefficients of 0.970 and 0.965, respectively. The random forest (RF) algorithm, when applied, achieved 99.12% overall training accuracy, 92.90% testing accuracy, and kappa coefficients of 0.986 and 0.882. Notably, with the temporal layer stacked image of the Sentinel-2 satellite, the RF algorithm reached exceptional performance with 99.79% training accuracy, 96.98% validation accuracy, and kappa coefficients of 0.996 and 0.954. In terms of forest cover estimation, the ANN algorithm identified 31.07% total forest coverage in the District Abbottabad region. In comparison, the RF algorithm recorded a slightly higher 31.17% of the total forested area. This research highlights the potential of advanced remote sensing techniques and machine learning algorithms in improving forest cover assessment and monitoring strategies.","<method>artificial neural network (ANN) supervised classification algorithm</method>, <method>random forest (RF) algorithm</method>"
2024,https://openalex.org/W4392144173,Engineering,Heterogeneous Catalyst Coating for Boosting the Activity and Chromium Tolerance of Cathodes for Solid Oxide Fuel Cells,"Abstract A challenge hindering the development of durable solid oxide fuel cells (SOFCs) is the significant performance degradation of cathodes owing to poisoning by volatile Cr originating from the Fe─Cr alloy interconnect. Herein, a heterogeneous catalyst coating, composed of Ba 1−x Ce 0.8 Gd 0.2 O 3–δ and BaCO 3 , remarkably improves the oxygen adsorption, dissociation capability, and Cr resistance of a La 0.6 Sr 0.4 Co 0.2 Fe 0.8 O 3–δ (LSCF) cathode is demonstrated. The coherent heterointerface interactions formed between the catalyst coating and LSCF result in varied levels of surface strain and electrostatic interactions, significantly suppressing Sr surface segregation on LSCF. A single cell with the catalyst coating‐decorated LSCF (CC‐LSCF) achieves a peak power density of 1.73 W cm −2 at 750 °C, with no noticeable performance degradation for 100 h. The CC‐LSCF cathode also exhibits outstanding durability under accelerated Cr poisoning conditions, compared with the tremendous degradation rate of 0.42% h −1 for the bare LSCF cathode. The enhanced Cr resistance is attributed to synergy induced by the stabilization of the lattice Sr cations by heterointerface interactions and the remarkable structural stability of the catalyst coating under Cr poisoning conditions. The novel heterointerface engineering strategy in this study provides insight into the design and development of active and Cr‐tolerant cathodes.",No methods found.
2024,https://openalex.org/W4392714183,Engineering,Explainability and Interpretability in Electric Load Forecasting Using Machine Learning Techniques – A Review,"Electric Load Forecasting (ELF) is the central instrument for planning and controlling demand response programs, electricity trading, and consumption optimization. Due to the increasing automation of these processes, meaningful and transparent forecasts become more and more important. Still, at the same time, the complexity of the used machine learning models and architectures increases. Because there is an increasing interest in interpretable and explainable load forecasting methods, this work conducts a literature review to present already applied approaches regarding explainability and interpretability for load forecasts using Machine Learning. Based on extensive literature research covering eight publication portals, recurring modeling approaches, trends, and modeling techniques are identified and clustered by properties to achieve more interpretable and explainable load forecasts. The results on interpretability show an increase in the use of probabilistic models, methods for time series decomposition and the use of fuzzy logic in addition to classically interpretable models. Dominant explainable approaches are Feature Importance and Attention mechanisms. The discussion shows that a lot of knowledge from the related field of time series forecasting still needs to be adapted to the problems in ELF. Compared to other applications of explainable and interpretable methods such as clustering, there are currently relatively few research results, but with an increasing trend.","<method>probabilistic models</method>, <method>time series decomposition</method>, <method>fuzzy logic</method>, <method>Feature Importance</method>, <method>Attention mechanisms</method>"
2024,https://openalex.org/W4396526654,Engineering,"Recent advancement of remaining useful life prediction of lithium-ion battery in electric vehicle applications: A review of modelling mechanisms, network configurations, factors, and outstanding issues","The remaining useful life (RUL) prediction of lithium-ion batteries (LIBs) plays a crucial role in battery management, safety assurance, and the anticipation of maintenance needs for reliable electric vehicle (EV) operation. An efficient prediction of RUL can ensure its safe operation and prevent both internal and external failures, as well as avoid any unwanted catastrophic events. However, achieving precise RUL prediction for electric vehicles presents a challenging task due to several issues related to intricate operational characteristics and dynamic shifts in model parameters throughout the aging process, battery parameters data extraction, data preprocessing, and hyperparameters tuning of the prediction model. This phenomenon significantly impacts the advancement of electric vehicle technology. To address these challenges, this study offers a comprehensive overview of various RUL prediction methods, presenting a comparative analysis of their outcomes, advantages, drawbacks, and associated research constraints. Emphasis is placed on the necessity of a battery management system (BMS) to ensure the safe and reliable functioning of LIBs. The review delves into crucial implementation factors, including battery test bench considerations, data selection, feature extraction, data preprocessing, performance evaluation indicators, and hyperparameter tuning. Additionally, the issues and challenges related to RUL prediction approaches such as; thermal runaway, material selection, cell balancing, battery aging, relaxation impact, training algorithms, data acquisition, and hyperparameter tuning were outlined to provide an in-depth understanding of the recent situations. The outcome of this review comprehensively examines various methods for predicting the RUL of LIB in EV applications, offering insights into their advantages, limitations, and research challenges. Recommendations for future trends in LIBs technology comprise enhancing prognostic accuracy and developing robust approaches to guarantee sustainable operation and management.",No methods found.
2024,https://openalex.org/W4396609541,Engineering,Robust Drone Delivery with Weather Information,"Problem definition: Drone delivery has recently garnered significant attention due to its potential for faster delivery at a lower cost than other delivery options. When scheduling drones from a depot for delivery to various destinations, the dispatcher must take into account the uncertain wind conditions, which affect the delivery times of drones to their destinations, leading to late deliveries. Methodology/results: To mitigate the risk of delivery delays caused by wind uncertainty, we propose a two-period drone scheduling model to robustly optimize the delivery schedule. In this framework, the scheduling decisions are made in the morning, with the provision for different delivery schedules in the afternoon that adapt to updated weather information available by midday. Our approach minimizes the essential riskiness index, which can simultaneously account for the probability of tardy delivery and the magnitude of lateness. Using wind observation data, we characterize the uncertain flight times via a cluster-wise ambiguity set, which has the benefit of tractability while avoiding overfitting the empirical distribution. A branch-and-cut (B&amp;C) algorithm is developed for this adaptive distributionally framework to improve its scalability. Our adaptive distributionally robust model can effectively reduce lateness in out-of-sample tests compared with other classical models. The proposed B&amp;C algorithm can solve instances to optimality within a shorter time frame than a general modeling toolbox. Managerial implications: Decision makers can use the adaptive robust model together with the cluster-wise ambiguity set to effectively reduce service lateness at customers for drone delivery systems. Funding: This work was supported by the National Natural Science Foundation of China [Grants 72101049 and 72232001], the Natural Science Foundation of Liaoning Province [Grant 2023-BS-091], the Fundamental Research Funds for the Central Universities [Grant DUT23RC(3)045], and the Major Project of the National Social Science Foundation [Grant 22&amp;ZD151]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/msom.2022.0339 .","<branch-and-cut (B&C) algorithm</method>, <adaptive distributionally robust model</method>, <cluster-wise ambiguity set</method>"
2024,https://openalex.org/W4392830014,Engineering,A Deep Learning-Based CAE Approach for Simulating 3D Vehicle Wheels Under Real-World Conditions,"The implementation of deep learning (DL) in computer-aided engineering (CAE) can significantly improve the accuracy and efficiency of simulating 3D vehicle wheels under real-world conditions. While traditional CAE methods can be time-consuming and computationally expensive, DL can reduce simulation time and development cycles across all industries. This work explores the role of DL and AI in virtual manufacturing and CAE and investigates how they can be used to improve the accuracy and efficiency of simulations for 3D vehicle wheels. Deep learning models can learn the complex relationships between different wheel design parameters, such as tire load distribution, stress distribution, and fatigue life. Once trained, these models can be embedded into CAE software, allowing for faster and more accurate simulations of wheel performance. This interdisciplinary study uses various deep learning techniques, including convolutional neural networks (CNNs), generative adversarial networks (GANs), and recurrent neural networks (RNNs), to create a more efficient and accurate relationship between CAD modeling and CAE simulation. The research aims to leverage the potential of deep learning models to automate 3D CAD design, accurately predict CAE results, and provide in-depth explanations and verifications. The benefits of this research are expected to extend to the automotive industry's pursuit of more robust and resilient wheel designs. By streamlining the product development process from conceptual design to engineering performance evaluation, this study has the potential to revolutionize the automotive industry's product development cycle.","<method>deep learning (DL)</method>, <method>convolutional neural networks (CNNs)</method>, <method>generative adversarial networks (GANs)</method>, <method>recurrent neural networks (RNNs)</method>"
2024,https://openalex.org/W4393055891,Engineering,A new intelligently optimized model reference adaptive controller using GA and WOA-based MPPT techniques for photovoltaic systems,"Recently, the integration of renewable energy sources, specifically photovoltaic (PV) systems, into power networks has grown in significance for sustainable energy generation. Researchers have investigated different control algorithms for maximum power point tracking (MPPT) to enhance the efficiency of PV systems. This article presents an innovative method to address the problem of maximum power point tracking in photovoltaic systems amidst swiftly changing weather conditions. MPPT techniques supply maximum power to the load during irradiance fluctuations and ambient temperatures. A novel optimal model reference adaptive controller is developed and designed based on the MIT rule to seek global maximum power without ripples rapidly. The suggested controller is also optimized through two popular meta-heuristic algorithms: The genetic algorithm (GA) and the whale optimization algorithm (WOA). These meta-heuristic approaches have been exploited to overcome the difficulty of selecting the adaptation gain of the MRAC controller. The reference voltage for MPPT is generated in the study through an adaptive neuro-fuzzy inference system. The suggested controller's performance is tested via MATLAB/Simulink software under varying temperature and radiation circumstances. Simulation is carried out using a Soltech 1sth-215-p module coupled to a boost converter, which powers a resistive load. Furthermore, to emphasize the recommended algorithm's performance, a comparative study was done between the optimal MRAC using GA and WOA and the conventional incremental conductance (INC) method.","<method>genetic algorithm (GA)</method>, <method>whale optimization algorithm (WOA)</method>, <method>adaptive neuro-fuzzy inference system</method>"
2024,https://openalex.org/W4393339929,Engineering,Optimizing landslide susceptibility mapping using machine learning and geospatial techniques,"Landslides present a substantial risk to human lives, the environment, and infrastructure. Consequently, it is crucial to highlight the regions prone to future landslides by examining the correlation between past landslides and various geo-environmental factors. This study aims to investigate the optimal data selection and machine learning model, or ensemble technique, for evaluating the vulnerability of areas to landslides and determining the most accurate approach. To attain our objectives, we considered two different scenarios for selecting landslide-free random points (a slope threshold and a buffer-based approach) and performed a comparative analysis of five machine learning models for landslide susceptibility mapping, namely: Support Vector Machine (SVM), Logistic Regression (LR), Linear Discriminant Analysis (LDA), Random Forest (RF), and Extreme Gradient Boosting (XGBoost). The study area for this research is an area in Polk County in Western North Carolina that has experienced fatal landslides, leading to casualties and significant damage to infrastructure, properties, and road networks. The model construction process involves the utilization of a dataset comprising 1215 historical landslide occurrences and 1215 non-landslide points. We integrated a total of fourteen geospatial data layers, consisting of topographic variables, soil data, geological data, and land cover attributes. We use various metrics to assess the models' performance, including accuracy, F1-score, Kappa score, and AUC-ROC. In addition, we used the seeded-cell area index (SCAI) to evaluate map consistency. The ensemble of the five models using Weighted Average produces outstanding results, with an AUC-ROC of 99.4% for the slope threshold scenario and 91.8% for the buffer-based scenario. Our findings emphasize the significant impact of non-landslide random sampling on model performance in landslide susceptibility mapping. Furthermore, by optimally identifying landslide-prone regions and hotspots that need urgent risk management and land use planning, our study demonstrates the effectiveness of machine learning models in analyzing landslide susceptibility and providing valuable insights for informed decision-making and disaster risk reduction initiatives.","<method>Support Vector Machine (SVM)</method>, <method>Logistic Regression (LR)</method>, <method>Linear Discriminant Analysis (LDA)</method>, <method>Random Forest (RF)</method>, <method>Extreme Gradient Boosting (XGBoost)</method>, <method>ensemble technique using Weighted Average</method>"
2024,https://openalex.org/W4399144385,Engineering,Assessment of technical water quality in mining based on machine learning methods,"Introduction. Mining requires water treatment and wastewater processing, abstraction and discharge during mining increases consumption several times. Since water consumption in mining and processing is usually associated with domestic, industrial and technical needs, the need for water supply systems required for water treatment increases. Water from different sources can be used for treatment: incoming water, process and reused water, and wastewater. But the water obtained from any of the sources must meet all the norms and requirements. Water quality is determined by physical, chemical and bacteriological properties. The main directions for improving water consumption by mining enterprises are to reduce the consumption of drinking water from rivers, lakes and municipal water supply, as well as to expand the use of mine and quarry water for domestic and technical needs. Materials and methods. As training data for training the neural network, a dataset that includes water quality data obtained from fresh water sources was selected for the methods work, and using machine learning, develops a model that predicts whether the water is suitable for technical use in mines. This dataset includes 2293 values (samples) as well as 9 attributes. Correlation, neural network, and decision tree methods were used to build the models in this study. Results. Various machine learning methods (neural network and decision trees) were used to build a predictive model to assess the quality of water that would be suitable for use in the mining industry for technical purposes. With the help of the built models were processed data obtained from public sources, when analyzing which it was found that the method of decision trees was more accurate. The constructed model, for determining dependencies, thus, has high accuracy (small error). To increase the practical significance of the study, a number of transformations of the initial data set were carried out, in particular, an experiment with the division of attributes into groups of importance, in relation to the data, taking into account the subject area. The results obtained made it clear that checking only for hazardous impurities does not guarantee the suitability of water, but almost completely excludes (low significance factor) samples with impurities that do not meet the requirements, and the model can have practical significance. Allocation of the group for rapid quality determination, showed that for the express test, in an emergency situation or under time constraints, the possibility of practical use of the obtained model, has a justification, due to the small error. In general, the conducted experiments have shown that when taking into account the costs (total) for data collection, it makes sense to use models, taking into account the reduction of collected data, on the parameters (factors) of technical water. Discussion. In general, on the basis of the conducted research, we can talk about the successful application of machine learning methods in determining the suitability of technical water in the mining industry. During the experiments, the decision tree method performed particularly well, with the lowest error values. In addition, further work can be carried out to reduce the error in the models, in particular, by possibly increasing the number of attributes, as well as more fine-tuning of the applied machine learning methods. Conclusions. The authors conclude that machine learning techniques can be successfully integrated to determine the quality and suitability of process water in the mining industry in today’s world. Resume. The paper compares machine learning methods such as decision trees and neural network method. The comparative analysis of these methods and their quality of information processing is shown on the example of a set of data on water quality in the mining industry. With the help of built models were processed data obtained from open sources, when analyzing which it was found that the method of decision trees was more accurate. The constructed model for determining dependencies has high accuracy (small error). Suggestions for practical applications and future research directions. This study can form the basis for research in this or related fields to conduct further studies on the reliability and accuracy of using machine learning to predict the quality of water used in the mining industry. Continued work in the above direction may be the rationale for wider use of the above methods to improve various meaningful production performance in this or related areas.","<method>neural network</method>, <method>decision tree</method>"
2024,https://openalex.org/W4401386421,Engineering,Machine learning prediction of mechanical properties in metal additive manufacturing,"Predicting mechanical properties in metal additive manufacturing (MAM) is essential for ensuring the performance and reliability of printed parts, as well as their suitability for specific applications. However, conducting experiments to estimate mechanical properties in MAM processes can be laborious and expensive, and they are often limited to specific materials and processes. Machine learning (ML) methods offer a more flexible and cost-effective approach to predicting mechanical properties based on processing parameters and material properties. In this study, we introduce a comprehensive framework for benchmarking ML models for predicting mechanical properties. We compiled an extensive experimental dataset from over 90 MAM articles and data sheets from a diverse range of sources, encompassing 140 different MAM data sheets. This dataset includes information on MAM processing conditions, machines, materials, and resulting mechanical properties such as yield strength, ultimate tensile strength, elastic modulus, elongation, hardness, and surface roughness. Our framework incorporates physics-aware featurization specific to MAM, adjustable ML models, and tailored evaluation metrics to construct a comprehensive learning framework for predicting mechanical properties. Additionally, we explore the Explainable AI method, specifically SHAP analysis, to elucidate and interpret the predicted values of ML models for mechanical properties. Furthermore, data-driven explicit models were developed to estimate mechanical properties based on processing parameters and material properties, offering enhanced interpretability compared to conventional ML models.","<method>Machine learning (ML) methods</method>, <method>Explainable AI method</method>, <method>SHAP analysis</method>, <method>data-driven explicit models</method>"
2024,https://openalex.org/W4402516998,Engineering,Systematic conservation prioritization with the prioritizr R package,"Abstract Plans for expanding protected area systems (prioritizations) need to fulfill conservation objectives. They also need to account for other factors, such as economic feasibility and anthropogenic land‐use requirements. Although prioritizations are often generated with decision support tools, most tools have limitations that hinder their use for decision‐making. We outlined how the prioritizr R package ( https://prioritizr.net ) can be used for systematic conservation prioritization. This decision support tool provides a flexible interface to build conservation planning problems. It can leverage a variety of commercial (e.g., Gurobi) and open‐source (e.g., CBC and SYMPHONY) exact algorithm solvers to identify optimal solutions in a short period. It is also compatible with a variety of spatially explicit (e.g., ESRI Shapefile, GeoTIFF) and nonspatial tabular (e.g., Microsoft Excel Spreadsheet) data formats. Additionally, it provides functionality for evaluating prioritizations, such as assessing the relative importance of different places selected by a prioritization. To showcase the prioritizr R package, we applied it to a case study based in Washington state (United States) for which we developed a prioritization to improve protected area coverage of native avifauna. We accounted for land acquisition costs, existing protected areas, places that might not be suitable for protected area establishment, and spatial fragmentation. We also conducted a benchmark analysis to examine the performance of different solvers. The prioritization identified 12,400 km 2 of priority areas for increasing the percentage of species’ distributions covered by protected areas. Although open source and commercial solvers were able to quickly solve large‐scale conservation planning problems, commercial solvers were required for complex, large‐scale problems.. The prioritizr R package is available on the Comprehensive R Archive Network (CRAN). In addition to reserve selection, it can inform habitat restoration, connectivity enhancement, and ecosystem service provisioning. It has been used in numerous conservation planning exercises to inform best practices and aid real‐world decision‐making.",No methods found.
2024,https://openalex.org/W4390483836,Engineering,Ranking Factors Affecting Sustainable Competitive Advantage from The Business Intelligence Perspective: Using Content Analysis And F-TOPSIS,"Sustainable competitive advantage, as a key factor in business success, ensures that the company is able to dominate the market with differentiated products and services over a long period of time. This advantage is especially achieved through business intelligence, since smart decisions, leveraging meaningful data and analytics, and continuous process improvement help the company maintain this advantage and experience sustainable growth. The aim of this study is to rank the factors influencing sustainable competitive advantage from a business intelligence standpoint. The research methodology consists of two stages: qualitative and quantitative. In the first step, content analysis was performed to extract indicators from previous studies. In the second step, indicators were ranked using the F-TOPSIS method. Factors affecting sustainable competitive advantage from the business intelligence viewpoint were categorized into 5 criteria, including 27 sub-criteria. The 5 main criteria are customer relationship management, smart marketing, soft and hard organizational factors, and the mental image of the product, respectively. In the second step, the sub-criteria in each criterion were ranked. In customer relationship management, the most important sub-criterion is effective interaction with customers. In smart marketing, the most important sub-criterion is feedback and continuous improvement. Among the soft and hard organizational factors, the most important sub-criteria are support from senior management and technology and infrastructure. In the mental image of the product, the most important sub-criterion is social responsibility.",No methods found.
2024,https://openalex.org/W4390748349,Engineering,Charging management of electric vehicles with the presence of renewable resources,"Considering the increasing use of electric vehicles, the establishment of charging stations to exchange power between the grid and electric devices, and the integration of charging stations with solar power generation sources, the optimal use of electric vehicle charging stations in the power system. The purpose of cost reduction in the presence of the intelligent environment is a challenge that must be investigated so that this platform is suitable for predicting the behaviour of vehicles and, as a result, optimizing their presence in the power network. This research presents a relatively complete radial distribution network development planning model in two scenarios. In the first scenario, the effects of electric vehicles are not considered, and only the effects of distributed production (renewable and dispatchable) are considered. Studies have been done on a sample 54-bus network, a common system in most Distribution expansion planning (DEP) articles for distribution networks. In addition, the real data of American highways have been used to create raw input data. Also, due to the distance limit, the information on vehicles under 100 miles has been received as electric vehicle information. The clustering method and Capiola multivariate probability distribution functions have created suitable vehicle scenarios during different planning years. Capiola's method increases the accuracy of vehicle load forecasting according to a predetermined growth rate. The DEP problem in this research is modeled as an optimization problem based on scenario, dynamic, and in 5 one-year time frames (5-year time horizon and one-year accuracy). The results indicate that, in the presence of electric vehicles and distributed production sources, the technical characteristics of the network are improved. Similarly, the use of DGs, in addition to reducing the cost of equipment, has reduced undistributed energy in the system. But 10,000 vehicles, which have been applied to the network as an uncontrolled load, have caused an increase in undistributed energy. The cost of equipment required for the network development is almost as much as 5%.","<method>clustering method</method>, <method>Capiola multivariate probability distribution functions</method>, <method>Capiola's method</method>"
2024,https://openalex.org/W4391612257,Engineering,Machine learning for the management of biochar yield and properties of biomass sources for sustainable energy,"Abstract Biochar is emerging as a potential solution for biomass conversion to meet the ever increasing demand for sustainable energy. Efficient management systems are needed in order to exploit fully the potential of biochar. Modern machine learning (ML) techniques, and in particular ensemble approaches and explainable AI methods, are valuable for forecasting the properties and efficiency of biochar properly. Machine‐learning‐based forecasts, optimization, and feature selection are critical for improving biomass management techniques. In this research, we explore the influences of these techniques on the accurate forecasting of biochar yield and properties for a range of biomass sources. We emphasize the importance of the interpretability of a model, as this improves human comprehension and trust in ML predictions. Sensitivity analysis is shown to be an effective technique for finding crucial biomass characteristics that influence the synthesis of biochar. Precision prognostics have far‐reaching ramifications, influencing industries such as biomass logistics, conversion technologies, and the successful use of biomass as renewable energy. These advances can make a substantial contribution to a greener future and can encourage the development of a circular biobased economy. This work emphasizes the importance of using sophisticated data‐driven methodologies such as ML in biochar synthesis, to usher in ecologically friendly energy solutions. These breakthroughs hold the key to a more sustainable and environmentally friendly future.","<method>ensemble approaches</method>, <method>explainable AI methods</method>, <method>machine-learning-based forecasts</method>, <method>optimization</method>, <method>feature selection</method>, <method>sensitivity analysis</method>"
2024,https://openalex.org/W4391878291,Engineering,Effective lung nodule detection using deep CNN with dual attention mechanisms,"Abstract Novel methods are required to enhance lung cancer detection, which has overtaken other cancer-related causes of death as the major cause of cancer-related mortality. Radiologists have long-standing methods for locating lung nodules in patients with lung cancer, such as computed tomography (CT) scans. Radiologists must manually review a significant amount of CT scan pictures, which makes the process time-consuming and prone to human error. Computer-aided diagnosis (CAD) systems have been created to help radiologists with their evaluations in order to overcome these difficulties. These systems make use of cutting-edge deep learning architectures. These CAD systems are designed to improve lung nodule diagnosis efficiency and accuracy. In this study, a bespoke convolutional neural network (CNN) with a dual attention mechanism was created, which was especially crafted to concentrate on the most important elements in images of lung nodules. The CNN model extracts informative features from the images, while the attention module incorporates both channel attention and spatial attention mechanisms to selectively highlight significant features. After the attention module, global average pooling is applied to summarize the spatial information. To evaluate the performance of the proposed model, extensive experiments were conducted using benchmark dataset of lung nodules. The results of these experiments demonstrated that our model surpasses recent models and achieves state-of-the-art accuracy in lung nodule detection and classification tasks.","<method>convolutional neural network (CNN)</method>, <method>dual attention mechanism</method>, <method>channel attention</method>, <method>spatial attention</method>, <method>global average pooling</method>"
2024,https://openalex.org/W4391097018,Engineering,Optimal energy management strategies for hybrid electric vehicles: A recent survey of machine learning approaches,"Hybrid Electric Vehicles (HEVs) have emerged as a viable option for reducing pollution and attaining fuel savings in addition to reducing emissions. The effectiveness of HEVs heavily relies on the energy management strategies (EMSs) employed, as it directly impacts vehicle fuel consumption. Developing suitable EMSs for HEVs poses a challenge, as the goal is to maximize fuel economy yet optimize vehicle performance. EMSs algorithms are critical in determining power distribution between the engine and motor in HEVs. Traditionally, EMSs for HEVs have been developed based on optimal control theory. However, in recent years, a rising number of people have been interested in utilizing machine-learning techniques to enhance EMSs performance. This article presents a current analysis of various EMSs proposed in the literature. It highlights the shift towards integrating machine learning and artificial intelligence (AI) breakthroughs in EMSs development. The study examines numerous case studies, and research works employing machine learning techniques across different categories to develop energy management strategies for HEVs. By leveraging advancements in machine learning and AI, researchers have explored innovative approaches to optimize HEVs' performance and fuel economy. Key conclusions from our investigation show that machine learning has made a substantial contribution to solving the complex problems associated with HEV energy management. We emphasize how machine learning algorithms may be adjusted to dynamic operating environments, how well they can identify intricate patterns in hybrid electric vehicle systems, and how well they can manage non-linear behaviors.","<method>optimal control theory</method>, <method>machine learning techniques</method>, <method>machine learning algorithms</method>"
2024,https://openalex.org/W4392157869,Engineering,Transfer learning with graph neural networks for improved molecular property prediction in the multi-fidelity setting,"Abstract We investigate the potential of graph neural networks for transfer learning and improving molecular property prediction on sparse and expensive to acquire high-fidelity data by leveraging low-fidelity measurements as an inexpensive proxy for a targeted property of interest. This problem arises in discovery processes that rely on screening funnels for trading off the overall costs against throughput and accuracy. Typically, individual stages in these processes are loosely connected and each one generates data at different scale and fidelity. We consider this setup holistically and demonstrate empirically that existing transfer learning techniques for graph neural networks are generally unable to harness the information from multi-fidelity cascades. Here, we propose several effective transfer learning strategies and study them in transductive and inductive settings. Our analysis involves a collection of more than 28 million unique experimental protein-ligand interactions across 37 targets from drug discovery by high-throughput screening and 12 quantum properties from the dataset QMugs. The results indicate that transfer learning can improve the performance on sparse tasks by up to eight times while using an order of magnitude less high-fidelity training data. Moreover, the proposed methods consistently outperform existing transfer learning strategies for graph-structured data on drug discovery and quantum mechanics datasets.","<method>graph neural networks</method>, <method>transfer learning</method>, <method>transfer learning strategies</method>"
2024,https://openalex.org/W4395444855,Engineering,Techno-economic feasibility of integrating hybrid battery-hydrogen energy storage system into an academic building,"Green hydrogen production and storage are vital in mitigating carbon emissions and sustainable transition. However, the high investment cost and management requirements are the bottleneck of utilizing hybrid hydrogen-based systems in microgrids. Given the necessity of cost-effective and optimal design of these systems, the present study examines techno-economic feasibility of integrating hybrid hydrogen-based systems into an outdoor test facility. With this perspective, several solar-driven hybrid scenarios are introduced at two energy storage levels, namely the battery and hydrogen energy storage systems, including the high-pressure gaseous hydrogen and metal hydride storage tanks. Dynamic simulations are carried out to address subtle interactions in components of the hybrid system by establishing a TRNSYS model coupled to a Fortran code simulating the metal hydride storage system. The OpenStudio-EnergyPlus plugin is used to simulate the building load, validate against experimental data, according to the measured data and monitored operating conditions. Aimed at enabling efficient integration of energy storage systems, a techno-enviro-economic optimization algorithm is developed to simultaneously minimize the levelized cost of the electricity and maximize the CO2 mitigation in each proposed hybrid scenario. The results indicate that integrating the gaseous hydrogen and metal hydride storages into the photovoltaic-alone scenario enhances 22.6% and 14.4% of the annual renewable factor. Accordingly, the inclusion of battery system to these hybrid scenarios gives a 30.4% and 20.3 % boost to the renewable factor value, respectively. Although the inclusion of battery energy storage into the hybrid systems increases the renewable factor, the results imply that it reduces the hydrogen production rate via electrolysis. The optimized values of the levelized cost of electricity and CO2 emission for different scenarios vary in the range of 0.376–0.789 $/kWh and 6.57–9.75 ton, respectively. The multi-criteria optimizations improve the levelized cost of electricity and CO2 emission by up to 46.2% and 11.3%, with respect to their preliminary design.",No methods found.
2024,https://openalex.org/W4396686667,Engineering,Optimal design of steel exoskeleton for the retrofitting of RC buildings via genetic algorithm,"In recent decades, steel exoskeletons have gathered significant attention as a seismic retrofitting technique for existing structures. The design methods proposed so far are focused on the identification of the system's overall parameters through simplified models. Although these methodologies provide helpful guidance at the preliminary design stage, they do not consider aspects such as the distribution of the exoskeletons and sizing of their components. To overcome these limitations, an optimization process based on the Genetic Algorithm is proposed in this paper to identify the optimal exoskeleton number and spatial arrangement, and to determine the optimal size of their constituent elements. The algorithm aims to minimize the weight of the retrofit solution while keeping the whole existing structure in the elastic field and ensuring the structural verification of the exoskeleton's elements. The analyses have been conducted using a finite-element code with an Open Application Programming Interface, which allows the models to be handled through automatic routines. The proposed optimization tool has been applied to several case studies, considering two different layouts for the exoskeletons. Finally, the effectiveness of the retrofit method has been demonstrated, and the proposed optimization tool has been able to significantly reduce the weight and cost of the intervention.",<method>Genetic Algorithm</method>
2024,https://openalex.org/W4398787803,Engineering,Transformer-based Generative Adversarial Networks in Computer Vision: A Comprehensive Survey,"Generative Adversarial Networks (GANs) have been very successful for synthesizing the images in a given dataset. The artificially generated images by GANs are very realistic. The GANs have shown potential usability in several computer vision applications, including image generation, image-to-image translation, video synthesis, etc. Conventionally, the generator network is the backbone of GANs, which generates the samples and the discriminator network is used to facilitate the training of the generator network. The generator and discriminator networks are usually a Convolutional Neural Network (CNN). The convolution-based networks exploit the local relationship in a layer, which requires the deep networks to extract the abstract features. However, recently developed Transformer networks are able to exploit the global relationship with tremendous performance improvement for several problems in computer vision. Motivated from the success of Transformer networks and GANs, recent works have tried to exploit the Transformers in GAN framework for the image/video synthesis. This paper presents a comprehensive survey on the developments and advancements in GANs utilizing the Transformer networks for computer vision applications. The performance comparison for several applications on benchmark datasets is also performed and analyzed. The conducted survey will be very useful to understand the research trends & gaps related with Transformer-based GANs and to develop the advanced GAN architectures by exploiting the global and local relationships for different applications.","<method>Generative Adversarial Networks (GANs)</method>, <method>Convolutional Neural Network (CNN)</method>, <method>Transformer networks</method>"
2024,https://openalex.org/W4400110237,Engineering,A new integrated intelligent computing paradigm for predicting joints shear strength,"Joints shear strength is a critical parameter during the design and construction of geotechnical engineering structures. The prevailing models mostly adopt the form of empirical functions, employing mathematical regression techniques to represent experimental data. As an alternative approach, this paper proposes a new integrated intelligent computing paradigm that aims to predict joints shear strength. Five metaheuristic optimization algorithms, including the chameleon swarm algorithm (CSA), slime mold algorithm, transient search optimization algorithm, equilibrium optimizer and social network search algorithm, were employed to enhance the performance of the multilayered perception (MLP) model. Efficiency comparisons were conducted between the proposed CSA-MLP model and twelve classical models, employing statistical indicators such as root mean square error (RMSE), correlation coefficient (R2), mean absolute error (MAE), and variance accounted for (VAF) to evaluate the performance of each model. The sensitivity analysis of parameters that impact joints shear strength was conducted. Finally, the feasibility and limitations of this study were discussed. The results revealed that, in comparison to other models, the CSA-MLP model exhibited the most appropriate performance in terms of R2 (0.88), RMSE (0.19), MAE (0.15), and VAF (90.32%) values. The result of sensitivity analysis showed that the normal stress and the joint roughness coefficient were the most critical factors influencing joints shear strength. This paper presented an efficacious attempt toward swift prediction of joints shear strength, thus avoiding the need for costly in-site and laboratory tests.","<method>chameleon swarm algorithm (CSA)</method>, <method>slime mold algorithm</method>, <method>transient search optimization algorithm</method>, <method>equilibrium optimizer</method>, <method>social network search algorithm</method>, <method>multilayered perception (MLP) model</method>"
2024,https://openalex.org/W4400721646,Engineering,"A Comprehensive Review on the Role of Artificial Intelligence in Power System Stability, Control, and Protection: Insights and Future Directions","This review comprehensively examines the burgeoning field of intelligent techniques to enhance power systems’ stability, control, and protection. As global energy demands increase and renewable energy sources become more integrated, maintaining the stability and reliability of both conventional power systems and smart grids is crucial. Traditional methods are increasingly insufficient for handling today’s power grids’ complex, dynamic nature. This paper discusses the adoption of advanced intelligence methods, including artificial intelligence (AI), deep learning (DL), machine learning (ML), metaheuristic optimization algorithms, and other AI techniques such as fuzzy logic, reinforcement learning, and model predictive control to address these challenges. It underscores the critical importance of power system stability and the new challenges of integrating diverse energy sources. The paper reviews various intelligent methods used in power system analysis, emphasizing their roles in predictive maintenance, fault detection, real-time control, and monitoring. It details extensive research on the capabilities of AI and ML algorithms to enhance the precision and efficiency of protection systems, showing their effectiveness in accurately identifying and resolving faults. Additionally, it explores the potential of fuzzy logic in decision-making under uncertainty, reinforcement learning for dynamic stability control, and the integration of IoT and big data analytics for real-time system monitoring and optimization. Case studies from the literature are presented, offering valuable insights into practical applications. The review concludes by identifying current limitations and suggesting areas for future research, highlighting the need for more robust, flexible, and scalable intelligent systems in the power sector. This paper is a valuable resource for researchers, engineers, and policymakers, providing a detailed understanding of the current and future potential of intelligent techniques in power system stability, control, and protection.","<method>artificial intelligence (AI)</method>, <method>deep learning (DL)</method>, <method>machine learning (ML)</method>, <method>metaheuristic optimization algorithms</method>, <method>fuzzy logic</method>, <method>reinforcement learning</method>, <method>model predictive control</method>"
2024,https://openalex.org/W4401289975,Engineering,Application of ANFIS approach for prediction of performance measures in wire electric discharge machining of SAE 1010,"Due to its exceptional quality, SAE 1010 is highly recommended for automotive applications, particularly in the manufacturing of headed fasteners and bolts. The primary application of this technology is in automobiles, while it also holds significant potential for various other technological disciplines. Utilizing alternative techniques for removing material has proven to be essential in overcoming numerous machining challenges that were previously difficult to solve. It possesses numerous practical applications in aircraft engineering and exhibits significant potential for implementation in other technical domains. Manufacturing complicated curved components using traditional machining methods might provide challenges. In order to prevent such issues, a wide range of cutting-edge machining methods have been developed. Wire Electrical Discharge Machining (WEDM) is a variance of Electrical Discharge Machining (EDM) that is suitable for this particular use. This study employs Taguchi's technique to examine the Wire Electrical Discharge Machining (WEDM) of SAE 1010 steel from an environmentally friendly viewpoint by employing a natural dielectric fluid in order to minimize its ecological footprint. This study aims to optimize the process variable and develop a hybrid predictive model based on grey approach for foretelling the necessary performance measures by considering various performance metrics, including material removal rate, surface roughness, and tolerance errors. The significance of process variables has been determined with the help of Analysis of variance (ANOVA) and it is inferred that pulse on duration is the most contributing factor for all the desired performance measures. A hybrid technique was used by an artificial intelligence technology to project the selected output measure. The outcomes on performance of the evolved ANFIS model shows the prediction capability of the model developed with least errors (MAPE – 0.0417, RMSE − 0.00023, MAE – 0.000419, Correlation coefficient 0.9997). The outcomes of the analysis indicate that the model is both efficient and accurate in its predictions, could be valuable to the manufacturer since it establishes targets for important performance indicators.","<method>Taguchi's technique</method>, <method>Analysis of variance (ANOVA)</method>, <method>hybrid predictive model based on grey approach</method>, <method>ANFIS model</method>"
2024,https://openalex.org/W4404436451,Engineering,"A Universal Interfacial Reconstruction Strategy Based on Converting Residual Alkali for Sodium Layered Oxide Cathodes: Marvelous Air Stability, Reversible Anion Redox, and Practical Full Cell","Mn-based layered oxide cathodes have attracted widespread attention due to high capacity and low cost, however, poor air stability, irreversible phase transitions, and slow kinetics inhibit their practical application. Here, we propose a universal interfacial reconstruction strategy based on converting residual alkali to tunnel phase Na0.44MnO2 for addressing the above mentioned issue simultaneously, using O3 NaNi0.4Fe0.2Mn0.4O2@2 mol % Na0.44MnO2 (NaNFM@NMO) as the prototype material. The optimized material exhibits an initial capacity and energy density comparable with lithium-ion batteries. The reversible anionic redox behavior and charge compensation mechanism of NaNFM@NMO were analyzed and verified by soft X-ray absorption spectrum and in situ X-ray absorption spectrum. Due to the intrinsic stability of the tunnel structure, excellent air stability and highly reversible structure evolution of the NaNFM@NMO cathode material are achieved, which are confirmed by contact angle test, rigorous aging test, and in situ X-ray diffraction. More importantly, the NaNFM@NMO cathode demonstrates a great match with the nonpresodiated hard carbon anode and shows excellent electrochemical performance of the full cell. Additionally, such a strategy could be also applied to modify P2-type cathodes, showing superior universality and good prospects in industrialized production. Overall, the proposed strategy could improve air stability while remaining interfacial and bulk stable simultaneously and will open up a whole new field for the optimization of other electrode materials.",No methods found.
2024,https://openalex.org/W4390721566,Engineering,Thousands of AI Authors on the Future of AI,"In the largest survey of its kind, 2,778 researchers who had published in top-tier artificial intelligence (AI) venues gave predictions on the pace of AI progress and the nature and impacts of advanced AI systems The aggregate forecasts give at least a 50% chance of AI systems achieving several milestones by 2028, including autonomously constructing a payment processing site from scratch, creating a song indistinguishable from a new song by a popular musician, and autonomously downloading and fine-tuning a large language model. If science continues undisrupted, the chance of unaided machines outperforming humans in every possible task was estimated at 10% by 2027, and 50% by 2047. The latter estimate is 13 years earlier than that reached in a similar survey we conducted only one year earlier [Grace et al., 2022]. However, the chance of all human occupations becoming fully automatable was forecast to reach 10% by 2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey). Most respondents expressed substantial uncertainty about the long-term value of AI progress: While 68.3% thought good outcomes from superhuman AI are more likely than bad, of these net optimists 48% gave at least a 5% chance of extremely bad outcomes such as human extinction, and 59% of net pessimists gave 5% or more to extremely good outcomes. Between 38% and 51% of respondents gave at least a 10% chance to advanced AI leading to outcomes as bad as human extinction. More than half suggested that ""substantial"" or ""extreme"" concern is warranted about six different AI-related scenarios, including misinformation, authoritarian control, and inequality. There was disagreement about whether faster or slower AI progress would be better for the future of humanity. However, there was broad agreement that research aimed at minimizing potential risks from AI systems ought to be prioritized more.",No methods found.
2024,https://openalex.org/W4390870882,Engineering,A domain adaptation approach to damage classification with an application to bridge monitoring,"Data-driven machine-learning algorithms generally suffer from a lack of labelled health-state data, mainly those referring to damage conditions. To address such an issue, population-based structural health monitoring seeks to enrich the original dataset by transferring knowledge from a population of monitored structures. Within this context, this paper presents a transfer learning approach, based on domain adaptation, to leverage information from completely-labelled bridge structure data to accurately predict new instances of an unknown target domain. Since intrinsic structural differences may cause distribution shifts, domain adaptation attempts to minimise the distance between the domains and to learn a mapping within a shared feature space. Specifically, the methodology involves the long-term acquisition of natural frequencies from several structural scenarios. Such damage-sensitive features are then aligned via domain adaptation so that a machine-learning algorithm can effectively utilise the labelled source domain data and generalise well to the unlabelled target-domain data. The described procedure is applied to two case studies, including the Z24 and the S101 benchmark bridges and their finite element models, respectively. The results demonstrate the successful exchange of health-state labels to identify the damage class within a population of bridges equipped with SHM systems, showing potential to reduce computational efforts and to deal with scarce or poor data sets in application to bridge network monitoring.","<method>transfer learning</method>, <method>domain adaptation</method>, <method>machine-learning algorithm</method>"
2024,https://openalex.org/W4391478629,Engineering,Enhancing photovoltaic parameter estimation: integration of non-linear hunting and reinforcement learning strategies with golden jackal optimizer,"Abstract The advancement of Photovoltaic (PV) systems hinges on the precise optimization of their parameters. Among the numerous optimization techniques, the effectiveness of each often rests on their inherent parameters. This research introduces a new methodology, the Reinforcement Learning-based Golden Jackal Optimizer (RL-GJO). This approach uniquely combines reinforcement learning with the Golden Jackal Optimizer to enhance its efficiency and adaptability in handling various optimization problems. Furthermore, the research incorporates an advanced non-linear hunting strategy to optimize the algorithm’s performance. The proposed algorithm is first validated using 29 CEC2017 benchmark test functions and five engineering-constrained design problems. Secondly, rigorous testing on PV parameter estimation benchmark datasets, including the single-diode model, double-diode model, three-diode model, and a representative PV module, was carried out to highlight the superiority of RL-GJO. The results were compelling: the root mean square error values achieved by RL-GJO were markedly lower than those of the original algorithm and other prevalent optimization methods. The synergy between reinforcement learning and GJO in this approach facilitates faster convergence and improved solution quality. This integration not only improves the performance metrics but also ensures a more efficient optimization process, especially in complex PV scenarios. With an average Freidman’s rank test values of 1.564 for numerical and engineering design problems and 1.742 for parameter estimation problems, the proposed RL-GJO is performing better than the original GJO and other peers. The proposed RL-GJO stands out as a reliable tool for PV parameter estimation. By seamlessly combining reinforcement learning with the golden jackal optimizer, it sets a new benchmark in PV optimization, indicating a promising avenue for future research and applications.","<method>Reinforcement Learning</method>, <method>Golden Jackal Optimizer</method>"
2024,https://openalex.org/W4391734045,Engineering,Novel hybrid kepler optimization algorithm for parameter estimation of photovoltaic modules,"Abstract The parameter identification problem of photovoltaic (PV) models is classified as a complex nonlinear optimization problem that cannot be accurately solved by traditional techniques. Therefore, metaheuristic algorithms have been recently used to solve this problem due to their potential to approximate the optimal solution for several complicated optimization problems. Despite that, the existing metaheuristic algorithms still suffer from sluggish convergence rates and stagnation in local optima when applied to tackle this problem. Therefore, this study presents a new parameter estimation technique, namely HKOA, based on integrating the recently published Kepler optimization algorithm (KOA) with the ranking-based update and exploitation improvement mechanisms to accurately estimate the unknown parameters of the third-, single-, and double-diode models. The former mechanism aims at promoting the KOA’s exploration operator to diminish getting stuck in local optima, while the latter mechanism is used to strengthen its exploitation operator to faster converge to the approximate solution. Both KOA and HKOA are validated using the RTC France solar cell and five PV modules, including Photowatt-PWP201, Ultra 85-P, Ultra 85-P, STP6-120/36, and STM6-40/36, to show their efficiency and stability. In addition, they are extensively compared to several optimization techniques to show their effectiveness. According to the experimental findings, HKOA is a strong alternative method for estimating the unknown parameters of PV models because it can yield substantially different and superior findings for the third-, single-, and double-diode models.","<method>Kepler optimization algorithm (KOA)</method>, <method>HKOA</method>"
2024,https://openalex.org/W4392106623,Engineering,Machine learning assisted prediction of solar to liquid fuel production: a case study,"In this era of heightened environmental awareness, the global community faces the critical challenge of climate change. Renewable energy (RE) emerges as a vital contender to mitigate global warming and meet increasing energy needs. Nonetheless, the fluctuating nature of renewable energy sources underscores the necessity for efficient conversion and storage strategies. This pioneering research focuses on the transformation of solar energy (SE) into liquid fuels, with a specific emphasis on formic acid (FA) as a case study, done in Binh Thuan, Vietnam. The paper unveils a technology designed to convert solar energy into formic acid, ensuring its stability and storage at ambient conditions. It involves detailed simulations to quantify the daily and monthly electricity output from photovoltaic (PV) systems and the corresponding mass of formic acid producible through solar energy. The simulation of a dual-axis solar tracking system for the PV panels, intended to maximize solar energy capture, is one of the project's illustrations. The elevation and azimuth angles, which are two essential tracking system parameters, are extensively studied in the present research. The project makes use of machine learning algorithms in the field of predictive modeling, specifically Artificial Neural Networks (ANN) and Support Vector Machines (SVM). These tools play a crucial role in modeling PV power output and formic acid production while accounting for a variety of influencing factors. A comparative study shows that SVM outperforms ANN in accurately predicting the production of FA and PV power generation, both of which are the major goals. This model is a predictive tool that can be used to forecast these goals based on certain causal variables. Overall, it is observed that the maximum power produced with 2-axis solar tracker was achieved in February as 2355 kW resulting in the highest formic acid production of 2.25 ×106 grams. The study's broad ramifications demonstrate solar liquid fuel technology's potential as a long-term fix in the field of renewable energy. In addition to advancing the field of renewable energy storage, the study represents a major step toward tackling the global challenge of climate change.","<method>Artificial Neural Networks (ANN)</method>, <method>Support Vector Machines (SVM)</method>"
2024,https://openalex.org/W4393999307,Engineering,Joint impact of service efficiency and salvage value on the manufacturer’s shared vehicle-type strategies,"With the rapid development of the sharing economy, many traditional automobile manufacturers have been choosing to provide the car sharing service. Some manufacturers share GVs, while others introduce EVs in the sharing market. We develop a model that a monopoly manufacturer who simultaneously sells GVs and EVs and discuss which type of vehicles should the manufacturer launch in the sharing market considering the service efficiency and the salvage value. Our findings are that no matter which type of vehicles the manufacturer shares, EV sales remain the same, but GV sales are reduced. This means that the manufacturer’s EV-sharing strategy always promotes EVs’ adoption. It is found that when both the service efficiency ratio of EV to GV and the salvage value gap between them are low or high, the manufacturer launches EVs; otherwise, the manufacturer launches GVs. We also find that the equilibrium vehicle-type strategy can maximize the manufacturer’s profit while being the most environmentally friendly only if the valuation of shared product is high. Through numerical analysis, we know that, although the manufacturer’s GV-sharing strategy worsens the environment, it always improves the social welfare. Notably, the manufacturer’s EV-sharing strategy is not always beneficial for the environment, especially if the service efficiency ratio is relatively high. Similarly, the manufacturer’s EV sharing does not always improve the social welfare, especially if the service efficiency ratio is in the middle range. The findings not only contribute to guiding the manufacturer’s vehicle-type strategies for car sharing, but also providing potential policy implications for the government’s effort in promoting EVs’ adoption.",No methods found.
2024,https://openalex.org/W4394585959,Engineering,Statistical Machine Learning for Power Flow Analysis Considering the Influence of Weather Factors on Photovoltaic Power Generation,"It is generally accepted that the impact of weather variation is gradually increasing in modern distribution networks with the integration of high-proportion photovoltaic (PV) power generation and weather-sensitive loads. This article analyzes power flow using a novel stochastic weather generator (SWG) based on statistical machine learning (SML). The proposed SML model, which incorporates generative adversarial networks (GANs), probability theory, and information theory, enables the generation and evaluation of simulated hourly weather data throughout the year. The GAN model captures various weather variation characteristics, including weather uncertainties, diurnal variations, and seasonal patterns. Compared to shallow learning models, the proposed deep learning model exhibits significant advantages in stochastic weather simulation. The simulated data generated by the proposed model closely resemble real data in terms of time-series regularity, integrity, and stochasticity. The SWG is applied to model PV power generation and weather-sensitive loads. Then, we actively conduct a power flow analysis (PFA) on a real distribution network in Guangdong, China, using simulated data for an entire year. The results provide evidence that the GAN-based SWG surpasses the shallow machine learning approach in terms of accuracy. The proposed model ensures accurate analysis of weather-related power flow and provides valuable insights for the analysis, planning, and design of distribution networks.","<method>statistical machine learning (SML)</method>, <method>generative adversarial networks (GANs)</method>, <method>shallow learning models</method>, <method>deep learning model</method>"
2024,https://openalex.org/W4391206025,Engineering,Short-term wind speed forecasting using an optimized three-phase convolutional neural network fused with bidirectional long short-term memory network model,"Wind energy is an environment friendly, low-carbon, and cost-effective renewable energy source. It is, however, difficult to integrate wind energy into a mixed energy grid due to its high volatility and intermittency. For wind energy conversion systems to be reliable and efficient, accurate wind speed (WS) forecasting is fundamental. This study cascades a convolutional neural network (CNN) with a bidirectional long short-term memory (BiLSTM) in order to obtain a model for hourly WS forecasting by utilizing several meteorological variables as model inputs to study their effects on predicted WS. For input selection, the mutation grey wolf optimizer (TMGWO) is used. For efficient optimization of CBiLSTM hyperparameters, a hybrid Bayesian Optimization and HyperBand (BOHB) algorithm is used. The combined usage of TMGWO, BOHB, and CBiLSTM leads to a three-phase hybrid model (i.e., 3P-CBiLSTM). The performance of 3P-CBiLSTM is benchmarked against the standalone and hybrid BiLSTMs, LSTMs, gradient boosting (GBRs), random forest (RFRs), and decision tree regressors (DTRs). The statistical analysis of forecasted WS reveals that the 3P-CBiLSTM is highly effective over the other benchmark forecasting methods. This objective model also registers the highest percentage of forecasted errors (≈ 53.4 – 81.8%) within the smallest error range ≤ |0.25| ms−1 amongst all tested study sites. Despite the remarkable results achieved, the CBiLSTM model cannot be generally understood, so the eXplainable Artificial Intelligence (xAI) technique was used for explaining local and global model outputs, based on Local Interpretable Model-Agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP). Both of the xAI methods determined that the antecedent WS is the most significant predictor of the short-term WS forecasting. Therefore, we aver that the proposed model can be employed to help wind farm operators in making quality decisions in maximizing wind power integration into the grid with reduced intermittency.","<method>convolutional neural network (CNN)</method>, <method>bidirectional long short-term memory (BiLSTM)</method>, <method>mutation grey wolf optimizer (TMGWO)</method>, <method>hybrid Bayesian Optimization and HyperBand (BOHB) algorithm</method>, <method>3P-CBiLSTM (three-phase hybrid model combining TMGWO, BOHB, and CBiLSTM)</method>, <method>long short-term memory (LSTM)</method>, <method>gradient boosting regressors (GBRs)</method>, <method>random forest regressors (RFRs)</method>, <method>decision tree regressors (DTRs)</method>, <method>Local Interpretable Model-Agnostic Explanations (LIME)</method>, <method>SHapley Additive exPlanations (SHAP)</method>"
2024,https://openalex.org/W4392450360,Engineering,Geographically weighted machine learning for modeling spatial heterogeneity in traffic crash frequency and determinants in US,"Spatial analyses of traffic crashes have drawn much interest due to the nature of the spatial dependence and spatial heterogeneity in the crash data. This study makes the best of Geographically Weighted Random Forest (GW-RF) model to explore the local associations between crash frequency and various influencing factors in the US, including road network attributes, socio-economic characteristics, and land use factors collected from multiple data sources. Special emphasis is put on modeling the spatial heterogeneity in the effects of a factor on crash frequency in different geographical areas in a data-driven way. The GW-RF model outperforms global models (e.g. Random Forest) and conventional geographically weighted regression, demonstrating superior predictive accuracy and elucidating spatial variations. The GW-RF model reveals spatial distinctions in the effects of certain factors on crash frequency. For example, the importance of intersection density varies significantly across regions, with high significance in the southern and northeastern areas. Low-grade road density emerges as influential in specific cities. The findings highlight the significance of different factors in influencing crash frequency across zones. Road network factors, particularly intersection density, exhibit high importance universally, while socioeconomic variables demonstrate moderate effects. Interestingly, land use variables show relatively lower importance. The outcomes could help to allocate resources and implement tailored interventions to reduce the likelihood of crashes.","<method>Geographically Weighted Random Forest (GW-RF)</method>, <method>Random Forest</method>, <method>geographically weighted regression</method>"
2024,https://openalex.org/W4392714432,Engineering,"Hybrid physics-machine learning models for predicting rate of penetration in the Halahatang oil field, Tarim Basin","Abstract Rate of penetration (ROP) is a key factor in drilling optimization, cost reduction and drilling cycle shortening. Due to the systematicity, complexity and uncertainty of drilling operations, however, it has always been a problem to establish a highly accurate and interpretable ROP prediction model to guide and optimize drilling operations. To solve this problem in the Tarim Basin, this study proposes four categories of hybrid physics-machine learning (ML) methods for modeling. One of which is residual modeling, in which an ML model learns to predict errors or residuals, via a physical model; the second is integrated coupling, in which the output of the physical model is used as an input to the ML model; the third is simple average, in which predictions from both the physical model and the ML model are combined; and the last is bootstrap aggregating (bagging), which follows the idea of ensemble learning to combine different physical models’ advantages. A total of 5655 real data points from the Halahatang oil field were used to test the performance of the various models. The results showed that the residual modeling model, with an R 2 of 0.9936, had the best performance, followed by the simple average model and bagging with R 2 values of 0.9394 and 0.5998, respectively. From the view of prediction accuracy, and model interpretability, the hybrid physics-ML model with residual modeling is the optimal method for ROP prediction.","<method>residual modeling</method>, <method>integrated coupling</method>, <method>simple average</method>, <method>bootstrap aggregating (bagging)</method>"
2024,https://openalex.org/W4395468525,Engineering,Optimizing energy solutions: A techno-economic analysis of solar-wind hybrid power generation in the coastal regions of Bangladesh,"Hybrid renewable energy systems have acquired attention worldwide for their ability to harness multiple renewable sources parallelly like solar, wind, and hydropower, presenting numerous advantages. Bangladesh is forced to rely on the traditional fossil fuel-powered power generation infrastructure in order to meet the nation's increasing need for electricity. The goal of this paper is to improve the percentage of renewable energy in Bangladesh's energy landscape by addressing the technical, economic, and environmental elements of building a specialized hybrid system at Patenga Sea Beach through a methodical approach. This study is the first to pinpoint and address Patenga Sea Beach's limits in light of Shah Amanat International Airport's nearby location in Chittagong. In order to build the suggested system, a daily load demand of 1000 kWh and an annual peak demand of 53.57 kW were taken into account. The analysis shows that this system has a levelized cost of energy (LCOE) of 0.03$, generating 4,604.591 MWh of power annually. Findings from HOMER reveal an initial capital outlay of $350,688 for initial capital investment, with an annual O&M cost of $3,821, contributing to its cost-effectiveness. During the span of its lifetime, the hybrid system can also avoid 5,767 tons of CO2. The simulation program PVsyst has been utilized to assess the PV system-specific performance, viability, and outcomes. According to the PVsyst evaluation, the PV system has an 89 % system performance ratio, a 0.017$ levelized cost of energy, and a break-even period of only 6.6 years. The sensitivity analysis examines renewable resources, factoring in climate change's effects on solar irradiation, wind speed, replacement cost, and operational expenses. It validates the hybrid system's viability across various environmental scenarios, demonstrating resilience to changes in renewable resource costs and availability. The suggested hybrid system can be placed to minimize costs associated with energy generation, circumvent structural and geographic limitations, improve the Patenga Sea beach's visual appeal, and lower greenhouse gas emissions. In addition to supporting future decisions about the deployment of hybrid systems, this research could assist in the integration of sustainable and renewable energy sources into the national grid.",No methods found.
2024,https://openalex.org/W4398155139,Engineering,Supersonic separation towards sustainable gas removal and carbon capture,"Carbon capture and storage is recognized as one of the most promising solutions to mitigate climate change. Compared to conventional separation technologies, supersonic separation is considered a new generation of technology for gas separation and carbon capture thanks to its advantages of cleaning and efficient processes which are achieved using energy conversion in supersonic flows. The supersonic separation works on two principles which both occur in supersonic flows: the energy conversion to generate microdroplets and supersonic swirling flows to remove the generated droplets. This review seeks to offer a detailed examination of the cutting-edge technology for gas separation and carbon dioxide removal in the new-generation supersonic separation technology, which plays a role in carbon capture and storage. The evaluation discusses the design, performance, financial feasibility, and practical uses of supersonic separators, emphasizing the most recent progress in the industry. Theoretical analysis, experiments, and numerical simulations are reviewed to examine in detail the advances in the nucleation and condensation characteristics and the mechanisms of supersonic separation, as well as new applications of this technology including the liquefaction of natural gas. We also provide the perspective of the challenges and opportunities for further development of supersonic separation. This survey contributes to an improved understanding of sustainable gas removal and carbon capture by using the new-generation supersonic separation technology to mitigate climate change.",No methods found.
2024,https://openalex.org/W4391243967,Engineering,Reviews and syntheses: Remotely sensed optical time series for monitoring vegetation productivity,"Abstract. Vegetation productivity is a critical indicator of global ecosystem health and is impacted by human activities and climate change. A wide range of optical sensing platforms, from ground-based to airborne and satellite, provide spatially continuous information on terrestrial vegetation status and functioning. As optical Earth observation (EO) data are usually routinely acquired, vegetation can be monitored repeatedly over time, reflecting seasonal vegetation patterns and trends in vegetation productivity metrics. Such metrics include gross primary productivity, net primary productivity, biomass, or yield. To summarize current knowledge, in this paper we systematically reviewed time series (TS) literature for assessing state-of-the-art vegetation productivity monitoring approaches for different ecosystems based on optical remote sensing (RS) data. As the integration of solar-induced fluorescence (SIF) data in vegetation productivity processing chains has emerged as a promising source, we also include this relatively recent sensor modality. We define three methodological categories to derive productivity metrics from remotely sensed TS of vegetation indices or quantitative traits: (i) trend analysis and anomaly detection, (ii) land surface phenology, and (iii) integration and assimilation of TS-derived metrics into statistical and process-based dynamic vegetation models (DVMs). Although the majority of used TS data streams originate from data acquired from satellite platforms, TS data from aircraft and unoccupied aerial vehicles have found their way into productivity monitoring studies. To facilitate processing, we provide a list of common toolboxes for inferring productivity metrics and information from TS data. We further discuss validation strategies of the RS data derived productivity metrics: (1) using in situ measured data, such as yield; (2) sensor networks of distinct sensors, including spectroradiometers, flux towers, or phenological cameras; and (3) inter-comparison of different productivity metrics. Finally, we address current challenges and propose a conceptual framework for productivity metrics derivation, including fully integrated DVMs and radiative transfer models here labelled as “Digital Twin”. This novel framework meets the requirements of multiple ecosystems and enables both an improved understanding of vegetation temporal dynamics in response to climate and environmental drivers and enhances the accuracy of vegetation productivity monitoring.","<method>trend analysis and anomaly detection</method>, <method>land surface phenology</method>, <method>integration and assimilation of time series-derived metrics into statistical and process-based dynamic vegetation models (DVMs)</method>, <method>fully integrated dynamic vegetation models (DVMs) and radiative transfer models (“Digital Twin” framework)</method>"
2024,https://openalex.org/W4391404217,Engineering,Wildfire spreading prediction using multimodal data and deep neural network approach,"Predicting wildfire spread behavior is an extremely important task for many countries. On a small scale, it is possible to ensure constant monitoring of the natural landscape through ground means. However, on the scale of large countries, this becomes practically impossible due to remote and vast forest territories. The most promising source of data in this case that can provide global monitoring is remote sensing data. Currently, the main challenge is the development of an effective pipeline that combines geospatial data collection and the application of advanced machine learning algorithms. Most approaches focus on short-term fire spreading prediction and utilize data from unmanned aerial vehicles (UAVs) for this purpose. In this study, we address the challenge of predicting fire spread on a large scale and consider a forecasting horizon ranging from 1 to 5 days. We train a neural network model based on the MA-Net architecture to predict wildfire spread based on environmental and climate data, taking into account spatial distribution features. Estimating the importance of features is another critical issue in fire behavior prediction, so we analyze their contribution to the model's results. According to the experimental results, the most significant features are wind direction and land cover parameters. The F1-score for the predicted burned area varies from 0.64 to 0.68 depending on the day of prediction (from 1 to 5 days). The study was conducted in northern Russian regions and shows promise for further transfer and adaptation to other regions. This geospatial data-based artificial intelligence (AI) approach can be beneficial for supporting emergency systems and facilitating rapid decision-making.",<method>neural network model based on the MA-Net architecture</method>
2024,https://openalex.org/W4391429082,Engineering,Simulation-based multi-objective genetic optimization for promoting energy efficiency and thermal comfort in existing buildings of hot climate,"This study conducts a detailed analysis to improve to enhance the energy performance of residential buildings in UAE through various retrofit measures. The applied methodology involved developing a calibrated building energy model for a two-story residential building, followed by a parametric analysis of six design variables, including wall and roof insulation, glazing, infiltration rate, window shading, and setpoint and setback temperatures to evaluate their impact on annual energy consumption. Additionally, a sensitivity analysis was conducted to assess the importance of the investigated design variables on building energy use. An optimization approach using the non-dominated sorting genetic algorithm (NSGA-II) was then implemented to optimize energy consumption while minimizing discomfort conditions. The key findings from the parametric simulations show significant energy savings: a 38.8 % reduction from improved wall insulation (achieving a U-value of 0.14 W/m2K), a 2.3 % decrease with better roof insulation, a 9.8 % saving from using triple clear glass glazing, a 9.6 % reduction by lowering the infiltration rate to 2.5 m³/h.m2, 7.5 % savings from window shading, and a 25.7 % decrease by optimizing cooling setpoints. A sensitivity analysis highlighted the dominant impact of wall insulation and cooling setpoint temperatures on energy usage. Followed by the cooling setpoint temperature. The subsequent NSGA-II optimization yielded 106 Pareto optimal solutions from 1897 iterations, offering a balance between reducing energy consumption (10,942 to 20,250 kWh/year, averaging 60 % savings) and minimizing discomfort hours (296–1230 h). These results provide actionable insights for stakeholders in the retrofitting process, emphasizing the significant energy-saving potential of specific retrofit measures.",<method>non-dominated sorting genetic algorithm (NSGA-II)</method>
2024,https://openalex.org/W4391665000,Engineering,A comprehensive review of critical analysis of biodegradable waste PCM for thermal energy storage systems using machine learning and deep learning to predict dynamic behavior,"This article explores the use of phase change materials (PCMs) derived from waste, in energy storage systems. It emphasizes the potential of these PCMs in addressing concerns related to fossil fuel usage and environmental impact. This article also highlights the aspects of these PCMs including reduced reliance on renewable resources minimized greenhouse gas emissions and waste reduction. The study also discusses approaches such as integrating nanotechnology to enhance thermal conductivity and utilizing machine learning and deep learning techniques for predicting dynamic behavior. The article provides an overall view of research on biodegradable waste-based PCMs and how they can play a promising role in achieving energy-efficient and sustainable thermal storage systems. However, specific conclusions drawn from the presented results are not explicitly outlined, leaving room, for investigation and exploration in this evolving field. Artificial neural network (ANN) predictive models for thermal energy storage devices perform differently. With a 4% adjusted mean absolute error, the Gaussian radial basis function kernel Support Vector Regression (SVR) model captured heat-related charging and discharging issues. The ANN model predicted finned tube heat and heat flux better than the numerical model. SVM models outperformed ANN and ANFIS in some datasets. Material property predictions favored gradient boosting, but Linear Regression and SVR models performed better, emphasizing application- and dataset-specific model selection. These predictive models provide insights into the complex thermal performance of building structures, aiding in the design and operation of energy-efficient systems. Biodegradable waste-based PCMs' sustainability includes carbon footprint, waste reduction, biodegradability, and circular economy alignment. Nanotechnology, machine learning, and deep learning improve thermal conductivity and prediction. Circular economy principles include waste reduction and carbon footprint reduction. Specific results-based conclusions are not stated. Presenting a comprehensive overview of current research highlights biodegradable waste-based PCMs' potential for energy-efficient and sustainable thermal storage systems.","<method>machine learning</method>, <method>deep learning</method>, <method>Artificial neural network (ANN)</method>, <method>Gaussian radial basis function kernel Support Vector Regression (SVR)</method>, <method>ANN model</method>, <method>numerical model</method>, <method>SVM models</method>, <method>ANFIS</method>, <method>gradient boosting</method>, <method>Linear Regression</method>"
2024,https://openalex.org/W4399369266,Engineering,Enhancing Skin Cancer Diagnosis Using Swin Transformer with Hybrid Shifted Window-Based Multi-head Self-attention and SwiGLU-Based MLP,"Abstract Skin cancer is one of the most frequently occurring cancers worldwide, and early detection is crucial for effective treatment. Dermatologists often face challenges such as heavy data demands, potential human errors, and strict time limits, which can negatively affect diagnostic outcomes. Deep learning–based diagnostic systems offer quick, accurate testing and enhanced research capabilities, providing significant support to dermatologists. In this study, we enhanced the Swin Transformer architecture by implementing the hybrid shifted window-based multi-head self-attention (HSW-MSA) in place of the conventional shifted window-based multi-head self-attention (SW-MSA). This adjustment enables the model to more efficiently process areas of skin cancer overlap, capture finer details, and manage long-range dependencies, while maintaining memory usage and computational efficiency during training. Additionally, the study replaces the standard multi-layer perceptron (MLP) in the Swin Transformer with a SwiGLU-based MLP, an upgraded version of the gated linear unit (GLU) module, to achieve higher accuracy, faster training speeds, and better parameter efficiency. The modified Swin model-base was evaluated using the publicly accessible ISIC 2019 skin dataset with eight classes and was compared against popular convolutional neural networks (CNNs) and cutting-edge vision transformer (ViT) models. In an exhaustive assessment on the unseen test dataset, the proposed Swin-Base model demonstrated exceptional performance, achieving an accuracy of 89.36%, a recall of 85.13%, a precision of 88.22%, and an F1-score of 86.65%, surpassing all previously reported research and deep learning models documented in the literature.","<method>Deep learning–based diagnostic systems</method>, <method>Swin Transformer architecture</method>, <method>hybrid shifted window-based multi-head self-attention (HSW-MSA)</method>, <method>shifted window-based multi-head self-attention (SW-MSA)</method>, <method>multi-layer perceptron (MLP)</method>, <method>SwiGLU-based MLP</method>, <method>gated linear unit (GLU) module</method>, <method>convolutional neural networks (CNNs)</method>, <method>vision transformer (ViT) models</method>"
2024,https://openalex.org/W4400227316,Engineering,FPSO/LNG hawser system lifetime assessment by Gaidai multivariate risk assessment method,"Abstract Floating Production Storage and Offloading (FPSO) unit being an offshore vessel, storing and producing crude oil, prior to crude oil being transported by accompanying shuttle tanker. Critical mooring/hawser strains during offloading operation have to be accurately predicted, in order to maintain operational safety and reliability. During certain types of offloading, excessive hawser tensions may occur, causing operational risks. Current study examines FPSO vessel’s dynamic reactions to hydrodynamic wave-induced loads, given realistic in situ environmental conditions, utilizing the AQWA software package. Current study advocates novel multi-dimensional spatiotemporal risks assessment approach, that is particularly well suited for large dataset analysis, based on numerical simulations (or measurements). Advocated multivariate reliability methodology may be useful for a variety of marine and offshore systems that must endure severe environmental stressors during their intended operational lifespan. Methodology, presented in this study provides advanced capability to efficiently, yet accurately evaluate dynamic system failure, hazard and damage risks, given representative dynamic record of multidimensional system’s inter-correlated critical components. Gaidai risk assessment method being novel dynamic multidimensional system’s lifetime assessment methodology. In order to validate and benchmark Gaidai risk assessment method, in this study it was applied to FPSO and potentially LNG (i.e., Liquid Natural Gas) vessels dynamics. Major advantage of the advocated approach is that there are no existing alternative risk assessment methods, able to tackle unlimited number of system’s dimensions. Accurate multi-dimensional risk assessment had been carried out, based on numerically simulated data, partially verified by available laboratory experiments. Confidence intervals had been given for predicted dynamic high-dimensional system risk levels.",<method>Gaidai risk assessment method</method>
2024,https://openalex.org/W4401203858,Engineering,"Generative AI as a transformative force for innovation: a review of opportunities, applications and challenges","Purpose This study examines the existing literature on generative artificial intelligence (Gen AI) and its impact across many sectors. This analysis explores the potential, applications, and challenges of Gen AI in driving innovation and creativity and generating ideas. Design/methodology/approach The study adopts a comprehensive literature review approach, carefully assessing current scientific articles on Gen AI published from 2022 to 2024. The analysis examines trends and insights derived from research. Findings The review indicates that Gen AI has significant potential to augment human creativity and innovation processes as a collaborative partner. However, it is imperative to prioritize responsible development and ethical frameworks in order to effectively tackle biases, privacy concerns, and other challenges. Gen AI is significantly transforming business models, processes, and value propositions in several industries, but with varying degrees of effect. Findings indicate also that despite the theory-driven approach to investigating Gen AI's creative and innovative potential, cutting-edge applications research prioritizes examining the possibilities of Gen AI models. Research limitations/implications Although this review offers a picture of great possibilities, it concurrently underlines the necessity for a deep knowledge of Gen AI nuances to fully harness its capabilities. The findings indicate that continuous research and exploration efforts are required to address the challenges of Gen AI and assure its responsible and ethical implementation. Therefore, more study is needed on enhancing human-AI collaboration and defining ethical norms for varied circumstances. Originality/value This study presents a relevant analysis of Gen AI's transformational potential as an innovation catalyst. It emphasizes major potential, applications across industries, and ethical issues for responsible integration.",No methods found.
2024,https://openalex.org/W4390483953,Engineering,THE ROLE OF BIG DATA IN BUSINESS STRATEGY: A CRITICAL REVIEW,"This paper critically examines the role of Big Data in shaping contemporary business strategies, highlighting its transformative impact across various sectors. The study delves into the multifaceted nature of Big Data, exploring its integration into business models, the challenges it presents, and the strategic advantages it offers. The research is grounded in a comprehensive literature review, focusing on peer-reviewed articles, conference papers, and academic book chapters from 2010 onwards, ensuring relevance and currency in the rapidly evolving field of Big Data. The aim of the paper is to provide a nuanced understanding of how Big Data influences business strategies, identifying existing research gaps and setting a clear direction for future scholarly inquiry. The scope encompasses the assessment of Big Data's integration in business strategies, its impact on decision-making processes, ethical implications, and the effectiveness of current technologies and methodologies. Key findings reveal that Big Data significantly enhances decision-making capabilities, customer experience, and competitive advantage. It also highlights the necessity for a strategic blueprint for Big Data integration, addressing cultural, governance, strategic, and technological aspects. The study concludes with recommendations for businesses to adopt a data-driven approach, integrating Big Data analytics into their strategic planning to enhance decision-making, customer engagement, and ethical compliance. In conclusion, the paper underscores the need for businesses to develop comprehensive strategies for Big Data integration, focusing on fostering a data-driven culture and ensuring data security and privacy. This strategic approach will enable businesses to leverage the full potential of Big Data, driving innovation and maintaining a competitive edge in the digital landscape.&#x0D; Keywords: Big Data, Business Strategy, Decision-Making, Data-Driven Culture, Ethical Implications.",No methods found.
2024,https://openalex.org/W4391304535,Engineering,Sustainable electric discharge machining using alumina-mixed deionized water as dielectric: Process modelling by artificial neural networks underpinning net-zero from industry,"The requirement for materials possessing both high strength and low density has garnered significant attention from industries and researchers in recent times. Among these materials, aluminum 6061 (Al6061) exhibits the desired properties. However, due to its diverse machining capabilities, powder-mixed electric discharge machining (PMEDM) has emerged as a viable option for cutting such materials. This method has been criticized for its high energy consumption and limited cutting efficiency. Furthermore, conventional dielectric (kerosene) employed in EDM has drastic environmental and operator's health concerns. To address the abovementioned issues, deionized water has been employed in this study which enhances the reusability of resources and minimizes the cost of the dielectric. Herein, to make the process sustainable, and to keep the environment free from hazardous fumes, generated during the machining process, deionized water has been used. In addition to that, to uplift the machining responses, alumina (Al2O3) nano-powder has been engaged. To conduct the study, response surface methodology (RSM) was employed. This investigation aimed to analyze the impact on the material removal rate (MRR), surface roughness (SR), and specific energy consumption (SEC) by using microscopy analysis, scanning electron microscopy (SEM), 3D surfaces profilometry, energy dispersive x-ray (EDX) analysis and after that, the machining responses are modelled using the artificial neural networks (ANN) technique. It was observed that by utilizing non-dominated sorting genetic algorithms (NSGA-II) an improvement of 87.42 % in MRR, 3.4 % better surface finish and 0.7 % better SEC have been obtained. Notably, CO2 emissions were found to be 94.27 % lower by using the deionized water as dielectric compared to those produced by kerosene oil.","<method>response surface methodology (RSM)</method>, <method>artificial neural networks (ANN)</method>, <method>non-dominated sorting genetic algorithms (NSGA-II)</method>"
2024,https://openalex.org/W4391605901,Engineering,Three‐in‐One Strategy Enables Single‐Component Organic Solar Cells with Record Efficiency and High Stability,"Abstract Single‐component organic solar cells (SCOSCs) with covalently bonding donor and acceptor are becoming increasingly attractive because of their superior stability over traditional multicomponent blend organic solar cells (OSCs). Nevertheless, the efficiency of SCOSCs is far behind the state‐of‐the‐art multicomponent OSCs. Herein, by combination of the advantages of three‐component and single‐component devices, this work reports an innovative three‐in‐one strategy to boost the performance of SCOSCs. In this three‐in‐one strategy, three independent components (PM6, D18, and PYIT) are covalently linked together to create a new single‐component active layer based on ternary conjugated block copolymer (TCBC) PM6‐D18 ‐b‐ PYIT by a facile polymerization. Precisely manipulating the component ratios in the polymer chains of PM6‐D18 ‐b‐ PYIT is able to broaden light utilization, promote charge dynamics, optimize, and stabilize film morphology, contributing to the simultaneously enhanced efficiency and stability of the SCOSCs. Ultimately, the PM6‐D18 ‐b‐ PYIT‐based device exhibits a power conversion efficiency (PCE) of 14.89%, which is the highest efficiency of the reported SCOSCs. Thanks to the aggregation restriction of each component and chain entanglement in the three‐in‐one system, the PM6‐D18 ‐b‐ PYIT‐based SCOSC displays significantly higher stability than the corresponding two‐component (PM6‐D18:PYIT) and three‐component (PM6:D18:PYIT). These results demonstrate that the three‐in‐one strategy is facile and promising for developing SCOSCs with superior efficiency and stability.",No methods found.
2024,https://openalex.org/W4391708122,Engineering,Application of machine learning approaches in supporting irrigation decision making: A review,"Irrigation decision-making has evolved from solely depending on farmers' decisions taken based on the visual analysis of field conditions to making decisions based on crop water need predictions generated using machine learning (ML) techniques. This paper reviews ML related articles to discuss how ML has been used to enhance irrigation decision making. We reviewed 16 studies that used ML approaches for irrigation scheduling prediction and decision-making focusing on the input features, algorithms used and their applicability in real world conditions. ML performances in terms of accuracy, water conservation compared to fixed or threshold-based methods are discussed along with modeling performances. Informed by the 16 research studies, we assessed constraints to the adoption of ML in irrigation decision making at field scale, which include limited data availability coupled with data sharing constraints, and a lack of uncertainty quantification as well as the need for physics informed ML based irrigation scheduling models. To address these limitations, we discussed approaches in future research such as integrating process-based models with ML, incorporating expert knowledge into the modeling procedure, and making data and tools Findable, Accessible, Interoperable, and Reusable (FAIR). These approaches will improve ML modeling outcomes and boost the availability of farm-related data and tools for FAIRer data-driven applications of irrigation modeling.","<method>machine learning (ML) techniques</method>, <method>ML approaches</method>, <method>process-based models integrated with ML</method>, <method>physics informed ML based irrigation scheduling models</method>"
2024,https://openalex.org/W4393144919,Engineering,A Novel Fuzzy Neural Network Architecture Search Framework for Defect Recognition With Uncertainties,"Defect recognition is an important task in intelligent manufacturing. Due to the subjectivity of human annotation, the collected defect data usually contains a lot of noise and unpredictable uncertainties, which have a great negative influence on defect recognition. It is a significant challenge to discover an effective defect recognition model with satisfactory uncertainty processing ability. A natural way is to automatically search for an efficient deep model, which can be realized by neural architecture search (NAS). To achieve this, we propose an efficient fuzzy NAS framework for defect recognition, where the searched architecture can effectively handle uncertain information from the given datasets. Specifically, we first design a fuzzy search space and the related encoding strategy for fuzzy NAS. Then, we propose a comparator-based evolutionary search approach, where an online end-to-end comparator is learned to directly determine the selection of candidate architectures from the evolutionary population. The comparator works in an end-to-end way and it transforms the complex ranking problem of evaluating architectures into a simple classification task, which overcomes the rank disorder issue suffered from traditional performance predictors. A series of experimental results demonstrate that the architecture with fewer #Params (1.22 M) search by fuzzy neural architecture search framework for defect recognition method achieves higher accuracy (92.26%) compared to the state-of-the-art results (i.e., DARTS-PV) on the ELPV dataset, as well as competitive results (accuracy = 76.4%, #Params = 1.04 M) on the CODEBRIM dataset. Experimental results show the effectiveness and efficiency of our proposed method in handling uncertain problems.","<method>neural architecture search (NAS)</method>, <method>fuzzy neural architecture search (fuzzy NAS)</method>, <method>comparator-based evolutionary search approach</method>"
2024,https://openalex.org/W4395479913,Engineering,Machinability investigation of natural fibers reinforced polymer matrix composite under drilling: Leveraging machine learning in bioengineering applications,"The growing demand for fiber-reinforced polymer (FRP) in industrial applications has prompted the exploration of natural fiber-based composites as a viable alternative to synthetic fibers. Using jute–rattan fiber-reinforced composite offers the potential for environmentally sustainable waste material decomposition and cost reduction compared to conventional fiber materials. This article focuses on the impact of different machining constraints on surface roughness and delamination during the drilling process of the jute–rattan FRP composite. Inspired by this unexplored research area, this article emphasizes the influence of various machining constraints on surface roughness and delamination in drilling jute–rattan FRP composite. Response surface methodology designs the experiment using drill bit material, spindle speed, and feed rate as input variables to measure surface roughness and delamination factors. The technique of order of preference by similarity to the ideal solution method is used to optimize the machining parameters, and for predicting surface roughness and delamination, two machine learning-based models named random forest (RF) and support vector machine (SVM) are utilized. To evaluate the accuracy of the predicted values, the correlation coefficient (R2), mean absolute percentage error, and mean squared error were used. RF performed better in comparison with SVM, with a higher value of R2 for both testing and training datasets, which is 0.997, 0.981, and 0.985 for surface roughness, entry delamination, and exit delamination, respectively. Hence, this study presents an innovative methodology for predicting surface roughness and delamination through machine learning techniques.","<method>random forest (RF)</method>, <method>support vector machine (SVM)</method>"
2024,https://openalex.org/W4396634174,Engineering,AISClean: AIS data-driven vessel trajectory reconstruction under uncertain conditions,"In maritime transportation, intelligent vessel surveillance has become increasingly prevalent and widespread by collecting and analyzing high massive spatial data from automatic identification system (AIS). The state-of-the-art AIS devices contain various functionalities, such as position transmission, tracking navigation, etc. Widely equipped shipboard AIS devices provide a large amount of real-time and historical vessel trajectory data for maritime management. However, the original AIS data often suffers from unwanted noise (i.e., poorly tracked timestamped points for vessel trajectories) and missing (i.e., no data is received or transmitted for a long term) data during signal acquisition, transmission, and analog-to-digital conversion. This degradation in data quality poses significant risks, including potential miscalculations in vessel collision avoidance systems, inaccuracies in emission calculations, and challenges in port management. In this work, a data-driven vessel trajectory reconstruction framework considering historical features is proposed to enhance the reliability of vessel trajectory. Specifically, a series of statistical methods are proposed to identify noisy data and missing data. Then, a model combining Geohash and dynamic time warping algorithms is developed to restore the trajectories degraded by random noise and missing data in vessel trajectories. Comparative experiments with baseline methods on multiple datasets verify the effectiveness of the proposed data-driven model.",<method>dynamic time warping</method>
2024,https://openalex.org/W4399568894,Engineering,Artificial intelligence capability and organizational performance: unraveling the mediating mechanisms of decision-making processes,"Purpose This study investigates the profound impact of artificial intelligence (AI) capabilities on decision-making processes and organizational performance, addressing a crucial gap in the literature by exploring the mediating role of decision-making speed and quality. Design/methodology/approach Drawing upon resource-based theory and prior research, this study constructs a comprehensive model and hypotheses to illuminate the influence of AI capabilities within organizations on decision-making speed, decision quality, and, ultimately, organizational performance. A dataset comprising 230 responses from diverse organizations forms the basis of the analysis, with the study employing a partial least squares structural equation model (PLS-SEM) for robust data examination. Findings The results demonstrate the pivotal role of AI capabilities in shaping organizational decision-making processes and performance. AI capability significantly and positively affects decision-making speed, decision quality, and overall organizational performance. Notably, decision-making speed is a critical factor contributing significantly to enhanced organizational performance. The study further uncovered partial mediation effects, suggesting that decision-making processes partially mediate the relationship between AI capabilities and organizational performance through decision-making speed. Originality/value This study contributes to the existing body of literature by providing empirical evidence of the multifaceted impact of AI capabilities on organizational decision-making and performance. Elucidating the mediating role of decision-making processes advances our understanding of the complex mechanisms through which AI capabilities drive organizational success.",<method>partial least squares structural equation model (PLS-SEM)</method>
2024,https://openalex.org/W4400234709,Engineering,Eco-friendly mix design of slag-ash-based geopolymer concrete using explainable deep learning,"Geopolymer concrete is a sustainable and eco-friendly substitute for traditional OPC (Ordinary Portland Cement) based concrete, as it reduces greenhouse gas emissions. With various supplementary cementitious materials, the compressive strength of geopolymer concrete should be accurately predicted. Recent studies have applied deep learning techniques to predict the compressive strength of geopolymer concrete yet its hidden decision-making criteria diminish the end-users' trust in predictions. To bridge this gap, the authors first developed three deep learning models: an artificial neural network (ANN), a deep neural network (DNN), and a 1D convolution neural network (CNN) to predict the compressive strength of slag ash-based geopolymer concrete. The performance indices for accuracy revealed that the DNN model outperforms the other two models. Subsequently, Shapley additive explanations (SHAP) were used to explain the best-performed deep learning model, DNN, and its compressive strength predictions. SHAP exhibited how the importance of each feature and its relationship contributes to the compressive strength prediction of the DNN model. Finally, the authors developed a novel DNN-based open-source software interface to predict the mix design proportions for a given target compressive strength (using inverse modeling technique) for slag ash-based geopolymer concrete. Additionally, the software calculates the Global Warming Potential (kg CO2 equivalent) for each mix design to select the mix designs with low greenhouse emissions.","<method>artificial neural network (ANN)</method>, <method>deep neural network (DNN)</method>, <method>1D convolution neural network (CNN)</method>, <method>Shapley additive explanations (SHAP)</method>"
2024,https://openalex.org/W4400496339,Engineering,Metal–Organic Framework Stability in Water and Harsh Environments from Data-Driven Models Trained on the Diverse WS24 Data Set,"Metal-organic frameworks (MOFs) are porous materials with applications in gas separations and catalysis, but a lack of water stability often limits their practical use given the ubiquity of water. Consequently, it is useful to predict whether a MOF is water-stable before investing time and resources into synthesis. Existing heuristics for designing water-stable MOFs lack generality and limit the diversity of explored chemistry due to narrowly defined criteria. Machine learning (ML) models offer the promise to improve the generality of predictions but require data. In an improvement on previous efforts, we enlarge the available training data for MOF water stability prediction by over 400%, adding 911 MOFs with water stability labels assigned through semiautomated manuscript analysis to curate the new data set WS24. The additional data are shown to improve ML model performance (test ROC-AUC > 0.8) over diverse chemistry for the prediction of both water stability and stability in harsher acidic conditions. We illustrate how the expanded data set and models can be used with a previously developed activation stability model in combination with genetic algorithms to quickly screen ∼10,000 MOFs from a space of hundreds of thousands for candidates with multivariate stability (upon activation, in water, and in acid). We uncover metal- and geometry-specific design rules for robust MOFs. The data set and ML models developed in this work, which we disseminate through an easy-to-use web interface, are expected to contribute toward the accelerated discovery of novel, water-stable MOFs for applications such as direct air gas capture and water treatment.","<method>Machine learning (ML) models</method>, <method>genetic algorithms</method>"
2024,https://openalex.org/W4401015316,Engineering,Deep learning approaches for visual faults diagnosis of photovoltaic systems: State-of-the-Art review,"PV systems are prone to external environmental conditions that affect PV system operations. Visual inspection of the impacts of faults on PV system is considered a better practice rather than onsite fault detection mechanisms. Faults such as hotspot, dark area, cracks, glass break, wavy lines, snail tracks, corrosion, discoloration, junction box failure and delamination faults have different visual symptoms. EL technology, infrared thermography, and photoluminescence approaches are used to extract and visualize the impact of faults on PV modules. DL based algorithms such as, CNN, ANN, RNN, AE, DBN, TL and hybrid algorithms have shown promising results in domain of visual PV fault detection. This article critically overviews working mechanism of DL algorithms in terms of their limitations, complexity, interpretability, training dataset requirements and capability to work with another DL algorithms. This research article also reviews, critically analyzes, and systematically presents different clustering algorithms based on their clustering mechanism, distance metrics, convergence criteria. Additionally, their performance is also evaluated in terms of DI, CHI, DBI, S-score, and homogeneity. Moreover, this research work explicitly identifies and explains the limitations and contributions of recent and older techniques employed for features extraction, data preprocessing, and decision making by performing SWOT analysis. This research work also recommends future research directions for industry and academia.","<method>CNN</method>, <method>ANN</method>, <method>RNN</method>, <method>AE</method>, <method>DBN</method>, <method>TL</method>, <method>hybrid algorithms</method>, <method>clustering algorithms</method>"
2024,https://openalex.org/W4401075136,Engineering,Metaheuristic optimization algorithms-based prediction modeling for titanium dioxide-Assisted photocatalytic degradation of air contaminants,"Airborne contaminants pose significant environmental and health challenges. Titanium dioxide (TiO2) has emerged as a leading photocatalyst in the degradation of air contaminants compared to other photocatalysts due to its inherent inertness, cost-effectiveness, and photostability. To assess its effectiveness, laboratory examinations are frequently employed to measure the photocatalytic degradation rate of TiO2. However, this approach involves time-consuming requirements, labor-intensive tasks, and high costs. In literature, ensemble or standalone models are commonly used for assessing the performance of TiO2 photocatalytic degradation of water and air contaminants. Nonetheless, the application of metaheuristic hybrid models has the potential to be more effective in predictive accuracy and efficiency. Accordingly, this research utilized hybrid machine learning (ML) algorithms to estimate the photo-degradation rate constants of organic air pollutants using TiO2 nanoparticles and exposure to ultraviolet light. Six metaheuristics optimization algorithms, namely, nuclear reaction optimization (NRO), differential evolution algorithm (DEA), human felicity algorithm (HFA), lightning search algorithm (LSA), Harris hawks algorithm (HHA), and tunicate swarm algorithm (TSA) were combined with random forest (RF) technique to establish the hybrid models. A database of 200 data points was acquired from experimental studies for model training and testing. Furthermore, multiple statistical indicators and 10-fold cross-validation were employed to examine the established hybrid model's accuracy and robustness. The TSA-RF model demonstrated superior prediction accuracy among the six suggested models, achieving an impressive correlation (R) of 0.90 and a lower root mean square error (RMSE) of 0.25. In contrast, the HFA-RF, HHA-RF, and NRO-RF models exhibited a slightly lower R-value of 0.88, with RMSE scores of 0.32. The DEA-RF and LSA-RF models, while effective, showed a marginally lower R-value of 0.85, with RMSE values of 0.45 and 0.44, respectively. Moreover, the SHapley Additive exPlanation (SHAP) results indicated that the degradation rates of air contaminants through photocatalysis were most notably influenced by factors such as the reactor sizes, photocatalyst dosage, humidity, and intensity.","<method>random forest (RF)</method>, <method>nuclear reaction optimization (NRO)</method>, <method>differential evolution algorithm (DEA)</method>, <method>human felicity algorithm (HFA)</method>, <method>lightning search algorithm (LSA)</method>, <method>Harris hawks algorithm (HHA)</method>, <method>tunicate swarm algorithm (TSA)</method>, <method>SHapley Additive exPlanation (SHAP)</method>"
2024,https://openalex.org/W4403158403,Engineering,Artificial intelligence alphafold model for molecular biology and drug discovery: a machine-learning-driven informatics investigation,"AlphaFold model has reshaped biological research. However, vast unstructured data in the entire AlphaFold field requires further analysis to fully understand the current research landscape and guide future exploration. Thus, this scientometric analysis aimed to identify critical research clusters, track emerging trends, and highlight underexplored areas in this field by utilizing machine-learning-driven informatics methods. Quantitative statistical analysis reveals that the AlphaFold field is enjoying an astonishing development trend (Annual Growth Rate = 180.13%) and global collaboration (International Co-authorship = 33.33%). Unsupervised clustering algorithm, time series tracking, and global impact assessment point out that Cluster 3 (Artificial Intelligence-Powered Advancements in AlphaFold for Structural Biology) has the greatest influence (Average Citation = 48.36 ± 184.98). Additionally, regression curve and hotspot burst analysis highlight ""structure prediction"" (s = 12.40, R2 = 0.9480, p = 0.0051), ""artificial intelligence"" (s = 5.00, R2 = 0.8096, p = 0.0375), ""drug discovery"" (s = 1.90, R2 = 0.7987, p = 0.0409), and ""molecular dynamics"" (s = 2.40, R2 = 0.8000, p = 0.0405) as core hotspots driving the research frontier. More importantly, the Walktrap algorithm further reveals that ""structure prediction, artificial intelligence, molecular dynamics"" (Relevance Percentage[RP] = 100%, Development Percentage[DP] = 25.0%), ""sars-cov-2, covid-19, vaccine design"" (RP = 97.8%, DP = 37.5%), and ""homology modeling, virtual screening, membrane protein"" (RP = 89.9%, DP = 26.1%) are closely intertwined with the AlphaFold model but remain underexplored, which implies a broad exploration space. In conclusion, through the machine-learning-driven informatics methods, this scientometric analysis offers an objective and comprehensive overview of global AlphaFold research, identifying critical research clusters and hotspots while prospectively pointing out underexplored critical areas.","<method>unsupervised clustering algorithm</method>, <method>Walktrap algorithm</method>"
2024,https://openalex.org/W4390663168,Engineering,Review of Prediction of Stress Corrosion Cracking in Gas Pipelines Using Machine Learning,"Pipeline integrity and safety depend on the detection and prediction of stress corrosion cracking (SCC) and other defects. In oil and gas pipeline systems, a variety of corrosion-monitoring techniques are used. The observed data exhibit characteristics of nonlinearity, multidimensionality, and noise. Hence, data-driven modeling techniques have been widely utilized. To accomplish intelligent corrosion prediction and enhance corrosion control, machine learning (ML)-based approaches have been developed. Some published papers related to SCC have discussed ML techniques and their applications, but none of the works has shown the real ability of ML to detect or predict SCC in energy pipelines, though fewer researchers have tested their models to prove them under controlled environments in laboratories, which is completely different from real work environments in the field. Looking at the current research status, the authors believe that there is a need to explore the best technologies and modeling approaches and to identify clear gaps; a critical review is, therefore, required. The objective of this study is to assess the current status of machine learning’s applications in SCC detection, identify current research gaps, and indicate future directions from a scientific research and application point of view. This review will highlight the limitations and challenges of employing machine learning for SCC prediction and also discuss the importance of incorporating domain knowledge and expert inputs to enhance the accuracy and reliability of predictions. Finally, a framework is proposed to demonstrate the process of the application of ML to condition assessments of energy pipelines.","<method>machine learning (ML)-based approaches</method>, <method>machine learning (ML) techniques</method>"
2024,https://openalex.org/W4390776100,Engineering,Hybrid KNN-SVM machine learning approach for solar power forecasting,"Predictions about solar power will have a significant impact on large-scale renewable energy plants. Photovoltaic (PV) power generation forecasting is particularly sensitive to measuring the uncertainty in weather conditions. Although several conventional techniques like long short-term memory (LSTM), support vector machine (SVM), etc. are available, but due to some restrictions, their application is limited. To enhance the precision of forecasting solar power from solar farms, a hybrid machine learning model that includes blends of the K-Nearest Neighbor (KNN) machine learning technique with the SVM to increase reliability for power system operators is proposed in this investigation. The conventional LSTM technique is also implemented to compare the performance of the proposed hybrid technique. The suggested hybrid model is improved by the use of structural diversity and data diversity in KNN and SVM, respectively. For the solar power predictions, the suggested method was tested on the Jodhpur real-time series dataset obtained from the data centers of weather stations using Meteonorm. The data set includes metrics such as Hourly Average Temperature (HAT), Hourly Total Sunlight Duration (HTSD), Hourly Total Global Solar Radiation (HTGSR), and Hourly Total Photovoltaic Energy Generation (HTPEG). The collated data has been segmented into training data, validation data, and testing data. Furthermore, the proposed technique performed better when evaluated on the three performance indices, viz., accuracy, sensitivity, and specificity. Compared with the conventional LSTM technique, the hybrid technique improved the prediction with 98% accuracy.","<method>long short-term memory (LSTM)</method>, <method>support vector machine (SVM)</method>, <method>K-Nearest Neighbor (KNN)</method>, <method>hybrid machine learning model (KNN + SVM)</method>"
2024,https://openalex.org/W4390939303,Engineering,A Reliable and Robust Deep Learning Model for Effective Recyclable Waste Classification,"In response to the growing waste problem caused by industrialization and modernization, the need for an automated waste sorting and recycling system for sustainable waste management has become ever more pressing. Deep learning has made significant advancements in image classification, making it ideally suited for waste sorting applications. This application depends on the development of a suitable deep learning model capable of accurately categorizing various categories of waste. In this study, we present RWC-Net (recyclable waste classification network), a novel deep learning model designed for the classification of six distinct waste categories using the TrashNet dataset of 2,527 images of waste. The performance of our model is subjected to intensive quantitative and qualitative evaluations and is compared to various state-of-art waste classification techniques. The proposed model outperformed several state-of-the-art models by obtaining a remarkable overall accuracy rate of 95.01 percent. In addition, it receives high F1-scores for each of the six waste categories: 97.24% for cardboard, 96.18% for glass, 94% for metal, 95.73% for paper, 93.67% for plastic, and 88.55% for litter. The reliability of the model is demonstrated qualitatively through the saliency maps generated by Score-CAM (class activation mapping) model, which provide visual insights into its performance across various waste categories. These results highlight the model's accuracy and demonstrate its potential as an effective automated waste classification and management solution.","<method>Deep learning</method>, <method>RWC-Net (recyclable waste classification network)</method>, <method>Score-CAM (class activation mapping)</method>"
2024,https://openalex.org/W4391202802,Engineering,Learning optimal inter-class margin adaptively for few-shot class-incremental learning via neural collapse-based meta-learning,"Few-Shot Class-Incremental Learning (FSCIL) aims to learn new classes incrementally with a limited number of samples per class. It faces issues of forgetting previously learned classes and overfitting on few-shot classes. An efficient strategy is to learn features that are discriminative in both base and incremental sessions. Current methods improve discriminability by manually designing inter-class margins based on empirical observations, which can be suboptimal. The emerging Neural Collapse (NC) theory provides a theoretically optimal inter-class margin for classification, serving as a basis for adaptively computing the margin. Yet, it is designed for closed, balanced data, not for sequential or few-shot imbalanced data. To address this gap, we propose a Meta-learning- and NC-based FSCIL method, MetaNC-FSCIL, to compute the optimal margin adaptively and maintain it at each incremental session. Specifically, we first compute the theoretically optimal margin based on the NC theory. Then we introduce a novel loss function to ensure that the loss value is minimized precisely when the inter-class margin reaches its theoretically best. Motivated by the intuition that ""learn how to preserve the margin"" matches the meta-learning's goal of ""learn how to learn"", we embed the loss function in base-session meta-training to preserve the margin for future meta-testing sessions. Experimental results demonstrate the effectiveness of MetaNC-FSCIL, achieving superior performance on multiple datasets. The code is available at https://github.com/qihangran/metaNC-FSCIL.","<method>Few-Shot Class-Incremental Learning (FSCIL)</method>, <method>Neural Collapse (NC) theory</method>, <method>Meta-learning</method>"
2024,https://openalex.org/W4392081637,Engineering,Hybrid deep learning models for time series forecasting of solar power,"Abstract Forecasting solar power production accurately is critical for effectively planning and managing renewable energy systems. This paper introduces and investigates novel hybrid deep learning models for solar power forecasting using time series data. The research analyzes the efficacy of various models for capturing the complex patterns present in solar power data. In this study, all of the possible combinations of convolutional neural network (CNN), long short-term memory (LSTM), and transformer (TF) models are experimented. These hybrid models also compared with the single CNN, LSTM and TF models with respect to different kinds of optimizers. Three different evaluation metrics are also employed for performance analysis. Results show that the CNN–LSTM–TF hybrid model outperforms the other models, with a mean absolute error (MAE) of 0.551% when using the Nadam optimizer. However, the TF–LSTM model has relatively low performance, with an MAE of 16.17%, highlighting the difficulties in making reliable predictions of solar power. This result provides valuable insights for optimizing and planning renewable energy systems, highlighting the significance of selecting appropriate models and optimizers for accurate solar power forecasting. This is the first time such a comprehensive work presented that also involves transformer networks in hybrid models for solar power forecasting.","<method>convolutional neural network (CNN)</method>, <method>long short-term memory (LSTM)</method>, <method>transformer (TF)</method>, <method>CNN–LSTM–TF hybrid model</method>, <method>TF–LSTM model</method>"
2024,https://openalex.org/W4392499245,Engineering,Exploring the role of skin temperature in thermal sensation and thermal comfort: A comprehensive review,"The role of skin temperature as a determinant of human thermal sensation and comfort has gained increasing recognition, prompting a need for a systematic review. This review examines the relationship between skin temperature and thermal sensation, synthesizing insights from 172 studies published since 2000. It uniquely focuses on the indispensable roles of local and mean skin temperatures, a perspective not comprehensively explored in previous literature. The review reveals that the most common measurement points for skin temperature are the face and hands, attributed to their higher thermal sensitivity and the practical ease of measurement. It establishes a clear linear relationship between mean skin temperature and user thermal sensation, though affected by the choice of measurement locations and number of points. A notable finding is the varying impact of local skin temperature on overall thermal sensation in changing environments, with local heating less influential than cooling. The review also uncovers significant demographic variations in thermal sensation, strongly influenced by differing skin temperatures across age groups, genders, and climatic regions. For example, elderly populations exhibit a decreased temperature sensitivity, especially towards warmth. Gender differences are also significant, with females experiencing higher skin temperatures in warmer environments and lower in colder ones. Machine learning (ML)-based methods, especially classification tree-based and support vector machine (SVM) techniques, dominate in predicting thermal sensation and comfort, leveraging skin temperature data. While ML methods are prevalent, statistical regression-based approaches offer valuable empirical insights. Thermo-physiological model-based methods provide reliable results by incorporating detailed skin temperature dynamics. The review identifies a gap in understanding how gender, age, and regional differences influence thermal comfort in diverse environments. The study recommends conducting more nuanced experiments to dissect the impact of these factors and proposes the integration of individual demographic variables into ML models to personalize thermal comfort predictions.","<method>classification tree-based</method>, <method>support vector machine (SVM)</method>, <method>statistical regression-based approaches</method>, <method>thermo-physiological model-based methods</method>"
2024,https://openalex.org/W4392509422,Engineering,A novel framework for predicting active flow control by combining deep reinforcement learning and masked deep neural network,"Active flow control (AFC) through deep reinforcement learning (DRL) is computationally demanding. To address this, a masked deep neural network (MDNN), aiming to replace the computational fluid dynamics (CFD) environment, is developed to predict unsteady flow fields under the influence of arbitrary object motion. Then, a novel DRL-MDNN framework that combines the MDNN-based environment with the DRL algorithm is proposed. To validate the reliability of the framework, a blind test in a pulsating baffle system is designed. Vibration damping is considered to be the objective, and a traditional DRL-CFD framework is constructed for comparison. After training, a spatiotemporal evolution of 200 time steps under the influence of arbitrary object motion is predicted by the MDNN. The details of the flow field are compared with the CFD results, and a relative error within 5% is achieved, which satisfies the accuracy of serving as an interactive environment for DRL algorithms. The DRL-MDNN and traditional DRL-CFD frameworks are then applied to the pulsating baffle system to find the optimal control strategy. The results indicate that both frameworks achieve similar control performance, reducing vibration by 90%. Considering the resources expended in establishing the database, the computational resource consumption of the DRL-MDNN framework is reduced by 95%, and the interactive response time during each episode is decreased by 98.84% compared to the traditional DRL-CFD framework.","<method>deep reinforcement learning (DRL)</method>, <method>masked deep neural network (MDNN)</method>, <method>DRL-MDNN framework</method>, <method>traditional DRL-CFD framework</method>"
2024,https://openalex.org/W4392890128,Engineering,"Motivation, work experience, and teacher performance: A comparative study","This research study investigates the effect of intrinsic and extrinsic motivation on employee performance, with a specific focus on the moderating role of employees' work experience. This investigation utilizes a proposed framework, focusing on higher educational institutions in West Bengal, India. It contributes to the human resource management field by comparing teacher performance in private and government academic institutions based on their motivation levels. The study employs a quantitative approach, collecting data from 250 teachers in West Bengal, India, using a structured questionnaire. The dataset underwent analysis employing Partial Least Squares Structural Equation Modeling (PLS-SEM) due to its inherent capacity to accommodate smaller sample sizes while delivering precise and insightful outcomes. The results indicate a strong positive relationship between intrinsic and extrinsic motivation and teacher performance in both types of institutions. Work experience moderates the connection between intrinsic motivation and performance in both sectors but has no significant impact on the relationship between extrinsic motivation and performance in private academic institutions. This study links a gap in the literature by empirically exploring the impact of teacher motivation on their performance and provides valuable insights into the complex interplay among motivation, work experience, and performance. Practically, it emphasizes the importance of employee motivation and accumulated work experience in enhancing performance. This study attempts to underscore the role of work experience as a moderating variable, thereby contributing to the novel discourse in the educational landscape of the post-pandemic era. The findings demand to identification of diverse organizational developmental drivers as work experience does not exhibit a strong mediation effect. However, limitations such as potential response bias should be considered in future research in this area.",<method>Partial Least Squares Structural Equation Modeling (PLS-SEM)</method>
2024,https://openalex.org/W4395069357,Engineering,Characterizing land use/land cover change dynamics by an enhanced random forest machine learning model: a Google Earth Engine implementation,"Abstract Land use and land cover (LULC) analysis is crucial for understanding societal development and assessing changes during the Anthropocene era. Conventional LULC mapping faces challenges in capturing changes under cloud cover and limited ground truth data. To enhance the accuracy and comprehensiveness of the descriptions of LULC changes, this investigation employed a combination of advanced techniques. Specifically, multitemporal 30 m resolution Landsat-8 satellite imagery was utilized, in addition to the cloud computing capabilities of the Google Earth Engine (GEE) platform. Additionally, the study incorporated the random forest (RF) algorithm. This study aimed to generate continuous LULC maps for 2014 and 2020 for the Shrirampur area of Maharashtra, India. A novel multiple composite RF approach based on LULC classification was utilized to generate the final LULC classification maps utilizing the RF-50 and RF-100 tree models. Both RF models utilized seven input bands (B1 to B7) as the dataset for LULC classification. By incorporating these bands, the models were able to influence the spectral information captured by each band to classify the LULC categories accurately. The inclusion of multiple bands enhanced the discrimination capabilities of the classifiers, increasing the comprehensiveness of the assessment of the LULC classes. The analysis indicated that RF-100 exhibited higher training and validation/testing accuracy for 2014 and 2020 (0.99 and 0.79/0.80, respectively). The study further revealed that agricultural land, built-up land, and water bodies have changed adequately and have undergone substantial variation among the LULC classes in the study area. Overall, this research provides novel insights into the application of machine learning (ML) models for LULC mapping and emphasizes the importance of selecting the optimal tree combination for enhancing the accuracy and reliability of LULC maps based on the GEE and different RF tree models. The present investigation further enabled the interpretation of pixel-level LULC interactions while improving image classification accuracy and suggested the best models for the classification of LULC maps through the identification of changes in LULC classes.","<method>random forest (RF) algorithm</method>, <method>multiple composite RF approach</method>, <method>RF-50 tree model</method>, <method>RF-100 tree model</method>, <method>machine learning (ML) models</method>"
2024,https://openalex.org/W4395445841,Engineering,Building information modeling (BIM) adoption for enhanced legal and contractual management in construction projects,"Building information modeling (BIM) adoption offers significant benefits for construction projects, including enhanced collaboration and legal/contractual management. However, the impacts of BIM deployment on construction contracts and dispute resolution remain unclear. This research analyzed the regulatory and legal implications of BIM adoption in construction industry via a framework evaluating Contractual Frameworks (CF), Dispute Resolution (DR), Legal Awareness (LA), and Risk Management (RM). Fifteen qualitative interviews with construction industry experts provided key insights. The results showed CF, DR, and LA have a large positive impact on BIM adoption, while RM has a smaller effect. Specifically, clearly defined BIM protocols in contracts, efficient dispute resolution mechanisms, and legal knowledge of BIM are essential to ensure effective deployment. These findings imply construction project managers should proactively address contractual and legal factors to manage risks, encourage collaboration, and adhere to regulations when implementing BIM. This research makes a significant contribution by addressing the lack of empirical insights on the contractual and legal aspects of BIM adoption. The qualitative methodology provides in-depth perspectives from experienced industry practitioners. The findings offer valuable insights for stakeholders worldwide seeking to leverage BIM. Further comparative research across regions and industries would continue to advance comprehension of the legal implications of BIM adoption.",No methods found.
2024,https://openalex.org/W4396656165,Engineering,Analysing LULC transformations using remote sensing data: insights from a multilayer perceptron neural network approach,"The study examines the complex dynamics of changes in LULC over three decades, focused on the years 1992, 2002, 2012, and 2022. The research highlights the significance of comprehending these alterations within the framework of environmental and socio-economic consequences. The changes in land use and land cover (LULC) have significant and far-reaching effects on ecosystems, biodiversity, and human livelihoods. This study offers useful information for politicians, conservationists, and urban planners by examining historical patterns and forecasting future changes. The study utilized a Multilayer Perceptron Neural Network (MLP-NN), a well-known machine learning technique that excels at collecting intricate patterns. This model's design had three layers: input, hidden, and output. The model underwent 10,000 iterations during its training process, and a thorough statistical analysis was conducted to assess the impact of each driving component. The MLP-NN model demonstrated impressive performance, with a skill measure of 0.8724 and an accuracy rate of 89.08%. The accuracy of the LULC estimates for 2022 was verified by comparing them with observed data, ensuring the model's reliability. Moreover, the presence of evidence likely was found to be a significant factor that had a substantial impact on the accuracy of the model. The study highlights the effectiveness of the MLP-NN model in accurately predicting changes in LULC. The model's exceptional accuracy and proficiency make it a powerful tool for future LULC forecasts. Identifying the primary causes of model performance and understanding their implications may help to enhance land management strategies, encourage spatial planning, guide accurate decision-making, and facilitate the development of policies that align with sustainable growth and development.",<method>Multilayer Perceptron Neural Network (MLP-NN)</method>
2024,https://openalex.org/W4399154552,Engineering,Seeking in Ride-on-Demand Service: A Reinforcement Learning Model With Dynamic Price Prediction,"Recent years witness the increasing popularity of ride-on-demand (RoD) services such as Uber and Didi. Compared with traditional taxi, RoD service is more ""data-driven"" and adopts dynamic pricing to manipulate the supply and demand in real time. Dynamic price could be viewed as an accurate and quantitative indicator of the supply and demand, and could provide clues to drivers, passengers, and the service providers, possibly reshaping the ways in which some problems are solved. In this paper, we focus on the seeking route recommendation problem that aims at increasing driver revenue by recommending highly profitable seeking routes to drivers of vacant cars with the help of dynamic prices. We first justify our motivation by showing the importance of route recommendation and answering why it is necessary to consider dynamic prices, based on the analysis of real service data. We then design a dynamic price prediction model to generate the dynamic prices at any given time and location based on multi-source urban data. After that, a reinforcement learning model is adopted to perform seeking route recommendation based on predicted dynamic prices. We conduct extensive experiments in different spatio-temporal combinations and make comparisons with multiple baselines. Results first show that our dynamic price prediction model achieves an accuracy ranging from 83.82% to 90.67% under different settings. It also proves that considering the real-time predicted dynamic prices significantly increases driver revenue by, for example, 12% and 47.5% during weekday evening rush hours, than merely using the average prices or completely ignoring dynamic prices.","<method>dynamic price prediction model</method>, <method>reinforcement learning model</method>"
2024,https://openalex.org/W4399734362,Engineering,Data oversampling and imbalanced datasets: an investigation of performance for machine learning and feature engineering,"Abstract The classification of imbalanced datasets is a prominent task in text mining and machine learning. The number of samples in each class is not uniformly distributed; one class contains a large number of samples while the other has a small number. Overfitting of the model occurs as a result of imbalanced datasets, resulting in poor performance. In this study, we compare different oversampling techniques like synthetic minority oversampling technique (SMOTE), support vector machine SMOTE (SVM-SMOTE), Border-line SMOTE, K-means SMOTE, and adaptive synthetic (ADASYN) oversampling to address the issue of imbalanced datasets and enhance the performance of machine learning models. Preprocessing significantly enhances the quality of input data by reducing noise, redundant data, and unnecessary data. This enables the machines to identify crucial patterns that facilitate the extraction of significant and pertinent information from the preprocessed data. This study preprocesses the data using various top-level preprocessing steps. Furthermore, two imbalanced Twitter datasets are used to compare the performance of oversampling techniques with six machine learning models including random forest (RF), SVM, K-nearest neighbor (KNN), AdaBoost (ADA), logistic regression (LR), and decision tree (DT). In addition, the bag of words (BoW) and term frequency and inverse document frequency (TF-IDF) features extraction approaches are used to extract features from the tweets. The experiments indicate that SMOTE and ADASYN perform much better than other techniques thus providing higher accuracy. Additionally, overall results show that SVM with ’linear’ kernel tends to attain the highest accuracy and recall score of 99.67% and 1.00% on ADASYN oversampled datasets and 99.57% accuracy on SMOTE oversampled dataset with TF-IDF features. The SVM model using 10-fold cross-validation experiments achieved 97.40 mean accuracy with a 0.008 standard deviation. Our approach achieved 2.62% greater accuracy as compared to other current methods.","<method>synthetic minority oversampling technique (SMOTE)</method>, <method>support vector machine SMOTE (SVM-SMOTE)</method>, <method>Border-line SMOTE</method>, <method>K-means SMOTE</method>, <method>adaptive synthetic (ADASYN) oversampling</method>, <method>random forest (RF)</method>, <method>SVM</method>, <method>K-nearest neighbor (KNN)</method>, <method>AdaBoost (ADA)</method>, <method>logistic regression (LR)</method>, <method>decision tree (DT)</method>"
2024,https://openalex.org/W4401437558,Engineering,Wind turbine gearbox reliability verification by multivariate Gaidai reliability method,"Reliability research is essential since WT (Wind turbines) are built to endure high wind and wave-induced stresses. Novel multivariate and 2D (bivariate) reliability methods presented in this investigation being conceived by authors to assist designers in accurately assessment of crucial stressors, acting inside key mechanical WT parts, such as gearbox and drivetrain. Current study made use of the recently created 2D modified Weibull-type approach. Since multivariate statistical analysis takes into consideration cross-correlations between various system components, it is more suited for design than univariate statistical analysis. To cross-validate 10-MW semi-submersible type FWT (i.e., Floating Wind Turbine) gearbox system failure or damage probability, forecasted by the multivariate Gaidai reliability methodology, current study employed 2D modified Weibull approach. Typical load types that FWTs and related parts are prone, include longitudinal, bending, twisting, and cyclic stresses. Stochastic nature of environmental loads, acting on FWTs in terms of windspeed, direction, shear, vorticity may cause excessive nonlinear structural dynamic effects, hence multivariate reliability analysis for FWT key components like gearbox and drivetrain is necessary. In this investigation dynamic, structural, aerodynamic, and control aspects of the FWT system had been modeled, using advanced numerical techniques – FWT drivetrain dynamics has been modeled utilizing SIMPACK (Multibody Simulation Method) software. Novel multivariate Gaidai risk assessment approach provided a reliable reliability evaluation for the coupled drivetrain's dynamics, given design return periods of interest. Weakness of bivariate approach has been highlighted, comprehensive solution has been presented in a form of novel multivariate reliability method.","<method>2D modified Weibull-type approach</method>, <method>multivariate Gaidai reliability methodology</method>, <method>multivariate reliability analysis</method>"
2024,https://openalex.org/W4390618081,Engineering,Making Sense of Machine Learning: A Review of Interpretation Techniques and Their Applications,"Transparency in AI models is essential for promoting human–AI collaboration and ensuring regulatory compliance. However, interpreting these models is a complex process influenced by various methods and datasets. This study presents a comprehensive overview of foundational interpretation techniques, meticulously referencing the original authors and emphasizing their pivotal contributions. Recognizing the seminal work of these pioneers is imperative for contextualizing the evolutionary trajectory of interpretation in the field of AI. Furthermore, this research offers a retrospective analysis of interpretation techniques, critically evaluating their inherent strengths and limitations. We categorize these techniques into model-based, representation-based, post hoc, and hybrid methods, delving into their diverse applications. Furthermore, we analyze publication trends over time to see how the adoption of advanced computational methods within various categories of interpretation techniques has shaped the development of AI interpretability over time. This analysis highlights a notable preference shift towards data-driven approaches in the field. Moreover, we consider crucial factors such as the suitability of these techniques for generating local or global insights and their compatibility with different data types, including images, text, and tabular data. This structured categorization serves as a guide for practitioners navigating the landscape of interpretation techniques in AI. In summary, this review not only synthesizes various interpretation techniques but also acknowledges the contributions of their original authors. By emphasizing the origins of these techniques, we aim to enhance AI model explainability and underscore the importance of recognizing biases, uncertainties, and limitations inherent in the methods and datasets. This approach promotes the ethical and practical use of interpretation insights, empowering AI practitioners, researchers, and professionals to make informed decisions when selecting techniques for responsible AI implementation in real-world scenarios.","<method>model-based methods</method>, <method>representation-based methods</method>, <method>post hoc methods</method>, <method>hybrid methods</method>, <method>data-driven approaches</method>"
2024,https://openalex.org/W4390686423,Engineering,Real-life data-driven model predictive control for building energy systems comparing different machine learning models,"By considering forecasts and exploiting storage effects, model predictive control can achieve significant energy and cost savings in the building sector. However, due to the high individual modeling effort, model predictive control lacks practical applicability. For that reason, data-driven process models, approximating the system behavior based on measurements, have become increasingly popular in recent years. Still, scientific literature lacks consent about the most promising model types and efficient workflows to integrate different machine learning models into a model predictive controller. With this work, we present a workflow to provide efficient model predictive controllers based on measurement data automatically. The main idea is to translate different machine learning models into optimization syntax to enable efficient optimization with full access to gradients. We currently consider artificial neural networks, gaussian process regression, and simple linear regression process models. We use a generic model ontology to automatize the controller generation further and test the methodology on two real-life use cases. The first use case is the application of five office rooms with smart thermostat valves. The second use case is a test hall with an air handling unit and a concrete core activation. Using only two days of initial training data, we deploy controllers based on the different model types for six weeks in the offices and apply online learning to improve the models continuously. We observe only minor differences in controller performance despite the artificial neural networks showing the highest prediction accuracy. The second use case shows that the simple linear models require less controller tuning effort. Thus, for practical applications, we recommend linear regression models.","<method>model predictive control</method>, <method>artificial neural networks</method>, <method>gaussian process regression</method>, <method>linear regression</method>"
2024,https://openalex.org/W4391684052,Engineering,Enhancing MPPT performance for partially shaded photovoltaic arrays through backstepping control with Genetic Algorithm-optimized gains,"As the significance and complexity of solar panel performance, particularly at their maximum power point (MPP), continue to grow, there is a demand for improved monitoring systems. The presence of variable weather conditions in Maroua, including potential partial shadowing caused by cloud cover or urban buildings, poses challenges to the efficiency of solar systems. This study introduces a new approach to tracking the Global Maximum Power Point (GMPP) in photovoltaic systems within the context of solar research conducted in Cameroon. The system utilizes Genetic Algorithm (GA) and Backstepping Controller (BSC) methodologies. The Backstepping Controller (BSC) dynamically adjusts the duty cycle of the Single Ended Primary Inductor Converter (SEPIC) to align with the reference voltage of the Genetic Algorithm (GA) in Maroua's dynamic environment. This environment, characterized by intermittent sunlight and the impact of local factors and urban shadowing, affects the production of energy. The Genetic Algorithm is employed to enhance the efficiency of BSC gains in Maroua's solar environment. This optimization technique expedites the tracking process and minimizes oscillations in the GMPP. The adaptability of the learning algorithm to specific conditions improves energy generation, even in the challenging environment of Maroua. This study introduces a novel approach to enhance the efficiency of photovoltaic systems in Maroua, Cameroon, by tailoring them to the specific solar dynamics of the region. In terms of performance, our approach surpasses the INC-BSC, P&O-BSC, GA-BSC, and PSO-BSC methodologies. In practice, the stabilization period following shadowing typically requires fewer than three iterations. Additionally, our Maximum Power Point Tracking (MPPT) technology is based on the Global Maximum Power Point (GMPP) methodology, contrasting with alternative technologies that prioritize the Local Maximum Power Point (LMPP). This differentiation is particularly relevant in areas with partial shading, such as Maroua, where the use of LMPP-based technologies can result in power losses. The proposed method demonstrates significant performance by achieving a minimum 33% reduction in power losses.","<method>Genetic Algorithm (GA)</method>, <method>Backstepping Controller (BSC)</method>, <method>INC-BSC</method>, <method>P&O-BSC</method>, <method>GA-BSC</method>, <method>PSO-BSC</method>, <method>Maximum Power Point Tracking (MPPT) based on Global Maximum Power Point (GMPP) methodology</method>"
2024,https://openalex.org/W4400896559,Engineering,Optimization of Raw Material Inventory using Always Better Control (ABC) Analysis and Economic Order Quantity (EOQ) Method Approach in the Warehouse of a Bolt Manufacturing Factory in Indonesia,"There is a problem in the raw material procurement process at the Contractor Company, such as running out and excess stock of raw materials, as well as difficulties in determining how many raw materials to order that meet the company's economic value. Running out of raw material stock results in delays in production activities, while excess raw material stock can fill warehouse capacity, thereby increasing storage costs. To overcome this problem, research was carried out using a quantitative descriptive method to determine the level of production cost efficiency and production effectiveness level in order to achieve optimization of raw material supplies using Always Better Control (ABC) Analysis and the Economic Order Quantity (EOQ) Method at Bolt Companies. ABC analysis plays a role in determining which raw materials have the highest level of demand and the EOQ method plays a role in determining the amount of raw materials to be ordered in order to meet the company's economic value. The research results show that the combination of ABC Analysis and EOQ Method can reduce ordering costs and raw material inventory. There are 4 items out of 10 raw material items that are included in Category A or the most prioritized category. From the results of calculations using the EOQ method, the Bolt Company can save total orders and raw material inventory (TIC) in the period January to December 2023 amounting to IDR 2,147,403,-.",No methods found.
2024,https://openalex.org/W4399685394,Engineering,Advanced Modelling of Soil Organic Carbon Content in Coal Mining Areas Using Integrated Spectral Analysis: A Dengcao Coal Mine Case Study,"Effective modelling and integrated spectral analysis approaches can advance modelling precision. To develop an integrated spectral forecast modelling of soil organic carbon (SOC), this research investigated a mining coal in Dengcao Coal Mine Area, Zhengzhou. The study utilizes the Lasso and Ranger algorithms were utilized in spectral band analysis. Four primary models employed during this process include Artificial Neural Network (ANN), Support Vector Machine, Random Forest (RF), and Partial Least Squares Regression (PLSR). The ideal model was chosen. The results showed that, in contrast to when band collection was based on Lasso algorithm modelling, model precision was higher when it was based on the Ranger algorithm. ANN model had an ideal goodness acceptance, and the modelling developed by RF showed the steadiest modelling consequences. Based on the results, a distinct method is proposed in this study for band assortment at the earlier stage of integrated spectral modelling of SOC. The Ranger method can be used to check the spectral particles, and RF or ANN can be chosen to develop the prediction modelling based on different statistics sets, which is appropriate to create the prediction modelling of SOC content in Dengcao Coal Mine Area. This research avails a position for the integrated spectral of Analysis for Advanced Modelling of Soil Organic Carbon Content in Coal Sources alongside a theoretical foundation for innovating portable device for the integrated spectral assessment of SOC content in coal mining habitats. This study might be significant for the changing modelling and monitoring of SOC in mining and environmental areas.","<method>Lasso</method>, <method>Ranger</method>, <method>Artificial Neural Network (ANN)</method>, <method>Support Vector Machine</method>, <method>Random Forest (RF)</method>, <method>Partial Least Squares Regression (PLSR)</method>"
2024,https://openalex.org/W4400837307,Engineering,Quality Control to Reduce Appearance Defects at PT. Musical Instrument,"This research was conducted at PT. Musical Instruments that aim to analyze quality control to reduce appearance defects in piano products on the assembling production line. The problem faced by the company is the high level of product defects which has an impact on decreasing quality and customer satisfaction. The research method used is Six sigma with a DMAIC (Define, Measure, Analyze, Improve, Control) approach. This type of research is quantitative, with data collected in the form of the number of production defects in pianos. To analyze the causes of defects, a fishbone diagram with 4M + 1E factors is used, namely Man, Machine, Method, Material, and Environment. The results of the analysis show that the main factors causing appearance defects in piano products include incompatibility with work methods, lack of worker training, use of non-standard materials, suboptimal jig conditions, and unsupportive working environment. Based on these findings, improvement proposals are given in the form of improving standard operating procedures, regular training for workers, the use of high-quality materials, regular maintenance and calibration of jigs, and improvement of work environment conditions. The implementation of this improvement proposal is expected to reduce the number of appearance defects in piano products, improve product quality, and meet the quality standards expected by PT. Musical instrument.",No methods found.
2024,https://openalex.org/W4394785938,Engineering,Interactive Tree of Life (iTOL) v6: recent updates to the phylogenetic tree display and annotation tool,"Abstract The Interactive Tree Of Life (https://itol.embl.de) is an online tool for the management, display, annotation and manipulation of phylogenetic and other trees. It is freely available and open to everyone. iTOL version 6 introduces a modernized and completely rewritten user interface, together with numerous new features. A new dataset type has been introduced (colored/labeled ranges), greatly upgrading the functionality of the previous simple colored range annotation function. Additional annotation options have been implemented for several existing dataset types. Dataset template files now support simple assignment of annotations to multiple tree nodes through substring matching, including full regular expression support. Node metadata handling has been greatly extended with novel display and exporting options, and it can now be edited interactively or bulk updated through annotation files. Tree labels can be displayed using multiple simultaneous font styles, with precise positioning, sizing and styling of each individual label part. Various bulk label editing functions have been implemented, simplifying large scale changes of all tree node labels. iTOL’s automatic taxonomy assignment functions now support trees based on the Genome Taxonomy Database (GTDB), in addition to the NCBI taxonomy. The functionality of the optional user account pages has been expanded, simplifying the management, navigation and sharing of projects and trees. iTOL currently handles more than one and a half million trees from &amp;gt;130 000 individual user accounts.",No methods found.
2024,https://openalex.org/W609405272,Engineering,Sonar Signal Processing 1995,"The performance of adaptive beamforming algorithms is known to degrade in a moving jammer environment.This degradation occurs due to the jammer motion that brings the interfering sources out of the sharp notches of the adapted pattern.In this paper, we consider a uni ed framework that allows tomake a broad class of adaptive array algorithms robust against jammer motion.The robustness is achieved by means of arti cially broadening the directional pattern null width in the jammer directions.For this purpose, we use a special type of data-dependent sidelobe derivative constraints that do not require any a priori information about the jammers. I. INTRODUCTIONThe performance of adaptive beamforming has been discussed for stationary (non-moving) jammer scenarios, [1],However, in the future fast moving platforms become more likely in various applications.The performance of adaptive arrays severely degrades if the weights are not able to adapt suf ciently fast to the changing jamming situation.Fast adaptation has therefore be the aim of research in these cases.Moving jammers represent a serious problem, because for large antennas the directional pattern nulls are extremely sharp and jammers may soon move out of the nulls, i.e. high gain antennas are very sensitive to this type of non-stationarity.Recently, a large number of robust adaptive beamforming methods has been studied [3],",No methods found.
2024,https://openalex.org/W4391135337,Engineering,ACCORD (ACcurate COnsensus Reporting Document): A reporting guideline for consensus methods in biomedicine developed via a modified Delphi,"Background In biomedical research, it is often desirable to seek consensus among individuals who have differing perspectives and experience. This is important when evidence is emerging, inconsistent, limited, or absent. Even when research evidence is abundant, clinical recommendations, policy decisions, and priority-setting may still require agreement from multiple, sometimes ideologically opposed parties. Despite their prominence and influence on key decisions, consensus methods are often poorly reported. Our aim was to develop the first reporting guideline dedicated to and applicable to all consensus methods used in biomedical research regardless of the objective of the consensus process, called ACCORD (ACcurate COnsensus Reporting Document). Methods and findings We followed methodology recommended by the EQUATOR Network for the development of reporting guidelines: a systematic review was followed by a Delphi process and meetings to finalize the ACCORD checklist. The preliminary checklist was drawn from the systematic review of existing literature on the quality of reporting of consensus methods and suggestions from the Steering Committee. A Delphi panel ( n = 72) was recruited with representation from 6 continents and a broad range of experience, including clinical, research, policy, and patient perspectives. The 3 rounds of the Delphi process were completed by 58, 54, and 51 panelists. The preliminary checklist of 56 items was refined to a final checklist of 35 items relating to the article title ( n = 1), introduction ( n = 3), methods ( n = 21), results ( n = 5), discussion ( n = 2), and other information ( n = 3). Conclusions The ACCORD checklist is the first reporting guideline applicable to all consensus-based studies. It will support authors in writing accurate, detailed manuscripts, thereby improving the completeness and transparency of reporting and providing readers with clarity regarding the methods used to reach agreement. Furthermore, the checklist will make the rigor of the consensus methods used to guide the recommendations clear for readers. Reporting consensus studies with greater clarity and transparency may enhance trust in the recommendations made by consensus panels.",No methods found.
2024,https://openalex.org/W4391359414,Engineering,Autopilot control unmanned aerial vehicle system for sewage defect detection using deep learning,"Abstract This work proposes the use of an unmanned aerial vehicle (UAV) with an autopilot to identify the defects present in municipal sewerage pipes. The framework also includes an effective autopilot control mechanism that can direct the flight path of a UAV within a sewer line. Both of these breakthroughs have been addressed throughout this work. The UAV's camera proved useful throughout a sewage inspection, providing important contextual data that helped analyze the sewerage line's internal condition. A plethora of information useful for understanding the sewerage line's inner functioning and extracting interior visual details can be obtained from camera‐recorded sewerage imagery if a defect is present. In the case of sewerage inspections, nevertheless, the impact of a false negative is significantly higher than that of a false positive. One of the trickiest parts of the procedure is identifying defective sewerage pipelines and false negatives. In order to get rid of the false negative outcome or false positive outcome, a guided image filter (GIF) is implemented in this proposed method during the pre‐processing stage. Afterwards, the algorithms Gabor transform (GT) and stroke width transform (SWT) were used to obtain the features of the UAV‐captured surveillance image. The UAV camera's sewerage image is then classified as “defective” or “not defective” using the obtained features by a Weighted Naive Bayes Classifier (WNBC). Next, images of the sewerage lines captured by the UAV are analyzed using speed‐up robust features (SURF) and deep learning to identify different types of defects. As a result, the proposed methodology achieved more favorable outcomes than prior existing approaches in terms of the following metrics: mean PSNR (71.854), mean MSE (0.0618), mean RMSE (0.2485), mean SSIM (98.71%), mean accuracy (98.372), mean specificity (97.837%), mean precision (93.296%), mean recall (94.255%), mean F1‐score (93.773%), and mean processing time (35.43 min).","<method>Weighted Naive Bayes Classifier (WNBC)</method>, <method>deep learning</method>"
2024,https://openalex.org/W4391092744,Engineering,DEA-Net: Single Image Dehazing Based on Detail-Enhanced Convolution and Content-Guided Attention,"Single image dehazing is a challenging ill-posed problem which estimates latent haze-free images from observed hazy images. Some existing deep learning based methods are devoted to improving the model performance via increasing the depth or width of convolution. The learning ability of Convolutional Neural Network (CNN) structure is still under-explored. In this paper, a Detail-Enhanced Attention Block (DEAB) consisting of Detail-Enhanced Convolution (DEConv) and Content-Guided Attention (CGA) is proposed to boost the feature learning for improving the dehazing performance. Specifically, the DEConv contains difference convolutions which can integrate prior information to complement the vanilla one and enhance the representation capacity. Then by using the re-parameterization technique, DEConv is equivalently converted into a vanilla convolution to reduce parameters and computational cost. By assigning the unique Spatial Importance Map (SIM) to every channel, CGA can attend more useful information encoded in features. In addition, a CGA-based mixup fusion scheme is presented to effectively fuse the features and aid the gradient flow. By combining above mentioned components, we propose our Detail-Enhanced Attention Network (DEA-Net) for recovering high-quality haze-free images. Extensive experimental results demonstrate the effectiveness of our DEA-Net, outperforming the state-of-the-art (SOTA) methods by boosting the PSNR index over 41 dB with only 3.653 M parameters. (The source code of our DEA-Net is available at https://github.com/cecret3350/DEA-Net.).","<method>Convolutional Neural Network (CNN)</method>, <method>Detail-Enhanced Attention Block (DEAB)</method>, <method>Detail-Enhanced Convolution (DEConv)</method>, <method>Content-Guided Attention (CGA)</method>, <method>re-parameterization technique</method>, <method>Spatial Importance Map (SIM)</method>, <method>CGA-based mixup fusion scheme</method>, <method>Detail-Enhanced Attention Network (DEA-Net)</method>"
2024,https://openalex.org/W4392367648,Engineering,Hardware implementation of memristor-based artificial neural networks,"Abstract Artificial Intelligence (AI) is currently experiencing a bloom driven by deep learning (DL) techniques, which rely on networks of connected simple computing units operating in parallel. The low communication bandwidth between memory and processing units in conventional von Neumann machines does not support the requirements of emerging applications that rely extensively on large sets of data. More recent computing paradigms, such as high parallelization and near-memory computing, help alleviate the data communication bottleneck to some extent, but paradigm- shifting concepts are required. Memristors, a novel beyond-complementary metal-oxide-semiconductor (CMOS) technology, are a promising choice for memory devices due to their unique intrinsic device-level properties, enabling both storing and computing with a small, massively-parallel footprint at low power. Theoretically, this directly translates to a major boost in energy efficiency and computational throughput, but various practical challenges remain. In this work we review the latest efforts for achieving hardware-based memristive artificial neural networks (ANNs), describing with detail the working principia of each block and the different design alternatives with their own advantages and disadvantages, as well as the tools required for accurate estimation of performance metrics. Ultimately, we aim to provide a comprehensive protocol of the materials and methods involved in memristive neural networks to those aiming to start working in this field and the experts looking for a holistic approach.","<method>deep learning (DL)</method>, <method>artificial neural networks (ANNs)</method>"
2024,https://openalex.org/W4400881081,Engineering,TransUNet: Rethinking the U-Net architecture design for medical image segmentation through the lens of transformers,"Medical image segmentation is crucial for healthcare, yet convolution-based methods like U-Net face limitations in modeling long-range dependencies. To address this, Transformers designed for sequence-to-sequence predictions have been integrated into medical image segmentation. However, a comprehensive understanding of Transformers' self-attention in U-Net components is lacking. TransUNet, first introduced in 2021, is widely recognized as one of the first models to integrate Transformer into medical image analysis. In this study, we present the versatile framework of TransUNet that encapsulates Transformers' self-attention into two key modules: (1) a Transformer encoder tokenizing image patches from a convolution neural network (CNN) feature map, facilitating global context extraction, and (2) a Transformer decoder refining candidate regions through cross-attention between proposals and U-Net features. These modules can be flexibly inserted into the U-Net backbone, resulting in three configurations: Encoder-only, Decoder-only, and Encoder+Decoder. TransUNet provides a library encompassing both 2D and 3D implementations, enabling users to easily tailor the chosen architecture. Our findings highlight the encoder's efficacy in modeling interactions among multiple abdominal organs and the decoder's strength in handling small targets like tumors. It excels in diverse medical applications, such as multi-organ segmentation, pancreatic tumor segmentation, and hepatic vessel segmentation. Notably, our TransUNet achieves a significant average Dice improvement of 1.06% and 4.30% for multi-organ segmentation and pancreatic tumor segmentation, respectively, when compared to the highly competitive nn-UNet, and surpasses the top-1 solution in the BrasTS2021 challenge. 2D/3D Code and models are available at https://github.com/Beckschen/TransUNet and https://github.com/Beckschen/TransUNet-3D, respectively.","<method>U-Net</method>, <method>Transformers</method>, <method>TransUNet</method>, <method>Transformer encoder</method>, <method>Transformer decoder</method>, <method>convolution neural network (CNN)</method>, <method>nn-UNet</method>"
2024,https://openalex.org/W4390587679,Engineering,"A Systematic Review and Meta-Analysis of Artificial Intelligence Tools in Medicine and Healthcare: Applications, Considerations, Limitations, Motivation and Challenges","Artificial intelligence (AI) has emerged as a transformative force in various sectors, including medicine and healthcare. Large language models like ChatGPT showcase AI’s potential by generating human-like text through prompts. ChatGPT’s adaptability holds promise for reshaping medical practices, improving patient care, and enhancing interactions among healthcare professionals, patients, and data. In pandemic management, ChatGPT rapidly disseminates vital information. It serves as a virtual assistant in surgical consultations, aids dental practices, simplifies medical education, and aids in disease diagnosis. A total of 82 papers were categorised into eight major areas, which are G1: treatment and medicine, G2: buildings and equipment, G3: parts of the human body and areas of the disease, G4: patients, G5: citizens, G6: cellular imaging, radiology, pulse and medical images, G7: doctors and nurses, and G8: tools, devices and administration. Balancing AI’s role with human judgment remains a challenge. A systematic literature review using the PRISMA approach explored AI’s transformative potential in healthcare, highlighting ChatGPT’s versatile applications, limitations, motivation, and challenges. In conclusion, ChatGPT’s diverse medical applications demonstrate its potential for innovation, serving as a valuable resource for students, academics, and researchers in healthcare. Additionally, this study serves as a guide, assisting students, academics, and researchers in the field of medicine and healthcare alike.","<method>Large language models</method>, <method>ChatGPT</method>, <method>systematic literature review using the PRISMA approach</method>"
2024,https://openalex.org/W4399657851,Engineering,Fusion of finite element and machine learning methods to predict rock shear strength parameters,"Abstract The trial-and-error method for calibrating rock mechanics parameters has the disadvantages of complexity, being time-consuming, and difficulty in ensuring accuracy. Harnessing the repeatability and scalability intrinsic to numerical simulation calculations and amalgamating them with the data-driven attributes of machine learning methods, this study uses the finite element analysis software RS2 to establish 252 sets of sandstone sample data. The recursive feature elimination and cross-validation method is employed for feature selection. The shear strength parameters of sandstone are predicted using machine learning models optimized by the particle swarm optimization (PSO) algorithm, including the backpropagation neural network, Bayesian ridge regression, support vector regression (SVR), and light gradient boosting machine. The predicted value of cohesion is proposed as the input feature to predict the friction angle. The results indicate that the optimal input characteristics for predicting cohesion are elastic modulus, Poisson's ratio, peak stress, and peak strain, while the optimal input characteristics for predicting friction angle are peak stress and cohesion. The PSO-SVR model demonstrates the best performance. The maximum error between the predicted values of cohesion and friction angle and the calculated results of RSData program are 3.5% and 4.31%, respectively. The finite element calculation is in good agreement with the stress–strain curve obtained in the laboratory. The sensitivity analysis indicates that SVR's prediction performance for cohesion and friction angle tends to be stable when the sample size is &amp;gt;25. These results offer a valuable reference for accurately predicting rock mechanics parameters.","<method>recursive feature elimination</method>, <method>cross-validation</method>, <method>particle swarm optimization (PSO)</method>, <method>backpropagation neural network</method>, <method>Bayesian ridge regression</method>, <method>support vector regression (SVR)</method>, <method>light gradient boosting machine</method>"
2024,https://openalex.org/W4392611940,Engineering,Deep learning-based structural health monitoring,"This article provides a comprehensive review of deep learning-based structural health monitoring (DL-based SHM). It encompasses a broad spectrum of DL theories and applications including nondestructive approaches; computer vision-based methods, digital twins, unmanned aerial vehicles (UAVs), and their integration with DL; vibration-based strategies including sensor fault and data recovery methods; and physics-informed DL approaches. Connections between traditional machine learning and DL-based methods as well as relations of local to global approaches including their extensive integrations are established. The state-of-the-art methods, including their advantages and limitations are presented. The review draws on current literature on the topic, also providing a synergistic analysis leading to the understanding of the evolution of DL as a basis for presenting the future research and development needs. Our overall finding is that despite the rapid progression of digital technology along with the progression of DL, the DL-based SHM appears to be in its infant stages with enormous potential for future developments to bring the SHM technology to a common practical use with wide scope applications, performance reliability, cost, and degree of automation. It is anticipated that this review paper will serve as a basic resource for readers seeking comprehensive and holistic understanding of the subject matter.","<method>deep learning-based structural health monitoring (DL-based SHM)</method>, <method>deep learning (DL)</method>, <method>computer vision-based methods</method>, <method>digital twins</method>, <method>vibration-based strategies</method>, <method>sensor fault and data recovery methods</method>, <method>physics-informed deep learning (DL) approaches</method>, <method>traditional machine learning</method>, <method>local to global approaches</method>"
2024,https://openalex.org/W4390508827,Engineering,3D printing of magneto-active smart materials for advanced actuators and soft robotics applications,"In the contemporary era, novel manufacturing technologies like additive manufacturing (AM) have revolutionized the different engineering sectors including biomedical, aerospace, electronics, etc. Four-dimensional (4D) printing aka AM of smart materials is gaining popularity among the scientific community, which has the excellent ability to make soft structures such as soft robots, actuators, and grippers. These soft structures are developed by applying various stimuli such as pH, temperature, magnetic field, and many combinations onto soft materials. Stimuli in 3D printing permit various shape-morphing behaviors such as bending, twisting, folding, swelling, rolling, shrinking, origami, or locomotion. A wide variety of soft magnetic structures can be fabricated through the incorporation of soft or hard magnetic particles into soft materials resulting in magneto-active soft materials (MASMs). With this integration, magneto-thermal coupling actuation allows diverse magneto-deformations, facilitating the development of personalized devices that are capable of enhanced deformation. In this review, guidelines are provided on the 3D printing for MASMs such as magneto-active polymers (MAPs), magneto-active composites, and magneto-active hydrogels (MAHs) on the booming development of various smart and flexible devices such as soft robots, wearable electronics, and biomimetic devices. Moreover, 3D-printed soft robotics have an outstanding capacity to adapt to complicated situations for many advanced actuating applications. Finally, some current challenges and emerging areas in this exciting technology have been proposed. Lastly, it is anticipated that technological advancements in developing smart and intelligent magneto-active structures will have a significant impact on the design of real-world applications.",No methods found.
2024,https://openalex.org/W4392872715,Engineering,GLC_FCS30D: the first global 30 m land-cover dynamics monitoring product with a fine classification system for the period from 1985 to 2022 generated using dense-time-series Landsat imagery and the continuous change-detection method,"Abstract. Land-cover change has been identified as an important cause or driving force of global climate change and is a significant research topic. Over the past few decades, global land-cover mapping has progressed; however, long-time-series global land-cover-change monitoring data are still sparse, especially those at 30 m resolution. In this study, we describe GLC_FCS30D, a novel global 30 m land-cover dynamics monitoring dataset containing 35 land-cover subcategories and covering the period 1985–2022 in 26 time steps (maps were updated every 5 years before 2000 and annually after 2000). GLC_FCS30D has been developed using continuous change detection and all available Landsat imagery based on the Google Earth Engine platform. Specifically, we first take advantage of the continuous change-detection model and the full time series of Landsat observations to capture the time points of changed pixels and identify the temporally stable areas. Then, we apply a spatiotemporal refinement method to derive the globally distributed and high-confidence training samples from these temporally stable areas. Next, local adaptive classification models are used to update the land-cover information for the changed pixels, and a temporal-consistency optimization algorithm is adopted to improve their temporal stability and suppress some false changes. Further, the GLC_FCS30D product is validated using 84 526 globally distributed validation samples from 2020. It achieves an overall accuracy of 80.88 % (±0.27 %) for the basic classification system (10 major land-cover types) and 73.04 % (±0.30 %) for the LCCS (Land Cover Classification System) level-1 validation system (17 LCCS land-cover types). Meanwhile, two third-party time-series datasets used for validation from the United States and Europe Union are also collected for analyzing accuracy variations, and the results show that GLC_FCS30D offers significant stability in terms of variation across the accuracy time series and achieves mean accuracies of 79.50 % (±0.50 %) and 81.91 % (±0.09 %) over the two regions. Lastly, we draw conclusions about the global land-cover-change information from the GLC_FCS30D dataset; namely, that forest and cropland variations have dominated global land-cover change over past 37 years, the net loss of forests reached about 2.5 million km2, and the net gain in cropland area is approximately 1.3 million km2. Therefore, the novel dataset GLC_FCS30D is an accurate land-cover-dynamics time-series monitoring product that benefits from its diverse classification system, high spatial resolution, and long time span (1985–2022); thus, it will effectively support global climate change research and promote sustainable development analysis. The GLC_FCS30D dataset is available via https://doi.org/10.5281/zenodo.8239305 (Liu et al., 2023).","<method>continuous change-detection model</method>, <method>spatiotemporal refinement method</method>, <method>local adaptive classification models</method>, <method>temporal-consistency optimization algorithm</method>"
2024,https://openalex.org/W4390706297,Engineering,TTST: A Top-<i>k</i> Token Selective Transformer for Remote Sensing Image Super-Resolution,"Transformer-based method has demonstrated promising performance in image super-resolution tasks, due to its long-range and global aggregation capability. However, the existing Transformer brings two critical challenges for applying it in large-area earth observation scenes: (1) redundant token representation due to most irrelevant tokens; (2) single-scale representation which ignores scale correlation modeling of similar ground observation targets. To this end, this paper proposes to adaptively eliminate the interference of irreverent tokens for a more compact self-attention calculation. Specifically, we devise a Residual Token Selective Group (RTSG) to grasp the most crucial token by dynamically selecting the top- <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula> keys in terms of score ranking for each query. For better feature aggregation, a Multi-scale Feed-forward Layer (MFL) is developed to generate an enriched representation of multi-scale feature mixtures during feed-forward process. Moreover, we also proposed a Global Context Attention (GCA) to fully explore the most informative components, thus introducing more inductive bias to the RTSG for an accurate reconstruction. In particular, multiple cascaded RTSGs form our final Top- <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula> Token Selective Transformer (TTST) to achieve progressive representation. Extensive experiments on simulated and real-world remote sensing datasets demonstrate our TTST could perform favorably against state-of-the-art CNN-based and Transformer-based methods, both qualitatively and quantitatively. In brief, TTST outperforms the state-of-the-art approach (HAT-L) in terms of PSNR by 0.14 dB on average, but only accounts for 47.26% and 46.97% of its computational cost and parameters. The code and pre-trained TTST will be available at <uri xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">https://github.com/XY-boy/TTST</uri> for validation.","<method>Transformer-based method</method>, <method>Residual Token Selective Group (RTSG)</method>, <method>Multi-scale Feed-forward Layer (MFL)</method>, <method>Global Context Attention (GCA)</method>, <method>Top-k Token Selective Transformer (TTST)</method>"
2024,https://openalex.org/W4391929423,Engineering,Porous Conductive Textiles for Wearable Electronics,"Over the years, researchers have made significant strides in the development of novel flexible/stretchable and conductive materials, enabling the creation of cutting-edge electronic devices for wearable applications. Among these, porous conductive textiles (PCTs) have emerged as an ideal material platform for wearable electronics, owing to their light weight, flexibility, permeability, and wearing comfort. This Review aims to present a comprehensive overview of the progress and state of the art of utilizing PCTs for the design and fabrication of a wide variety of wearable electronic devices and their integrated wearable systems. To begin with, we elucidate how PCTs revolutionize the form factors of wearable electronics. We then discuss the preparation strategies of PCTs, in terms of the raw materials, fabrication processes, and key properties. Afterward, we provide detailed illustrations of how PCTs are used as basic building blocks to design and fabricate a wide variety of intrinsically flexible or stretchable devices, including sensors, actuators, therapeutic devices, energy-harvesting and storage devices, and displays. We further describe the techniques and strategies for wearable electronic systems either by hybridizing conventional off-the-shelf rigid electronic components with PCTs or by integrating multiple fibrous devices made of PCTs. Subsequently, we highlight some important wearable application scenarios in healthcare, sports and training, converging technologies, and professional specialists. At the end of the Review, we discuss the challenges and perspectives on future research directions and give overall conclusions. As the demand for more personalized and interconnected devices continues to grow, PCT-based wearables hold immense potential to redefine the landscape of wearable technology and reshape the way we live, work, and play.",No methods found.
2024,https://openalex.org/W4393993191,Engineering,Generative AI for Customizable Learning Experiences,"The introduction of accessible generative artificial intelligence opens promising opportunities for the implementation of personalized learning methods in any educational environment. Personalized learning has been conceptualized for a long time, but it has only recently become realistic and truly achievable. In this paper, we propose an affordable and sustainable approach toward personalizing learning materials as part of the complete educational process. We have created a tool within a pre-existing learning management system at a software engineering college that automatically generates learning materials based on the learning outcomes provided by the professor for a particular class. The learning materials were composed in three distinct styles, the initial one being the traditional professor style and the other two variations adopting a pop-culture influence, namely Batman and Wednesday Addams. Each lesson, besides being delivered in three different formats, contained automatically generated multiple-choice questions that students could use to check their progress. This paper contains complete instructions for developing such a tool with the help of large language models using OpenAI’s API and an analysis of the preliminary experiment of its usage performed with the help of 20 college students studying software engineering at a European university. Participation in the study was optional and on voluntary basis. Each student’s tool usage was quantified, and two questionnaires were conducted: one immediately after subject completion and another 6 months later to assess both immediate and long-term effects, perceptions, and preferences. The results indicate that students found the multiple variants of the learning materials really engaging. While predominantly utilizing the traditional variant of the learning materials, they found this approach inspiring, would recommend it to other students, and would like to see it more in classes. The most popular feature were the automatically generated quiz-style tests that they used to assess their understanding. Preliminary evidence suggests that the use of various versions of learning materials leads to an increase in students’ study time, especially for students who have not mastered the topic otherwise. The study’s small sample size of 20 students restricts its ability to generalize its findings, but its results provide useful early insights and lay the groundwork for future research on AI-supported educational strategies.",<method>large language models</method>
2024,https://openalex.org/W4399450035,Engineering,Power Hungry Processing: Watts Driving the Cost of AI Deployment?,"Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of ""generality"" comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and 'general-purpose' models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.","<method>finetuned models</method>, <method>multi-purpose generative architectures</method>"
2024,https://openalex.org/W4393203949,Engineering,"Bifurcations, chaotic behavior, and optical solutions for the complex Ginzburg–Landau equation","This research focuses on investigating an extended version of the Ginzburg–Landau (GL) equation that describes the motion of particles in a plasma. The other applications of this model can be found in optics, and other related fields. As part of our approach, we seek to discover some new wave solutions to the model, and then use the Galilean transformation in order to determine the model's dynamical properties. After that, we use planar dynamical system theory to perform an in-depth bifurcation analysis, which gives us valuable insight into the system. By conducting thorough mathematical and bifurcation analyses, we identify the fundamental dynamics and critical points that regulate the system's behavior. Moreover we explore the chaotic nature of the system, uncovering potential chaotic tendencies and their consequences. Further, we obtain several categories of optical solutions of the complex GL equation by utilizing new logarithmic transformations, offering valuable insight into its behavior and potential applications in optics. Our analytical technique yields closed-form solutions expressing elementary functions. In order to ensure the reliability of our findings, we rigorously validate the obtained solutions by substituting them back into the original model. Our research helps us better understand this equation's characteristics and its relevance across a wide range of disciplines.",No methods found.
2024,https://openalex.org/W4401070841,Engineering,Transformer-Based Visual Segmentation: A Survey,"Visual segmentation seeks to partition images, video frames, or point clouds into multiple segments or groups. This technique has numerous real-world applications, such as autonomous driving, image editing, robot sensing, and medical analysis. Over the past decade, deep learning-based methods have made remarkable strides in this area. Recently, transformers, a type of neural network based on self-attention originally designed for natural language processing, have considerably surpassed previous convolutional or recurrent approaches in various vision processing tasks. Specifically, vision transformers offer robust, unified, and even simpler solutions for various segmentation tasks. This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements. We first review the background, encompassing problem definitions, datasets, and prior convolutional methods. Next, we summarize a meta-architecture that unifies all recent transformer-based approaches. Based on this meta-architecture, we examine various method designs, including modifications to the meta-architecture and associated applications. We also present several specific subfields, including 3D point cloud segmentation, foundation model tuning, domain-aware segmentation, efficient segmentation, and medical segmentation. Additionally, we compile and re-evaluate the reviewed methods on several well-established datasets. Finally, we identify open challenges in this field and propose directions for future research. The project page can be found at <uri xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">https://github.com/lxtGH/Awesome-Segmentation-With-Transformer</uri> .","<method>deep learning-based methods</method>, <method>transformers</method>, <method>convolutional approaches</method>, <method>recurrent approaches</method>, <method>vision transformers</method>"
2024,https://openalex.org/W4391844002,Engineering,Texture Exposure of Unconventional (101)<sub>Zn</sub> Facet: Enabling Dendrite‐Free Zn Deposition on Metallic Zinc Anodes,"Abstract Texturing metallic zinc anodes (MZAs) for selective exposure of (002) Zn plane with high thermodynamical stability is an efficient scheme for dendrite‐free Zn electrodeposition. However, fundamental factors that influence Zn deposition morphology via surface crystallographic texture engineering are not well understood. Herein, different from traditional cognition, MZAs with preferential exposure of (101) Zn facet are demonstrated to be equally effective in promoting dendrite‐free Zn deposition, which is enabled by introducing trace amount (0.01 m ) of theophylline into ZnSO 4 electrolyte. Experimental results and mathematical model corroborate, indicating mechanistically that the theophylline derived cations preferentially adsorb on the (002) Zn crystal plane due to higher adsorption energy, thereby accelerating its growth through increased binding affinity with Zn 2+ ions. Consequently, this phenomenon facilitates the texture exposure of (101) Zn facet to achieve ordered surface crystallographic orientation of MZAs (101‐Zn), thus enabling electrodeposition/dissolution cycling over 650 h under a depth of discharge up to 40% and significantly boosting the rechargeability (76.7% capacity retention after 1000 cycles) of the 101‐Zn||carbon‐cloth@MnO 2 full battery relative to counterpart without theophylline additive (36.3%). The work offers deep insights on the scientific links between the surface crystallographic orientation of MZAs and Zn deposition morphology, while opens up vast untapped opportunities to realize dendrite‐free MZAs.",No methods found.
2024,https://openalex.org/W4391974599,Engineering,"Generative AI for Transformative Healthcare: A Comprehensive Study of Emerging Models, Applications, Case Studies, and Limitations","Generative artificial intelligence (GAI) can be broadly described as an artificial intelligence system capable of generating images, text, and other media types with human prompts. GAI models like ChatGPT, DALL-E, and Bard have recently caught the attention of industry and academia equally. GAI applications span various industries like art, gaming, fashion, and healthcare. In healthcare, GAI shows promise in medical research, diagnosis, treatment, and patient care and is already making strides in real-world deployments. There has yet to be any detailed study concerning the applications and scope of GAI in healthcare. Addressing this research gap, we explore several applications, real-world scenarios, and limitations of GAI in healthcare. We examine how GAI models like ChatGPT and DALL-E can be leveraged to aid in the applications of medical imaging, drug discovery, personalized patient treatment, medical simulation and training, clinical trial optimization, mental health support, healthcare operations and research, medical chatbots, human movement simulation, and a few more applications. Along with applications, we cover four real-world healthcare scenarios that employ GAI: visual snow syndrome diagnosis, molecular drug optimization, medical education, and dentistry. We also provide an elaborate discussion on seven healthcare-customized LLMs like Med-PaLM, BioGPT, DeepHealth, etc.,Since GAI is still evolving, it poses challenges like the lack of professional expertise in decision making, risk of patient data privacy, issues in integrating with existing healthcare systems, and the problem of data bias which are elaborated on in this work along with several other challenges. We also put forward multiple directions for future research in GAI for healthcare.","<method>Generative artificial intelligence (GAI)</method>, <method>GAI models like ChatGPT</method>, <method>GAI models like DALL-E</method>, <method>healthcare-customized LLMs like Med-PaLM</method>, <method>healthcare-customized LLMs like BioGPT</method>, <method>healthcare-customized LLMs like DeepHealth</method>"
2024,https://openalex.org/W4390837884,Engineering,The deep learning applications in IoT-based bio- and medical informatics: a systematic literature review,"Abstract Nowadays, machine learning (ML) has attained a high level of achievement in many contexts. Considering the significance of ML in medical and bioinformatics owing to its accuracy, many investigators discussed multiple solutions for developing the function of medical and bioinformatics challenges using deep learning (DL) techniques. The importance of DL in Internet of Things (IoT)-based bio- and medical informatics lies in its ability to analyze and interpret large amounts of complex and diverse data in real time, providing insights that can improve healthcare outcomes and increase efficiency in the healthcare industry. Several applications of DL in IoT-based bio- and medical informatics include diagnosis, treatment recommendation, clinical decision support, image analysis, wearable monitoring, and drug discovery. The review aims to comprehensively evaluate and synthesize the existing body of the literature on applying deep learning in the intersection of the IoT with bio- and medical informatics. In this paper, we categorized the most cutting-edge DL solutions for medical and bioinformatics issues into five categories based on the DL technique utilized: convolutional neural network , recurrent neural network , generative adversarial network , multilayer perception , and hybrid methods. A systematic literature review was applied to study each one in terms of effective properties, like the main idea, benefits, drawbacks, methods, simulation environment, and datasets. After that, cutting-edge research on DL approaches and applications for bioinformatics concerns was emphasized. In addition, several challenges that contributed to DL implementation for medical and bioinformatics have been addressed, which are predicted to motivate more studies to develop medical and bioinformatics research progressively. According to the findings, most articles are evaluated using features like accuracy, sensitivity, specificity, F -score, latency, adaptability, and scalability.","<method>convolutional neural network</method>, <method>recurrent neural network</method>, <method>generative adversarial network</method>, <method>multilayer perception</method>, <method>hybrid methods</method>"
2024,https://openalex.org/W4391018556,Engineering,Battery safety: Machine learning-based prognostics,"Lithium-ion batteries play a pivotal role in a wide range of applications, from electronic devices to large-scale electrified transportation systems and grid-scale energy storage. Nevertheless, they are vulnerable to both progressive aging and unexpected failures, which can result in catastrophic events such as explosions or fires. Given their expanding global presence, the safety of these batteries and potential hazards from serious malfunctions are now major public concerns. Over the past decade, scholars and industry experts are intensively exploring methods to monitor battery safety, spanning from materials to cell, pack and system levels and across various spectral, spatial, and temporal scopes. In this Review, we start by summarizing the mechanisms and nature of battery failures. Following this, we explore the intricacies in predicting battery system evolution and delve into the specialized knowledge essential for data-driven, machine learning models. We offer an exhaustive review spotlighting the latest strides in battery fault diagnosis and failure prognosis via an array of machine learning approaches. Our discussion encompasses: (1) supervised and reinforcement learning integrated with battery models, apt for predicting faults/failures and probing into failure causes and safety protocols at the cell level; (2) unsupervised, semi-supervised, and self-supervised learning, advantageous for harnessing vast data sets from battery modules/packs; (3) few-shot learning tailored for gleaning insights from scarce examples, alongside physics-informed machine learning to bolster model generalization and optimize training in data-scarce settings. We conclude by casting light on the prospective horizons of comprehensive, real-world battery prognostics and management.","<method>supervised learning</method>, <method>reinforcement learning</method>, <method>unsupervised learning</method>, <method>semi-supervised learning</method>, <method>self-supervised learning</method>, <method>few-shot learning</method>, <method>physics-informed machine learning</method>"
2024,https://openalex.org/W4394018687,Engineering,"Exploring the synergies between collaborative robotics, digital twins, augmentation, and industry 5.0 for smart manufacturing: A state-of-the-art review","Industry 5.0 aims at establishing an inclusive, smart and sustainable production process that encourages human creativity and expertise by leveraging enhanced automation and machine intelligence. Collaborative robotics, or ""cobotics"",is a major enabling technology of Industry 5.0, which aspires at improving human dexterity by elevating robots to extensions of human capabilities and, ultimately, even as team members. A pivotal element that has the potential to operate as an interface for the teaming aspiration of Industry 5.0 is the adoption of novel technologies such as virtual reality (VR), augmented reality (AR), mixed reality (MR) and haptics, together known as ""augmentation"". Industry 5.0 also benefit from Digital Twins (DTs), which are digital representations of a physical assets that serves as their counterpart — or twins. Another essential component of Industry 5.0 is artificial intelligence (AI), which has the potential to create a more intelligent and efficient manufacturing process. In this study, a systematic review of the state of the art is presented to explore the synergies between cobots, DTs, augmentation, and Industry 5.0 for smart manufacturing. To the best of the author's knowledge, this is the first attempt in the literature to provide a comprehensive review of the synergies between the various components of Industry 5.0. This work aims at increasing the global efforts to realize the large variety of application possibilities offered by Industry 5.0 and to provide an up-to-date reference as a stepping-stone for new research and development within this field.",<method>artificial intelligence (AI)</method>
2024,https://openalex.org/W4391383756,Engineering,High-quality semiconductor fibres via mechanical design,"Abstract Recent breakthroughs in fibre technology have enabled the assembly of functional materials with intimate interfaces into a single fibre with specific geometries 1–11 , delivering diverse functionalities over a large area, for example, serving as sensors, actuators, energy harvesting and storage, display, and healthcare apparatus 12–17 . As semiconductors are the critical component that governs device performance, the selection, control and engineering of semiconductors inside fibres are the key pathways to enabling high-performance functional fibres. However, owing to stress development and capillary instability in the high-yield fibre thermal drawing, both cracks and deformations in the semiconductor cores considerably affect the performance of these fibres. Here we report a mechanical design to achieve ultralong, fracture-free and perturbation-free semiconductor fibres, guided by a study on stress development and capillary instability at three stages of the fibre formation: the viscous flow, the core crystallization and the subsequent cooling stage. Then, the exposed semiconductor wires can be integrated into a single flexible fibre with well-defined interfaces with metal electrodes, thereby achieving optoelectronic fibres and large-scale optoelectronic fabrics. This work provides fundamental insights into extreme mechanics and fluid dynamics with geometries that are inaccessible in traditional platforms, essentially addressing the increasing demand for flexible and wearable optoelectronics.",No methods found.
2024,https://openalex.org/W4391385913,Engineering,Chained machine learning model for predicting load capacity and ductility of steel fiber–reinforced concrete beams,"Abstract One of the main issues associated with steel fiber–reinforced concrete (SFRC) beams is the ability to anticipate their flexural response. With a comprehensive grid search, several stacked models (i.e., chained, parallel) consisting of various machine learning (ML) algorithms and artificial neural networks (ANNs) were developed to predict the flexural response of SFRC beams. The flexural performance of SFRC beams under bending was assessed based on 193 experimental specimens from real‐life beam models. The ML techniques were applied to predict SFRC beam responses to bending load as functions of the steel fiber properties, concrete elastic modulus, beam dimensions, and reinforcement details. The accuracy of the models was evaluated using the coefficient of determination (), mean absolute error (MAE), and root mean square error (RMSE) of actual versus predicted values. The findings revealed that the proposed technique exhibited notably superior performance, delivering faster and more accurate predictions compared to both the ANNs and parallel models. Shapley diagrams were used to analyze variable contributions quantitatively. Shapley values show that the chained model prediction of ductility index is highly affected by two other targets (peak load and peak deflection) that show the chained algorithm utilizing the prediction of previous steps for enhancing the prediction of the target feature. The proposed model can be viewed as a function of significant input variables that permit the quick assessment of the likely performance of SFRC beams in bending.","<method>stacked models (chained)</method>, <method>stacked models (parallel)</method>, <method>machine learning (ML) algorithms</method>, <method>artificial neural networks (ANNs)</method>"
2024,https://openalex.org/W4394929444,Engineering,AI literacy and its implications for prompt engineering strategies,"Artificial intelligence technologies are rapidly advancing. As part of this development, large language models (LLMs) are increasingly being used when humans interact with systems based on artificial intelligence (AI), posing both new opportunities and challenges. When interacting with LLM-based AI system in a goal-directed manner, prompt engineering has evolved as a skill of formulating precise and well-structured instructions to elicit desired responses or information from the LLM, optimizing the effectiveness of the interaction. However, research on the perspectives of non-experts using LLM-based AI systems through prompt engineering and on how AI literacy affects prompting behavior is lacking. This aspect is particularly important when considering the implications of LLMs in the context of higher education. In this present study, we address this issue, introduce a skill-based approach to prompt engineering, and explicitly consider the role of non-experts' AI literacy (students) in their prompt engineering skills. We also provide qualitative insights into students' intuitive behaviors towards LLM-based AI systems. The results show that higher-quality prompt engineering skills predict the quality of LLM output, suggesting that prompt engineering is indeed a required skill for the goal-directed use of generative AI tools. In addition, the results show that certain aspects of AI literacy can play a role in higher quality prompt engineering and targeted adaptation of LLMs within education. We, therefore, argue for the integration of AI educational content into current curricula to enable a hybrid intelligent society in which students can effectively use generative AI tools such as ChatGPT.",No methods found.
2024,https://openalex.org/W4391968719,Engineering,Artificial intelligence and IoT driven technologies for environmental pollution monitoring and management,"Detecting hazardous substances in the environment is crucial for protecting human wellbeing and ecosystems. As technology continues to advance, artificial intelligence (AI) has emerged as a promising tool for creating sensors that can effectively detect and analyze these hazardous substances. The increasing advancements in information technology have led to a growing interest in utilizing this technology for environmental pollution detection. AI-driven sensor systems, AI and Internet of Things (IoT) can be efficiently used for environmental monitoring, such as those for detecting air pollutants, water contaminants, and soil toxins. With the increasing concerns about the detrimental impact of legacy and emerging hazardous substances on ecosystems and human health, it is necessary to develop advanced monitoring systems that can efficiently detect, analyze, and respond to potential risks. Therefore, this review aims to explore recent advancements in using AI, sensors and IOTs for environmental pollution monitoring, taking into account the complexities of predicting and tracking pollution changes due to the dynamic nature of the environment. Integrating machine learning (ML) methods has the potential to revolutionize environmental science, but it also poses challenges. Important considerations include balancing model performance and interpretability, understanding ML model requirements, selecting appropriate models, and addressing concerns related to data sharing. Through examining these issues, this study seeks to highlight the latest trends in leveraging AI and IOT for environmental pollution monitoring.","<method>artificial intelligence (AI)</method>, <method>machine learning (ML) methods</method>"
2024,https://openalex.org/W4392386497,Engineering,Clinical applications of artificial intelligence in robotic surgery,"Abstract Artificial intelligence (AI) is revolutionizing nearly every aspect of modern life. In the medical field, robotic surgery is the sector with some of the most innovative and impactful advancements. In this narrative review, we outline recent contributions of AI to the field of robotic surgery with a particular focus on intraoperative enhancement. AI modeling is allowing surgeons to have advanced intraoperative metrics such as force and tactile measurements, enhanced detection of positive surgical margins, and even allowing for the complete automation of certain steps in surgical procedures. AI is also Query revolutionizing the field of surgical education. AI modeling applied to intraoperative surgical video feeds and instrument kinematics data is allowing for the generation of automated skills assessments. AI also shows promise for the generation and delivery of highly specialized intraoperative surgical feedback for training surgeons. Although the adoption and integration of AI show promise in robotic surgery, it raises important, complex ethical questions. Frameworks for thinking through ethical dilemmas raised by AI are outlined in this review. AI enhancements in robotic surgery is some of the most groundbreaking research happening today, and the studies outlined in this review represent some of the most exciting innovations in recent years.",<method>AI modeling</method>
2024,https://openalex.org/W4392130427,Engineering,Artificial intelligence and machine learning applications in the project lifecycle of the construction industry: A comprehensive review,"The construction industry faces many challenges, including schedule and cost overruns, productivity constraints, and workforce shortages. Compared to other sectors, it lags in digitalization in every project phase. Artificial Intelligence (AI) and Machine Learning (ML) have emerged as transformative technologies revolutionizing the construction sector. However, a discernible gap persists in systematically categorizing the applications of these technologies throughout the various phases of the construction project life cycle. In response to this gap, this research aims to present a thorough assessment of the deployment of AI and ML across diverse phases in construction projects, with the ultimate goal of furnishing valuable insights for the effective integration of these intelligent systems within the construction sector. A thorough literature review was performed to identify AI and ML applications in the building sector. After scrutinizing the literature, the applications of AI and ML were presented based on a construction project life cycle. A critical review of existing literature on AI and ML applications in the building industry showed that AI and ML applications are more frequent in the planning and construction stages. Moreover, the opportunities for AI and ML applications in other stages were discussed based on the life cycle categorization and presented in this study. The practical contribution of the study lies in providing valuable insights for the effective integration of intelligent systems within the construction sector. Academically, the research contributes by conducting a thorough literature review, categorizing AI and ML applications based on the construction project life cycle, and identifying opportunities for their deployment in different stages.",No methods found.
2024,https://openalex.org/W4396973073,Engineering,DA-TransUNet: integrating spatial and channel dual attention with transformer U-net for medical image segmentation,"Accurate medical image segmentation is critical for disease quantification and treatment evaluation. While traditional U-Net architectures and their transformer-integrated variants excel in automated segmentation tasks. Existing models also struggle with parameter efficiency and computational complexity, often due to the extensive use of Transformers. However, they lack the ability to harness the image’s intrinsic position and channel features. Research employing Dual Attention mechanisms of position and channel have not been specifically optimized for the high-detail demands of medical images. To address these issues, this study proposes a novel deep medical image segmentation framework, called DA-TransUNet, aiming to integrate the Transformer and dual attention block (DA-Block) into the traditional U-shaped architecture. Also, DA-TransUNet tailored for the high-detail requirements of medical images, optimizes the intermittent channels of Dual Attention (DA) and employs DA in each skip-connection to effectively filter out irrelevant information. This integration significantly enhances the model’s capability to extract features, thereby improving the performance of medical image segmentation. DA-TransUNet is validated in medical image segmentation tasks, consistently outperforming state-of-the-art techniques across 5 datasets. In summary, DA-TransUNet has made significant strides in medical image segmentation, offering new insights into existing techniques. It strengthens model performance from the perspective of image features, thereby advancing the development of high-precision automated medical image diagnosis. The codes and parameters of our model will be publicly available at https://github.com/SUN-1024/DA-TransUnet .","<method>U-Net architectures</method>, <method>Transformer-integrated variants</method>, <method>Dual Attention mechanisms</method>, <method>DA-TransUNet</method>"
2024,https://openalex.org/W4401819987,Engineering,"Artificial intelligence for geoscience: Progress, challenges and perspectives","Public summary•What does AI bring to geoscience? AI has been accelerating and deepening our understanding of Earth Systems in an unprecedented way, including the atmosphere, lithosphere, hydrosphere, cryosphere, biosphere, anthroposphere and the interactions between spheres.•What are the noteworthy challenges of AI in geoscience? As we embrace the huge potential of AI in geoscience, several challenges arise including reliability and interpretability, ethical issues, data security, and high demand and cost.•What is the future of AI in geoscience? The synergy between traditional principles and modern AI-driven techniques holds immense promise and will shape the trajectory of geoscience in upcoming years.AbstractThis paper explores the evolution of geoscientific inquiry, tracing the progression from traditional physics-based models to modern data-driven approaches facilitated by significant advancements in artificial intelligence (AI) and data collection techniques. Traditional models, which are grounded in physical and numerical frameworks, provide robust explanations by explicitly reconstructing underlying physical processes. However, their limitations in comprehensively capturing Earth's complexities and uncertainties pose challenges in optimization and real-world applicability. In contrast, contemporary data-driven models, particularly those utilizing machine learning (ML) and deep learning (DL), leverage extensive geoscience data to glean insights without requiring exhaustive theoretical knowledge. ML techniques have shown promise in addressing Earth science-related questions. Nevertheless, challenges such as data scarcity, computational demands, data privacy concerns, and the ""black-box"" nature of AI models hinder their seamless integration into geoscience. The integration of physics-based and data-driven methodologies into hybrid models presents an alternative paradigm. These models, which incorporate domain knowledge to guide AI methodologies, demonstrate enhanced efficiency and performance with reduced training data requirements. This review provides a comprehensive overview of geoscientific research paradigms, emphasizing untapped opportunities at the intersection of advanced AI techniques and geoscience. It examines major methodologies, showcases advances in large-scale models, and discusses the challenges and prospects that will shape the future landscape of AI in geoscience. The paper outlines a dynamic field ripe with possibilities, poised to unlock new understandings of Earth's complexities and further advance geoscience exploration.Graphical abstract","<method>machine learning (ML)</method>, <method>deep learning (DL)</method>, <method>hybrid models</method>"
2024,https://openalex.org/W4398243924,Engineering,Optimizing renewable energy systems through artificial intelligence: Review and future prospects,"The global transition toward sustainable energy sources has prompted a surge in the integration of renewable energy systems (RES) into existing power grids. To improve the efficiency, reliability, and economic viability of these systems, the synergistic application of artificial intelligence (AI) methods has emerged as a promising avenue. This study presents a comprehensive review of the current state of research at the intersection of renewable energy and AI, highlighting key methodologies, challenges, and achievements. It covers a spectrum of AI utilizations in optimizing different facets of RES, including resource assessment, energy forecasting, system monitoring, control strategies, and grid integration. Machine learning algorithms, neural networks, and optimization techniques are explored for their role in complex data sets, enhancing predictive capabilities, and dynamically adapting RES. Furthermore, the study discusses the challenges faced in the implementation of AI in RES, such as data variability, model interpretability, and real-time adaptability. The potential benefits of overcoming these challenges include increased energy yield, reduced operational costs, and improved grid stability. The review concludes with an exploration of prospects and emerging trends in the field. Anticipated advancements in AI, such as explainable AI, reinforcement learning, and edge computing, are discussed in the context of their potential impact on optimizing RES. Additionally, the paper envisions the integration of AI-driven solutions into smart grids, decentralized energy systems, and the development of autonomous energy management systems. This investigation provides important insights into the current landscape of AI applications in RES.","<method>machine learning algorithms</method>, <method>neural networks</method>, <method>optimization techniques</method>, <method>explainable AI</method>, <method>reinforcement learning</method>"
2024,https://openalex.org/W4392931267,Engineering,Dynamic Event-Triggered Control for a Class of Uncertain Strict-Feedback Systems via an Improved Adaptive Neural Networks Backstepping Approach,"This article focuses on a dynamic event-triggered adaptive neural networks backstepping control for a class of uncertain strict-feedback systems with communication constraints. The uncertain terms including external disturbances and unknown nonlinear functions are approximated by radial basis function neural networks, in which the weight update laws are obtained via the gradient descent algorithm, ensuring the local boundedness of the approximation error of neural networks. Then, to enhance the transmission efficiency of control signals, a dynamic event-triggered mechanism is introduced, which enables the dynamic adjustment of threshold parameters in response to the actual tracking performance. It is strictly proved via the Lyapunov stability criterion that the tracking error can converge to a desired small neighborhood of the origin, and all signals in the closed-loop system are bounded. Finally, the validity of the control strategy is demonstrated through a simulation example. <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Note to Practitioners</i> — In practical network control systems, control signals are typically transmitted continuously or periodically to devices through the communication network in the form of data packets. As communication networks are usually shared by various system nodes, and resources such as communication channel bandwidth and computational capabilities are limited, improving the transmission efficiency of control signals becomes a crucial design problem for controllers in network control systems. Therefore, This study introduces a control method via event-triggered sampling, aiming to enhance sampling efficiency while ensuring the stability and reliability of the system. The proposed control method is suitable for a broad category of strict-feedback nonlinear systems with communication constraints, offering notable advantages such as low-complexity design and straightforward implementation.","<method>radial basis function neural networks</method>, <method>gradient descent algorithm</method>"
2024,https://openalex.org/W4391244205,Engineering,Research on the evolution of China's photovoltaic technology innovation network from the perspective of patents,"Photovoltaic (PV) technology, as a low-carbon energy technology, is crucial to mitigating climate change and achieving sustainable development. China has the largest total number of PV technology patents in the world, but the lack of core technologies has restricted the further innovative development of China's PV industry. Therefore, it is necessary to clarify China's current PV technology accumulation to better catch up with key technology areas. To clearly describe the structural characteristics of China's PV technology innovation network, this study uses China's patent PV technology data over the past 20 years from the Incopat global patent database and analyses the structural characteristics of the network from the perspectives of one-mode and two-mode networks, using method of social network analysis (SNA). The results show that 1) the leading PV enterprises have basically formed relatively stable internal collaborations and that the scale of innovation network development has expanded rapidly, with very strong stamina; 2) with the development of China's PV industry, many innovative PV techniques have been developed by leading enterprises in the field of innovation and research and development (R&D) of PV technology, and among patent applicants with strong collaboration, kinship collaboration with investment relationships is dominant; 3) provinces participating in PV technology innovation are increasing significantly, the network is more influenced by leading nodes, and the eastern coastal provinces are pioneers in the innovation and R&D of PV technology; and 4) PV technological innovation collaboration between patent applicants and cities has changed from local collaboration to cross-regional collaboration, high-value areas are basically concentrated in the eastern coastal region of China, with scattered spatial characteristics, and cross-regional collaboration presents a ""triangular"" spatial structure, with the Yangtze River Delta, Pearl River Delta, and Beijing-Tianjin-Hebei as cores. The conclusions can provide patent information support for scientific research on energy conservation and emission reduction to achieve low-carbon goals, and can also provide reference for policy formulation of renewable energy development and green development strategies.",<method>social network analysis (SNA)</method>
2024,https://openalex.org/W4392503764,Engineering,Mental-LLM,"Advances in large language models (LLMs) have empowered a variety of applications. However, there is still a significant gap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health. In this work, we present a comprehensive evaluation of multiple LLMs on various mental health prediction tasks via online text data, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4. We conduct a broad range of experiments, covering zero-shot prompting, few-shot prompting, and instruction fine-tuning. The results indicate a promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9% on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8%. They further perform on par with the state-of-the-art task-specific language model. We also conduct an exploratory case study on LLMs' capability on mental health reasoning tasks, illustrating the promising capability of certain models such as GPT-4. We summarize our findings into a set of action guidelines for potential methods to enhance LLMs' capability for mental health tasks. Meanwhile, we also emphasize the important limitations before achieving deployability in real-world mental health settings, such as known racial and gender bias. We highlight the important ethical risks accompanying this line of research.","<method>zero-shot prompting</method>, <method>few-shot prompting</method>, <method>instruction fine-tuning</method>"
2024,https://openalex.org/W4391689754,Engineering,The use of ChatGPT in teaching and learning: a systematic review through SWOT analysis approach,"Introduction The integration of ChatGPT, an advanced AI-powered chatbot, into educational settings, has caused mixed reactions among educators. Therefore, we conducted a systematic review to explore the strengths and weaknesses of using ChatGPT and discuss the opportunities and threats of using ChatGPT in teaching and learning. Methods Following the PRISMA flowchart guidelines, 51 articles were selected among 819 studies collected from Scopus, ERIC and Google Scholar databases in the period from 2022-2023. Results The synthesis of data extracted from the 51 included articles revealed 32 topics including 13 strengths, 10 weaknesses, 5 opportunities and 4 threats of using ChatGPT in teaching and learning. We used Biggs’s Presage-Process-Product (3P) model of teaching and learning to categorize topics into three components of the 3P model. Discussion In the Presage stage, we analyzed how ChatGPT interacts with student characteristics and teaching contexts to ensure that the technology adapts effectively to diverse needs and backgrounds. In the Process stage, we analyzed how ChatGPT impacted teaching and learning activities to determine its ability to provide personalized, adaptive, and effective instructional support. Finally, in the Product stage, we evaluated how ChatGPT contributed to student learning outcomes. By carefully considering its application in each stage of teaching and learning, educators can make informed decisions, leveraging the strengths and addressing the weaknesses of ChatGPT to optimize its integration into teaching and learning processes.",No methods found
2024,https://openalex.org/W4392239564,Engineering,Human-AI collaboration patterns in AI-assisted academic writing,"Artificial Intelligence (AI) has increasingly influenced higher education, notably in academic writing where AI-powered assisting tools offer both opportunities and challenges. Recently, the rapid growth of generative AI (GAI) has brought its impacts into sharper focus, yet the dynamics of its utilisation in academic writing remain largely unexplored. This paper focuses on examining the nature of human-AI interactions in academic writing, specifically investigating the strategies doctoral students employ when collaborating with a GAI-powered assisting tool. This study involves 626 recorded activities on how ten doctoral students interact with GAI-powered assisting tool during academic writing. AI-driven learning analytics approach was adopted for three layered analyses: (1) data pre-processing and analysis with quantitative content analysis, (2) sequence analysis with Hidden Markov Model (HMM) and hierarchical sequence clustering, and (3) pattern analysis with process mining. Findings indicate that doctoral students engaging in iterative, highly interactive processes with the GAI-powered assisting tool generally achieve better performance in the writing task. In contrast, those who use GAI merely as a supplementary information source, maintaining a linear writing approach, tend to get lower writing performance. This study points to the need for further investigations into human-AI collaboration in learning in higher education, with implications for tailored educational strategies and solutions.","<method>AI-driven learning analytics</method>, <method>quantitative content analysis</method>, <method>Hidden Markov Model (HMM)</method>, <method>hierarchical sequence clustering</method>, <method>process mining</method>"
2024,https://openalex.org/W4393072087,Engineering,FI-NPI: Exploring Optimal Control in Parallel Platform Systems,"Typically, the current and speed loop closure of servo motor of the parallel platform is accomplished with incremental PI regulation. The control method has strong robustness, but the parameter tuning process is cumbersome, and it is difficult to achieve the optimal control state. In order to further optimize the performance, this paper proposes a double-loop control structure based on fuzzy integral and neuron proportional integral (FI-NPI). The structure makes full use of the control advantages of the fuzzy controller and integrator to improve the performance of speed closed-loop control. And through the feedforward branch, the speed error is used as the teacher signal for neuron supervised learning, which improves the effect of current closed-loop control. Through comparative simulation experiments, this paper verifies that the FI-NPI controller has a faster dynamic response speed than the traditional PI controller. Finally, in this paper, the FI-NPI controller is implemented in C language in the servo-driven lower computer, and the speed closed-loop test of the BLDC motor is carried out. The experimental results show that the FI-NPI double-loop controller is better than the traditional double-PI controller in performance indicators such as convergence rate and RMSE, which confirms that the FI-NPI double-loop controller is more suitable for BLDC servo control.","<method>fuzzy integral</method>, <method>neuron proportional integral (FI-NPI)</method>, <method>neuron supervised learning</method>"
2024,https://openalex.org/W4391288564,Engineering,Innovations in hydrogel-based manufacturing: A comprehensive review of direct ink writing technique for biomedical applications,"Direct ink writing (DIW) stands as a pioneering additive manufacturing technique that holds transformative potential in the field of hydrogel fabrication. This innovative approach allows for the precise deposition of hydrogel inks layer by layer, creating complex three-dimensional structures with tailored shapes, sizes, and functionalities. By harnessing the versatility of hydrogels, DIW opens up possibilities for applications spanning from tissue engineering to soft robotics and wearable devices. This comprehensive review investigates DIW as applied to hydrogels and its multifaceted applications. The paper introduces a diverse range of printing techniques while providing a thorough exploration of DIW for hydrogel-based printing. The investigation aims to explain the progress made, challenges faced, and potential trajectories that lie ahead for DIW in hydrogel-based manufacturing. The fundamental principles underlying DIW are carefully examined, specifically focusing on rheological attributes and printing parameters, prompting a comprehensive survey of the wide variety of hydrogel materials. These encompass both natural and synthetic variations, all of which can be effectively harnessed for this purpose. Furthermore, the review explores the latest applications of DIW for hydrogels in biomedical areas, with a primary focus on tissue engineering, wound dressing, and drug delivery systems. The document not only consolidates the existing state of DIW within the context of hydrogel-based manufacturing but also charts potential avenues for further research and innovative breakthroughs.",No methods found.
2024,https://openalex.org/W4399039179,Engineering,Distributed Event-Triggered Output-Feedback Time-Varying Formation Fault-Tolerant Control for Nonlinear Multi-Agent Systems,"This paper studies the event-triggered time-varying formation control problem for nonlinear multi-agent systems with actuator faults. Based on the neural network approximation technique, a neural observer is constructed to estimate the unmeasured states of systems. Then, a distributed adaptive event-triggered time-varying formation control manner is proposed utilizing the intermittent estimated states information from the agent and its neighbors. To overcome the problem that estimated states triggering leads to virtual control laws is non-differentiable, a distributed continuous control scheme under regular output-feedback is designed firstly, upon which a distributed event-triggered controller is constructed by replacing estimated states with intermittent estimated ones. It is shown that the designed event-triggered output-feedback time-varying formation fault-tolerant controller can compensate for actuator faults, and all signals in closed-loop systems are semi-globally uniformly ultimately bounded. Finally, simulation results of a practical example are given to verify the effectiveness of the proposed control manner. <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Note to Practitioners</i> —Formation control has broad application prospects in modern military and civilian fields, such as combat aircraft flying formation, satellite formation, autonomous vehicle formation, etc. In formation control systems, when agents occur actuator faults, it may break the original formation and even cause collision between agents. As a result, the security of formation control systems is facing great challenges in practical engineering applications. On the other hand, communication bandwidth is limited in practical engineering systems, and how to make systems quickly form formation under the limited communication bandwidth has become a key topic. Inspired by the above discussions, a distributed state-triggered output-feedback time-varying formation fault-tolerant control scheme is designed in this paper, in which actuator faults are compensated by using adaptive technology. Meanwhile, to sufficiently save the usage of system communication resources, a dual-channel event-triggered mechanism is designed.","<method>neural network approximation technique</method>, <method>neural observer</method>, <method>distributed adaptive event-triggered time-varying formation control</method>, <method>distributed continuous control scheme under regular output-feedback</method>, <method>distributed event-triggered controller</method>, <method>distributed state-triggered output-feedback time-varying formation fault-tolerant control scheme</method>, <method>adaptive technology</method>, <method>dual-channel event-triggered mechanism</method>"
2024,https://openalex.org/W4390770894,Engineering,"A review on microgrid optimization with meta-heuristic techniques: Scopes, trends and recommendation","Microgrids (MGs) use renewable sources to meet the growing demand for energy with increasing consumer needs and technological advancement. They operate independently as small-scale energy networks using distributed energy resources. However, the intermittent nature of renewable energy sources and poor power quality are essential operational problems that must be mitigated to improve the MG's performance. To address these challenges, researchers have introduced heuristic optimization mechanisms for MGs. However, local minima and the inability to find a global minimum in heuristic methods create errors in non-linear and nonconvex optimization, posing challenges in dealing with several operational aspects of MG such as energy management optimization, cost-effective dispatch, dependability, storage sizing, cyber-attack minimization, and grid integration. These challenges affect MG's performance by adding complexity to the management of storage capacity, cost minimization, reliability assurance, and balance of renewable sources, which accelerates the need for meta-heuristic optimization algorithms (MHOAs). This paper presents a state-of-the-art review of MHOAs and their role in improving the operational performance of MGs. Firstly, the fundamentals of MG optimization are discussed to explore the scopes, requisites, and opportunities of MHOAs in MG networks. Secondly, several MHOAs in the MG domain are described, and their recent trends in MG's techno-economic analysis, load forecasting, resiliency improvement, control operation, fault diagnosis, and energy management are summarized. The summary reveals that nearly 25% of the research in these areas utilizes the particle swarm optimization method, while the genetic and grey wolf algorithms are utilized by nearly 10% and 5% of the works studied in this paper, respectively, for optimizing the MG's performance. This result summarizes that MHOA presents a system-agnostic optimization approach, offering a new avenue for enhancing the effectiveness of future MGs. Finally, we highlight some challenges that emerge during the integration of MHOAs into MGs, potentially motivating researchers to conduct further studies in this area.","<method>particle swarm optimization</method>, <method>genetic algorithm</method>, <method>grey wolf algorithm</method>"
2024,https://openalex.org/W4391505626,Engineering,Professionalizing Legal Translator Training: Prospects and Opportunities,"Legal transactions have permeated every aspect of our life. Much of this is accomplished through legal translators who, by their outputs, impact our personal and professional future. That said, this article seeks to tackle the challenges and opportunities in preparing legal translators for professional practice. The article is a quality review in its nature which adopts the descriptive approach. The interactionist perspective is adopted in this present article to examine the challenges faced by and the opportunities offered to legal translators under training. This examination is placed within the context of the rapidly evolving translation industry and its related interdisciplinary research, which covers the technology and legal translation, quality in legal translation, and training pathways for legal translators. The subjective perspective is acknowledged as the human experience is involved to explain the individual phenomena within broader context of legal translation profession. The article draws that there is a need to make changes in the legal translation status because we need to improve the translator’s perception of their role. Moreover, training models adopted to prepare legal translators have to be updated by revising the outdated practices of legal translation, and integrating the social role to face the new challenges as the translators are the intercultural mediators who facilitate the international legal communication.",No methods found.
2024,https://openalex.org/W4392465065,Engineering,A novel Swin transformer approach utilizing residual multi-layer perceptron for diagnosing brain tumors in MRI images,"Abstract Serious consequences due to brain tumors necessitate a timely and accurate diagnosis. However, obstacles such as suboptimal imaging quality, issues with data integrity, varying tumor types and stages, and potential errors in interpretation hinder the achievement of precise and prompt diagnoses. The rapid identification of brain tumors plays a pivotal role in ensuring patient safety. Deep learning-based systems hold promise in aiding radiologists to make diagnoses swiftly and accurately. In this study, we present an advanced deep learning approach based on the Swin Transformer. The proposed method introduces a novel Hybrid Shifted Windows Multi-Head Self-Attention module (HSW-MSA) along with a rescaled model. This enhancement aims to improve classification accuracy, reduce memory usage, and simplify training complexity. The Residual-based MLP (ResMLP) replaces the traditional MLP in the Swin Transformer, thereby improving accuracy, training speed, and parameter efficiency. We evaluate the Proposed-Swin model on a publicly available brain MRI dataset with four classes, using only test data. Model performance is enhanced through the application of transfer learning and data augmentation techniques for efficient and robust training. The Proposed-Swin model achieves a remarkable accuracy of 99.92%, surpassing previous research and deep learning models. This underscores the effectiveness of the Swin Transformer with HSW-MSA and ResMLP improvements in brain tumor diagnosis. This method introduces an innovative diagnostic approach using HSW-MSA and ResMLP in the Swin Transformer, offering potential support to radiologists in timely and accurate brain tumor diagnosis, ultimately improving patient outcomes and reducing risks.","<method>Deep learning-based systems</method>, <method>Swin Transformer</method>, <method>Hybrid Shifted Windows Multi-Head Self-Attention module (HSW-MSA)</method>, <method>Residual-based MLP (ResMLP)</method>, <method>transfer learning</method>, <method>data augmentation</method>"
2024,https://openalex.org/W4394935921,Engineering,Machine learning-based predictive model for thermal comfort and energy optimization in smart buildings,"In the current context of energy transition and increasing climate change, optimizing building performance has become a critical objective. Efficient energy use and occupant comfort are paramount considerations in building design and operation. To address these challenges, this study introduces a predictive model leveraging Machine Learning (ML) algorithms. The model aims to predict thermal comfort levels and optimize energy consumption in Heating, Ventilation, and Air Conditioning (HVAC) systems. Four distinct ML algorithms Support Vector Machine (SVM), Artificial Neural Network (ANN), Random Forest (RF), and EXtreme Gradient Boosting (XGBOOST) are employed for this purpose. Data for the model is collected using a network of Raspberry Pi boards equipped with multiple sensors. Performance evaluation of the ML algorithms is conducted using statistical error metrics, including, Root Mean Square Error (RMSE), Mean Square Error (MSE), Mean Absolute Error (MAE), and coefficient of determination (R2). Results reveal that the RF and XGBOOST algorithms exhibit superior performance, achieving accuracies of 96.7% and 9.64% respectively. In contrast, the SVM algorithm demonstrates inferior performance with a R2 of 81.1%. These findings underscore the predictive capability of the RF and XGBOOST model in forecasting Predicted Mean Vote (PMV) values. The proposed model holds promise for enhancing occupant thermal comfort in buildings while simultaneously optimizing energy consumption in HVAC systems. Further research could explore the practical applications of these findings in building design and operation.","<method>Support Vector Machine (SVM)</method>, <method>Artificial Neural Network (ANN)</method>, <method>Random Forest (RF)</method>, <method>EXtreme Gradient Boosting (XGBOOST)</method>"
2024,https://openalex.org/W4391071215,Engineering,Automatic assessment of text-based responses in post-secondary education: A systematic review,"Text-based open-ended questions in academic formative and summative assessments help students become deep learners and prepare them to understand concepts for a subsequent conceptual assessment. However, grading text-based questions, especially in large (>50 enrolled students) courses, is tedious and time-consuming for instructors. Text processing models continue progressing with the rapid development of Artificial Intelligence (AI) tools and Natural Language Processing (NLP) algorithms. Especially after breakthroughs in Large Language Models (LLM), there is immense potential to automate rapid assessment and feedback of text-based responses in education. This systematic review adopts a scientific and reproducible literature search strategy based on the PRISMA process using explicit inclusion and exclusion criteria to study text-based automatic assessment systems in post-secondary education, screening 838 papers and synthesizing 93 studies. To understand how text-based automatic assessment systems have been developed and applied in education in recent years, three research questions are considered: 1) What types of automated assessment systems can be identified using input, output, and processing framework? 2) What are the educational focus and research motivations of studies with automated assessment systems? 3) What are the reported research outcomes in automated assessment systems and the next steps for educational applications? All included studies are summarized and categorized according to a proposed comprehensive framework, including the input and output of the system, research motivation, and research outcomes, aiming to answer the research questions accordingly. Additionally, the typical studies of automated assessment systems, research methods, and application domains in these studies are investigated and summarized. This systematic review provides an overview of recent educational applications of text-based assessment systems for understanding the latest AI/NLP developments assisting in text-based assessments in higher education. Findings will particularly benefit researchers and educators incorporating LLMs such as ChatGPT into their educational activities.","<method>Natural Language Processing (NLP) algorithms</method>, <method>Large Language Models (LLM)</method>"
2024,https://openalex.org/W4391484310,Engineering,Multimodal Soft Robotic Actuation and Locomotion,"Diverse and adaptable modes of complex motion observed at different scales in living creatures are challenging to reproduce in robotic systems. Achieving dexterous movement in conventional robots can be difficult due to the many limitations of applying rigid materials. Robots based on soft materials are inherently deformable, compliant, adaptable, and adjustable, making soft robotics conducive to creating machines with complicated actuation and motion gaits. This review examines the mechanisms and modalities of actuation deformation in materials that respond to various stimuli. Then, strategies based on composite materials are considered to build toward actuators that combine multiple actuation modes for sophisticated movements. Examples across literature illustrate the development of soft actuators as free-moving, entirely soft-bodied robots with multiple locomotion gaits via careful manipulation of external stimuli. The review further highlights how the application of soft functional materials into robots with rigid components further enhances their locomotive abilities. Finally, taking advantage of the shape-morphing properties of soft materials, reconfigurable soft robots have shown the capacity for adaptive gaits that enable transition across environments with different locomotive modes for optimal efficiency. Overall, soft materials enable varied multimodal motion in actuators and robots, positioning soft robotics to make real-world applications for intricate and challenging tasks.",No methods found.
2024,https://openalex.org/W4400020165,Engineering,"Big data, machine learning, and digital twin assisted additive manufacturing: A review","Additive manufacturing (AM) has undergone significant development over the past decades, resulting in vast amounts of data that carry valuable information. Numerous research studies have been conducted to extract insights from AM data and utilize it for optimizing various aspects such as the manufacturing process, supply chain, and real-time monitoring. Data integration into proposed digital twin frameworks and the application of machine learning techniques is expected to play pivotal roles in advancing AM in the future. In this paper, we provide an overview of machine learning and digital twin-assisted AM. On one hand, we discuss the research domain and highlight the machine-learning methods utilized in this field, including material analysis, design optimization, process parameter optimization, defect detection and monitoring, and sustainability. On the other hand, we examine the status of digital twin-assisted AM from the current research status to the technical approach and offer insights into future developments and perspectives in this area. This review paper aims to examine present research and development in the convergence of big data, machine learning, and digital twin-assisted AM. Although there are numerous review papers on machine learning for additive manufacturing and others on digital twins for AM, no existing paper has considered how these concepts are intrinsically connected and interrelated. Our paper is the first to integrate the three concepts big data, machine learning, and digital twins and propose a cohesive framework for how they can work together to improve the efficiency, accuracy, and sustainability of AM processes. By exploring latest advancements and applications within these domains, our objective is to emphasize the potential advantages and future possibilities associated with integration of these technologies in AM.",<method>machine learning</method>
2024,https://openalex.org/W4390494339,Engineering,"A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions","Automating the monitoring of industrial processes has the potential to enhance efficiency and optimize quality by promptly detecting abnormal events and thus facilitating timely interventions. Deep learning, with its capacity to discern non-trivial patterns within large datasets, plays a pivotal role in this process. Standard deep learning methods are suitable to solve a specific task given a specific type of data. During training, deep learning demands large volumes of labeled data. However, due to the dynamic nature of the industrial processes and environment, it is impractical to acquire large-scale labeled data for standard deep learning training for every slightly different case anew. Deep transfer learning offers a solution to this problem. By leveraging knowledge from related tasks and accounting for variations in data distributions, the transfer learning framework solves new tasks with little or even no additional labeled data. The approach bypasses the need to retrain a model from scratch for every new setup and dramatically reduces the labeled data requirement. This survey first provides an in-depth review of deep transfer learning, examining the problem settings of transfer learning and classifying the prevailing deep transfer learning methods. Moreover, we delve into applications of deep transfer learning in the context of a broad spectrum of time series anomaly detection tasks prevalent in primary industrial domains, e.g., manufacturing process monitoring, predictive maintenance, energy management, and infrastructure facility monitoring. We discuss the challenges and limitations of deep transfer learning in industrial contexts and conclude the survey with practical directions and actionable suggestions to address the need to leverage diverse time series data for anomaly detection in an increasingly dynamic production environment.","<method>deep learning</method>, <method>standard deep learning methods</method>, <method>deep transfer learning</method>, <method>transfer learning framework</method>"
2024,https://openalex.org/W4393162950,Engineering,Predicting the thermal distribution in a convective wavy fin using a novel training physics-informed neural network method,"Abstract Fins are widely used in many industrial applications, including heat exchangers. They benefit from a relatively economical design cost, are lightweight, and are quite miniature. Thus, this study investigates the influence of a wavy fin structure subjected to convective effects with internal heat generation. The thermal distribution, considered a steady condition in one dimension, is described by a unique implementation of a physics-informed neural network (PINN) as part of machine-learning intelligent strategies for analyzing heat transfer in a convective wavy fin. This novel research explores the use of PINNs to examine the effect of the nonlinearity of temperature equation and boundary conditions by altering the hyperparameters of the architecture. The non-linear ordinary differential equation (ODE) involved with heat transfer is reduced into a dimensionless form utilizing the non-dimensional variables to simplify the problem. Furthermore, Runge–Kutta Fehlberg’s fourth–fifth order (RKF-45) approach is implemented to evaluate the simplified equations numerically. To predict the wavy fin's heat transfer properties, an advanced neural network model is created without using a traditional data-driven approach, the ability to solve ODEs explicitly by incorporating a mean squared error-based loss function. The obtained results divulge that an increase in the thermal conductivity variable upsurges the thermal distribution. In contrast, a decrease in temperature profile is caused due to the augmentation in the convective-conductive variable values.","<method>physics-informed neural network (PINN)</method>, <method>Runge–Kutta Fehlberg’s fourth–fifth order (RKF-45) approach</method>"
2024,https://openalex.org/W4399326707,Engineering,Enhancing precision agriculture: A comprehensive review of machine learning and AI vision applications in all-terrain vehicle for farm automation,"The automation of all-terrain vehicles (ATVs) through the integration of advanced technologies such as machine learning (ML) and artificial intelligence (AI) vision has significantly changed precision agriculture. This paper aims to analyse and develop trends to provide comprehensive knowledge of the current state of ATV-based precision agriculture and the future possibilities of ML and AI. A bibliometric analysis was conducted through network diagram with keywords taken from previous publications in the domain. This review comprehensively analyses the potential of machine learning and artificial intelligence in transforming farming operations through the automation of tasks and the deployment of all-terrain vehicles. The research extensively analyses how machine learning methods have influenced several aspects of agricultural activities, such as planting, harvesting, spraying, weeding, crop monitoring, and others. AI vision systems are being researched for their ability to enhance precise and prompt decision-making in ATV-driven agricultural automation. These technologies have been thoroughly tested to show how they can improve crop yield, reducing overall investment, and make farming more efficient. Examples include machine learning-based seeding accuracy, AI-enabled crop health monitoring, and the use of AI vision for accurate pesticide application. The assessment examines challenges such as data privacy problems and scalability constraints, along with potential advancements and future prospects in the field. This will assist researchers and practitioners in making well-informed judgments regarding farming practices that are efficient, sustainable, and technologically robust.","<method>machine learning</method>, <method>artificial intelligence vision</method>, <method>machine learning methods</method>, <method>AI vision systems</method>, <method>machine learning-based seeding accuracy</method>, <method>AI-enabled crop health monitoring</method>, <method>AI vision for accurate pesticide application</method>"
2024,https://openalex.org/W4400343988,Engineering,Failure Characteristics and Cooperative Control Strategies for Gob-Side Entry Driving near an Advancing Working Face: A Case Study,"Gob-side entry driving near an advancing working face can improve the recovery rate of coal resources and keep the balance between mining and development. However, the large displacement of the gob-side entry caused by the mining dynamics of abutment pressure challenges the safety and processes of coal mining. This article takes the 15102 tailentry of Xizhang Coal Mine in Changzhi City, Shanxi Province, as an example to study the stability of the coal pillar and the failure characteristics of the surrounding rock and proposes cooperative control strategies of surrounding rock stability. Field tests indicated that when the coal pillar width was 15 m, the displacements of the entry floor, roof, coal pillar side, and solid coal side were 1121 mm, 601 mm, 783 mm, and 237 mm, respectively. A meticulously validated numerical model, incorporating a double-yield model for the gob materials and calibrated parameters, was developed to investigate the stress changes and yield zone distribution across the coal pillar with different sizes. The results of the simulation indicate that the influence range of the dynamic abutment pressure caused by mining in the upper section of gob-side entry driving is 30 m ahead and 70 m behind. When the coal pillar width increases from 7 m to 20 m, the internal stress of the coal pillar increases continuously, while the internal stress of the solid coal decreases continuously. It is estimated that the reasonable coal pillar width should be 7 m, which is subjected to a lower load. The cooperative control strategies comprising a narrow coal pillar, hydraulic fracturing roof cutting for pressure relief, and entry dynamic support were proposed and applied in the 15103 tailentry. The final displacements of the floor, roof, coal pillar side, and solid coal side were 66.01%, 62.06%, 61.05%, and 63.30% lower than that of the 15102 tailentry in the same period, respectively, which effectively controlled the stability of surrounding rock. In addition, this finding for the gob-side entry driving near an advancing working face in this study can potentially be applied to other similar projects.",No methods found.
2024,https://openalex.org/W4391527655,Engineering,"Micro(nano)plastics in the Human Body: Sources, Occurrences, Fates, and Health Risks","The increasing global attention on micro(nano)plastics (MNPs) is a result of their ubiquity in the water, air, soil, and biosphere, exposing humans to MNPs on a daily basis and threatening human health. However, crucial data on MNPs in the human body, including the sources, occurrences, behaviors, and health risks, are limited, which greatly impedes any systematic assessment of their impact on the human body. To further understand the effects of MNPs on the human body, we must identify existing knowledge gaps that need to be immediately addressed and provide potential solutions to these issues. Herein, we examined the current literature on the sources, occurrences, and behaviors of MNPs in the human body as well as their potential health risks. Furthermore, we identified key knowledge gaps that must be resolved to comprehensively assess the effects of MNPs on human health. Additionally, we addressed that the complexity of MNPs and the lack of efficient analytical methods are the main barriers impeding current investigations on MNPs in the human body, necessitating the development of a standard and unified analytical method. Finally, we highlighted the need for interdisciplinary studies from environmental, biological, medical, chemical, computer, and material scientists to fill these knowledge gaps and drive further research. Considering the inevitability and daily occurrence of human exposure to MNPs, more studies are urgently required to enhance our understanding of their potential negative effects on human health.",No methods found.
2024,https://openalex.org/W4391610762,Engineering,Insights into Civilian Electromagnetic Absorption Materials: Challenges and Innovative Solutions,"Abstract The emergence and widespread adoption of fifth‐generation wireless technology in civilian sectors have advanced the information revolution, while simultaneously leading to a variety of negative impacts. Foremost issues in the civilian domain include electromagnetic (EM) pollution, interference, and the inefficient use of EM energy. Despite growing global awareness, progress in devising effective solutions is sluggish, underlining the complexity of these issues. The delay in addressing EM concerns arises from several factors, including a considerable gap in comprehensive understanding and the absence of a unified stance on these critical issues. Moreover, the emergence of new challenges like EM interference and energy inefficiency indicates a clear need for innovative ideas and approaches. Addressing these civilian issues extends beyond the inherent properties of EM absorbers, requiring combination of tailored macro‐structural designs and external energy field management for a comprehensive solution. This situation emphasizes the urgent need for diverse strategies and creative thinking to navigate the challenges posed by the civilian EM issues. This perspective underscores these identified challenges and systematically proposes practical approaches to each, aiming to foster both conversation and action in this crucial area. The insights provided are expected to substantially aid in confronting and alleviating these pressing EM concerns.",No methods found.
2024,https://openalex.org/W4400093331,Engineering,Human-Centered and Sustainable Artificial Intelligence in Industry 5.0: Challenges and Perspectives,"The aim of this position paper is to identify a specific focus and the major challenges related to the human-centered artificial intelligence (HCAI) approach in the field of Industry 5.0 and the circular economy. A first step towards the opening of a line of research is necessary to aggregate multidisciplinary and interdisciplinary skills to promote and take into consideration the different aspects related to this topic, from the more technical and engineering aspects to the social ones and the repercussions in terms of sustainability. The proposal and vision of this preliminary work is to identify and discuss a suitable field for such interaction. This field has been identified, specifically, within additive manufacturing (AM) in the context of Industry 5.0. Additive manufacturing (AM), is a disruptive opportunity for more sustainable production systems that can be better optimized with AI, becoming an ideal platform for interconnection between different levels of application and integration of HCAI concepts, and at the same time able to prove them. In this context, two prospective areas with a high application impact of HCAI are those of AM-oriented supply chain and product customization in the AM field, enabled by a plethora of recently emerging technologies such as the internet of things, cloud and edge computing, and next-generation networks (5G). The paper concludes with the challenges HCAI poses to public policymakers, who face significant policy challenges in regulating artificial intelligence, and addressing the socioeconomic and technological impacts. Decision-makers are required to address these challenges by adopting some tentative policy recommendations.",No methods found.
2024,https://openalex.org/W4390604402,Engineering,A novel and dynamic land use/cover change research framework based on an improved PLUS model and a fuzzy multiobjective programming model,"Spatial reconstruction and scenario simulation of historical processes and future trends of land use/cover change (LUCC) can help to reveal the historical background of land conversion and the spatial distribution of future land. Moreover, there is a close relationship between the spatiotemporal dynamics of land use/cover and changes in different ecosystem services (ESs). Using this relationship to simulate future land use scenarios is important. In this study, an LUCC dynamic analysis framework (LSTM-PLUS-FMOP) was constructed based on a deep learning time series forecasting model (LSTM), a parallelized urban land use simulation (PLUS) model and a fuzzy multiobjective programming (FMOP) model. The PLUS model was used to analyze the driving mechanism of land expansion and explore the land conversion pattern. In addition, three land conversion scenarios were established: natural land expansion (NLE), economic development priority (EDP) and regional sustainable development (RSD). The FMOP model and the relationship between LUCC and ES were used to perform a spatial simulation of land conversion. The uncertainty parameters in the model were treated by intuitionistic fuzzy numbers (IFSs). This study applied the constructed framework to the Yellow River Basin of Shaanxi Province (YRB-SX). The results showed that (1) from 2000 to 2020, the cropland area of the YRB-SX continuously decreased by 12.67 × 104 ha, while the built-up area continuously increased by 28.25 × 104 ha. The net reduction in woodland and grassland area was 13.90 × 104 ha. (2) The relative error range of land prediction using the LSTM model was 0.0003– 0.0042. This model had better accuracy than the Markov chain prediction model. (3) The cropland area decreased by 0.26% (NLE), 0.85% (EDP) and 1.68% (RSD) under the three scenarios. The built-up area increased by 25.01%, 32.76% and 14.72%, respectively. The RSD scenario followed the principles of ecological protection and spatial constraints, which mitigated the degradation of the ecosystem to some extent. This coupled simulation framework will help to obtain land allocation schemes that meet the requirements of ecological protection and provide solutions for rational land management.","<method>LSTM</method>, <method>PLUS model</method>, <method>fuzzy multiobjective programming (FMOP) model</method>"
2024,https://openalex.org/W4391168980,Engineering,Utilizing Hybrid Machine Learning and Soft Computing Techniques for Landslide Susceptibility Mapping in a Drainage Basin,"The hydrological system of thebasin of Lake Urmia is complex, deriving its supply from a network comprising 13 perennial rivers, along withnumerous small springs and direct precipitation onto the lake’s surface. Among these contributors, approximately half of the inflow is attributed to the Zarrineh River and the Simineh River. Remarkably, Lake Urmia lacks a natural outlet, with its water loss occurring solely through evaporation processes. This study employed a comprehensive methodology integrating ground surveys, remote sensing analyses, and meticulous documentation of historical landslides within the basin as primary information sources. Through this investigative approach, we preciselyidentified and geolocated a total of 512 historical landslide occurrences across the Urmia Lake drainage basin, leveraging GPS technology for precision. Thisarticle introduces a suite of hybrid machine learning predictive models, such as support-vector machine (SVM), random forest (RF), decision trees (DT), logistic regression (LR), fuzzy logic (FL), and the technique for order of preference by similarity to the ideal solution (TOPSIS). These models were strategically deployed to assess landslide susceptibility within the region. The outcomes of the landslide susceptibility assessment reveal that the main high susceptible zones for landslide occurrence are concentrated in the northwestern, northern, northeastern, and some southern and southeastern areas of the region. Moreover, when considering the implementation of predictions using different algorithms, it became evident that SVM exhibited superior performance regardingboth accuracy (0.89) and precision (0.89), followed by RF, with and accuracy of 0.83 and a precision of 0.83. However, it is noteworthy that TOPSIS yielded the lowest accuracy value among the algorithms assessed.","<method>support-vector machine (SVM)</method>, <method>random forest (RF)</method>, <method>decision trees (DT)</method>, <method>logistic regression (LR)</method>, <method>fuzzy logic (FL)</method>, <method>technique for order of preference by similarity to the ideal solution (TOPSIS)</method>"
2024,https://openalex.org/W4391473457,Engineering,"“HOT” ChatGPT: The Promise of ChatGPT in Detecting and Discriminating Hateful, Offensive, and Toxic Comments on Social Media","Harmful textual content is pervasive on social media, poisoning online communities and negatively impacting participation. A common approach to this issue is developing detection models that rely on human annotations. However, the tasks required to build such models expose annotators to harmful and offensive content and may require significant time and cost to complete. Generative AI models have the potential to understand and detect harmful textual content. We used ChatGPT to investigate this potential and compared its performance with MTurker annotations for three frequently discussed concepts related to harmful textual content on social media: Hateful, Offensive, and Toxic (HOT). We designed five prompts to interact with ChatGPT and conducted four experiments eliciting HOT classifications. Our results show that ChatGPT can achieve an accuracy of approximately 80% when compared to MTurker annotations. Specifically, the model displays a more consistent classification for non-HOT comments than HOT comments compared to human annotations. Our findings also suggest that ChatGPT classifications align with the provided HOT definitions. However, ChatGPT classifies “hateful” and “offensive” as subsets of “toxic.” Moreover, the choice of prompts used to interact with ChatGPT impacts its performance. Based on these insights, our study provides several meaningful implications for employing ChatGPT to detect HOT content, particularly regarding the reliability and consistency of its performance, its understanding and reasoning of the HOT concept, and the impact of prompts on its performance. Overall, our study provides guidance on the potential of using generative AI models for moderating large volumes of user-generated textual content on social media.","<method>Generative AI models</method>, <method>ChatGPT</method>"
2024,https://openalex.org/W4391558404,Engineering,Exploring the Potential of ChatGPT in Automated Code Refinement: An Empirical Study,"Code review is an essential activity for ensuring the quality and maintainability of software projects. However, it is a time-consuming and often error-prone task that can significantly impact the development process. Recently, ChatGPT, a cutting-edge language model, has demonstrated impressive performance in various natural language processing tasks, suggesting its potential to automate code review processes. However, it is still unclear how well ChatGPT performs in code review tasks. To fill this gap, in this paper, we conduct the first empirical study to understand the capabilities of ChatGPT in code review tasks, specifically focusing on automated code refinement based on given code reviews. To conduct the study, we select the existing benchmark CodeReview and construct a new code review dataset with high quality. We use CodeReviewer, a state-of-the-art code review tool, as a baseline for comparison with ChatGPT. Our results show that ChatGPT outperforms CodeReviewer in code refinement tasks. Specifically, our results show that ChatGPT achieves higher EM and BLEU scores of 22.78 and 76.44 respectively, while the state-of-the-art method achieves only 15.50 and 62.88 on a high-quality code review dataset. We further identify the root causes for ChatGPT's underperformance and propose several strategies to mitigate these challenges. Our study provides insights into the potential of ChatGPT in automating the code review process, and highlights the potential research directions.","<method>ChatGPT</method>, <method>CodeReviewer</method>"
2024,https://openalex.org/W4391831565,Engineering,Comparison of Random Forest and XGBoost Classifiers Using Integrated Optical and SAR Features for Mapping Urban Impervious Surface,"The integration of optical and SAR datasets through ensemble machine learning models shows promising results in urban remote sensing applications. The integration of multi-sensor datasets enhances the accuracy of information extraction. This research presents a comparison of two ensemble machine learning classifiers (random forest and extreme gradient boost (XGBoost)) classifiers using an integration of optical and SAR features and simple layer stacking (SLS) techniques. Therefore, Sentinel-1 (SAR) and Landsat 8 (optical) datasets were used with SAR textures and enhanced modified indices to extract features for the year 2023. The classification process utilized two machine learning algorithms, random forest and XGBoost, for urban impervious surface extraction. The study focused on three significant East Asian cities with diverse urban dynamics: Jakarta, Manila, and Seoul. This research proposed a novel index called the Normalized Blue Water Index (NBWI), which distinguishes water from other features and was utilized as an optical feature. Results showed an overall accuracy of 81% for UIS classification using XGBoost and 77% with RF while classifying land use land cover into four major classes (water, vegetation, bare soil, and urban impervious). However, the proposed framework with the XGBoost classifier outperformed the RF algorithm and Dynamic World (DW) data product and comparatively showed higher classification accuracy. Still, all three results show poor separability with bare soil class compared to ground truth data. XGBoost outperformed random forest and Dynamic World in classification accuracy, highlighting its potential use in urban remote sensing applications.","<method>ensemble machine learning models</method>, <method>random forest</method>, <method>extreme gradient boost (XGBoost)</method>, <method>simple layer stacking (SLS)</method>"
2024,https://openalex.org/W4392543906,Engineering,A Multilevel Multimodal Fusion Transformer for Remote Sensing Semantic Segmentation,"Accurate semantic segmentation of remote sensing data plays a crucial role in the success of geoscience research and applications. Recently, multimodal fusion-based segmentation models have attracted much attention due to their outstanding performance as compared to conventional single-modal techniques. However, most of these models perform their fusion operation using convolutional neural networks (CNN) or the vision transformer (Vit), resulting in insufficient local-global contextual modeling and representative capabilities. In this work, a multilevel multimodal fusion scheme called FTransUNet is proposed to provide a robust and effective multimodal fusion backbone for semantic segmentation by integrating both CNN and Vit into one unified fusion framework. Firstly, the shallow-level features are first extracted and fused through convolutional layers and shallow-level feature fusion (SFF) modules. After that, deep-level features characterizing semantic information and spatial relationships are extracted and fused by a well-designed Fusion Vit (FVit). It applies Adaptively Mutually Boosted Attention (Ada-MBA) layers and Self-Attention (SA) layers alternately in a three-stage scheme to learn cross-modality representations of high inter-class separability and low intra-class variations. Specifically, the proposed Ada-MBA computes SA and Cross-Attention (CA) in parallel to enhance intra- and cross-modality contextual information simultaneously while steering attention distribution towards semantic-aware regions. As a result, FTransUNet can fuse shallow-level and deep-level features in a multilevel manner, taking full advantage of CNN and transformer to accurately characterize local details and global semantics, respectively. Extensive experiments confirm the superior performance of the proposed FTransUNet compared with other multimodal fusion approaches on two fine-resolution remote sensing datasets, namely ISPRS Vaihingen and Potsdam. The source code in this work is available at https://github.com/sstary/SSRS.","<method>convolutional neural networks (CNN)</method>, <method>vision transformer (Vit)</method>, <method>FTransUNet</method>, <method>shallow-level feature fusion (SFF) modules</method>, <method>Fusion Vit (FVit)</method>, <method>Adaptively Mutually Boosted Attention (Ada-MBA) layers</method>, <method>Self-Attention (SA) layers</method>, <method>Cross-Attention (CA)</method>"
2024,https://openalex.org/W4392627614,Engineering,How to conduct a bibliometric content analysis: Guidelines and contributions of content co‐occurrence or co‐word literature reviews,"Abstract Literature reviews summarize existing literature, uncover research gaps, and offer future research directions, thus aiding in theoretical and methodological development. Informetric research including bibliometric, scientometric, webometric, cybermetric, patentometric, and altmetric methods are becoming increasingly prevalent in conducting literature review studies. Looking at the common informetric literature review methods—citation, co‐citation, co‐author, bibliographic coupling, and content co‐occurrence analyses, this study aims to serve as a guide in using content co‐occurrence also known as co‐word analysis to conduct literature reviews. This study outlines a variety of informetric research methods and how they are utilized to conduct review and evidence‐based conceptual studies. In addition to the analyses, the study highlights different informetric software packages like Bibliometrix, Biblioshiny, Leximancer, NVivo, and CiteSpace including their comparison. The study further discusses contributions of algorithm‐based content analyses including offering taxonomies, definitions, classifications, typologies, comparisons, and theoretical development to constitute integrative literature reviews. Finally, this study offers step‐by‐step guidelines for conducting a review study using VOSviewer content co‐occurrence analysis while providing a systems view of informetric research in social science. The study also notes the emergence of generative artificial intelligence (AI) like Open AI's ChatGPT, Google's Bard, Elicit, Scite, Research Rabbit, and ChatPDF among others, and its potential in contributing to the literature review methods and, as such, being an interesting direction for future research.","<method>content co‐occurrence analysis (co‐word analysis)</method>, <method>algorithm‐based content analyses</method>, <method>generative artificial intelligence (AI)</method>"
2024,https://openalex.org/W4390486945,Engineering,4400 TEU cargo ship dynamic analysis by Gaidai reliability method,"Abstract Modern cargo vessel transport constitutes an important part of global economy; hence it is of paramount importance to develop novel, more efficient reliability methods for cargo ships, especially if onboard recorded data is available. Classic reliability methods, dealing with timeseries, do not have the advantage of dealing efficiently with system high dimensionality and cross-correlation between different dimensions. This study validates novel structural reliability method suitable for multi-dimensional structural systems versus a well-established bivariate statistical method. An example of this reliability study was a chosen container ship subjected to large deck panel stresses during sailing. Risk of losing containers, due to extreme motions is the primary concern for ship cargo transport. Due to non-stationarity and complicated nonlinearities of both waves and ship motions, it is challenging to model such a phenomenon. In the case of extreme motions, the role of nonlinearities dramatically increases, activating effects of second and higher order. Moreover, laboratory tests may also be questioned. Therefore, data measured on actual ships during their voyages in harsh weather provides a unique insight into statistics of ship motions. This study aimed at benchmarking and validation of the state-of-the-art method, which enables extraction of the necessary information about the extreme system dynamics from onboard measured time histories. The method proposed in this study opens up broad possibilities of predicting simply, yet efficiently potential failure or structural damage risks for the nonlinear multi-dimensional cargo vessel dynamic systems as a whole. Note that advocated novel reliability method can be used for a wide range of complex engineering systems, thus not limited to cargo ship only.",No methods found.
2024,https://openalex.org/W4390584313,Engineering,A Conceptual Model for Inclusive Technology: Advancing Disability Inclusion through Artificial Intelligence,"Artificial intelligence (AI) has ushered in transformative changes, championing inclusion and accessibility for individuals with disabilities. This article delves into the remarkable AI-driven solutions that have revolutionized their lives across various domains. From assistive technologies such as voice recognition and AI-powered smart glasses catering to diverse needs, to healthcare benefiting from early disease detection algorithms and wearable devices that monitor vital signs and alert caregivers in emergencies, AI has steered in significant enhancements. Moreover, AI-driven prosthetics and exoskeletons have substantially improved mobility for those with limb impairments. The realm of education has not been left untouched, with AI tools creating inclusive learning environments that adapt to individual learning styles, paving the way for academic success among students with disabilities. However, the boundless potential of AI also presents ethical concerns and challenges. Issues like safeguarding data privacy, mitigating algorithmic bias, and bridging the digital divide must be thoughtfully addressed to fully harness AI’s potential in empowering individuals with disabilities. To complement these achievements, a robust conceptual model for AI disability inclusion serves as the theoretical framework, guiding the development of tailored AI solutions. By striking a harmonious balance between innovation and ethics, AI has the power to significantly enhance the overall quality of life for individuals with disabilities across a spectrum of vital areas.","<method>voice recognition</method>, <method>early disease detection algorithms</method>"
2024,https://openalex.org/W4390665705,Engineering,The ethical implications of using generative chatbots in higher education,"Incorporating artificial intelligence (AI) into education, specifically through generative chatbots, can transform teaching and learning for education professionals in both administrative and pedagogical ways. However, the ethical implications of using generative chatbots in education must be carefully considered. Ethical concerns about advanced chatbots have yet to be explored in the education sector. This short article introduces the ethical concerns associated with introducing platforms such as ChatGPT in education. The article outlines how handling sensitive student data by chatbots presents significant privacy challenges, thus requiring adherence to data protection regulations, which may not always be possible. It highlights the risk of algorithmic bias in chatbots, which could perpetuate societal biases, which can be problematic. The article also examines the balance between fostering student autonomy in learning and the potential impact on academic self-efficacy, noting the risk of over-reliance on AI for educational purposes. Plagiarism continues to emerge as a critical ethical concern, with AI-generated content threatening academic integrity. The article advocates for comprehensive measures to address these ethical issues, including clear policies, advanced plagiarism detection techniques, and innovative assessment methods. By addressing these ethical challenges, the article argues that educators, AI developers, policymakers, and students can fully harness the potential of chatbots in education, creating a more inclusive, empowering, and ethically sound educational future.",No methods found.
2024,https://openalex.org/W4390511794,Engineering,Firefly algorithm based WSN-IoT security enhancement with machine learning for intrusion detection,"Abstract A Wireless Sensor Network (WSN) aided by the Internet of Things (IoT) is a collaborative system of WSN systems and IoT networks are work to exchange, gather, and handle data. The primary objective of this collaboration is to enhance data analysis and automation to facilitate improved decision-making. Securing IoT with the assistance of WSN necessitates the implementation of protective measures to confirm the safety and reliability of the interconnected WSN and IoT components. This research significantly advances the current state of the art in IoT and WSN security by synergistically harnessing the potential of machine learning and the Firefly Algorithm. The contributions of this work are twofold: firstly, the proposed FA-ML technique exhibits an exceptional capability to enhance intrusion detection accuracy within the WSN-IoT landscape. Secondly, the amalgamation of the Firefly Algorithm and machine learning introduces a novel dimension to the domain of security-oriented optimization techniques. The implications of this research resonate across various sectors, ranging from critical infrastructure protection to industrial automation and beyond, where safeguarding the integrity of interconnected systems are of paramount importance. The amalgamation of cutting-edge machine learning and bio-inspired algorithms marks a pivotal step forward in crafting robust and intelligent security measures for the evolving landscape of IoT-driven technologies. For intrusion detection in the WSN-IoT, the FA-ML method employs a support vector machine (SVM) machine model for classification with parameter tuning accomplished using a Grey Wolf Optimizer (GWO) algorithm. The experimental evaluation is simulated using NSL-KDD Dataset, revealing the remarkable enhancement of the FA-ML technique, achieving a maximum accuracy of 99.34%. In comparison, the KNN-PSO and XGBoost models achieved lower accuracies of 96.42% and 95.36%, respectively. The findings validate the potential of the FA-ML technique as an active security solution for WSN-IoT systems, harnessing the power of machine learning and the Firefly Algorithm to bolster intrusion detection capabilities.","<method>Firefly Algorithm</method>, <method>machine learning</method>, <method>FA-ML technique</method>, <method>support vector machine (SVM)</method>, <method>Grey Wolf Optimizer (GWO)</method>, <method>KNN-PSO</method>, <method>XGBoost</method>"
2024,https://openalex.org/W4391220430,Engineering,Disturbance observer-based sliding mode control of active vertical suspension for high-speed rail vehicles,"Disturbance observer-based sliding mode control (SMC) on the active vertical suspension (AVS) is developed to suppress the car body's rigid and flexible vibration to improve ride comfort for a high-speed rail vehicle. A vertical-dynamics-intended vehicle model is foremost built with the bounce, pitch, and bending mode of the car body involved. Two sliding surfaces and disturbance observers are then designed for SMC, which only require the car body states, obviating the bogies' states. The observer is successful in estimating the effect of unknown track irregularities perturbation as well as externally straightforwardly excitations on the car body with good accuracy. Control efficacy is verified through comparison with passive cases and sky-hook controls. With modal damping below 2% in passive cases, the mode shape function, especially with large displacement at the suspension support, would enlarge the car body acceleration. Moreover, the longitudinal traction rod can significantly multiply the flexible vibration at certain speeds. Comparative studies indicate that SMC provides satisfactory outcomes in the rigid and bending vibration suppression when all the car body's three modes are considered in the sliding surface. However, it requires a high-frequency response actuator and allows a tighter tolerance of the control time delay than the sky-hook control.",No methods found.
2024,https://openalex.org/W4391512775,Engineering,Peak and ultimate stress-strain model of confined ultra-high-performance concrete (UHPC) using hybrid machine learning model with conditional tabular generative adversarial network,"Ultra-high-performance concrete (UHPC) has gained prominence owing to its exceptional physical and mechanical properties and improved sustainability, making it ideal for large-scale structural applications. While numerous analytical studies have focused on predicting the stress-strain response of unconfined UHPC, there remains a lack of a reliable model for predicting the stress-strain response of confined UHPC, which poses challenges to efficient design and broader adoption, particularly in seismically active regions. To bridge this gap, the present study introduces a framework that implements machine learning (ML) models augmented by a state-of-the-art conditional tabular generative adversarial network (CTGAN) and Optuna, which a next-generation optimization framework, to accurately predict the peak and ultimate axial stress-strain responses of UHPC confined with either normal-strength steel or high-strength steel. The Optuna-optimized CTGAN is employed to address the issue of limited data by generating synthetic datasets of hypothetical confined UHPC specimens. A comprehensive database of confined UHPC stress-strain responses was compiled from existing literature and used to condition the CTGAN. The augmented database is then leveraged to develop a hybrid ML model that integrates extreme gradient boosting, gradient boosting machine, support vector regression, and K-nearest neighbors for predicting peak and ultimate stress-strain responses of confined UHPC. The predictive accuracy of the proposed hybrid ML model is evaluated and compared with a diverse set of ML models of varying complexity, and the results demonstrate its superior performance in predicting the peak and ultimate stress-strain response of confined UHPC. Furthermore, a graphical user interface of the proposed model is developed to facilitate its practical implementation and provide a rapid, autonomous, and accurate prediction of the stress-strain response of confined UHPC at both peak and ultimate states.","<method>conditional tabular generative adversarial network (CTGAN)</method>, <method>Optuna</method>, <method>extreme gradient boosting</method>, <method>gradient boosting machine</method>, <method>support vector regression</method>, <method>K-nearest neighbors</method>"
2024,https://openalex.org/W4392462461,Engineering,Sentiment Analysis in the Age of Generative AI,"Abstract In the rapidly advancing age of Generative AI, Large Language Models (LLMs) such as ChatGPT stand at the forefront of disrupting marketing practice and research. This paper presents a comprehensive exploration of LLMs’ proficiency in sentiment analysis, a core task in marketing research for understanding consumer emotions, opinions, and perceptions. We benchmark the performance of three state-of-the-art LLMs, i.e., GPT-3.5, GPT-4, and Llama 2, against established, high-performing transfer learning models. Despite their zero-shot nature, our research reveals that LLMs can not only compete with but in some cases also surpass traditional transfer learning methods in terms of sentiment classification accuracy. We investigate the influence of textual data characteristics and analytical procedures on classification accuracy, shedding light on how data origin, text complexity, and prompting techniques impact LLM performance. We find that linguistic features such as the presence of lengthy, content-laden words improve classification performance, while other features such as single-sentence reviews and less structured social media text documents reduce performance. Further, we explore the explainability of sentiment classifications generated by LLMs. The findings indicate that LLMs, especially Llama 2, offer remarkable classification explanations, highlighting their advanced human-like reasoning capabilities. Collectively, this paper enriches the current understanding of sentiment analysis, providing valuable insights and guidance for the selection of suitable methods by marketing researchers and practitioners in the age of Generative AI.","<method>Large Language Models (LLMs)</method>, <method>GPT-3.5</method>, <method>GPT-4</method>, <method>Llama 2</method>, <method>transfer learning models</method>"
2024,https://openalex.org/W4393001808,Engineering,Multi-Source and Multi-modal Deep Network Embedding for Cross-Network Node Classification,"In recent years, to address the issue of networked data sparsity in node classification tasks, cross-network node classification (CNNC) leverages the richer information from a source network to enhance the performance of node classification in the target network, which typically has sparser information. However, in real-world applications, labeled nodes may be collected from multiple sources with multiple modalities (e.g., text, vision, and video). Naive application of single-source and single-modal CNNC methods may result in sub-optimal solutions. To this end, in this article, we propose a model called Multi-source and Multi-modal Cross-network Deep Network Embedding (M 2 CDNE) for cross-network node classification. In M 2 CDNE, we propose a deep multi-modal network embedding approach that combines the extracted deep multi-modal features to make the node vector representations network invariant. In addition, we apply dynamic adversarial adaptation to assess the significance of marginal and conditional probability distributions between each source and target network to make node vector representations label discriminative. Furthermore, we devise to classify nodes in the target network through the related source classifier and aggregate different predictions utilizing respective network weights, corresponding to the discrepancy between each source and target network. Extensive experiments performed on real-world datasets demonstrate that the proposed M 2 CDNE significantly outperforms the state-of-the-art approaches.","<method>cross-network node classification (CNNC)</method>, <method>Multi-source and Multi-modal Cross-network Deep Network Embedding (M2CDNE)</method>, <method>deep multi-modal network embedding</method>, <method>dynamic adversarial adaptation</method>"
2024,https://openalex.org/W4393933468,Engineering,The future of bioplastics in food packaging: An industrial perspective,"Plastics have formed an indispensable component of virtually every consumer good. However, they have also contributed substantially to contamination of the marine environment. The world lacks a well-meaning effort to curtail plastic waste. The plastic pollution crisis arising from inappropriate disposal or mismanaged plastic wastes has driven strategies to repurpose, reuse, and recycle plastic products. Extensive research is continuing aiming to produce a new generation of plastics that are collectively referred to as bioplastics. Ideally, bioplastics are degraded and utilized by surrounding microorganisms, eliminating them from our environment. However, biodegradable bioplastics are still only a small percentage of the global plastics market, and many bioplastics are not biodegradable. While bioplastics are good for the environment, their position as a singular and credible solution for plastic waste is debated. In addition, there are misunderstandings and misconceptions about their use, leading to lack of consumer awareness and acceptance. Food packaging is currently undergoing rapid change partly due to the commercial manufacturing of bioplastics. In order to better understand the future of bioplastics, this review will assess the drivers, and barriers of bioplastics in the context of food packaging, and the role of bioplastic packaging in the circular economy. The purpose of this review is to provide an industrial and strategic roadmap to guide researchers, plastic experts, policy makers, and regulating bodies in identifying the future development path of bioplastics.",No methods found.
2024,https://openalex.org/W4396798020,Engineering,"The triple exposure nexus of microplastic particles, plastic-associated chemicals, and environmental pollutants from a human health perspective","The presence of microplastics (MPs) is increasing at a dramatic rate globally, posing risks for exposure and subsequent potential adverse effects on human health. Apart from being physical objects, MP particles contain thousands of plastic-associated chemicals (i.e., monomers, chemical additives, and non-intentionally added substances) captured within the polymer matrix. These chemicals are often migrating from MPs and can be found in various environmental matrices and human food chains; increasing the risks for exposure and health effects. In addition to the physical and chemical attributes of MPs, plastic surfaces effectively bind exogenous chemicals, including environmental pollutants (e.g., heavy metals, persistent organic pollutants). Therefore, MPs can act as vectors of environmental pollution across air, drinking water, and food, further amplifying health risks posed by MP exposure. Critically, fragmentation of plastics in the environment increases the risk for interactions with cells, increases the presence of available surfaces to leach plastic-associated chemicals, and adsorb and transfer environmental pollutants. This review proposes the so-called triple exposure nexus approach to comprehensively map existing knowledge on interconnected health effects of MP particles, plastic-associated chemicals, and environmental pollutants. Based on the available data, there is a large knowledge gap in regard to the interactions and cumulative health effects of the triple exposure nexus. Each component of the triple nexus is known to induce genotoxicity, inflammation, and endocrine disruption, but knowledge about long-term and inter-individual health effects is lacking. Furthermore, MPs are not readily excreted from organisms after ingestion and they have been found accumulated in human blood, cardiac tissue, placenta, etc. Even though the number of studies on MPs-associated health impacts is increasing rapidly, this review underscores that there is a pressing necessity to achieve an integrated assessment of MPs' effects on human health in order to address existing and future knowledge gaps.",No methods found.
2024,https://openalex.org/W4390494361,Engineering,"A Comprehensive Survey on 5G-and-Beyond Networks With UAVs: Applications, Emerging Technologies, Regulatory Aspects, Research Trends and Challenges","The rapid advancement of fifth-generation (5G)-and-beyond networks coupled with unmanned aerial vehicles (UAVs) has opened up exciting possibilities for diverse applications and cutting-edge technologies, revolutionizing the way connections, communications, and innovations unfold in the digital age. This paper presents a comprehensive survey of the deployment scenarios, applications, emerging technologies, regulatory aspects, research trends, and challenges associated with the use of UAVs in 5G-and-beyond networks. It begins with a succinct background and motivation, followed by a systematic UAV classification and a review of relevant works. The survey covers UAV deployment scenarios, including single and multiple UAV configurations. The categorization of UAV applications in 5G is presented, along with investigations into emerging technologies for enhancing UAV communications. Regulatory considerations encompassing flight guidelines, spectrum allocation, privacy, and safety are discussed. Moreover, light is shed on the latest research trends and open challenges in the field, with promising directions for future investigations identified, concluding with a summary of key findings and contributions. This survey serves as a valuable resource for researchers, practitioners, and policymakers in the UAV and communication domains. Additionally, it offers a comprehensive foundation for informed decision-making, fostering collaboration, and driving advancements in UAV and communication technologies to address the evolving needs of our interconnected world.",No methods found.
2024,https://openalex.org/W4390533101,Engineering,A vehicular network based intelligent transport system for smart cities using machine learning algorithms,"Abstract Smart cities and the Internet of Things have enabled the integration of communicating devices for efficient decision-making. Notably, traffic congestion is one major problem faced by daily commuters in urban cities. In developed countries, specialized sensors are deployed to gather traffic information to predict traffic patterns. Any traffic updates are shared with the commuters via the Internet. Such solutions become impracticable when physical infrastructure and Internet connectivity are either non-existent or very limited. In case of developing countries, no roadside units are available and Internet connectivity is still an issue in remote areas. Internet traffic analysis is a thriving field of study due to the myriad ways in which it may be put to practical use. In the intelligent Internet-of-Vehicles (IOVs), traffic congestion can be predicted and identified using cutting-edge technologies. Using tree-based decision-tree, random-forest, extra-tree, and XGBoost machine learning (ML) strategies, this research proposes an intelligent-transport-system for the IOVs-based vehicular network traffic in a smart city set-up. The suggested system uses ensemble learning and averages the selection of crucial features to give high detection accuracy at minimal computational costs, as demonstrated by the simulation results. For IOV-based vehicular network traffic, the tree-based ML approaches with feature-selection (FS) outperformed those without FS. When contrasted to the lowest KNN accuracy of 96.6% and the highest SVM accuracy of 98.01%, the Stacking approach demonstrates superior accuracy as 99.05%.","<method>decision-tree</method>, <method>random-forest</method>, <method>extra-tree</method>, <method>XGBoost</method>, <method>ensemble learning</method>, <method>feature-selection (FS)</method>, <method>KNN</method>, <method>SVM</method>, <method>Stacking</method>"
2024,https://openalex.org/W4390817508,Engineering,Conventional to Deep Ensemble Methods for Hyperspectral Image Classification: A Comprehensive Survey,"Hyperspectral image classification has become a hot research topic. HSI has been widely used in a wide range of real-world application areas due to the in-depth spectral information stored within each pixel. Noticeably, the detailed features - i.e., a nonlinear correlation between the obtained spectral data and the correlating HSI data object, generate efficient classification results that are complex for traditional techniques. Deep Learning (DL) has recently been validated as an influential feature extractor that efficiently identifies the nonlinear issues that have arisen in various computer vision challenges. This motivates using DL for Hyperspectral Image Classification (HSIC), which shows promising results. This survey provides a brief description of DL for HSIC and compares cutting-edge methodologies in the field. We will first summarize the key challenges for HSIC, and then we will discuss the superiority of DL and DL-ensemble in addressing these issues. In this article, we divide the state-of-the-art DL methodologies and DL with ensemble into spectral features, spatial features, and combined spatial-spectral features in order to comprehensively and critically evaluate the progress (future research directions as well) of such methodologies for HSIC. Furthermore, we will take into account that DL involves a substantial percentage of labeled training images, whereas obtaining such a number for HSI is time and cost-consuming. As a result, this survey describes some methodologies for improving the classification performance of DL techniques, which can serve as future recommendations.","<method>Deep Learning (DL)</method>, <method>DL-ensemble</method>"
2024,https://openalex.org/W4392232974,Engineering,Business analytics and decision science: A review of techniques in strategic business decision making,"Business analytics and decision science have emerged as pivotal domains in enhancing strategic business decision-making processes. This review delves into various techniques that organizations employ to optimize their operations and achieve competitive advantages. At the forefront of strategic decision-making is data analytics, where vast amounts of data are analyzed to extract valuable insights. Descriptive analytics provides a historical perspective by examining past data trends, enabling businesses to understand their performance over time. This retrospective analysis serves as a foundation for predictive analytics, which utilizes statistical models and machine learning algorithms to forecast future trends and outcomes. By leveraging predictive analytics, organizations can anticipate market shifts, customer preferences, and potential risks, thereby making informed decisions. Prescriptive analytics uses predictive models to guide strategic decision-making, utilizing optimization algorithms and simulation tools to identify optimal actions. Decision science integrates analytical techniques with human judgment, focusing on consumer behavior and psychological factors to tailor marketing strategies and product offerings. Additionally, artificial intelligence (AI) and machine learning (ML) technologies are revolutionizing strategic decision-making by automating complex tasks and providing real-time insights. Natural language processing (NLP) algorithms analyze unstructured data sources, such as customer reviews and social media posts, to extract valuable information and sentiment analysis. This enables businesses to gauge customer satisfaction levels and identify areas for improvement promptly. Decision trees, regression analysis, and clustering techniques are widely used in business analytics to segment customers, identify patterns, forecast sales trends, evaluate alternatives, assess risks, and optimize resource allocation. In conclusion, business analytics and decision science offer a plethora of techniques that empower organizations to make informed, data-driven decisions. By leveraging descriptive, predictive, and prescriptive analytics, along with AI and ML technologies, businesses can navigate complex environments, capitalize on opportunities, and mitigate risks effectively. This review underscores the importance of integrating analytical techniques with human expertise to achieve strategic objectives and sustainable growth.","<method>machine learning algorithms</method>, <method>artificial intelligence (AI)</method>, <method>machine learning (ML) technologies</method>, <method>natural language processing (NLP) algorithms</method>, <method>decision trees</method>, <method>regression analysis</method>, <method>clustering techniques</method>"
2024,https://openalex.org/W4394776334,Engineering,Emerging underwater survey technologies: A review and future outlook,"Emerging underwater survey technologies are revolutionizing the way we explore and understand the underwater world. This review examines the latest advancements in underwater survey equipment, highlighting their operational benefits and potential areas for future development. Recent developments in underwater survey technologies have led to significant improvements in accuracy, efficiency, and data quality. Advanced sonar systems, such as multibeam and sidescan sonars, provide high-resolution images of the seafloor, allowing for detailed mapping of underwater features. Autonomous underwater vehicles (AUVs) equipped with sophisticated sensors and cameras enable precise data collection in challenging environments, such as deep-sea areas or complex underwater structures. The operational benefits of these technologies are vast. They allow for faster surveying, reduced costs, and improved safety for personnel. Additionally, the high-quality data obtained from these surveys can lead to better decision-making in various industries, including offshore energy, marine research, and underwater archaeology. Looking ahead, the future of underwater survey technologies is promising. There is a growing interest in developing integrated systems that combine multiple sensors and data processing capabilities to provide a more comprehensive view of underwater environments. Artificial intelligence and machine learning algorithms are also being increasingly utilized to analyze large datasets and extract valuable insights. However, several challenges remain, such as the need for better underwater communication systems, improved battery life for autonomous vehicles, and enhanced data processing capabilities. Addressing these challenges will be crucial for the continued advancement of underwater survey technologies. In conclusion, the latest advancements in underwater survey technologies offer exciting opportunities for enhancing our understanding of the underwater world. By leveraging these technologies and addressing key challenges, we can unlock new possibilities for underwater exploration and research.","<method>Artificial intelligence</method>, <method>machine learning algorithms</method>"
2024,https://openalex.org/W4395683385,Engineering,NAVIGATING THE FUTURE: INTEGRATING AI AND MACHINE LEARNING IN HR PRACTICES FOR A DIGITAL WORKFORCE,"As organizations navigate the complexities of the digital age, the role of Human Resources (HR) is evolving to meet the demands of a digital workforce. This review explores the integration of Artificial Intelligence (AI) and Machine Learning (ML) in HR practices to enhance efficiency, effectiveness, and employee satisfaction in the digital era. AI and ML technologies offer HR departments the opportunity to streamline operations, improve decision-making processes, and enhance employee experiences. By leveraging AI and ML, HR professionals can automate routine tasks such as recruitment, onboarding, training, and performance evaluation, allowing them to focus on more strategic initiatives that drive organizational success. One of the key advantages of integrating AI and ML in HR practices is the ability to personalize employee experiences. These technologies can analyze large volumes of data to identify patterns and trends, enabling HR professionals to tailor programs and policies to meet the unique needs of individual employees. This personalization can lead to higher levels of employee engagement, satisfaction, and retention. Furthermore, AI and ML can help HR departments make more informed decisions by providing data-driven insights. These technologies can analyze employee data to identify areas for improvement, predict future trends, and develop strategies to address challenges proactively. By leveraging these insights, HR professionals can make strategic decisions that align with the organization's goals and objectives. However, integrating AI and ML in HR practices also presents challenges, such as data privacy concerns, ethical considerations, and the need for upskilling HR professionals to use these technologies effectively. Organizations must address these challenges to realize the full potential of AI and ML in HR practices. In conclusion, integrating AI and ML in HR practices offers organizations the opportunity to enhance efficiency, effectiveness, and employee satisfaction in the digital age. By leveraging these technologies, HR departments can streamline operations, personalize employee experiences, and make more informed decisions that drive organizational success. As organizations increasingly turn to digital solutions, the role of artificial intelligence (AI) and machine learning (ML) in Human Resources becomes pivotal. This paper will focus on how AI and ML are being integrated into HR functions such as recruitment, onboarding, and employee engagement. It will also discuss the ethical implications and the challenges of maintaining human touch in an increasingly automated workplace. Case studies of companies leading in digital HR practices will be highlighted to provide real-world insights. Keywords: Digital Force, HR Practices, AI, Machine Learning, Future.","<method>Artificial Intelligence (AI)</method>, <method>Machine Learning (ML)</method>"
2024,https://openalex.org/W4391301691,Engineering,AI-Driven Digital Twin Model for Reliable Lithium-Ion Battery Discharge Capacity Predictions,"The present study proposes a novel method for predicting the discharge capabilities of lithium-ion (Li-ion) batteries using a digital twin model in practice. By combining cutting-edge machine learning techniques, such as AdaBoost and long short-term memory (LSTM) network, with a semiempirical mathematical structure, the digital twin (DT)—a virtual representation that mimics the behavior of actual batteries in real time is constructed. Various metaheuristic optimization methods, such as antlion, grey wolf optimization (GWO), and improved grey wolf optimization (IGWO), are used to adjust hyperparameters in order to optimize the models. As indicators of performance, mean absolute error (MAE) and root-mean-square error (RMSE) are applied to the models after they have undergone extensive training and ten-fold cross-validation. The models are rigorously trained and cross-validated using the NASA battery aging dataset, a widely accepted benchmark dataset for battery research. The IGWO-AdaBoost digital twin model emerges as the standout performer, achieving exceptional accuracy in predicting the discharge capacity. This model demonstrates the lowest mean absolute error (MAE) of 0.01, showcasing its superior precision in estimating discharge capabilities. Additionally, the root mean square error (RMSE) for the IGWO-AdaBoost DT model is also the lowest at 0.01. The findings of this study offer insightful information about the potential utilization of the digital twin model to accurately predict the discharge capacity of batteries.","<method>AdaBoost</method>, <method>long short-term memory (LSTM) network</method>, <method>antlion optimization</method>, <method>grey wolf optimization (GWO)</method>, <method>improved grey wolf optimization (IGWO)</method>"
2024,https://openalex.org/W4391559452,Engineering,Multi-USV Task Planning Method Based on Improved Deep Reinforcement Learning,"A safe and reliable task planning method is a prerequisite for the collaborative execution of ocean observation data collection tasks by multiple unmanned surface vessels (multi-USVs). Deep Reinforcement Learning (DRL) combines the powerful nonlinear function-fitting capabilities of deep neural networks with the decision-making and control abilities of reinforcement learning, providing a novel approach to solving the multi-USV task planning problem. However, when applied to the field of multi-USV task planning, it faces challenges such as a vast exploration space, extended training times, and unstable training process. To this end, this paper proposes a multi-USV task planning method based on improved deep reinforcement learning. The proposed method draws on the idea of a value decomposition network, breaking down the multi-USV task planning problem into two subproblems: task allocation and autonomous collision avoidance. Different state spaces, action spaces, and reward functions are designed for the various subproblems. Based on this, a deep neural network is used to map the state space of each subproblem to the action space of each USV, and the generated strategy of the deep neural network is assessed based on the corresponding reward function. This successfully integrates task allocation and path planning into a comprehensive task planning framework. Deep neural networks consist of the Actor networks and the Critic networks. During the training phase of the Critic network, different methods are used to train different Critic networks to improve the convergence speed of the algorithm. An improved temporal difference error method is specifically applied to train the Critic network for evaluating autonomous collision avoidance strategies, resulting in improving the autonomous collision avoidance ability of USVs. At the same time, to improve the efficiency of task allocation, hierarchical mechanisms, and regional division mechanisms are introduced to construct sub-system task planning models, which further decompose the task planning problem. A combination of successor features and an improved temporal difference error method is specifically applied to train another Critic network for evaluating the sub-systems task allocation schemes and collaborative motion trajectories, aiming to enhance the allocation efficiency of the sub-systems. Furthermore, transfer learning is employed to merge the sub-system task planning, using it as a constraint to direct the exploration and assessment of both the cluster task allocation schemes and the cluster collaborative motion trajectories. This enables rapid and accurate learning for task allocation within the multi-USV cluster. During the training phase of the Actor network, the introduction of the experience replay method and target network technique is employed to enhance the proximal policy optimization algorithm. This facilitates distributed joint training of the Actor network, thereby improving the accuracy of the algorithm. Simulation results validate the effectiveness and superiority of this method.","<method>Deep Reinforcement Learning (DRL)</method>, <method>value decomposition network</method>, <method>deep neural network</method>, <method>Actor networks</method>, <method>Critic networks</method>, <method>improved temporal difference error method</method>, <method>hierarchical mechanisms</method>, <method>regional division mechanisms</method>, <method>successor features</method>, <method>transfer learning</method>, <method>experience replay method</method>, <method>target network technique</method>, <method>proximal policy optimization algorithm</method>"
2024,https://openalex.org/W4392653416,Engineering,Exploring and leveraging aggregation effects on reactive oxygen species generation in photodynamic therapy,"Abstract Aggregate‐level photodynamic therapy (PDT) has attracted significant interest and driven substantial advances in multifunction phototheranostic platforms. As exemplified by two typical instances of aggregation‐caused quenching of reactive oxygen species (ROS) and aggregation‐induced generation of ROS, the aggregation effect plays a significant role on the ROS generation of photosensitizers (PSs), which is worthy of in‐depth exploration and full utilization. However, in contrast to the well‐developed researches on the aggregation effect on luminescence, the studies concerning the aggregation effect on ROS generation are currently in a relatively nascent and disjointed stage, lacking guidance from a firmly established research paradigm. To advance this regard, this review aims at providing a consolidated overview of the fundamental principles and research status of aggregation effects on the ROS generation. Here, the research status can be organized into two main facets. One involves the comparison between isolated state and aggregated state, which is mainly conducted by two methods of changing solvent environments and adding adjuvants into a given solvent. The other underscores the distinctions between different aggregate states, consisting of three parts, namely comparison within the same or between different categories based on the classification of single‐component and multicomponent aggregates. In this endeavor, we will present our views on current research methodologies that explore how aggregation affects ROS generation and highlight the design strategies to leverage the aggregation effect to optimize PS regiments. We aspire this review to propel the advancement of phototheranostic platforms and accelerate the clinical implementation of precision medicine, and inspire more contributions to aggregate‐level photophysics and photochemistry, pushing the aggregate science and materials forward.",No methods found.
2024,https://openalex.org/W4394788952,Engineering,A comparative review of subsea navigation technologies in offshore engineering projects,"Subsea navigation technologies play a critical role in enhancing precision and efficiency in offshore engineering projects. This comparative review examines the evolution and application of subsea navigation systems, with a focus on improving precision in offshore construction and surveying. The study evaluates various subsea navigation technologies, including acoustic positioning systems, inertial navigation systems, and optical imaging systems, highlighting their capabilities and limitations. The review begins by discussing the historical development of subsea navigation technologies, tracing their evolution from early acoustic systems to modern integrated solutions. It then explores the key components and operation principles of each technology, providing insights into their functionalities and suitability for different offshore applications. The study also examines the challenges and advancements in subsea navigation, including issues related to signal interference, accuracy, and real-time data processing. It discusses how these challenges have been addressed through technological innovations, such as improved sensor technologies and advanced data fusion algorithms. Furthermore, the review assesses the practical application of subsea navigation technologies in offshore engineering projects, including pipeline installation, subsea infrastructure maintenance, and underwater surveying. It analyzes case studies and industry practices to illustrate the effectiveness of these technologies in improving precision, reducing costs, and mitigating risks in offshore operations. Overall, this comparative review provides a comprehensive overview of subsea navigation technologies in offshore engineering projects. It highlights the evolution, capabilities, and applications of these technologies, emphasizing their role in enhancing precision and efficiency in offshore construction and surveying. The study concludes with recommendations for future research and development to further improve the performance and reliability of subsea navigation systems in offshore operations.",No methods found.
2024,https://openalex.org/W4401691402,Engineering,Optimal truss design with MOHO: A multi-objective optimization perspective,"This research article presents the Multi-Objective Hippopotamus Optimizer (MOHO), a unique approach that excels in tackling complex structural optimization problems. The Hippopotamus Optimizer (HO) is a novel approach in meta-heuristic methodology that draws inspiration from the natural behaviour of hippos. The HO is built upon a trinary-phase model that incorporates mathematical representations of crucial aspects of Hippo's behaviour, including their movements in aquatic environments, defense mechanisms against predators, and avoidance strategies. This conceptual framework forms the basis for developing the multi-objective (MO) variant MOHO, which was applied to optimize five well-known truss structures. Balancing safety precautions and size constraints concerning stresses on individual sections and constituent parts, these problems also involved competing objectives, such as reducing the weight of the structure and the maximum nodal displacement. The findings of six popular optimization methods were used to compare the results. Four industry-standard performance measures were used for this comparison and qualitative examination of the finest Pareto-front plots generated by each algorithm. The average values obtained by the Friedman rank test and comparison analysis unequivocally showed that MOHO outperformed other methods in resolving significant structure optimization problems quickly. In addition to finding and preserving more Pareto-optimal sets, the recommended algorithm produced excellent convergence and variance in the objective and decision fields. MOHO demonstrated its potential for navigating competing objectives through diversity analysis. Additionally, the swarm plots effectively visualize MOHO's solution distribution of MOHO across iterations, highlighting its superior convergence behaviour. Consequently, MOHO exhibits promise as a valuable method for tackling complex multi-objective structure optimization issues.","<method>Multi-Objective Hippopotamus Optimizer (MOHO)</method>, <method>Hippopotamus Optimizer (HO)</method>"
2024,https://openalex.org/W4390667445,Engineering,Automated data processing and feature engineering for deep learning and big data applications: A survey,"Modern approach to artificial intelligence (AI) aims to design algorithms that learn directly from data. This approach has achieved impressive results and has contributed significantly to the progress of AI, particularly in the sphere of supervised deep learning. It has also simplified the design of machine learning systems as the learning process is highly automated. However, not all data processing tasks in conventional deep learning pipelines have been automated. In most cases data has to be manually collected, preprocessed and further extended through data augmentation before they can be effective for training. Recently, special techniques for automating these tasks have emerged. The automation of data processing tasks is driven by the need to utilize large volumes of complex, heterogeneous data for machine learning and big data applications. Today, end-to-end automated data processing systems based on automated machine learning (AutoML) techniques are capable of taking raw data and transforming them into useful features for Big Data tasks by automating all intermediate processing stages. In this work, we present a thorough review of approaches for automating data processing tasks in deep learning pipelines, including automated data preprocessing– e.g., data cleaning, labeling, missing data imputation, and categorical data encoding–as well as data augmentation (including synthetic data generation using generative AI methods) and feature engineering–specifically, automated feature extraction, feature construction and feature selection. In addition to automating specific data processing tasks, we discuss the use of AutoML methods and tools to simultaneously optimize all stages of the machine learning pipeline.","<method>supervised deep learning</method>, <method>data augmentation</method>, <method>automated machine learning (AutoML)</method>, <method>automated data preprocessing</method>, <method>data cleaning</method>, <method>labeling</method>, <method>missing data imputation</method>, <method>categorical data encoding</method>, <method>synthetic data generation using generative AI methods</method>, <method>automated feature extraction</method>, <method>feature construction</method>, <method>feature selection</method>"
2024,https://openalex.org/W4390880765,Engineering,Opportunities and Challenges of Generative AI in Construction Industry: Focusing on Adoption of Text-Based Models,"In the last decade, despite rapid advancements in artificial intelligence (AI) transforming many industry practices, construction largely lags in adoption. Recently, the emergence and rapid adoption of advanced large language models (LLMs) like OpenAI’s GPT, Google’s PaLM, and Meta’s Llama have shown great potential and sparked considerable global interest. However, the current surge lacks a study investigating the opportunities and challenges of implementing Generative AI (GenAI) in the construction sector, creating a critical knowledge gap for researchers and practitioners. This underlines the necessity to explore the prospects and complexities of GenAI integration. Bridging this gap is fundamental to optimizing GenAI’s early stage adoption within the construction sector. Given GenAI’s unprecedented capabilities to generate human-like content based on learning from existing content, we reflect on two guiding questions: What will the future bring for GenAI in the construction industry? What are the potential opportunities and challenges in implementing GenAI in the construction industry? This study delves into reflected perception in literature, analyzes the industry perception using programming-based word cloud and frequency analysis, and integrates authors’ opinions to answer these questions. This paper recommends a conceptual GenAI implementation framework, provides practical recommendations, summarizes future research questions, and builds foundational literature to foster subsequent research expansion in GenAI within the construction and its allied architecture and engineering domains.",No methods found.
2024,https://openalex.org/W4391479301,Engineering,Comparative Assessment of Two Global Sensitivity Approaches Considering Model and Parameter Uncertainty,"Abstract Global Sensitivity Analysis (GSA) is key to assisting appraisal of the behavior of hydrological systems through model diagnosis considering multiple sources of uncertainty. Uncertainty sources typically comprise incomplete knowledge in (a) conceptual and mathematical formulation of models and (b) parameters embedded in the models. In this context, there is the need for detailed investigations aimed at a robust quantification of the importance of model and parameter uncertainties in a rigorous multi‐model context. This study aims at evaluating and comparing two modern multi‐model GSA methodologies. These are the first GSA approaches embedding both model and parameter uncertainty sources and encompass the variance‐based framework based on Sobol indices (as derived by Dai &amp; Ye, 2015, https://doi.org/10.1016/j.jhydrol.2015.06.034 ) and the moment‐based approach upon which the formulation of the multi‐model AMA indices (as derived by Dell'Oca et al., 2020, https://doi.org/10.1029/2019wr025754 ) is based. We provide an assessment of various aspects of sensitivity upon considering a joint analysis of these two approaches in a multi‐model context. Our work relies on well‐established scenarios that comprise (a) a synthetic setting related to reactive transport across a groundwater system and (b) an experimentally‐based study considering heavy metal sorption onto a soil. Our study documents that the joint use of these GSA approaches can provide different while complementary information to assess mutual consistency of approaches and to enrich the information content provided by GSA under model and parameter uncertainty. While being related to groundwater settings, our results can be considered as reference for future GSA studies coping with model and parameter uncertainty.",No methods found.
2024,https://openalex.org/W4392106982,Engineering,AI hype as a cyber security risk: the moral responsibility of implementing generative AI in business,"Abstract This paper examines the ethical obligations companies have when implementing generative Artificial Intelligence (AI). We point to the potential cyber security risks companies are exposed to when rushing to adopt generative AI solutions or buying into “AI hype”. While the benefits of implementing generative AI solutions for business have been widely touted, the inherent risks associated have been less well publicised. There are growing concerns that the race to integrate generative AI is not being accompanied by adequate safety measures. The rush to buy into the hype of generative AI and not fall behind the competition is potentially exposing companies to broad and possibly catastrophic cyber-attacks or breaches. In this paper, we outline significant cyber security threats generative AI models pose, including potential ‘backdoors’ in AI models that could compromise user data or the risk of ‘poisoned’ AI models producing false results. In light of these the cyber security concerns, we discuss the moral obligations of implementing generative AI into business by considering the ethical principles of beneficence, non-maleficence, autonomy, justice, and explicability. We identify two examples of ethical concern, overreliance and over-trust in generative AI, both of which can negatively influence business decisions, leaving companies vulnerable to cyber security threats. This paper concludes by recommending a set of checklists for ethical implementation of generative AI in business environment to minimise cyber security risk based on the discussed moral responsibilities and ethical concern.",No methods found.
2024,https://openalex.org/W4392547070,Engineering,Challenges in High-Throughput Inorganic Materials Prediction and Autonomous Synthesis,"Materials discovery lays the foundation for many technological advancements. The prediction and discovery of new materials are not simple tasks. Here, we outline some basic principles of solid-state chemistry, which might help to advance both, and discuss pitfalls and challenges in materials discovery. Using the recent work of Szymanski [Nature 624, 86 (2023)], which reported the autonomous discovery of 43 novel materials, as an example, we discuss problems that can arise in unsupervised materials discovery and hope that by addressing these, autonomous materials discovery can be brought closer to reality. We discuss all 43 synthetic products and point out four common shortfalls in the analysis. These errors unfortunately lead to the conclusion that no new materials have been discovered in that work. We conclude that there are two important points of improvement that require future work from the community, as follows. (i) Automated Rietveld analysis of powder x-ray diffraction data is not yet reliable. Future improvement of such, and the development of a reliable artificial-intelligence-based tool for Rietveld fitting, would be very helpful, not only for autonomous materials discovery but also for the community in general. (ii) We find that disorder in materials is often neglected in predictions. The predicted compounds investigated herein have all their elemental components located on distinct crystallographic positions but in reality, elements can share crystallographic sites, resulting in higher-symmetry space groups and—very often—known alloys or solid solutions. This error might be related to the difficulty of modeling disorder in a computationally economical way and needs to be addressed both by computational and experimental material scientists. We find that two thirds of the claimed successful materials in Szymanski are likely to be known compositionally disordered versions of the predicted ordered compounds. We highlight important issues in materials discovery, computational chemistry, and autonomous interpretation of x-ray diffraction. We discuss concepts of materials discovery from an experimentalist point of view, which we hope will be helpful for the community to further advance this important new aspect of our field. Published by the American Physical Society 2024",No methods found.
2024,https://openalex.org/W4392611134,Engineering,"Employing advanced control, energy storage, and renewable technologies to enhance power system stability","As the world witnesses a surge in the adoption of renewable energy sources to meet the surging global power demands, the dynamic and intermittent nature of these sources emerges as a significant hurdle. This article extensively explores the potential of advanced control systems, energy storage technologies, and renewable resources to fortify stability within power systems. Advanced control methodologies are strategically amalgamated with energy storage deployment and the utilization of renewable energy, to advance the reliability, predictability, and sustainability of power systems. The stability analysis, with a dedicated focus on Input-to-input-to-state stability (ISS), is conducted meticulously by applying the Lyapunov function and the Reciprocally Convex Approach, resulting in an impressive stability rate of 23.6%. Additionally, Positive Realness is substantiated by extracting Linear Matrix Inequalities (LMI) in the context of Enhancing Grid Stability with Wind Power. The study places particular emphasis on evaluating ISS and Passivity in both delayed and non-delayed systems, with a specific focus on neutral time-delay systems. This evaluation involves the selection of an appropriate Lyapunov-Krasovskii Functional (LKF) and its derivation, coupled with the integration of reciprocally convex methods, descriptive approach, and Jensen inequality. The outcomes of these analyses shed light on the causes of excess energy and its effective storage, along with highlighting the synergistic impact of integrating renewable sources and controlling grid frequency, voltage, and power in real time. The study also accentuates the robustness achieved through Enhanced Stability and the mitigation of Reduced Fluctuations, especially in the context of renewable energy sources.",No methods found.
2024,https://openalex.org/W4392980686,Engineering,Data-driven evolution of water quality models: An in-depth investigation of innovative outlier detection approaches-A case study of Irish Water Quality Index (IEWQI) model,"Recently, there has been a significant advancement in the water quality index (WQI) models utilizing data-driven approaches, especially those integrating machine learning and artificial intelligence (ML/AI) technology. Although, several recent studies have revealed that the data-driven model has produced inconsistent results due to the data outliers, which significantly impact model reliability and accuracy. The present study was carried out to assess the impact of data outliers on a recently developed Irish Water Quality Index (IEWQI) model, which relies on data-driven techniques. To the author's best knowledge, there has been no systematic framework for evaluating the influence of data outliers on such models. For the purposes of assessing the outlier impact of the data outliers on the water quality (WQ) model, this was the first initiative in research to introduce a comprehensive approach that combines machine learning with advanced statistical techniques. The proposed framework was implemented in Cork Harbour, Ireland, to evaluate the IEWQI model's sensitivity to outliers in input indicators to assess the water quality. In order to detect the data outlier, the study utilized two widely used ML techniques, including Isolation Forest (IF) and Kernel Density Estimation (KDE) within the dataset, for predicting WQ with and without these outliers. For validating the ML results, the study used five commonly used statistical measures. The performance metric (R2) indicates that the model performance improved slightly (R2 increased from 0.92 to 0.95) in predicting WQ after removing the data outlier from the input. But the IEWQI scores revealed that there were no statistically significant differences among the actual values, predictions with outliers, and predictions without outliers, with a 95% confidence interval at p < 0.05. The results of model uncertainty also revealed that the model contributed <1% uncertainty to the final assessment results for using both datasets (with and without outliers). In addition, all statistical measures indicated that the ML techniques provided reliable results that can be utilized for detecting outliers and their impacts on the IEWQI model. The findings of the research reveal that although the data outliers had no significant impact on the IEWQI model architecture, they had moderate impacts on the rating schemes' of the model. This finding indicated that detecting the data outliers could improve the accuracy of the IEWQI model in rating WQ as well as be helpful in mitigating the model eclipsing problem. In addition, the results of the research provide evidence of how the data outliers influenced the data-driven model in predicting WQ and reliability, particularly since the study confirmed that the IEWQI model's could be effective for accurately rating WQ despite the presence of the data outliers in the input. It could occur due to the spatio-temporal variability inherent in WQ indicators. However, the research assesses the influence of data input outliers on the IEWQI model and underscores important areas for future investigation. These areas include expanding temporal analysis using multi-year data, examining spatial outlier patterns, and evaluating detection methods. Moreover, it is essential to explore the real-world impacts of revised rating categories, involve stakeholders in outlier management, and fine-tune model parameters. Analysing model performance across varying temporal and spatial resolutions and incorporating additional environmental data can significantly enhance the accuracy of WQ assessment. Consequently, this study offers valuable insights to strengthen the IEWQI model's robustness and provides avenues for enhancing its utility in broader WQ assessment applications. Moreover, the study successfully adopted the framework for evaluating how data input outliers affect the data-driven model, such as the IEWQI model. The current study has been carried out in Cork Harbour for only a single year of WQ data. The framework should be tested across various domains for evaluating the response of the IEWQI model's in terms of the spatio-temporal resolution of the domain. Nevertheless, the study recommended that future research should be conducted to adjust or revise the IEWQI model's rating schemes and investigate the practical effects of data outliers on updated rating categories. However, the study provides potential recommendations for enhancing the IEWQI model's adaptability and reveals its effectiveness in expanding its applicability in more general WQ assessment scenarios.","<method>Isolation Forest (IF)</method>, <method>Kernel Density Estimation (KDE)</method>"
2024,https://openalex.org/W4400058707,Engineering,Computer vision in smart agriculture and precision farming: Techniques and applications,"The transformation of age-old farming practices through the integration of digitization and automation has sparked a revolution in agriculture that is driven by cutting-edge computer vision and artificial intelligence (AI) technologies. This transformation not only promises increased productivity and economic growth, but also has the potential to address important global issues such as food security and sustainability. This survey paper aims to provide a holistic understanding of the integration of vision-based intelligent systems in various aspects of precision agriculture. By providing a detailed discussion on key areas of digital life cycle of crops, this survey contributes to a deeper understanding of the complexities associated with the implementation of vision-guided intelligent systems in challenging agricultural environments. The focus of this survey is to explore widely used imaging and image analysis techniques being utilized for precision farming tasks. This paper first discusses various salient crop metrics used in digital agriculture. Then this paper illustrates the usage of imaging and computer vision techniques in various phases of digital life cycle of crops in precision agriculture, such as image acquisition, image stitching and photogrammetry, image analysis, decision making, treatment, and planning. After establishing a thorough understanding of related terms and techniques involved in the implementation of vision-based intelligent systems for precision agriculture, the survey concludes by outlining the challenges associated with implementing generalized computer vision models for real-time deployment of fully autonomous farms.",<method>computer vision</method>
2024,https://openalex.org/W4390825883,Engineering,Transforming Object Design and Creation: Biomaterials and Contemporary Manufacturing Leading the Way,"In the field of three-dimensional object design and fabrication, this paper explores the transformative potential at the intersection of biomaterials, biopolymers, and additive manufacturing. Drawing inspiration from the intricate designs found in the natural world, this study contributes to the evolving landscape of manufacturing and design paradigms. Biomimicry, rooted in emulating nature's sophisticated solutions, serves as the foundational framework for developing materials endowed with remarkable characteristics, including adaptability, responsiveness, and self-transformation. These advanced engineered biomimetic materials, featuring attributes such as shape memory and self-healing properties, undergo rigorous synthesis and characterization procedures, with the overarching goal of seamless integration into the field of additive manufacturing. The resulting synergy between advanced manufacturing techniques and nature-inspired materials promises to revolutionize the production of objects capable of dynamic responses to environmental stimuli. Extending beyond the confines of laboratory experimentation, these self-transforming objects hold significant potential across diverse industries, showcasing innovative applications with profound implications for object design and fabrication. Through the reduction of waste generation, minimization of energy consumption, and the reduction of environmental footprint, the integration of biomaterials, biopolymers, and additive manufacturing signifies a pivotal step towards fostering ecologically conscious design and manufacturing practices. Within this context, inanimate three-dimensional objects will possess the ability to transcend their static nature and emerge as dynamic entities capable of evolution, self-repair, and adaptive responses in harmony with their surroundings. The confluence of biomimicry and additive manufacturing techniques establishes a seminal precedent for a profound reconfiguration of contemporary approaches to design, manufacturing, and ecological stewardship, thereby decisively shaping a more resilient and innovative global milieu.",No methods found.
2024,https://openalex.org/W4391057570,Engineering,Scalable deposition techniques for large-area perovskite photovoltaic technology: A multi-perspective review,"Perovskite photovoltaic technology holds great promise for the solar energy industry due to its high efficiency and fascinating potentialities for low-cost production. However, overcoming stability and scalability challenges by addressing environmental concerns is crucial for their successful deployment in the market. While perovskite solar cells have shown promising power conversion efficiency and long-term lifetime once produced by using small-scale production facilities, the scaling up of the device production without losing in performance and stability, still remains a not-trivial task. As matter of the fact, research and development efforts are currently ongoing to bring the perovskite solar technology closer to the market. From this point of view, the choice and the optimization of a specific deposition method depends on several factors like scalability, cost-effectiveness, and peculiar requirements required by the featured application. In this review, a comprehensive description of deposition methods for large area substrate is provided followed by an exhaustive collection of the main published progresses on large area perovskite solar devices. In particular, the large-area compatible deposition techniques will be categorized in the two main families based on solution and physical deposition processes, while pros and cons of both families are investigated in view of their potentialities for low-cost and high-throughput industrial production. Not least, hybrid deposition methods combining specific features of both solution and vacuum deposition techniques will be suggested as a way to leverage both their advantages. In addition, the main characterization techniques for large area devices will be described as a powerful way to select the proper deposition technique for supporting the development of commercial product based on perovskite technology. The opened challenges and the new routes for moving perovskite from lab to fab are finally suggested and discussed.",No methods found.
2024,https://openalex.org/W4393070891,Engineering,Economic and environmental benefits of digital agricultural technologies in crop production: A review,"This comprehensive review delved into the economic and environmental benefits of Digital Agricultural Technologies (DATs) in crop production, synthesising data from 136 peer-reviewed papers and 28 documents with empirical data from relevant EU projects. This analysis highlighted the substantial contribution of DATs across five key categories: Recording and Mapping Technologies (RMT), Guidance and Controlled Traffic Farming (CTF) Technologies, Variable Rate Technologies (VRT), Robotic Systems or Smart Machines (RSSM), and Farm Management Information Systems (FMIS). Specifically, it provided an overview of the various benefits that these technologies can deliver with the most significant ones revealing reductions of up to 80% in fertiliser usage with RMT and CTF applications, while VRT demonstrated a 60% decrease in fertiliser usage and up to 80% reduction in pesticide use. VRT also showed an increase in yield by 62%. RSSM was able to reduce labour by 97% and diesel consumption by 50%. FMIS improved yield by 10% to 15%, facilitating simultaneous reductions in labour and input costs, illustrating the critical role of integrated digital solutions in enhancing agricultural efficiency and sustainability. From an environmental point of view, VRT has emerged as a major factor in environmental sustainability, demonstrating water savings of 20% to 50% in vineyards and pear orchards and a significant reduction in greenhouse gas emissions. These findings highlighted the significant benefits of DATs on enhancing productivity and promoting environmental sustainability. They provided a compelling case for further investment and research in DATs through quantifiable benefits in crop production.",No methods found.
2024,https://openalex.org/W4394015596,Engineering,Predicting the mechanical properties of plastic concrete: An optimization method by using genetic programming and ensemble learners,"This study presents a comparative analysis of individual and ensemble learning algorithms (ELAs) to predict the compressive strength (CS) and flexural strength (FS) of plastic concrete. Multilayer perceptron neuron network (MLPNN), Support vector machine (SVM), random forest (RF), and decision tree (DT) were used as base learners, which were then combined with bagging and Adaboost methods to improve the predictive performance. In addition, gene expression programming (GEP) was used to develop computational equations that can be used to predict the CS and FS of plastic concrete. An extensive database containing 357 and 125 data points was obtained from the literature, and the eight most impactful ingredients were used in the model's development. The accuracy of all models was assessed using several statistical measures, including an error matrix, Akaike information criterion (AIC), K-fold cross-validation, and other external validation equations. Furthermore, sensitivity and SHAP analysis were performed to evaluate input variables' relative significance and impact on the anticipated CS and FS. Based on statistical measures and other validation criteria, GEP outpaces all other individual models, whereas, in ELAs, the SVR ensemble with Adaboost and RF modified with the Bagging technique demonstrated superior performance. SHapley Additive exPlanations (SHAP) and sensitivity analysis reveal that plastic, cement, water, and the age of the specimens have the highest influence, while superplasticizer has the lowest impact, which is consistent with experimental studies. Moreover, GUI and GEP-based simple mathematical correlation can enhance the practical scope of this study and be an effective tool for the pre-mix design of plastic concrete.","<method>Multilayer perceptron neuron network (MLPNN)</method>, <method>Support vector machine (SVM)</method>, <method>random forest (RF)</method>, <method>decision tree (DT)</method>, <method>bagging</method>, <method>Adaboost</method>, <method>gene expression programming (GEP)</method>, <method>K-fold cross-validation</method>, <method>SHapley Additive exPlanations (SHAP)</method>"
2024,https://openalex.org/W4391089359,Engineering,Groundwater level prediction using an improved SVR model integrated with hybrid particle swarm optimization and firefly algorithm,"The demand for water resources has increased due to rapid increase of metropolitan areas brought on by growth in population and industrialisation. In addition, the groundwater recharge is being afftected by shifting land use pattern caused by urban development. Using precise and trustworthy estimates of groundwater level is vital for the sustainable groundwater resources management in the face of changing climatic circumstances. In this context, machine learning (ML) methods offer a new and promising approach for accurately forecasting long-term changes in the groundwater level (GWL) without computational effort of developing a comprehensive flow model. In order to simulate GWL, five data-driven (DD) models, including the hybridization of support vector regression (SVR) with two optimisation algorithms i.e., firefly algorithm and particle swarm optimisation (FFAPSO), SVR-FFA, SVR-PSO, SVR and Multilayer perception (MLP), have been examined in the present study. Spatial clustering was utilised to choose four observation wells within Cuttack district in order to study and assess the water levels. Six scenarios were created by incorporating numerous variables, such as GWL in the previous months, evapotranspiration, temperature, precipitation, and river discharge. The goal was to identify the variables that were most efficient in predicting GWL. The SVR-FFAPSO model performs best in GWL forecasting for Khuntuni station, according to the quantitative analysis with correlation coefficient (R) = 0.9978, Nash–Sutcliffe efficiency (NSE) = 0.9933, mean absolute error (MAE) = 0.00025 (m), root mean squared error (RMSE) = 0.00775 (m) during the training phase. It is advised that groundwater monitoring network and data collecting system are strengthen in India for ensuring effective modelling of long-term management of groundwater resources.","<method>support vector regression (SVR)</method>, <method>firefly algorithm</method>, <method>particle swarm optimisation (PSO)</method>, <method>SVR-FFA</method>, <method>SVR-PSO</method>, <method>Multilayer perception (MLP)</method>"
2024,https://openalex.org/W4391855187,Engineering,Machine learning-assisted in-situ adaptive strategies for the control of defects and anomalies in metal additive manufacturing,"In metal additive manufacturing (AM), the material microstructure and part geometry are formed incrementally. Consequently, the resulting part could be defect- and anomaly-free if sufficient care is taken to deposit each layer under optimal process conditions. Conventional closed-loop control (CLC) engineering solutions which sought to achieve this were deterministic and rule-based, thus resulting in limited success in the stochastic environment experienced in the highly dynamic AM process. On the other hand, emerging machine learning (ML) based strategies are better suited to providing the robustness, scope, flexibility, and scalability required for process control in an uncertain environment. Offline ML models that help optimise AM process parameters before a build begins and online ML models that efficiently processed in-situ sensory data to detect and diagnose flaws in real-time (or near-real-time) have been developed. However, ML models that enable a process to take evasive or corrective actions in relation to flaws via on the fly decision-making are only emerging. These models must possess prognostic capabilities to provide context-sensitive recommendations for in-situ process control based on real-time diagnostics. In this article, we pinpoint the shortcomings in traditional CLC strategies, and provide a framework for defect and anomaly control through ML-assisted CLC in AM. We discuss flaws in terms of their causes, in-situ detectability, and controllability, and examine their management under three scenarios: avoidance, mitigation, and repair. Then, we summarise the research into ML models developed for offline optimisation and in-situ diagnosis before initiating a detailed conversation on the implementation of ML-assisted in-situ process control. We found that researchers favoured reinforcement learning approaches or inverse ML models for making rapid, situation-aware control decisions. We also observed that, to-date, the defects addressed were those that may be quantified relatively easily autonomously, and that mitigation (rather than avoidance or repair) was the aim of ML-assisted in-situ control strategies. Additionally, we highlight the various technologies that must seamlessly combine to advance the field of autonomous in-situ control so that it becomes a reality in industrial settings. Finally, we raise awareness of seldom discussed, yet highly pertinent, topics relevant to adaptive control. Our work closes a significant gap in the current AM literature by broaching wide-ranging discussions on matters relevant to in-situ adaptive control in AM.","<method>machine learning (ML) based strategies</method>, <method>offline ML models</method>, <method>online ML models</method>, <method>reinforcement learning approaches</method>, <method>inverse ML models</method>"
2024,https://openalex.org/W4391973028,Engineering,A comprehensive evaluation of large Language models on benchmark biomedical text processing tasks,"Recently, Large Language Models (LLMs) have demonstrated impressive capability to solve a wide range of tasks. However, despite their success across various tasks, no prior work has investigated their capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of LLMs on benchmark biomedical tasks. For this purpose, a comprehensive evaluation of 4 popular LLMs in 6 diverse biomedical tasks across 26 datasets has been conducted. To the best of our knowledge, this is the first work that conducts an extensive evaluation and comparison of various LLMs in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot LLMs even outperform the current state-of-the-art models when they were fine-tuned only on the training set of these datasets. This suggests that pre-training on large text corpora makes LLMs quite specialized even in the biomedical domain. We also find that not a single LLM can outperform other LLMs in all tasks, with the performance of different LLMs may vary depending on the task. While their performance is still quite poor in comparison to the biomedical models that were fine-tuned on large training sets, our findings demonstrate that LLMs have the potential to be a valuable tool for various biomedical tasks that lack large annotated data.","<method>Large Language Models (LLMs)</method>, <method>zero-shot learning</method>, <method>fine-tuning</method>"
2024,https://openalex.org/W4392640075,Engineering,Performance assessment of machine learning algorithms for mapping of land use/land cover using remote sensing data,"The rapid increase in population accelerates the rate of change of Land use/Land cover (LULC) in various parts of the world. This phenomenon caused a huge strain for natural resources. Hence, continues monitoring of LULC changes gained a significant importance for management of natural resources and assessing the climate change impacts. Recently, application of machine learning algorithms on RS (remote sensing) data for rapid and accurate mapping of LULC gained significant importance due to growing need of LULC estimation for ecosystem services, natural resource management and environmental management. Hence, it is crucial to access and compare the performance of different machine learning classifiers for accurate mapping of LULC. The primary objective of this study was to compare the performance of CART (Classification and Regression Tree), RF (Random Forest) and SVM (Support Vector Machine) for LULC estimation by processing RS data on Google Earth Engine (GEE). In total four classes of LULC (Water Bodies, Vegetation Cover, Urban Land and Barren Land) for city of Lahore were extracted using satellite images from Landsat-7, Landsat-8 and Landsat-9 for years 2008, 2015 and 2022, respectively. According to results, RF is the best performing classifier with maximum overall accuracy of 95.2% and highest Kappa coefficient value of 0.87, SVM achieved maximum accuracy of 89.8% with highest Kappa of 0.84 and CART showed maximum overall accuracy of 89.7% with Kappa value of 0.79. Results from this study can give assistance for decision makers, planners and RS experts to choose a suitable machine learning algorithm for LULC classification in an unplanned urbanized city like Lahore.","<method>Classification and Regression Tree (CART)</method>, <method>Random Forest (RF)</method>, <method>Support Vector Machine (SVM)</method>"
2024,https://openalex.org/W4393405236,Engineering,Transformer and Graph Convolution-Based Unsupervised Detection of Machine Anomalous Sound Under Domain Shifts,"Thanks to the development of deep learning, machine abnormal sound detection (MASD) based on unsupervised learning has exhibited excellent performance. However, in the task of unsupervised MASD, there are discrepancies between the acoustic characteristics of the test set and the training set under the physical parameter changes (domain shifts) of the same machine's operating conditions. Existing methods not only struggle to stably learn the sound signal features under various domain shifts but also inevitably increase computational overhead. To address these issues, we propose an unsupervised machine abnormal sound detection model based on Transformer and Dynamic Graph Convolution (Unsuper-TDGCN) in this paper. Firstly, we design a network that models time-frequency domain features to capture both global and local spatial and time-frequency interactions, thus improving the model's stability under domain shifts. Then, we introduce a Dynamic Graph Convolutional Network (DyGCN) to model the dependencies between features under domain shifts, enhancing the model's ability to perceive changes in domain features. Finally, a Domain Self-adaptive Network (DSN) is employed to compensate for the performance decline caused by domain shifts, thereby improving the model's adaptive ability for detecting anomalous sounds in MASD tasks under domain shifts. The effectiveness of our proposed model has been validated on multiple datasets.","<method>unsupervised learning</method>, <method>Transformer</method>, <method>Dynamic Graph Convolutional Network (DyGCN)</method>, <method>Domain Self-adaptive Network (DSN)</method>"
2024,https://openalex.org/W4396652595,Engineering,Geological survey techniques and carbon storage: Optimizing renewable energy site selection and carbon sequestration,"Geological survey techniques play a crucial role in optimizing site selection for renewable energy projects and identifying suitable locations for carbon storage to mitigate climate change. This abstract provides an overview of how geological survey techniques can be used to achieve these objectives. Renewable energy development, particularly solar and wind power, requires careful site selection to maximize energy generation efficiency and minimize environmental impacts. Geological surveys are instrumental in assessing factors such as subsurface geology, topography, soil composition, and hydrological conditions. These surveys help identify suitable locations with optimal wind or solar resources and geologic conditions for infrastructure development. Additionally, geological surveys are essential for identifying suitable sites for carbon storage, a critical component of carbon capture and storage (CCS) technologies aimed at reducing greenhouse gas emissions. Geological formations, such as deep saline aquifers, depleted oil and gas reservoirs, and unmineable coal seams, can serve as storage reservoirs for captured carbon dioxide (CO2). Geological surveys help characterize these formations to assess their suitability for long-term CO2 storage, considering factors such as porosity, permeability, and sealing integrity. Optimizing site selection for renewable energy projects and carbon storage requires a comprehensive understanding of subsurface geology and environmental conditions. Advanced geological survey techniques, such as seismic imaging, remote sensing, and geophysical surveys, are essential for acquiring detailed subsurface data. These techniques enable scientists and engineers to assess site suitability, evaluate risks, and design effective mitigation measures. In conclusion, geological survey techniques are invaluable tools for optimizing site selection for renewable energy projects and identifying suitable locations for carbon storage. By leveraging these techniques, stakeholders can make informed decisions that promote sustainable energy development and mitigate the impacts of climate change.",No methods found.
2024,https://openalex.org/W4401667275,Engineering,Artificial intelligence for literature reviews: opportunities and challenges,"Abstract This paper presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous and organised methodology that assesses and integrates prior research on a given topic. Numerous tools have been developed to assist and partially automate the SLR process. The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews. Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases. We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features. We also analyse 11 recent tools that leverage large language models for searching the literature and assisting academic writing. Finally, the paper discusses current trends in the field, outlines key research challenges, and suggests directions for future research. We highlight three primary research challenges: integrating advanced AI solutions, such as large language models and knowledge graphs, improving usability, and developing a standardised evaluation framework. We also propose best practices to ensure more robust evaluations in terms of performance, usability, and transparency. Overall, this review offers a detailed overview of AI-enhanced SLR tools for researchers and practitioners, providing a foundation for the development of next-generation AI solutions in this field.","<method>large language models</method>, <method>knowledge graphs</method>"
2024,https://openalex.org/W357964437,Engineering,Women's Medical Work in Early Modern France,"For a majority of the French population during the period known as the Renaissance, most medical care would come at the hands of women. Women's medical work, like that of other providers, needs to be situated in specific historical and social contexts. This book adopts a number of methodological approaches which will help to highlight and understand women's medical practices, and may provide new ways to perceive their contribution to the history of medicine more generally. It focuses on women because, as practitioners, they cut across most sectors of medical practice. The book is structured in such a way as to demonstrate how different contexts and communities responded to women's medical work in varied and sometimes contrasting ways. It explores religious understandings of female healing work as lay and religious women. The book presents the study of women's domestic and charitable medical labour, by exploring the impact of print in the context of women as readers and patrons of medical literature, with a focus on the publication of manuals contributing to the domestic care discourse. It examines the role of women in the municipally organised systems of poor relief and child care for foundlings and orphans. The book also follows women's gynaecological and reproductive knowledge, particularly in the contexts of elite and royal court life.",No methods found.
2024,https://openalex.org/W4390480832,Engineering,Joint Optimization Risk Factor and Energy Consumption in IoT Networks With TinyML-Enabled Internet of UAVs,"The high mobility of Internet of Unmanned Aerial Vehicles (IUAVs) has attracted attention in the field of data collection. With the rapid development of the Internet of Things (IoT), more and more data are generated by IoT networks. IUAV-aided IoT networks can efficiently collect data in specific areas, which is of great significance in disaster relief. In the data collection task, it is necessary to plan the flight trajectory for the data collector—IUAV, so that the IUAV can collect data efficiently. However, existing research basically only considers the efficiency of data collection by IUAVs, but rarely considers the safety of IUAVs during flight. Therefore, this paper proposes an IUAV trajectory planning algorithm that integrates energy efficiency and safety using local search to address the issues mentioned above. At the same time, a Tiny Machine Learning (TinyML) algorithm is designed to assist the IUAV in making real-time decisions during flight. First, we build a general mathematical model that describes the risk in a particular region. Then consider guiding the IUAV to a safer trajectory by introducing virtual nodes in the flight trajectory. Furthermore, we designed a local search algorithm for the three tasks of IUAV access sequence, IoT Networks cluster heads selection and virtual nodes selection, and solved them through iterative optimization. We also consider the unreachable situation of the virtual nodes and use TinyML technology to help the IUAV adjust the position of the virtual nodes in real time in case of an emergency.In the end, an IUAV trajectory is obtained that can efficiently collect IoT networks' data and fly safely. We have conducted a large number of simulation experiments to demonstrate the efficiency of the proposed algorithm compared to the baseline algorithm.","<method>Tiny Machine Learning (TinyML)</method>, <method>local search algorithm</method>"
2024,https://openalex.org/W4390754233,Engineering,Groundwater Quality Assessment and Irrigation Water Quality Index Prediction Using Machine Learning Algorithms,"The evaluation of groundwater quality is crucial for irrigation purposes; however, due to financial constraints in developing countries, such evaluations suffer from insufficient sampling frequency, hindering comprehensive assessments. Therefore, associated with machine learning approaches and the irrigation water quality index (IWQI), this research aims to evaluate the groundwater quality in Naama, a region in southwest Algeria. Hydrochemical parameters (cations, anions, pH, and EC), qualitative indices (SAR,RSC,Na%,MH,and PI), as well as geospatial representations were used to determine the groundwater’s suitability for irrigation in the study area. In addition, efficient machine learning approaches for forecasting IWQI utilizing Extreme Gradient Boosting (XGBoost), Support vector regression (SVR), and K-Nearest Neighbours (KNN) models were implemented. In this research, 166 groundwater samples were used to calculate the irrigation index. The results showed that 42.18% of them were of excellent quality, 34.34% were of very good quality, 6.63% were good quality, 9.64% were satisfactory, and 4.21% were considered unsuitable for irrigation. On the other hand, results indicate that XGBoost excels in accuracy and stability, with a low RMSE (of 2.8272 and a high R of 0.9834. SVR with only four inputs (Ca2+, Mg2+, Na+, and K) demonstrates a notable predictive capability with a low RMSE of 2.6925 and a high R of 0.98738, while KNN showcases robust performance. The distinctions between these models have important implications for making informed decisions in agricultural water management and resource allocation within the region.","<method>Extreme Gradient Boosting (XGBoost)</method>, <method>Support Vector Regression (SVR)</method>, <method>K-Nearest Neighbours (KNN)</method>"
2024,https://openalex.org/W4391062409,Engineering,"Critical analysis of the technological affordances, challenges and future directions of Generative AI in education: a systematic review","Generative artificial intelligence has been regarded as a transformative tool. While responsible and ethical applications could bring opportunities to education, their misuse could pose demanding challenges. It is necessary to clarify the technological affordances and challenges in a normative way to lay the foundation for future development. This study addressed the dearth of literature by performing a systematic review, aiming to (i) explore the utility and availability from the technological affordances perspective; (ii) summarize the current challenges in risks prevention; and (iii) propose possible directions for future research and practice. A total of 27 academic articles published in core journals between 2020 and 2023 were analyzed, and the inductive grounded approach was used to categorize the coding schemes. The findings revealed four technological affordances: accessibility, personalization, automation, and interactivity; and five challenges: academic integrity risk, response errors and bias, over-dependence risk, the widening digital divide, and privacy and security. We propose future directions, encourage educational organizations to formulate guidelines for the ethical use of AI in education, call on educators to embrace future trends in AI education instead of shunning its use, and guide students to treat it as a thought aid and reference, rather than relying on it entirely.",No methods found.
2024,https://openalex.org/W4391147899,Engineering,Integration of cascaded coordinated rolling horizon control for output power smoothing in islanded wind–solar microgrid with multiple hydrogen storage tanks,"This paper presents a strategy based on the hierarchical rolling horizon control, also called model predictive control (MPC), for efficiently managing a hydrogen-energy storage system (HESS) within an islanded wind-solar microgrid. An electrolyzer uses electricity generated from renewable sources to produce clean hydrogen, which is then re-electrified by a fuel cell as needed to meet the microgrid's loads. The main contribution lies in the incorporation of multiple hydrogen storage tanks in the HESS, distinguishing it from existing literature, which typically focuses on a single tank. The incorporation of multiple tanks in the HESS enables the storage of large volumes of hydrogen for long-term use, allowing the microgrid to operate autonomously without interaction with the utility grid. In order to ensure optimal performance, the selection of the most suitable device for operation at each time-step is crucial. The proposed control strategy takes into account the economic and operational costs, degradation aspects, and physical constraints of the HESS, while simultaneously ensuring the tracking of reference demands and with the highest priority smoothing out the variations of renewable energy sources. Numerical simulations and a lab-scale microgrid setup demonstrate that the controller effectively manages the HESS thus satisfying economic constraints and optimizing device costs, even when deviations occur between the predicted and real-time scenarios. Furthermore, the inclusion of multiple hydrogen tanks allows the microgrid to both mitigate fluctuations in renewable power sources and effectively meet load demand.",No methods found.
2024,https://openalex.org/W4391262045,Engineering,Monolithic 2D Perovskites Enabled Artificial Photonic Synapses for Neuromorphic Vision Sensors,"Abstract Neuromorphic visual sensors (NVS) based on photonic synapses hold a significant promise to emulate the human visual system. However, current photonic synapses rely on exquisite engineering of the complex heterogeneous interface to realize learning and memory functions, resulting in high fabrication cost, reduced reliability, high energy consumption and uncompact architecture, severely limiting the up‐scaled manufacture, and on‐chip integration. Here a photo‐memory fundamental based on ion‐exciton coupling is innovated to simplify synaptic structure and minimize energy consumption. Due to the intrinsic organic/inorganic interface within the crystal, the photodetector based on monolithic 2D perovskite exhibits a persistent photocurrent lasting about 90 s, enabling versatile synaptic functions. The electrical power consumption per synaptic event is estimated to be≈1.45 × 10 −16 J, one order of magnitude lower than that in a natural biological system. Proof‐of‐concept image preprocessing using the neuromorphic vision sensors enabled by photonic synapse demonstrates 4 times enhancement of classification accuracy. Furthermore, getting rid of the artificial neural network, an expectation‐based thresholding model is put forward to mimic the human visual system for facial recognition. This conceptual device unveils a new mechanism to simplify synaptic structure, promising the transformation of the NVS and fostering the emergence of next generation neural networks.",<method>expectation-based thresholding model</method>
2024,https://openalex.org/W4391312031,Engineering,Artificial intelligence (AI) in renewable energy: A review of predictive maintenance and energy optimization,"The integration of Artificial Intelligence (AI) in the renewable energy sector has emerged as a transformative force, enhancing the efficiency and sustainability of energy systems. This paper provides a comprehensive review of the application of AI in two critical aspects of renewable energy in relation to predictive maintenance and energy optimization. Predictive maintenance, enabled by AI, has revolutionized the renewable energy landscape by predicting and preventing equipment failures before they occur. Utilizing machine learning algorithms, AI analyzes vast amounts of data from sensors and historical performance to identify patterns indicative of potential faults. This proactive approach not only minimizes downtime but also extends the lifespan of renewable energy infrastructure, resulting in substantial cost savings and improved reliability. Furthermore, AI plays a pivotal role in optimizing the energy output of renewable sources. Through advanced data analytics and real-time monitoring, AI algorithms can adapt to changing environmental conditions, predicting energy production patterns and optimizing resource allocation. This ensures maximum energy yield from renewable sources, making them more competitive with traditional energy sources. The paper delves into specific AI techniques such as deep learning, neural networks, and predictive analytics employed for predictive maintenance and energy optimization in various renewable energy systems like solar, wind, and hydropower. Challenges and opportunities associated with implementing AI in renewable energy are discussed, including data security, interoperability, and the need for standardized frameworks. The synthesis of AI technologies with renewable energy not only addresses operational challenges but also contributes to the global transition towards sustainable and clean energy solutions. This review serves as a valuable resource for researchers, practitioners, and policymakers seeking insights into the evolving landscape of AI applications in the renewable energy sector. As technology continues to advance, the synergies between AI and renewable energy are poised to shape the future of the global energy paradigm.","<method>machine learning algorithms</method>, <method>deep learning</method>, <method>neural networks</method>, <method>predictive analytics</method>"
2024,https://openalex.org/W4391679299,Engineering,Generative Pre-Trained Transformer (GPT) in Research: A Systematic Review on Data Augmentation,"GPT (Generative Pre-trained Transformer) represents advanced language models that have significantly reshaped the academic writing landscape. These sophisticated language models offer invaluable support throughout all phases of research work, facilitating idea generation, enhancing drafting processes, and overcoming challenges like writer’s block. Their capabilities extend beyond conventional applications, contributing to critical analysis, data augmentation, and research design, thereby elevating the efficiency and quality of scholarly endeavors. Strategically narrowing its focus, this review explores alternative dimensions of GPT and LLM applications, specifically data augmentation and the generation of synthetic data for research. Employing a meticulous examination of 412 scholarly works, it distills a selection of 77 contributions addressing three critical research questions: (1) GPT on Generating Research data, (2) GPT on Data Analysis, and (3) GPT on Research Design. The systematic literature review adeptly highlights the central focus on data augmentation, encapsulating 48 pertinent scholarly contributions, and extends to the proactive role of GPT in critical analysis of research data and shaping research design. Pioneering a comprehensive classification framework for “GPT’s use on Research Data”, the study classifies existing literature into six categories and 14 sub-categories, providing profound insights into the multifaceted applications of GPT in research data. This study meticulously compares 54 pieces of literature, evaluating research domains, methodologies, and advantages and disadvantages, providing scholars with profound insights crucial for the seamless integration of GPT across diverse phases of their scholarly pursuits.","<method>Generative Pre-trained Transformer (GPT)</method>, <method>Large Language Models (LLM)</method>"
2024,https://openalex.org/W4399054066,Engineering,The ethics of using artificial intelligence in scientific research: new guidance needed for a new tool,"Using artificial intelligence (AI) in research offers many important benefits for science and society but also creates novel and complex ethical issues. While these ethical issues do not necessitate changing established ethical norms of science, they require the scientific community to develop new guidance for the appropriate use of AI. In this article, we briefly introduce AI and explain how it can be used in research, examine some of the ethical issues raised when using it, and offer nine recommendations for responsible use, including: (1) Researchers are responsible for identifying, describing, reducing, and controlling AI-related biases and random errors; (2) Researchers should disclose, describe, and explain their use of AI in research, including its limitations, in language that can be understood by non-experts; (3) Researchers should engage with impacted communities, populations, and other stakeholders concerning the use of AI in research to obtain their advice and assistance and address their interests and concerns, such as issues related to bias; (4) Researchers who use synthetic data should (a) indicate which parts of the data are synthetic; (b) clearly label the synthetic data; (c) describe how the data were generated; and (d) explain how and why the data were used; (5) AI systems should not be named as authors, inventors, or copyright holders but their contributions to research should be disclosed and described; (6) Education and mentoring in responsible conduct of research should include discussion of ethical use of AI.",No methods found.
2024,https://openalex.org/W4390480870,Engineering,GraphGST: Graph Generative Structure-Aware Transformer for Hyperspectral Image Classification,"Transformer holds significance in deep learning (DL) research. Node embedding (NE) and positional encoding (PE) are usually two indispensable components in a Transformer. The former can excavate hidden correlations from the data, while the latter can store locational relationships between nodes. Recently, the Transformer has been applied for hyperspectral image (HSI) classification because the model can capture long-range dependencies to aggregate global features for representation learning. In an HSI, adjacent pixels tend to be homogeneous, while the NE does not identify the positional information of pixels. Therefore, PE is crucial for Transformers to understand locational relationships between pixels. However, in this area, most Transformer-based methods randomly generate PEs without considering their physical meaning, which leads to weak representations. This article proposes a new graph generative structure-aware Transformer (GraphGST) to solve the above-mentioned PE problem when implementing HSI classification. In our GraphGST, a new absolute PE (APE) is established to acquire pixels' absolute positional sequences (APSs) and is integrated into the Transformer architecture. Moreover, a generative mechanism with self-supervised learning is developed to achieve cross-view contrastive learning (CL), aiming to enhance the representation learning of the Transformer. The proposed GraphGST model can capture local-to-global correlations, and the extracted APSs can complement the spectral features of pixels to assist in NE. Several experiments with real HSIs are conducted to evaluate the effectiveness of our GraphGST. The proposed method demonstrates very competitive performance compared with other state-of-the-art (SOTA) approaches. Our source codes will be provided in the following link <uri xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">https://github.com/yuanchaosu/TGRS-graphGST</uri> .","<method>Transformer</method>, <method>Node embedding (NE)</method>, <method>Positional encoding (PE)</method>, <method>Graph generative structure-aware Transformer (GraphGST)</method>, <method>Absolute positional encoding (APE)</method>, <method>Self-supervised learning</method>, <method>Cross-view contrastive learning (CL)</method>"
2024,https://openalex.org/W4391130239,Engineering,Learning to Aggregate Multi-Scale Context for Instance Segmentation in Remote Sensing Images,"The task of instance segmentation in remote sensing images, aiming at performing per-pixel labeling of objects at the instance level, is of great importance for various civil applications. Despite previous successes, most existing instance segmentation methods designed for natural images encounter sharp performance degradations when they are directly applied to top-view remote sensing images. Through careful analysis, we observe that the challenges mainly come from the lack of discriminative object features due to severe scale variations, low contrasts, and clustered distributions. In order to address these problems, a novel context aggregation network (CATNet) is proposed to improve the feature extraction process. The proposed model exploits three lightweight plug-and-play modules, namely, dense feature pyramid network (DenseFPN), spatial context pyramid (SCP), and hierarchical region of interest extractor (HRoIE), to aggregate global visual context at feature, spatial, and instance domains, respectively. DenseFPN is a multi-scale feature propagation module that establishes more flexible information flows by adopting interlevel residual connections, cross-level dense connections, and feature reweighting strategy. Leveraging the attention mechanism, SCP further augments the features by aggregating global spatial context into local regions. For each instance, HRoIE adaptively generates RoI features for different downstream tasks. Extensive evaluations of the proposed scheme on iSAID, DIOR, NWPU VHR-10, and HRSID datasets demonstrate that the proposed approach outperforms state-of-the-arts under similar computational costs. Source code and pretrained models are available at https://github.com/yeliudev/CATNet.","<method>context aggregation network (CATNet)</method>, <method>dense feature pyramid network (DenseFPN)</method>, <method>spatial context pyramid (SCP)</method>, <method>hierarchical region of interest extractor (HRoIE)</method>, <method>attention mechanism</method>"
2024,https://openalex.org/W4391248672,Engineering,Pedestrian 3D Shape Understanding for Person Re-Identification via Multi-View Learning,"Recent development in computing power has resulted in performance improvements on holistic(none-occluded) person Re-Identification (ReID) tasks. Nevertheless, the precision of the recent research will diminish when a pedestrian is obstructed by obstacles. Within the realm of 2D space, the loss of information from obstructed objects continues to pose significant challenges in the context of person ReID. Person is a 3D non-grid object, and thus semantic representation learning in only 2D space limits the understanding of occluded person. In the present work, we propose a network based on 3D multi-view learning, allowing it to acquire geometric and shape details of an occluded pedestrian from 3D space. Simultaneously, it capitalizes on advancements in 2D-based networks to extract semantic representations from 3D multi-views. Specifically, the surface random selection strategy is proposed to convert images of 2D RGB into 3D multi-views. Using this strategy, we build four extensive 3D multi-view data collections for person ReID. After that, Pedestrian 3D Shape Understanding for Person Re-Identification via Multi-View Learning(MV-3DSReID), is proposed for identifying the person by learning person geometry and structure representation from the groups of multi-view images. In comparison to alternative data formats (e.g., 2D RGB, 3D point cloud), multi-view images complement each other's detailed features of the 3D object by adjusting rendering viewpoints, thus facilitating a more comprehensive understanding of the person for both holistic and occluded ReID situations. Experiments on occluded and holistic ReID tasks demonstrate performance levels comparable to state-of-the-art methods, validating the effectiveness of our proposed approach in tackling challenges related to occlusion. The code is available at https://github.com/hangjiaqi1/MV-TransReID.","<method>3D multi-view learning</method>, <method>2D-based networks</method>, <method>surface random selection strategy</method>, <method>Pedestrian 3D Shape Understanding for Person Re-Identification via Multi-View Learning (MV-3DSReID)</method>"
2024,https://openalex.org/W4391814337,Engineering,Normalizing Large Scale Sensor-Based MWD Data: An Automated Method toward A Unified Database,"In the context of geo-infrastructures and specifically tunneling projects, analyzing the large-scale sensor-based measurement-while-drilling (MWD) data plays a pivotal role in assessing rock engineering conditions. However, handling the big MWD data due to multiform stacking is a time-consuming and challenging task. Extracting valuable insights and improving the accuracy of geoengineering interpretations from MWD data necessitates a combination of domain expertise and data science skills in an iterative process. To address these challenges and efficiently normalize and filter out noisy data, an automated processing approach integrating the stepwise technique, mode, and percentile gate bands for both single and peer group-based holes was developed. Subsequently, the mathematical concept of a novel normalizing index for classifying such big datasets was also presented. The visualized results from different geo-infrastructure datasets in Sweden indicated that outliers and noisy data can more efficiently be eliminated using single hole-based normalizing. Additionally, a relational unified PostgreSQL database was created to store and automatically transfer the processed and raw MWD as well as real time grouting data that offers a cost effective and efficient data extraction tool. The generated database is expected to facilitate in-depth investigations and enable application of the artificial intelligence (AI) techniques to predict rock quality conditions and design appropriate support systems based on MWD data.",No methods found.
2024,https://openalex.org/W4391843476,Engineering,Artificial intelligence (AI) cybersecurity dimensions: a comprehensive framework for understanding adversarial and offensive AI,"Abstract As Artificial Intelligence (AI) rapidly advances and integrates into various domains, cybersecurity emerges as a critical field grappling with both the benefits and pitfalls of AI technologies. This paper explores the multifaceted dimensions of AI-driven cyberattacks, offering insights into their implications, mitigation strategies, underlying motivations, and profound societal impacts. The research centres on developing and presenting the AI Cybersecurity Dimensions (AICD) Framework, a comprehensive, multidimensional schema designed to guide academics, policymakers, and industry professionals in understanding and combating the evolving challenges posed by AI-driven cyber threats. The research unveils the complex dynamics of offensive AI, stressing the need for adaptive defences and ethical considerations. Concurrently, the study highlights adversarial AI threats, calling for proactive measures to address their potential ramifications. Through rigorous textual analyses and extensive literature reviews, the paper underscores the urgency for interdisciplinary approaches to bridge the technology-humanity chasm traditionally observed in cybersecurity discussions. By synthesising these diverse elements, the AICD Framework emerges as an instrumental tool for holistic understanding and practical interventions in the AI-infused cybersecurity landscape. The paper concludes with an urgent call for collaborative efforts in research and practice to navigate the intricate challenges and capitalise on the opportunities borne from the convergence of AI and cybersecurity.",No methods found.
2024,https://openalex.org/W4391973098,Engineering,The use of machine learning techniques to investigate the properties of metakaolin-based geopolymer concrete,"The construction industry significantly contributes to global greenhouse gas emissions, highlighting the imperative for developing environmentally friendly construction materials. Geopolymers, particularly those utilizing metakaolin (MK), have emerged as a promising green alternative to conventional concrete. However, the acquisition of MK-based geopolymer concrete with optimal mechanical properties poses challenges due to numerous influential factors, disagreement over various findings, and the lack of a reliable predictive model. This study aimed to address this gap by employing a wide range of machine learning methods, namely gradient boosting machine, random forest, decision tree, artificial neural network, and support vector machine. Different optimization and regularization techniques were used to comprehensively understand the factors affecting the compressive strength of MK-based geopolymer concrete, including mixture design, chemical characteristics of the initial binder and activators, and different curing regimes. The results demonstrated the exceptional performance of the gradient boosting machine in predicting the compressive strength of MK-based geopolymer concrete, achieving a coefficient of determination of 0.983 and a mean absolute error of 1.615 MPa. Additionally, the study employed partial dependence plots, feature importance analysis, and SHapley Additive exPlanations (SHAP) to elucidate the proposed models. The coarse-to-fine aggregate ratio, H2O/Na2O molar ratio, extra water content, and sodium hydroxide concentration were identified as the most critical parameters affecting the compressive strength of MK-based geopolymer concrete. This research contributes to advancing the development of sustainable construction materials, streamlining experimental tasks, minimizing the need for labor and materials, improving time efficiency, and providing valuable insights for optimizing the design of MK-based geopolymer concrete.","<method>gradient boosting machine</method>, <method>random forest</method>, <method>decision tree</method>, <method>artificial neural network</method>, <method>support vector machine</method>"
2024,https://openalex.org/W4392752316,Engineering,"AI literacy for users – A comprehensive review and future research directions of learning methods, components, and effects","The rapid advancement of artificial intelligence (AI) has brought transformative changes to various aspects of human life, leading to an exponential increase in the number of AI users. The broad access and usage of AI enable immense benefits but also give rise to significant challenges. One way for AI users to address these challenges is to develop AI literacy, referring to human proficiency in different subject areas of AI that enable purposeful, efficient, and ethical usage of AI technologies. This study aims to comprehensively understand and structure the research on AI literacy for AI users through a systematic, scoping literature review. Therefore, we synthesize the literature, provide a conceptual framework, and develop a research agenda. Our review paper holistically assesses the fragmented AI literacy research landscape (68 papers) while critically examining its specificity to different user groups and its distinction from other technology literacies, exposing that research efforts are partly not well integrated. We organize our findings in an overarching conceptual framework structured along the learning methods leading to, the components constituting, and the effects stemming from AI literacy. Our research agenda – oriented along the developed conceptual framework – sheds light on the most promising research opportunities to prepare AI users for an AI-powered future of work and society.",No methods found.
2024,https://openalex.org/W4394685122,Engineering,Sequence Training and Data Shuffling to Enhance the Accuracy of Recurrent Neural Network Based Battery Voltage Models,"&lt;div class=""section abstract""&gt;&lt;div class=""htmlview paragraph""&gt;Battery terminal voltage modelling is crucial for various applications, including electric vehicles, renewable energy systems, and portable electronics. Terminal voltage models are used to determine how a battery will respond under load and can be used to calculate run-time, power capability, and heat generation and as a component of state estimation approaches, such as for state of charge. Previous studies have shown better voltage modelling accuracy for long short-term memory (LSTM) recurrent neural networks than other traditional methods (e.g., equivalent circuit and electrochemical models). This study presents two new approaches – sequence training and data shuffling – to improve LSTM battery voltage models further, making them an even better candidate for the high-accuracy modelling of lithium-ion batteries. Because the LSTM memory captures information from past time steps, it must typically be trained using one series of continuous data. Instead, the proposed sequence training approach feeds a fixed window of prior data (e.g., 100 seconds) into the LSTM at each time step to initialize the memory states properly and then only uses the output at the current time step. With this method, the LSTM just requires the prior data window to be continuous, thereby allowing the handling of discontinuities. This also means that during the training process, the data can be shuffled randomly, enabling mini-batches to speed up the training significantly. When these approaches were applied, LSTM voltage estimation error was reduced by 22%, from 28.5 mV to 22.3 mV RMS error over four drive cycles and temperatures from -20 to 25°C.&lt;/div&gt;&lt;/div&gt;","<method>long short-term memory (LSTM) recurrent neural networks</method>, <method>sequence training</method>, <method>data shuffling</method>"
2024,https://openalex.org/W4390533386,Engineering,"A Systematic Literature Review of Digital Twin Research for Healthcare Systems: Research Trends, Gaps, and Realization Challenges","Using the PRISMA approach, we present the first systematic literature review of digital twin (DT) research in healthcare systems (HSs). This endeavor stems from the pressing need for a thorough analysis of this emerging yet fragmented research area, with the goal of consolidating knowledge to catalyze its growth. Our findings are structured around three research questions aimed at identifying: (i) current research trends, (ii) gaps, and (iii) realization challenges. Current trends indicate global interest and interdisciplinary collaborations to address complex HS challenges. However, existing research predominantly focuses on conceptualization; research on integration, verification, and implementation is nascent. Additionally, we document that a substantial body of papers mislabel their work, often disregarding modeling and twinning methods that are necessary elements of a DT. Furthermore, we provide a non-exhaustive classification of the literature based on two axes: <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">the object</i> (i.e., product or process) and <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">the context</i> (i.e., patient's body, medical procedures, healthcare facilities, and public health). While this is a testament to the diversity of the field, it implies a specific pattern that could be reimagined. We also identify two gaps: (i) considering the human-in-the-loop nature of HSs with a focus on provider decision-making and (ii) implementation research. Lastly, we discuss two challenges for broad-scale implementation of DTs in HSs: improving virtual-to-physical connectivity and data-related issues. In conclusion, this study suggests that DT research could potentially help alleviate the acute shortcomings of HSs that are often manifested in the inability to concurrently improve the quality of care, provider wellbeing, and cost efficiency.",No methods found.
2024,https://openalex.org/W4390751384,Engineering,Robotics in Healthcare: A Survey,"Abstract Research and innovation in the area of robotics in healthcare has seen significant growth in recent years. Global trends indicate that patients are getting older and sicker, while demands in healthcare workers are increasing their chance of injury. Robotic technology has the potential to enable high levels of patient care, clinical productivity and safety for both patients and healthcare workers. This paper surveys the state-of-the-art in robotics in healthcare and well-being, with particular attention to the key barriers and enablers to the implementation of this technology in real-world settings. Desktop research was used to identify available and emerging robotic technology currently in use (or with potential use) in healthcare settings. Primary sources of information included: academic publications, international organisations, commercial websites and online news agencies. In this paper, applications of robots in healthcare were divided into five main areas: service, assistive, socially-assistive, teleoperated and interventional robots. The maturity and readiness of different products is still an open challenge, with service and interventional robots leading the way. Wide-spread adoption of robots is likely to happen as the cost of the technology reduces, and wide evidence of beneficial long-term impact is available. This manuscript identified the main drivers, challenges, opportunities and considerations for implementing robots in healthcare. We hope this manuscript will raise awareness about robotics in healthcare among a wider audience to maximise availability, quality, and acceptability this technology.",No methods found.
2024,https://openalex.org/W4390975281,Engineering,Semantic and Instance Segmentation in Coastal Urban Spatial Perception: A Multi-Task Learning Framework with an Attention Mechanism,"With the continuous acceleration of urbanization, urban planning and design require more in-depth research and development. Street view images can express rich urban features and guide residents’ emotions toward a city, thereby providing the most intuitive reflection of their perception of the city’s spatial quality. However, current researchers mainly conduct research on urban spatial quality through subjective experiential judgment, which includes problems such as a high cost and a low judgment accuracy. In response to these problems, this study proposes a multi-task learning urban spatial attribute perception model that integrates an attention mechanism. Via this model, the existing attributes of urban street scenes are analyzed. Then, the model is improved by introducing semantic segmentation and instance segmentation to identify and match the qualities of the urban space. The experimental results show that the multi-task learning urban spatial attribute perception model with an integrated attention mechanism has prediction accuracies of 79.54%, 78.62%, 79.68%, 77.42%, 78.45%, and 76.98% for the urban spatial attributes of beauty, boredom, depression, liveliness, safety, and richness, respectively. The accuracy of the multi-task learning urban spatial scene feature image segmentation model with an integrated attention mechanism is 95.4, 94.8, 96.2, 92.1, and 96.7 for roads, walls, sky, vehicles, and buildings, respectively. The multi-task learning urban spatial scene feature image segmentation model with an integrated attention mechanism has a higher recognition accuracy for urban spatial buildings than other models. These research results indicate the model’s effectiveness in matching urban spatial quality with public perception.","<method>multi-task learning</method>, <method>attention mechanism</method>, <method>semantic segmentation</method>, <method>instance segmentation</method>"
2024,https://openalex.org/W4391404732,Engineering,"The Convergence of Intelligent Tutoring, Robotics, and IoT in Smart Education for the Transition from Industry 4.0 to 5.0","This review paper provides a comprehensive analysis of the automation of smart education in the context of Industry 5.0 from 78 papers, focusing on the integration of advanced technologies and the development of innovative, effective, and ethical educational solutions for the future workforce. As the world transitions into an era characterized by human–machine collaboration and rapidly evolving technologies, there is an urgent need to recognize the pivotal role of smart education in preparing individuals for the opportunities and challenges presented by the new industrial landscape. The paper examines key components of smart education, including intelligent tutoring systems, adaptive learning environments, learning analytics, and the application of the Internet of Things (IoT) in education. It also discusses the role of advanced technologies such as artificial intelligence (AI), machine learning (ML), robotics, and augmented and virtual reality (AR/VR) in shaping personalized and immersive learning experiences. The review highlights the importance of smart education in addressing the growing demand for upskilling and reskilling, fostering a culture of lifelong learning, and promoting adaptability, resilience, and self-improvement among learners. Furthermore, the paper delves into the challenges and ethical considerations associated with the implementation of smart education, addressing issues such as data privacy, the digital divide, teacher and student readiness, and the potential biases in AI-driven systems. Through a presentation of case studies and examples of successful smart education initiatives, the review aims to inspire educators, policymakers, and industry stakeholders to collaborate and innovate in the design and implementation of effective smart education solutions. Conclusively, the paper outlines emerging trends, future directions, and potential research opportunities in the field of smart education, emphasizing the importance of continuous improvement and the integration of new technologies to ensure that education remains relevant and effective in the context of Industry 5.0. By providing a holistic understanding of the key components, challenges, and potential solutions associated with smart education, this review paper seeks to contribute to the ongoing discourse surrounding the automation of smart education and its role in preparing the workforce for the future of work.","<method>artificial intelligence (AI)</method>, <method>machine learning (ML)</method>"
2024,https://openalex.org/W4391482136,Engineering,A comprehensive analysis of the emerging modern trends in research on photovoltaic systems and desalination in the era of artificial intelligence and machine learning,"Integration of photovoltaic (PV) systems, desalination technologies, and Artificial Intelligence (AI) combined with Machine Learning (ML) has introduced a new era of remarkable research and innovation. This review article thoroughly examines the recent advancements in the field, focusing on the interplay between PV systems and water desalination within the framework of AI and ML applications, along with it analyses current research to identify significant patterns, obstacles, and prospects in this interdisciplinary field. Furthermore, review examines the incorporation of AI and ML methods in improving the performance of PV systems. This includes raising their efficiency, implementing predictive maintenance strategies, and enabling real-time monitoring. It also explores the transformative influence of intelligent algorithms on desalination techniques, specifically addressing concerns pertaining to energy usage, scalability, and environmental sustainability. This article provides a thorough analysis of the current literature, identifying areas where research is lacking and suggesting potential future avenues for investigation. These advancements have resulted in increased efficiency, decreased expenses, and improved sustainability of PV system. By utilizing artificial intelligence technologies, freshwater productivity can increase by 10 % and efficiency. This review offers significant and informative perspectives for researchers, engineers, and policymakers involved in renewable energy and water technology. It sheds light on the latest advancements in photovoltaic systems and desalination, which are facilitated by AI and ML. The review aims to guide towards a more sustainable and technologically advanced future.","<method>Artificial Intelligence (AI)</method>, <method>Machine Learning (ML)</method>, <method>predictive maintenance strategies</method>, <method>intelligent algorithms</method>"
2024,https://openalex.org/W4391484346,Engineering,Sensors for Emerging Water Contaminants: Overcoming Roadblocks to Innovation,"Ensuring water quality and safety requires the effective detection of emerging contaminants, which present significant risks to both human health and the environment. Field deployable low-cost sensors provide solutions to detect contaminants at their source and enable large-scale water quality monitoring and management. Unfortunately, the availability and utilization of such sensors remain limited. This Perspective examines current sensing technologies for detecting emerging contaminants and analyzes critical barriers, such as high costs, lack of reliability, difficulties in implementation in real-world settings, and lack of stakeholder involvement in sensor design. These technical and nontechnical barriers severely hinder progression from proof-of-concepts and negatively impact user experience factors such as ease-of-use and actionability using sensing data, ultimately affecting successful translation and widespread adoption of these technologies. We provide examples of specific sensing systems and explore key strategies to address the remaining scientific challenges that must be overcome to translate these technologies into the field such as improving sensitivity, selectivity, robustness, and performance in real-world water environments. Other critical aspects such as tailoring research to meet end-users' requirements, integrating cost considerations and consumer needs into the early prototype design, establishing standardized evaluation and validation protocols, fostering academia-industry collaborations, maximizing data value by establishing data sharing initiatives, and promoting workforce development are also discussed. The Perspective describes a set of guidelines for the development, translation, and implementation of water quality sensors to swiftly and accurately detect, analyze, track, and manage contamination.",No methods found.
2024,https://openalex.org/W4391574742,Engineering,Optimum tuned mass damper inerter under near-fault pulse-like ground motions of buildings including soil-structure interaction,"This study investigates the effectiveness of the tuned mass damper inerter (TMDI) in mitigating building response, considering the soil structure interaction (SSI). Three types of models are examined: single degree of freedom (SDOF), low-rise multi-degree of freedom (MDOF), and high-rise MDOF. Additionally, the natural period of the SDOF model is varied to explore the TMDI's efficacy across different ranges. Frequency and time domain analysis are conducted under pulse-like ground motions. The H2 and genetic algorithm (GA) are used to optimize the parameters of the TMDI. In this optimization method the transfer function for displacement response is minimized. In time domain analysis we used Newmark's integration method to solve the equation of motion for all the cases considered. It is found that the optimized TMDI proves highly effective in mitigating the displacement response of the buildings, accounting for SSI. Notably, its efficiency is more pronounced when pulse period aligns closely with the buildings' natural period. In addition, a notable pattern emerges, wherein the TMDI excels in mitigating response for buildings experiencing large motion, thereby enhancing safety under severe conditions. These findings offer valuable insights into the application and optimization of the TMDI to enhance seismic performance in various buildings, while considering complex interaction with the soil.",<method>genetic algorithm (GA)</method>
2024,https://openalex.org/W4391693184,Engineering,3DUV-NetR+: A 3D hybrid semantic architecture using transformers for brain tumor segmentation with MultiModal MR images,"Brain tumor segmentation plays a substantial role in Medical Image Analysis (MIS). In this regard, automatic segmentation methods facilitate precise and efficient segmentation, significantly contributing to diagnosis and treatment planning in medical applications. Recently, several Deep Learning-based architectures have been proposed to revolutionize the MIS field. Particularly, the combination of Convolution Neural Networks (CNNs) and Transformers has greatly enhanced and developed segmentation results. Moreover, the Attention mechanism in Transformers allows the modeling of long-range contextual features extracted from CNNs' encoder part. This paper proposes a hybrid advanced 3D model for brain tumor segmentation using multi-modal magnetic resonance images. The model benefits from the features extracted from the encoder of 3DU-Net and V-Net architectures at each depth. Then, a concatenation between these features and their fusion is carried out at each decoder depth to build new significant features followed by a 3D convolution layer and Transformers block for more contextual information. In addition, a final convolution block is applied to get the segmented tumor. To this end, the model is evaluated on the BraTS 2020 dataset to segment different sub-regions of brain tumors. The obtained results demonstrate the effectiveness of the proposed model in terms of dice similarity coefficient (DSC) and Hausdorff Distance (HD). For DSC, 91.95% and 82.80% and 81.70% for Whole Tumor(WT), Tumor Core (TC), and Enhancing Tumor(ET), respectively are archived, while for HD, 4.9 mm, 6.0 mm and 3.8 mm for WT, TC and ET are accomplished.","<method>Deep Learning-based architectures</method>, <method>Convolution Neural Networks (CNNs)</method>, <method>Transformers</method>, <method>Attention mechanism in Transformers</method>, <method>3DU-Net</method>, <method>V-Net</method>, <method>3D convolution layer</method>, <method>Transformers block</method>"
2024,https://openalex.org/W4391867349,Engineering,Topology Optimization of Adaptive Structures: New Limits of Material Economy,"Adaptive structures can counteract the effect of external loads and other environmental actions through active manipulation of the internal force flow (i.e., the load path) and geometry (i.e., form or shape). Previous studies have shown that adaptation enables significant mass and energy saving compared to conventional structures that resist the effect of loading solely through material strength and stiffness (i.e., passive structures). The computational synthesis of adaptive structures is a challenging process since it involves optimization of the structural layout as well as sensor and actuator placement, which is, generally, a Mixed-Integer Non-linear Programming (MINLP) problem. Previous formulations employ sizing and/or geometry optimization in combination with actuator placement optimization. No method has yet been formulated for the simultaneous synthesis of the structural topology, element sizing, and placement of actuators. This paper offers the first-ever formulation for the All-In-One (AIO) topology optimization of adaptive structures based on the Ground Structure approach. The objective function comprises the mass of structural elements and actuators. The design variables are structural topology, element cross-section areas, and actuator locations. State variables include element forces and deformations, nodal displacements, and actuator commands. Constraints ensure that feasible solutions satisfy equilibrium and geometric compatibility as well as limits on stress, stability, nodal displacements, and actuator forces. Auxiliary constraints are implemented to enable the simultaneous synthesis of the structural layout and actuator placement and linearize the formulation into a Mixed-Integer Linear Problem (MILP) that can be solved to a global optimum. Due to a large number of variables, the AIO formulation can be typically applied to small-scale problems. To reduce the computational cost, a two-step sequential formulation is developed and benchmarked against the AIO method. Results show that the sequential method produces solutions of similar quality compared to the AIO one albeit with a significantly reduced computational cost. Results confirm that the optimal adaptive solutions vastly outperform topology-optimized conventional (i.e., passive) solutions. Adaptive solutions approach the limit of material economy (fully stressed design, e.g., Michell trusses) and, in parallel, satisfy important constraints including displacements and stability that would not be possible without adaptation.",No methods found.
2024,https://openalex.org/W4392529708,Engineering,A machine learning-based framework for clustering residential electricity load profiles to enhance demand response programs,"Load shapes derived from smart meter data are frequently employed to analyze daily energy consumption patterns, particularly in the context of applications like Demand Response (DR). Nevertheless, one of the most important challenges to this endeavor lies in identifying the most suitable consumer clusters with similar consumption behaviors. In this paper, we present a novel machine learning based framework in order to achieve optimal load profiling through a real case study, utilizing data from almost 5000 households in London. Four widely used clustering algorithms are applied specifically K-means, K-medoids, Hierarchical Agglomerative Clustering and Density-based Spatial Clustering. An empirical analysis as well as multiple evaluation metrics are leveraged to assess those algorithms. Following that, we redefine the problem as a probabilistic classification one, with the classifier emulating the behavior of a clustering algorithm, leveraging Explainable AI (xAI) to enhance the interpretability of our solution. According to the clustering algorithm analysis the optimal number of clusters for this case is seven. Despite that, our methodology shows that two of the clusters, almost 10% of the dataset, exhibit significant internal dissimilarity. As a result, these clusters have been excluded from consideration for DR programs. The scalability and versatility of our solution makes it an ideal choice for power utility companies aiming to segment their users for creating more targeted DR programs.","<method>K-means</method>, <method>K-medoids</method>, <method>Hierarchical Agglomerative Clustering</method>, <method>Density-based Spatial Clustering</method>, <method>probabilistic classification</method>, <method>Explainable AI (xAI)</method>"
2024,https://openalex.org/W4400937555,Engineering,The diagnostic and triage accuracy of the GPT-3 artificial intelligence model: an observational study,"BackgroundArtificial intelligence (AI) applications in health care have been effective in many areas of medicine, but they are often trained for a single task using labelled data, making deployment and generalisability challenging. How well a general-purpose AI language model performs diagnosis and triage relative to physicians and laypeople is not well understood.MethodsWe compared the predictive accuracy of Generative Pre-trained Transformer 3 (GPT-3)'s diagnostic and triage ability for 48 validated synthetic case vignettes (<50 words; sixth-grade reading level or below) of both common (eg, viral illness) and severe (eg, heart attack) conditions to a nationally representative sample of 5000 lay people from the USA who could use the internet to find the correct options and 21 practising physicians at Harvard Medical School. There were 12 vignettes for each of four triage categories: emergent, within one day, within 1 week, and self-care. The correct diagnosis and triage category (ie, ground truth) for each vignette was determined by two general internists at Harvard Medical School. For each vignette, human respondents and GPT-3 were prompted to list diagnoses in order of likelihood, and the vignette was marked as correct if the ground-truth diagnosis was in the top three of the listed diagnoses. For triage accuracy, we examined whether the human respondents' and GPT-3's selected triage was exactly correct according to the four triage categories, or matched a dichotomised triage variable (emergent or within 1 day vs within 1 week or self-care). We estimated GPT-3's diagnostic and triage confidence on a given vignette using a modified bootstrap resampling procedure, and examined how well calibrated GPT-3's confidence was by computing calibration curves and Brier scores. We also performed subgroup analysis by case acuity, and an error analysis for triage advice to characterise how its advice might affect patients using this tool to decide if they should seek medical care immediately.FindingsAmong all cases, GPT-3 replied with the correct diagnosis in its top three for 88% (42/48, 95% CI 75–94) of cases, compared with 54% (2700/5000, 53–55) for lay individuals (p<0.0001) and 96% (637/666, 94–97) for physicians (p=0·012). GPT-3 triaged 70% correct (34/48, 57–82) versus 74% (3706/5000, 73–75; p=0.60) for lay individuals and 91% (608/666, 89–93%; p<0.0001) for physicians. As measured by the Brier score, GPT-3 confidence in its top prediction was reasonably well calibrated for diagnosis (Brier score=0·18) and triage (Brier score=0·22). We observed an inverse relationship between case acuity and GPT-3 accuracy (p<0·0001) with a fitted trend line of –8·33% decrease in accuracy for every level of increase in case acuity. For triage error analysis, GPT-3 deprioritised truly emergent cases in seven instances.InterpretationA general-purpose AI language model without any content-specific training could perform diagnosis at levels close to, but below, physicians and better than lay individuals. We found that GPT-3's performance was inferior to physicians for triage, sometimes by a large margin, and its performance was closer to that of lay individuals. Although the diagnostic performance of GPT-3 was comparable to physicians, it was significantly better than a typical person using a search engine.FundingThe National Heart, Lung, and Blood Institute.","<method>Generative Pre-trained Transformer 3 (GPT-3)</method>, <method>modified bootstrap resampling procedure</method>"
2024,https://openalex.org/W4390492410,Engineering,Deep Learning for Integrated Origin–Destination Estimation and Traffic Sensor Location Problems,"Traffic control and management applications require the full realization of traffic flow data. Frequently, such data are acquired by traffic sensors with two issues: it is not practicable or even possible to place traffic sensors on every link in a network; sensors do not provide direct information about origin–destination (O–D) demand flows. Therefore, it is imperative to locate the best places to deploy traffic sensors and then augment the knowledge obtained from this link flow sample to predict the entire traffic flow of the network. This article provides a resilient deep learning (DL) architecture combined with a global sensitivity analysis tool to solve O–D estimation and sensor location problems simultaneously. The proposed DL architecture is based on the stacked sparse autoencoder (SAE) model for accurately estimating the entire O–D flows of the network using link flows, thus reversing the conventional traffic assignment problem. The SAE model extracts traffic flow characteristics and derives a meaningful relationship between traffic flow data and network topology. To train the proposed DL architecture, synthetic link flow data were created randomly from the historical demand data of the network. Finally, a global sensitivity analysis was implemented to prioritize the importance of each link in the O–D estimation step to solve the sensor location problem. Two networks of different sizes were used to validate the performance of the model. The efficiency of the proposed method for solving the combination of traffic flow estimation and sensor location problems was confirmed from a low root-mean-square error with a reduction in the number of link flows required.","<method>deep learning (DL) architecture</method>, <method>stacked sparse autoencoder (SAE) model</method>, <method>global sensitivity analysis</method>"
2024,https://openalex.org/W4400798015,Engineering,Evaluation of artificial intelligence-powered screening for sexually transmitted infections-related skin lesions using clinical images and metadata,"Abstract Background Sexually transmitted infections (STIs) pose a significant global public health challenge. Early diagnosis and treatment reduce STI transmission, but rely on recognising symptoms and care-seeking behaviour of the individual. Digital health software that distinguishes STI skin conditions could improve health-seeking behaviour. We developed and evaluated a deep learning model to differentiate STIs from non-STIs based on clinical images and symptoms. Methods We used 4913 clinical images of genital lesions and metadata from the Melbourne Sexual Health Centre collected during 2010–2023. We developed two binary classification models to distinguish STIs from non-STIs: (1) a convolutional neural network (CNN) using images only and (2) an integrated model combining both CNN and fully connected neural network (FCN) using images and metadata. We evaluated the model performance by the area under the ROC curve (AUC) and assessed metadata contributions to the Image-only model. Results Our study included 1583 STI and 3330 non-STI images. Common STI diagnoses were syphilis (34.6%), genital warts (24.5%) and herpes (19.4%), while most non-STIs (80.3%) were conditions such as dermatitis, lichen sclerosis and balanitis. In both STI and non-STI groups, the most frequently observed groups were 25–34 years (48.6% and 38.2%, respectively) and heterosexual males (60.3% and 45.9%, respectively). The Image-only model showed a reasonable performance with an AUC of 0.859 (SD 0.013). The Image + Metadata model achieved a significantly higher AUC of 0.893 (SD 0.018) compared to the Image-only model ( p &lt; 0.01). Out of 21 metadata, the integration of demographic and dermatological metadata led to the most significant improvement in model performance, increasing AUC by 6.7% compared to the baseline Image-only model. Conclusions The Image + Metadata model outperformed the Image-only model in distinguishing STIs from other skin conditions. Using it as a screening tool in a clinical setting may require further development and evaluation with larger datasets.","<method>convolutional neural network (CNN)</method>, <method>fully connected neural network (FCN)</method>"
2024,https://openalex.org/W4394845368,Engineering,Perspectives in the Development of Tools to Assess Vaccine Literacy,"Vaccine literacy (VL) is the ability to find, understand, and evaluate vaccination-related information to make appropriate decisions about immunization. The tools developed so far for its evaluation have produced consistent results. However, some dimensions may be underestimated due to the complexity of factors influencing VL. Moreover, the heterogeneity of methods used in studies employing these tools hinders a comprehensive understanding of its role even more. To overcome these limitations, a path has been sought to propose new instruments. This has necessitated updating earlier literature reviews on VL and related tools, exploring its relationship with vaccine hesitancy (VH), and examining associated variables like beliefs, attitudes, and behaviors towards immunization. Based on the current literature, and supported by the re-analysis of a dataset from an earlier study, we propose a theoretical framework to serve as the foundation for creating future assessment tools. These instruments should not only evaluate the psychological factors underlying the motivational aspect of VL, but also encompass knowledge and competencies. The positioning of VL in the framework at the intersection between sociodemographic antecedents and attitudes, leading to behaviors and outcomes, explains why and how VL can directly or indirectly influence vaccination decisions by countering VH and operating at personal, as well as at organizational and community levels.",No methods found.
2024,https://openalex.org/W4391137578,Social Sciences,A multinational study on the factors influencing university students’ attitudes and usage of ChatGPT,"Abstract Artificial intelligence models, like ChatGPT, have the potential to revolutionize higher education when implemented properly. This study aimed to investigate the factors influencing university students’ attitudes and usage of ChatGPT in Arab countries. The survey instrument “TAME-ChatGPT” was administered to 2240 participants from Iraq, Kuwait, Egypt, Lebanon, and Jordan. Of those, 46.8% heard of ChatGPT, and 52.6% used it before the study. The results indicated that a positive attitude and usage of ChatGPT were determined by factors like ease of use, positive attitude towards technology, social influence, perceived usefulness, behavioral/cognitive influences, low perceived risks, and low anxiety. Confirmatory factor analysis indicated the adequacy of the “TAME-ChatGPT” constructs. Multivariate analysis demonstrated that the attitude towards ChatGPT usage was significantly influenced by country of residence, age, university type, and recent academic performance. This study validated “TAME-ChatGPT” as a useful tool for assessing ChatGPT adoption among university students. The successful integration of ChatGPT in higher education relies on the perceived ease of use, perceived usefulness, positive attitude towards technology, social influence, behavioral/cognitive elements, low anxiety, and minimal perceived risks. Policies for ChatGPT adoption in higher education should be tailored to individual contexts, considering the variations in student attitudes observed in this study.",No methods found.
2024,https://openalex.org/W4390609372,Social Sciences,Investigation of the moderation effect of gender and study level on the acceptance and use of generative <scp>AI</scp> by higher education students: Comparative evidence from Poland and Egypt,"Abstract This study delves into the implications of incorporating AI tools, specifically ChatGPT, in higher education contexts. With a primary focus on understanding the acceptance and utilization of ChatGPT among university students, the research utilizes the Unified Theory of Acceptance and Use of Technology (UTAUT) as the guiding framework. The investigation probes into four crucial constructs of UTAUT—performance expectancy, effort expectancy, social influence and facilitating conditions—to understand their impact on the intent and actual use behaviour of students. The study relies on data collected from six universities in two countries and assessed through descriptive statistics and structural equation modelling techniques, and also takes into account participants' gender and study level. The key findings show that performance expectancy, effort expectancy, and social influence significantly influence behavioural intention. Furthermore, behavioural intention, when considered alongside facilitating conditions, influences actual use behaviour. This research also explores the moderating impact of gender and study level on the relationships among these variables. The results not only augment our comprehension of technology acceptance in the context of AI tools but also provide valuable input for formulating strategies that promote effective incorporation of ChatGPT in higher education. The study underscores the need for effective awareness initiatives, bespoke training programmes, and intuitive tool designs to bolster students' perceptions and foster the wider adoption of AI tools in education. Practitioner notes What is already known about this topic ChatGPT is a tool that is quickly gaining worldwide recognition. ChatGPT helps with writing essays and solving assignments. ChatGPT raises ethical concerns about authorship, plagiarism and ethics. What this paper adds This study explores students' acceptance of ChatGPT as an aid in their education, which has not been studied previously. We used the extended Unified Technology Acceptance and Use of Technology theory to test what factors mostly influence the use of ChatGPT by students. We conducted a multiple study in Poland and Egypt based on sampling strategy from six universities. Implications for practice and/or policy ChatGPT is a global game changer and should be incorporated into study programmes. The limitations of ChatGPT should be well explained and known since it is prone to making mistakes. Higher education teachers should be aware of ChatGPT's capabilities.",No methods found.
2024,https://openalex.org/W4391776447,Social Sciences,Theories of motivation: A comprehensive analysis of human behavior drivers,"This paper explores theories of motivation, including instinct theory, arousal theory, incentive theory, intrinsic theory, extrinsic theory, the ARCS model, self-determination theory, expectancy-value theory, and goal-orientation theory. Each theory is described in detail, along with its key concepts, assumptions, and implications for behavior. Intrinsic theory suggests that individuals are motivated by internal factors like enjoyment and satisfaction, while extrinsic theory suggests that external factors like rewards and social pressure drive behavior. Arousal theory says that to feel motivated, people try to keep an optimal level of activation or excitement. Incentive theory suggests that behavior is driven by the promise of rewards or the threat of punishment. The ARCS model, designed to motivate learners, incorporates elements of attention, relevance, confidence, and satisfaction. Self-determination theory proposes that individuals are motivated by their needs for autonomy, competence, and relatedness. The expectation-value theory suggests that behavior is influenced by individuals' beliefs about their ability to succeed and the value they place on the task. The goal-orientation theory suggests that individuals have different goals for engaging in a behavior. By understanding these different theories of motivation, educators, coaches, managers, and individuals may analyze what drives behavior and how to harness it to achieve their goals. In essence, a nuanced comprehension of these diverse motivation theories equips individuals across varied domains with a strategic toolkit to navigate the complex landscape of human behavior, fostering a more profound understanding of what propels actions and how to channel these insights toward the attainment of overarching goals.",No methods found.
2024,https://openalex.org/W4391243055,Social Sciences,Systematic literature review: Quantum machine learning and its applications,"Quantum physics has changed the way we understand our environment, and one of its branches, quantum mechanics, has demonstrated accurate and consistent theoretical results. Quantum computing is the process of performing calculations using quantum mechanics. This field studies the quantum behavior of certain subatomic particles (photons, electrons, etc.) for subsequent use in performing calculations, as well as for large-scale information processing. These advantages are achieved through the use of quantum features, such as entanglement or superposition. These capabilities can give quantum computers an advantage in terms of computational time and cost over classical computers. Nowadays, scientific challenges are impossible to perform by classical computation due to computational complexity (more bytes than atoms in the observable universe) or the time it would take (thousands of years), and quantum computation is the only known answer. However, current quantum devices do not have yet the necessary qubits and are not fault-tolerant enough to achieve these goals. Nonetheless, there are other fields like machine learning, finance, or chemistry where quantum computation could be useful with current quantum devices. This manuscript aims to present a review of the literature published between 2017 and 2023 to identify, analyze, and classify the different types of algorithms used in quantum machine learning and their applications. The methodology follows the guidelines related to Systematic Literature Review methods, such as the one proposed by Kitchenham and other authors in the software engineering field. Consequently, this study identified 94 articles that used quantum machine learning techniques and algorithms and shows their implementation using computational quantum circuits or ansatzs. The main types of found algorithms are quantum implementations of classical machine learning algorithms, such as support vector machines or the k-nearest neighbor model, and classical deep learning algorithms, like quantum neural networks. One of the most relevant applications in the machine learning field is image classification. Many articles, especially within the classification, try to solve problems currently answered by classical machine learning but using quantum devices and algorithms. Even though results are promising, quantum machine learning is far from achieving its full potential. An improvement in quantum hardware is required for this potential to be achieved since the existing quantum computers lack enough quality, speed, and scale to allow quantum computing to achieve its full potential.","<method>support vector machines</method>, <method>k-nearest neighbor model</method>, <method>quantum neural networks</method>"
2024,https://openalex.org/W638137987,Social Sciences,State Formation and Political Legitimacy,"evolution of the state from earlier forms of political organization is associated with revolutionary changes in the structure of inequality. These magnify distinctions in rank and power that outweigh anything previously known in so-called primitive societies. This volume explains how and why people came to accept and even identify themselves with this new form of authority. introduction provides a new theory of legitimacy by synthesizing and uniting earlier theories from psychological, cultural-materialist, rational choice, and Marxist approaches. case studies which follow present a wide range of materials on cultures in both Western and non-Western settings, and across a number of different historical periods. Included are examples from Africa, Asia, Europe, and the New World. Older states such as Ur, Inca, and medieval France are examined along with more contemporary states including Indonesia, Tanzania, and the revolutionary beginnings of the United States. Using a variety of approaches the contributors show in each instance how the state obtained and used its power, then attempted to have its power accepted as the natural order under the protection of supra-naturally ordained authority. No matter how tyrannical or benign, the cases show that state power must be justified by faith and experience that demonstrates its value to the participants. Through such analysis, the book demonstrates that states must be capable of enforcing their rule, but that they cannot deceive populations into accepting state domination. Indeed, the book suggests that social evolution moves toward less coercive rule and increased democratization. Ronald Cohen is a political anthropologist who has taught at the Universities of Toronto, McGill, Northwestern, and Ahmadu Bello, and is on the faculty of the University of Florida. He has carried out field research in Africa, the Arctic and Washington. His major works include The Kanuri of Borno, Dominance and Defiance, Origins of the State,  and a book in preparation on food policy and agricultural transformation in Africa. Judith D. Toland is a lecturer at University College, Northwestern University, and the College of Arts and Sciences, Loyola University of Chicago. She is the director of her own corporate and non-profit consulting firm. She has done fieldwork in Ayacucho, Peru and has written widely on the Inca State.",No methods found.
2024,https://openalex.org/W4391490182,Social Sciences,Digital capabilities to manage agri-food supply chain uncertainties and build supply chain resilience during compounding geopolitical disruptions,"Purpose The agricultural supply chain is susceptible to disruptive geopolitical events. Therefore, agri-food firms must devise robust resilience strategies to hasten recovery and mitigate global food security effects. Hence, the central aim of this paper is to investigate how supply chains could leverage digital technologies to design resilience strategies to manage uncertainty stemming from the external environment disrupted by a geopolitical event. The context of the study is the African agri-food supply chain during the Russian invasion of Ukraine. Design/methodology/approach The authors employ strategic contingency and dynamic capabilities theory arguments to explore the scenario and conditions under which African agri-food firms could leverage digital technologies to formulate contingency strategies and devise mitigation countermeasures. Then, the authors used a multi-case-study analysis of 14 African firms of different sizes and tiers within three main agri-food sectors (i.e. livestock farming, food-crop and fisheries-aquaculture) to explore, interpret and present data and their findings. Findings Downstream firms (wholesalers and retailers) of the African agri-food supply chain are found to extensively use digital seizing and transforming capabilities to formulate worst-case assumptions amid geopolitical disruption, followed by proactive mitigation actions. These capabilities are mainly supported by advanced technologies such as blockchain and additive manufacturing. On the other hand, smaller upstream partners (SMEs, cooperatives and smallholders) are found to leverage less advanced technologies, such as mobile apps and cloud-based data analytics, to develop sensing capabilities necessary to formulate a “wait-and-see” strategy, allowing them to reduce perceptions of heightened supply chain uncertainty and take mainly reactive mitigation strategies. Finally, the authors integrate their findings into a conceptual framework that advances the research agenda on managing supply chain uncertainty in vulnerable areas. Originality/value This study is the first that sought to understand the contextual conditions (supply chain characteristics and firm characteristics) under which companies in the African agri-food supply chain could leverage digital technologies to manage uncertainty. The study advances contingency and dynamic capability theories by providing a new way of interacting in one specific context. In practice, this study assists managers in developing suitable strategies to manage uncertainty during geopolitical disruptions.",No methods found.
2024,https://openalex.org/W4392034144,Social Sciences,"From Industry 4.0 Digital Manufacturing to Industry 5.0 Digital Society: a Roadmap Toward Human-Centric, Sustainable, and Resilient Production","Abstract The present study addresses two critical controversies surrounding the emerging Industry 5.0 agenda. Firstly, it seeks to elucidate the driving forces behind the accelerated momentum of the Industry 5.0 agenda amidst the ongoing digital industrial transformation. Secondly, it explores how the agenda’s sustainability values can be effectively realised. The study conducted a comprehensive content-centric literature synthesis and identified how Industry 4.0 shortcomings adversely impacted sustainability values. Furthermore, the study implements a novel approach that determines how and in what order the sustainability functions of Industry 4.0 should be leveraged to promote the sustainability objectives of Industry 5.0. Results reveal that Industry 4.0 has benefited economic and environmental sustainability values most at the organisational and supply chain levels. Nonetheless, most micro and meso-social sustainability values have been adversely impacted by Industry 4.0. Similarly, Industry 4.0 has been worryingly detrimental to macro sustainability values like social or economic growth equality. These contradictory implications of Industry 4.0 have pulled the Industry 5.0 agenda. However, the results identified nine sustainability functions of Industry 4.0 that, when leveraged appropriately and in the correct order, can offer important implications for realising the economic and socio-environmental goals of Industry 5.0. For example, under extreme unpredictability of business world uncertainties, the business should first leverage the automation and integration capabilities of Industry 4.0 to gain the necessary cost-saving, resource efficiency, risk management capability, and business antifragility that allow them to introduce sustainable innovation into their business model without jeopardising their survival. Various scenarios for empowering Industry 5.0 sustainability values identified in the present study offer important implications for knowledge and practice.",No methods found.
2024,https://openalex.org/W4390499487,Social Sciences,"Cryptocurrency awareness, acceptance, and adoption: the role of trust as a cornerstone","Abstract Cryptocurrencies—i.e., digital or virtual currencies secured by cryptography based on blockchain technology, such as Bitcoin and Ethereum—have brought transformative changes to the global economic landscape. These innovative transaction methods have rapidly made their mark in the financial sector, reshaping the dynamics of the global economy. However, there remains a notable hesitation in its widespread acceptance and adoption, largely due to misconceptions and lack of proper guidance about its use. Such gaps in understanding create an opportunity to address these concerns. Using the technology acceptance model (TAM), this study develops a parsimonious model to explain the awareness, acceptance, and adoption of cryptocurrency. The model was assessed using partial least squares structural equation modeling (PLS-SEM) with a sample of 332 participants aged 18 to 40 years. The findings suggest that cryptocurrency awareness plays a direct, positive, and significant role in shaping cryptocurrency adoption and that this positive relationship is mediated by factors that exemplify cryptocurrency acceptance, namely the ease of use and usefulness of cryptocurrency. The results also reveal that trust is a significant factor that strengthens these direct and mediating relationships. These insights emphasize the necessity of fostering an informed understanding of cryptocurrencies to accelerate their broader adoption in the financial ecosystem. By addressing the misconceptions and reinforcing factors like ease of use, usefulness, and trust, policymakers and financial institutions can better position themselves to integrate and promote cryptocurrency in mainstream financial systems.",No methods found.
2024,https://openalex.org/W4391092266,Social Sciences,Research on the impact of digital transformation on the production efficiency of manufacturing enterprises: Institution-based analysis of the threshold effect,"Digital transformation is the core strategic orientation in the digital economy era and is evolving into a pivotal competitive advantage among countries, industries and enterprises. Based on the analysis of its mechanism, this paper focuses on the data of A-share listed manufacturing enterprises in China from 2011 to 2021; measures and analyses the variables relating to the impact of digital transformation on the production efficiency of manufacturing enterprises; and conducts empirical tests. The research shows that digital transformation plays a significant role in promoting the production efficiency of manufacturing enterprises, and the promotion effect is more obvious for high-tech enterprises and non-state-owned enterprises than for non-high-tech enterprises and state-owned enterprises; digital transformation has a greater promotional effect on the production efficiency of manufacturing enterprises with formal institutions or informal institutions switching from low value intervals to high value intervals. Accordingly, this paper proposes accelerating the digital transformation process of manufacturing enterprises and adopting targeted policies in accordance with the principle of resource endowment. Moreover, from the perspective of institutional environment optimization in digital transformation, this paper proposes building a proper formal institutional environment and strengthening the construction of informal institutions. Our study provides new ideas for constructing enterprise digital transformation indicators. We also offer new insights into the different institutional levels of the effect of digital transformation.",No methods found.
2024,https://openalex.org/W4391528827,Social Sciences,Deep learning-aided decision support for diagnosis of skin disease across skin tones,"Abstract Although advances in deep learning systems for image-based medical diagnosis demonstrate their potential to augment clinical decision-making, the effectiveness of physician–machine partnerships remains an open question, in part because physicians and algorithms are both susceptible to systematic errors, especially for diagnosis of underrepresented populations. Here we present results from a large-scale digital experiment involving board-certified dermatologists ( n = 389) and primary-care physicians ( n = 459) from 39 countries to evaluate the accuracy of diagnoses submitted by physicians in a store-and-forward teledermatology simulation. In this experiment, physicians were presented with 364 images spanning 46 skin diseases and asked to submit up to four differential diagnoses. Specialists and generalists achieved diagnostic accuracies of 38% and 19%, respectively, but both specialists and generalists were four percentage points less accurate for the diagnosis of images of dark skin as compared to light skin. Fair deep learning system decision support improved the diagnostic accuracy of both specialists and generalists by more than 33%, but exacerbated the gap in the diagnostic accuracy of generalists across skin tones. These results demonstrate that well-designed physician–machine partnerships can enhance the diagnostic accuracy of physicians, illustrating that success in improving overall diagnostic accuracy does not necessarily address bias.",<method>deep learning system decision support</method>
2024,https://openalex.org/W4391216329,Social Sciences,"Student psychological well-being in higher education: The role of internal team environment, institutional, friends and family support and academic engagement","Psychological well-being of students is an area of concern in higher education institutes across the world. Although several studies have explored the factors associated with students’ psychological well-being, limited research has focused on the relation between the overall support for students and psychological well-being. Students of higher education may get formal support, in the form of team environment and institutional support; and informal support, in the form of family and friends’ support. The purpose of this study is to examine the relation of these four kinds of support with psychological well-being of management students. We also examine the intervening role of academic engagement in this relationship. Analysis using structural equation modeling and hierarchical regression on data collected from 309 management students from Indian universities, shows that positive internal team environment, and institutional and family support positively relate to students’ psychological well-being. Academic engagement partially mediates the relation between positive internal team environment and psychological well-being, and family support and psychological well-being. Also, academic engagement fully mediates the relation between institutional support and psychological well-being. The study highlights the significance of internal team environment and institutional support for students’ academic engagement and psychological well-being, and the role of academic engagement in determining well-being. Based on these findings, we suggest interventions that can be undertaken by educational institutions to enhance psychological well-being of students. Theoretical implications and research avenues are discussed.",No methods found.
2024,https://openalex.org/W4394681533,Social Sciences,REVIEWING THE IMPACT OF HEALTH INFORMATION TECHNOLOGY ON HEALTHCARE MANAGEMENT EFFICIENCY,"This research paper explores the intricate relationship between Health Information Technology (HIT) and healthcare management efficiency, investigating current trends, emerging technologies, and their potential implications. The study encompasses a thorough literature review, highlighting the impact of HIT on operational and clinical aspects of healthcare delivery. Key findings reveal the transformative role of technology in streamlining administrative processes, improving communication, and enhancing overall patient care. Ethical considerations, patient privacy, and regulation compliance are crucial factors in successfully implementing HIT. Looking towards the future, the paper anticipates the integration of emerging technologies such as Artificial Intelligence, Blockchain, and the Internet of Things, signalling a paradigm shift in healthcare management. While acknowledging the potential benefits, the research also underscores the importance of ethical frameworks, transparency, and user-centred design in adopting these technologies. The study concludes with reflections on the limitations of the research, suggesting avenues for future exploration. Recommendations emphasize the need for ongoing research, longitudinal studies, and a global perspective to ensure healthcare organizations effectively leverage technology while maintaining ethical standards. The findings of this research carry implications for healthcare practitioners, policymakers, and technology innovators, encouraging a strategic and ethical approach to the ever-evolving landscape of health information technology.&#x0D; Keywords: Health Information Technology, Healthcare Management Efficiency, Emerging Technologies, Ethical Considerations, Patient Privacy.",<method>Artificial Intelligence</method>
2024,https://openalex.org/W4390637043,Social Sciences,Blockchain meets machine learning: a survey,"Abstract Blockchain and machine learning are two rapidly growing technologies that are increasingly being used in various industries. Blockchain technology provides a secure and transparent method for recording transactions, while machine learning enables data-driven decision-making by analyzing large amounts of data. In recent years, researchers and practitioners have been exploring the potential benefits of combining these two technologies. In this study, we cover the fundamentals of blockchain and machine learning and then discuss their integrated use in finance, medicine, supply chain, and security, including a literature review and their contribution to the field such as increased security, privacy, and decentralization. Blockchain technology enables secure and transparent decentralized record-keeping, while machine learning algorithms can analyze vast amounts of data to derive valuable insights. Together, they have the potential to revolutionize industries by enhancing efficiency through automated and trustworthy processes, enabling data-driven decision-making, and strengthening security measures by reducing vulnerabilities and ensuring the integrity of information. However, there are still some important challenges to be handled prior to the common use of blockchain and machine learning such as security issues, strategic planning, information processing, and scalable workflows. Nevertheless, until the difficulties that have been identified are resolved, their full potential will not be achieved.",<method>machine learning algorithms</method>
2024,https://openalex.org/W4393119757,Social Sciences,Unmasking bias in artificial intelligence: a systematic review of bias detection and mitigation strategies in electronic health record-based models,"Abstract Objectives Leveraging artificial intelligence (AI) in conjunction with electronic health records (EHRs) holds transformative potential to improve healthcare. However, addressing bias in AI, which risks worsening healthcare disparities, cannot be overlooked. This study reviews methods to handle various biases in AI models developed using EHR data. Materials and Methods We conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and Meta-analyses guidelines, analyzing articles from PubMed, Web of Science, and IEEE published between January 01, 2010 and December 17, 2023. The review identified key biases, outlined strategies for detecting and mitigating bias throughout the AI model development, and analyzed metrics for bias assessment. Results Of the 450 articles retrieved, 20 met our criteria, revealing 6 major bias types: algorithmic, confounding, implicit, measurement, selection, and temporal. The AI models were primarily developed for predictive tasks, yet none have been deployed in real-world healthcare settings. Five studies concentrated on the detection of implicit and algorithmic biases employing fairness metrics like statistical parity, equal opportunity, and predictive equity. Fifteen studies proposed strategies for mitigating biases, especially targeting implicit and selection biases. These strategies, evaluated through both performance and fairness metrics, predominantly involved data collection and preprocessing techniques like resampling and reweighting. Discussion This review highlights evolving strategies to mitigate bias in EHR-based AI models, emphasizing the urgent need for both standardized and detailed reporting of the methodologies and systematic real-world testing and evaluation. Such measures are essential for gauging models’ practical impact and fostering ethical AI that ensures fairness and equity in healthcare.","<method>resampling</method>, <method>reweighting</method>"
2024,https://openalex.org/W4390629445,Social Sciences,Investigating pre-service teachers’ artificial intelligence perception from the perspective of planned behavior theory,"There is a need for teachers who are prepared to teach Artificial Intelligence (AI) across the K-12 learning contexts. Owing to the dearth of teacher education programmes on AI, it is helpful to explore factors to be considered in designing an effective AI programme for future teachers. We posit that understanding how to encourage pre-service teachers to learn AI is thus critical for practitioners and policymakers while designing effective instructional AI teacher education programmes. This exploratory study examined the perceptions of pre-service teachers and their behavioral intention to learn AI, by identifying factors that might affect learning and promoting AI in teacher preparation programmes. This study proposed a research model supported by the theory of planned behavior and expanded with other constructs. The factors that were examined include basic knowledge of AI, subjective norm, AI for social good, perceived self-efficacy, self-transcendent goals, personal relevance, AI anxiety, behavioral intention to learn AI, and actual learning of AI. Using a duly validated questionnaire, we surveyed 796 pre-service teachers in Nigerian Universities. Through structural equation modeling approach analyses, our proposed model explains about 79% of the variance in pre-service teachers' intention to learn AI. Basic knowledge and subjective norm were found to be the most important determinant in pre-service teachers' intention to learn AI. All our hypotheses were supported except for self-efficacy and personal relevance, personal relevance and social good, and behavioral intention and actual learning behavior. The findings provide practitioners, researchers, and policymakers with valuable information to consider in designing effective AI teacher education programmes.",No methods found.
2024,https://openalex.org/W4396636942,Social Sciences,Artificial intelligence in digital pathology: a systematic review and meta-analysis of diagnostic test accuracy,"Abstract Ensuring diagnostic performance of artificial intelligence (AI) before introduction into clinical practice is essential. Growing numbers of studies using AI for digital pathology have been reported over recent years. The aim of this work is to examine the diagnostic accuracy of AI in digital pathology images for any disease. This systematic review and meta-analysis included diagnostic accuracy studies using any type of AI applied to whole slide images (WSIs) for any disease. The reference standard was diagnosis by histopathological assessment and/or immunohistochemistry. Searches were conducted in PubMed, EMBASE and CENTRAL in June 2022. Risk of bias and concerns of applicability were assessed using the QUADAS-2 tool. Data extraction was conducted by two investigators and meta-analysis was performed using a bivariate random effects model, with additional subgroup analyses also performed. Of 2976 identified studies, 100 were included in the review and 48 in the meta-analysis. Studies were from a range of countries, including over 152,000 whole slide images (WSIs), representing many diseases. These studies reported a mean sensitivity of 96.3% (CI 94.1–97.7) and mean specificity of 93.3% (CI 90.5–95.4). There was heterogeneity in study design and 99% of studies identified for inclusion had at least one area at high or unclear risk of bias or applicability concerns. Details on selection of cases, division of model development and validation data and raw performance data were frequently ambiguous or missing. AI is reported as having high diagnostic accuracy in the reported areas but requires more rigorous evaluation of its performance.",No methods found.
2024,https://openalex.org/W4402827393,Social Sciences,Larger and more instructable language models become less reliable,"Abstract The prevailing methods to make large language models more powerful and amenable have been based on continuous scaling up (that is, increasing their size, data volume and computational resources 1 ) and bespoke shaping up (including post-filtering 2,3 , fine tuning or use of human feedback 4,5 ). However, larger and more instructable large language models may have become less reliable. By studying the relationship between difficulty concordance, task avoidance and prompting stability of several language model families, here we show that easy instances for human participants are also easy for the models, but scaled-up, shaped-up models do not secure areas of low difficulty in which either the model does not err or human supervision can spot the errors. We also find that early models often avoid user questions but scaled-up, shaped-up models tend to give an apparently sensible yet wrong answer much more often, including errors on difficult questions that human supervisors frequently overlook. Moreover, we observe that stability to different natural phrasings of the same question is improved by scaling-up and shaping-up interventions, but pockets of variability persist across difficulty levels. These findings highlight the need for a fundamental shift in the design and development of general-purpose artificial intelligence, particularly in high-stakes areas for which a predictable distribution of errors is paramount.","<method>continuous scaling up</method>, <method>post-filtering</method>, <method>fine tuning</method>, <method>use of human feedback</method>"
2024,https://openalex.org/W4391609543,Social Sciences,Does AI-Driven Technostress Promote or Hinder Employees’ Artificial Intelligence Adoption Intention? A Moderated Mediation Model of Affective Reactions and Technical Self-Efficacy,"Purpose: The increasing integration of Artificial Intelligence (AI) within enterprises is generates significant technostress among employees, potentially influencing their intention to adopt AI. However, existing research on the psychological effects of this phenomenon remains inconclusive. Drawing on the Affective Events Theory (AET) and the Challenge–Hindrance Stressor Framework (CHSF), the current study aims to explore the “black box” between challenge and hindrance technology stressors and employees’ intention to adopt AI, as well as the boundary conditions of this mediation relationship. Methods: The study employs a quantitative approach and utilizes three-wave data. Data were collected through the snowball sampling technique and a structured questionnaire survey. The sample comprises employees from 11 distinct organizations located in Guangdong Province, China. We received 301 valid questionnaires, representing an overall response rate of 75%. The theoretical model was tested through confirmatory factor analysis and regression analyses using Mplus and the Process macro for SPSS. Results: The results indicate that positive affect mediates the positive relationship between challenge technology stressors and AI adoption intention, whereas AI anxiety mediates the negative relationship between hindrance technology stressors and AI adoption intention. Furthermore, the results reveal that technical self-efficacy moderates the effects of challenge and hindrance technology stressors on affective reactions and the indirect effects of challenge and hindrance technology stressors on AI adoption intention through positive affect and AI anxiety, respectively. Conclusion: Overall, our study suggests that AI-driven challenge technology stressors positively impact AI adoption intention through the cultivation of positive affect, while hindrance technology stressors impede AI adoption intention by triggering AI anxiety. Additionally, technical self-efficacy emerges as a crucial moderator in shaping these relationships. This research has the potential to make a meaningful contribution to the literature on AI adoption intention, deepening our holistic understanding of the influential mechanisms involved. Furthermore, the study affirms the applicability and relevance of Affective Events Theory (AET) and the Challenge-Hindrance Stressor Framework (CHSF). In practical terms, the research provides actionable insights for organizations to effectively manage employees’ AI adoption intention. Keywords: challenge and hindrance technology stressors, AI adoption intention, positive affect, AI anxiety, technical self-efficacy",No methods found.
2024,https://openalex.org/W4392343921,Social Sciences,Data extraction for evidence synthesis using a large language model: A proof‐of‐concept study,"Abstract Data extraction is a crucial, yet labor‐intensive and error‐prone part of evidence synthesis. To date, efforts to harness machine learning for enhancing efficiency of the data extraction process have fallen short of achieving sufficient accuracy and usability. With the release of large language models (LLMs), new possibilities have emerged to increase efficiency and accuracy of data extraction for evidence synthesis. The objective of this proof‐of‐concept study was to assess the performance of an LLM (Claude 2) in extracting data elements from published studies, compared with human data extraction as employed in systematic reviews. Our analysis utilized a convenience sample of 10 English‐language, open‐access publications of randomized controlled trials included in a single systematic review. We selected 16 distinct types of data, posing varying degrees of difficulty (160 data elements across 10 studies). We used the browser version of Claude 2 to upload the portable document format of each publication and then prompted the model for each data element. Across 160 data elements, Claude 2 demonstrated an overall accuracy of 96.3% with a high test–retest reliability (replication 1: 96.9%; replication 2: 95.0% accuracy). Overall, Claude 2 made 6 errors on 160 data items. The most common errors ( n = 4) were missed data items. Importantly, Claude 2's ease of use was high; it required no technical expertise or labeled training data for effective operation (i.e., zero‐shot learning). Based on findings of our proof‐of‐concept study, leveraging LLMs has the potential to substantially enhance the efficiency and accuracy of data extraction for evidence syntheses.","<method>large language models (LLMs)</method>, <method>zero‐shot learning</method>"
2024,https://openalex.org/W4392755249,Social Sciences,Leveraging technology and financial literacy for women’s empowerment in SMEs: A conceptual framework for sustainable development,"In the landscape of small and medium-sized enterprises (SMEs), fostering women's empowerment through technology and financial literacy emerges as a pivotal strategy for sustainable development. This abstract presents a conceptual framework delineating the integration of technology and financial literacy to empower women entrepreneurs within SMEs, fostering economic growth, and societal advancement. The framework begins by recognizing the significance of leveraging technology as a catalyst for women's empowerment. Technology adoption facilitates access to markets, networks, and resources, leveling the playing field for women entrepreneurs. Through digital platforms, women can overcome geographical barriers, tap into global markets, and enhance their competitiveness in the digital economy. Moreover, the conceptual framework emphasizes the critical role of financial literacy in enabling women's empowerment within SMEs. Financial literacy equips women entrepreneurs with the knowledge and skills to manage finances effectively, make informed decisions, and access capital markets. By enhancing financial literacy, women can navigate complex financial landscapes, secure funding for business growth, and mitigate financial risks. Central to the framework is the intersectionality of technology and financial literacy, synergistically driving women's empowerment in SMEs. Technology serves as an enabler for financial literacy initiatives, providing accessible and scalable platforms for delivering financial education and services to women entrepreneurs. Conversely, financial literacy enhances women's ability to leverage technology effectively, maximizing the benefits of digital tools and platforms for business growth and sustainability. The conceptual framework underscores the importance of ecosystem support in facilitating women's empowerment within SMEs. Collaboration among government agencies, financial institutions, technology providers, and civil society organizations is essential for creating an enabling environment that fosters women's access to technology and financial resources. By fostering partnerships and collaborations, stakeholders can amplify the impact of initiatives aimed at promoting women's empowerment and sustainable development in SMEs. In conclusion, the conceptual framework presents a holistic approach to leveraging technology and financial literacy for women's empowerment in SMEs, offering insights into the interconnectedness of technology adoption, financial literacy, and ecosystem support in driving sustainable development outcomes. Implementing this framework requires concerted efforts from stakeholders across sectors to create an inclusive and supportive environment where women entrepreneurs can thrive and contribute to economic growth and societal advancement.",No methods found.
2024,https://openalex.org/W4392855331,Social Sciences,The Making of the “Good Bad” Job: How Algorithmic Management Manufactures Consent Through Constant and Confined Choices,"This research explores how a new relation of production—the shift from human managers to algorithmic managers on digital platforms—manufactures workplace consent. While most research has argued that the task standardization and surveillance that accompany algorithmic management will give rise to the quintessential “bad job” (Kalleberg, Reskin, and Hudson, 2000; Kalleberg, 2011), I find that, surprisingly, many workers report liking and finding choice while working under algorithmic management. Drawing on a seven-year qualitative study of the largest sector in the gig economy, the ride-hailing industry, I describe how workers navigate being managed by an algorithm. I begin by showing how algorithms segment the work at multiple sites of human–algorithm interactions and how this configuration of the work process allows for more-frequent and narrow choice. I find that workers use two sets of tactics. In engagement tactics, individuals generally follow the algorithmic nudges and do not try to get around the system; in deviance tactics, individuals manipulate their input into the algorithmic management system. While the behaviors associated with these tactics are practical opposites, they both elicit consent, or active, enthusiastic participation by workers to align their efforts with managerial interests, and both contribute to workers seeing themselves as skillful agents. However, this choice-based consent can mask the more-structurally problematic elements of the work, contributing to the growing popularity of what I call the “good bad” job.",No methods found.
2024,https://openalex.org/W4390569131,Social Sciences,Rapport with a chatbot? The underlying role of anthropomorphism in socio-cognitive perceptions of rapport and e-word of mouth,"This study examines the impact of rapport with chatbots on electronic word of mouth (e-WOM), in the first phase, by considering several antecedents including anthropomorphism. In the second phase, deeper insights are provided into the moderated mediation role of rapport and the moderated moderation effect of value creation and hedonic motivation on e-WOM engagement. With tourism services as the research context, a survey was conducted among 257 visitors from three countries (China, India and New Zealand), selected due to their diverse cultural backgrounds and higher number of inbound visitors to Australia. The partial least squares method was used for data analysis along with multi-group analysis. Findings report the positive role of anthropomorphism in developing rapport with chatbots in digital interactions. Interestingly, rapport had the highest moderated mediation impact in the data from China followed by the data from India. The moderated moderation impact of hedonic motivation was only significant in the data from China, whereas value creation was a significant moderator in the data from both China and New Zealand. The study extends social exchange theory in a human–chatbot or artificial intelligence (AI) interaction context with cultural implications. The findings are useful for organizations relying on customer rapport with AI-based chatbots to ensure long-term customer service through digital interactions.","<method>partial least squares method</method>, <method>multi-group analysis</method>"
2024,https://openalex.org/W4391508432,Social Sciences,Artificial intelligence-driven virtual rehabilitation for people living in the community: A scoping review,"Abstract Virtual Rehabilitation (VRehab) is a promising approach to improving the physical and mental functioning of patients living in the community. The use of VRehab technology results in the generation of multi-modal datasets collected through various devices. This presents opportunities for the development of Artificial Intelligence (AI) techniques in VRehab, namely the measurement, detection, and prediction of various patients’ health outcomes. The objective of this scoping review was to explore the applications and effectiveness of incorporating AI into home-based VRehab programs. PubMed/MEDLINE, Embase, IEEE Xplore, Web of Science databases, and Google Scholar were searched from inception until June 2023 for studies that applied AI for the delivery of VRehab programs to the homes of adult patients. After screening 2172 unique titles and abstracts and 51 full-text studies, 13 studies were included in the review. A variety of AI algorithms were applied to analyze data collected from various sensors and make inferences about patients’ health outcomes, most involving evaluating patients’ exercise quality and providing feedback to patients. The AI algorithms used in the studies were mostly fuzzy rule-based methods, template matching, and deep neural networks. Despite the growing body of literature on the use of AI in VRehab, very few studies have examined its use in patients’ homes. Current research suggests that integrating AI with home-based VRehab can lead to improved rehabilitation outcomes for patients. However, further research is required to fully assess the effectiveness of various forms of AI-driven home-based VRehab, taking into account its unique challenges and using standardized metrics.","<method>fuzzy rule-based methods</method>, <method>template matching</method>, <method>deep neural networks</method>"
2024,https://openalex.org/W4390483800,Social Sciences,Comprehensive Risk Analysis and Decision-Making Model for Hydroelectricity Energy Investments,"The risks of hydroelectricity energy investments should be managed effectively to increase the performance of these projects. Thus, more significant risks should be identified to take effective measures for risk management without experiencing high costs. Accordingly, the purpose of this study is to define critical risks in hydroelectricity energy investment projects by making a priority analysis. Within this scope, a new decision-making model is created. In the first stage, five different risks are examined by considering Spherical fuzzy Entropy. Moreover, the second stage consists of ranking emerging seven countries with the help of Spherical fuzzy multi-attribute ideal-real comparative assessment (MAIRCA). The main contribution of this study is that more important risks of hydroelectricity energy investments can be identified by the help of the priority analysis. This situation provides an opportunity to implement effective strategies to increase these investments without having high costs. Additionally, considering Spherical fuzzy sets has a positive impact on the appropriateness of the results. Since these numbers use a wider data range, the effectiveness of the analysis results can increase. It is determined that the most important risk is environmental risk with the highest weight value of 0.2478. Financial risks and personnel risks are other significant factors that affect the performance of the hydroelectricity energy investments. Furthermore, as a result of ranking the alternatives, it is seen that China is the most suitable country for hydroelectric energy investments. India and Mexico are other successful countries in this respect. However, Turkey and Indonesia have lower performance for this situation.","<method>Spherical fuzzy Entropy</method>, <method>Spherical fuzzy multi-attribute ideal-real comparative assessment (MAIRCA)</method>"
2024,https://openalex.org/W4393092671,Social Sciences,CFSSynergy: Combining Feature-Based and Similarity-Based Methods for Drug Synergy Prediction,"Drug synergy prediction plays a vital role in cancer treatment. Because experimental approaches are labor-intensive and expensive, computational-based approaches get more attention. There are two types of computational methods for drug synergy prediction: feature-based and similarity-based. In feature-based methods, the main focus is to extract more discriminative features from drug pairs and cell lines to pass to the task predictor. In similarity-based methods, the similarities among all drugs and cell lines are utilized as features and fed into the task predictor. In this work, a novel approach, called CFSSynergy, that combines these two viewpoints is proposed. First, a discriminative representation is extracted for paired drugs and cell lines as input. We have utilized transformer-based architecture for drugs. For cell lines, we have created a similarity matrix between proteins using the Node2Vec algorithm. Then, the new cell line representation is computed by multiplying the protein–protein similarity matrix and the initial cell line representation. Next, we compute the similarity between unique drugs and unique cells using the learned representation for paired drugs and cell lines. Then, we compute a new representation for paired drugs and cell lines based on the similarity-based features and the learned features. Finally, these features are fed to XGBoost as a task predictor. Two well-known data sets were used to evaluate the performance of our proposed method: DrugCombDB and OncologyScreen. The CFSSynergy approach consistently outperformed existing methods in comparative evaluations. This substantiates the efficacy of our approach in capturing complex synergistic interactions between drugs and cell lines, setting it apart from conventional similarity-based or feature-based methods.","<method>transformer-based architecture</method>, <method>Node2Vec algorithm</method>, <method>XGBoost</method>"
2024,https://openalex.org/W4391531696,Social Sciences,Artificial intelligence in the risk prediction models of cardiovascular disease and development of an independent validation screening tool: a systematic review,"Abstract Background A comprehensive overview of artificial intelligence (AI) for cardiovascular disease (CVD) prediction and a screening tool of AI models (AI-Ms) for independent external validation are lacking. This systematic review aims to identify, describe, and appraise AI-Ms of CVD prediction in the general and special populations and develop a new independent validation score (IVS) for AI-Ms replicability evaluation. Methods PubMed, Web of Science, Embase, and IEEE library were searched up to July 2021. Data extraction and analysis were performed for the populations, distribution, predictors, algorithms, etc. The risk of bias was evaluated with the prediction risk of bias assessment tool (PROBAST). Subsequently, we designed IVS for model replicability evaluation with five steps in five items, including transparency of algorithms, performance of models, feasibility of reproduction, risk of reproduction, and clinical implication, respectively. The review is registered in PROSPERO (No. CRD42021271789). Results In 20,887 screened references, 79 articles (82.5% in 2017–2021) were included, which contained 114 datasets (67 in Europe and North America, but 0 in Africa). We identified 486 AI-Ms, of which the majority were in development ( n = 380), but none of them had undergone independent external validation. A total of 66 idiographic algorithms were found; however, 36.4% were used only once and only 39.4% over three times. A large number of different predictors (range 5–52,000, median 21) and large-span sample size (range 80–3,660,000, median 4466) were observed. All models were at high risk of bias according to PROBAST, primarily due to the incorrect use of statistical methods. IVS analysis confirmed only 10 models as “recommended”; however, 281 and 187 were “not recommended” and “warning,” respectively. Conclusion AI has led the digital revolution in the field of CVD prediction, but is still in the early stage of development as the defects of research design, report, and evaluation systems. The IVS we developed may contribute to independent external validation and the development of this field.",No methods found.
2024,https://openalex.org/W4399800741,Social Sciences,Do competitive strategies affect working capital management efficiency?,"Purpose This study examines the effects of CLS and DS on companies' WCME and analyses the differences in WCME at company and market levels. Design/methodology/approach This study adopts the DEA approach, regression, differences, and additional analyses to achieve its objectives. This study employs 235 non-financial companies and 1,175 company-year observations from eight active industries in the United States from 2016 to 2020. Findings The findings indicate that CLS and DS strategies positively influence companies' WCME. Additionally, WCME differed across size categories and industries, with large companies and those operating in the communication services industry showing better WCME. By contrast, WCME did not differ between the periods before and during the COVID-19 pandemic. Practical implications This study scrutinizes the impact of CLS and DS strategies on companies' WCME to bridge the gap in this field. It extends the investigation of competitive strategies as explanatory variables for a company's WCME and examines the differences in companies' WCME at the company and market levels, which may assist decision-makers in improving their strategies and efficiencies for continuous improvement. Originality/value This study enhances current knowledge by uncovering the influence of CLS and DS strategies on improving companies' WCME, an underexplored topic. It also explores companies' WCME trends and patterns regarding company size, industry type, and the pandemic period to draw interesting conclusions about the essence of WCME.",No methods found.
2024,https://openalex.org/W4400015499,Social Sciences,Building entrepreneurial resilience during crisis using generative AI: An empirical study on SMEs,"Recently, Gen AI has garnered significant attention across various sectors of society, particularly capturing the interest of small business due to its capacity to allow them to reassess their business models with minimal investment. To understand how small and medium-sized firms have utilised Gen AI-based tools to cope with the market's high level of turbulence caused by the COVID-19 pandemic, geopolitical crises, and economic slowdown, researchers have conducted an empirical study. Although Gen AI is receiving more attention, there remains a dearth of empirical studies that investigate how it influences the entrepreneurial orientation of firms and their ability to cultivate entrepreneurial resilience amidst market turbulence. Most of the literature offers anecdotal evidence. To address this research gap, the authors have grounded their theoretical model and research hypotheses in the contingent view of dynamic capability. They tested the research hypotheses using cross-sectional data from a pre-tested survey instrument, which yielded 87 useable responses from small and medium enterprises in France. The authors used variance-based structural equation modelling with the commercial WarpPLS 7.0 software to test the theoretical model. The study's findings suggest that Gen AI and EO have a significant influence on building entrepreneurial resilience as higher-order and lower-order dynamic capabilities. However, market turbulence has a negative moderating effect on the path that joins entrepreneurial orientation and entrepreneurial resilience. The results suggest that the assumption that high market turbulence will have positive effects on dynamic capabilities and competitive advantage is not always true, and the linear assumption does not hold, which is consistent with some scholars' assumptions. The study's results offer significant contributions to the contingent view of dynamic capabilities and open new research avenues that require further investigation into the non-linear relationship of market turbulence.",<method>variance-based structural equation modelling</method>
2024,https://openalex.org/W4390685244,Social Sciences,Make every dollar count: The impact of green credit regulation on corporate green investment efficiency,"This paper systematically examines the impact of green credit regulation on the efficiency of corporate green investment. The results show that green credit policy significantly decreases green investment efficiency for heavily polluting firms. This is further evidenced through the fact that these firms are more inclined to make symbolic efforts to pursue more credit resources rather than engaging in substantive green investments to drive real green transition. This negative effect is more pronounced for small, non-state-owned and non-foreign-funded firms. Our further analysis suggests that the intensity of environmental law enforcement, the level of financial development, and intellectual property protection can mitigate this negative effect of green credit policy on green investment efficiency. Our study is groundbreaking in that it makes the first attempt to calculate the future value that green investment can create, which serves the basis for analyzing the economic effects of green investment at the industry level. The findings indicate that labor-intensive industries with close ties to consumers' daily lives have a higher future value for green investment. Conversely, capital-intensive industries such as the metallurgical industry have a lower future value for green investment. These findings emphasize the need to improve green credit regulation and make genuine green investment to accelerate real green transition in emerging economies.",No methods found.
2024,https://openalex.org/W4390755438,Social Sciences,A voting gray wolf optimizer-based ensemble learning models for intrusion detection in the Internet of Things,"Abstract The Internet of Things (IoT) has garnered considerable attention from academic and industrial circles as a pivotal technology in recent years. The escalation of security risks is observed to be associated with the growing interest in IoT applications. Intrusion detection systems (IDS) have been devised as viable instruments for identifying and averting malicious actions in this context. Several techniques described in academic papers are thought to be very accurate, but they cannot be used in the real world because the datasets used to build and test the models do not accurately reflect and simulate the IoT network. Existing methods, on the other hand, deal with these issues, but they are not good enough for commercial use because of their lack of precision, low detection rate, receiver operating characteristic (ROC), and false acceptance rate (FAR). The effectiveness of these solutions is predominantly dependent on individual learners and is consequently influenced by the inherent limitations of each learning algorithm. This study introduces a new approach for detecting intrusion attacks in an IoT network, which involves the use of an ensemble learning technique based on gray wolf optimizer (GWO). The novelty of this study lies in the proposed voting gray wolf optimizer (GWO) ensemble model, which incorporates two crucial components: a traffic analyzer and a classification phase engine. The model employs a voting technique to combine the probability averages of the base learners. Secondly, the combination of feature selection and feature extraction techniques is to reduce dimensionality. Thirdly, the utilization of GWO is employed to optimize the parameters of ensemble models. Similarly, the approach employs the most authentic intrusion detection datasets that are accessible and amalgamates multiple learners to generate ensemble learners. The hybridization of information gain (IG) and principal component analysis (PCA) was employed to reduce dimensionality. The study utilized a novel GWO ensemble learning approach that incorporated a decision tree, random forest, K-nearest neighbor, and multilayer perceptron for classification. To evaluate the efficacy of the proposed model, two authentic datasets, namely, BoT-IoT and UNSW-NB15, were scrutinized. The GWO-optimized ensemble model demonstrates superior accuracy when compared to other machine learning-based and deep learning models. Specifically, the model achieves an accuracy rate of 99.98%, a DR of 99.97%, a precision rate of 99.94%, an ROC rate of 99.99%, and an FAR rate of 1.30 on the BoT-IoT dataset. According to the experimental results, the proposed ensemble model optimized by GWO achieved an accuracy of 100%, a DR of 99.9%, a precision of 99.59%, an ROC of 99.40%, and an FAR of 1.5 when tested on the UNSW-NB15 dataset.","<method>ensemble learning technique</method>, <method>gray wolf optimizer (GWO)</method>, <method>voting technique</method>, <method>feature selection</method>, <method>feature extraction</method>, <method>information gain (IG)</method>, <method>principal component analysis (PCA)</method>, <method>decision tree</method>, <method>random forest</method>, <method>K-nearest neighbor</method>, <method>multilayer perceptron</method>"
2024,https://openalex.org/W4391715819,Social Sciences,The impact of a blockchain-based food traceability system on the online purchase intention of organic agricultural products,"The issue of food safety has become a crucial public health issue worldwide. Blockchain technology allows consumers to track the flow of food and effectively record the sources and processes of their products, and reduces the elimination of associated food fraud phenomena, such as counterfeiting and dilution. Using signaling theory, structural equation model method and an online sample of 415 Chinese participants, this study investigates the impact of how blockchain-based food traceability system influences consumers' online purchase intentions of organic agricultural products. The results illustrate that: (1) blockchain-based food traceability system can increase consumers' willingness to buy organic agricultural products. (2) Blockchain-based food traceability system can increase consumers' product quality perceptions, product trust and environmental information transparency perceptions. (3) Product quality perceptions, product trust and environmental information transparency perceptions can also promote consumers' purchase intentions. (4) Product quality perceptions, product trust and environmental information transparency perceptions all have significant mediation effects in the in the relationship between blockchain-based food traceability system and consumer purchase intention. The findings of the research are valuable information for business managers and retailers who want to increase sales of their products, as well as helping relevant policy makers to achieve the ultimate goal of green and sustainable development in the food industry.",No methods found.
2024,https://openalex.org/W4394580101,Social Sciences,A systematic review and multivariate meta-analysis of the physical and mental health benefits of touch interventions,"Receiving touch is of critical importance, as many studies have shown that touch promotes mental and physical well-being. We conducted a pre-registered (PROSPERO: CRD42022304281) systematic review and multilevel meta-analysis encompassing 137 studies in the meta-analysis and 75 additional studies in the systematic review (n = 12,966 individuals, search via Google Scholar, PubMed and Web of Science until 1 October 2022) to identify critical factors moderating touch intervention efficacy. Included studies always featured a touch versus no touch control intervention with diverse health outcomes as dependent variables. Risk of bias was assessed via small study, randomization, sequencing, performance and attrition bias. Touch interventions were especially effective in regulating cortisol levels (Hedges' g = 0.78, 95% confidence interval (CI) 0.24 to 1.31) and increasing weight (0.65, 95% CI 0.37 to 0.94) in newborns as well as in reducing pain (0.69, 95% CI 0.48 to 0.89), feelings of depression (0.59, 95% CI 0.40 to 0.78) and state (0.64, 95% CI 0.44 to 0.84) or trait anxiety (0.59, 95% CI 0.40 to 0.77) for adults. Comparing touch interventions involving objects or robots resulted in similar physical (0.56, 95% CI 0.24 to 0.88 versus 0.51, 95% CI 0.38 to 0.64) but lower mental health benefits (0.34, 95% CI 0.19 to 0.49 versus 0.58, 95% CI 0.43 to 0.73). Adult clinical cohorts profited more strongly in mental health domains compared with healthy individuals (0.63, 95% CI 0.46 to 0.80 versus 0.37, 95% CI 0.20 to 0.55). We found no difference in health benefits in adults when comparing touch applied by a familiar person or a health care professional (0.51, 95% CI 0.29 to 0.73 versus 0.50, 95% CI 0.38 to 0.61), but parental touch was more beneficial in newborns (0.69, 95% CI 0.50 to 0.88 versus 0.39, 95% CI 0.18 to 0.61). Small but significant small study bias and the impossibility to blind experimental conditions need to be considered. Leveraging factors that influence touch intervention efficacy will help maximize the benefits of future interventions and focus research in this field.",No methods found.
2024,https://openalex.org/W4396905964,Social Sciences,A systematic review and meta-analysis of artificial intelligence versus clinicians for skin cancer diagnosis,"Scientific research of artificial intelligence (AI) in dermatology has increased exponentially. The objective of this study was to perform a systematic review and meta-analysis to evaluate the performance of AI algorithms for skin cancer classification in comparison to clinicians with different levels of expertise. Based on PRISMA guidelines, 3 electronic databases (PubMed, Embase, and Cochrane Library) were screened for relevant articles up to August 2022. The quality of the studies was assessed using QUADAS-2. A meta-analysis of sensitivity and specificity was performed for the accuracy of AI and clinicians. Fifty-three studies were included in the systematic review, and 19 met the inclusion criteria for the meta-analysis. Considering all studies and all subgroups of clinicians, we found a sensitivity (Sn) and specificity (Sp) of 87.0% and 77.1% for AI algorithms, respectively, and a Sn of 79.78% and Sp of 73.6% for all clinicians (overall); differences were statistically significant for both Sn and Sp. The difference between AI performance (Sn 92.5%, Sp 66.5%) vs. generalists (Sn 64.6%, Sp 72.8%), was greater, when compared with expert clinicians. Performance between AI algorithms (Sn 86.3%, Sp 78.4%) vs expert dermatologists (Sn 84.2%, Sp 74.4%) was clinically comparable. Limitations of AI algorithms in clinical practice should be considered, and future studies should focus on real-world settings, and towards AI-assistance.",No methods found.
2024,https://openalex.org/W4390843293,Social Sciences,Assessing the Banking Sector of Bosnia and Herzegovina: An Analysis of Financial Indicators through the MEREC and MARCOS Methods,"Abstract The banking sector assumes a pivotal role in the economic development of nations. The assessment of financial indicators pertaining to banks holds fundamental importance in the evaluation of bank stability and sustainability. This research employs the MEREC (Method based on the Removal Effects of Criteria) and MARCOS (Measurement of Alternatives and Ranking according to COmpromise Solution) methodologies to delve deeper into the financial landscape of the banking sector in Bosnia and Herzegovina (BiH). Specifically, the objective is to rank banks according to their financial indicators, utilizing financial data from the year 2022. The MEREC method is applied to gauge the significance of financial indicators and ascertain their respective weights, while the MARCOS method is employed to rank banks within BiH based on their financial indicators. The examination of financial indicators within the BiH banking sector, facilitated by the MEREC and MARCOS methodologies, yields a more comprehensive understanding of the sector’s present condition. Limitations of this research, which primarily stem from its reliance on available financial data and predefined methodologies, lies within limited consideration for external factors. To provide a broader contextual perspective, the inclusion of additional financial indicators and comparative analyses with banking sectors of other nations would be imperative. The findings of this research reveal that Raiffeisen Bank exhibits the most favourable financial indicators and demonstrates the highest level of efficiency within this context. Consequently, this research offers insights into identifying exemplary banks that can serve as models for enhancing the performance of others.","<method>MEREC (Method based on the Removal Effects of Criteria)</method>, <method>MARCOS (Measurement of Alternatives and Ranking according to COmpromise Solution)</method>"
2024,https://openalex.org/W4391454392,Social Sciences,UANet: An Uncertainty-Aware Network for Building Extraction From Remote Sensing Images,"Building extraction aims to segment building pixels from remote sensing images and plays an essential role in many applications, such as city planning and urban dynamic monitoring. Over the past few years, deep learning methods with encoder–decoder architectures have achieved remarkable performance due to their powerful feature representation capability. Nevertheless, due to the varying scales and styles of buildings, conventional deep learning models always suffer from uncertain predictions and cannot accurately distinguish the complete footprints of the building from the complex distribution of ground objects, leading to a large degree of omission and commission. In this paper, we realize the importance of uncertain prediction and propose a novel and straightforward Uncertainty-Aware Network (UANet) to alleviate this problem. Specifically, we first apply a general encoder–decoder network to obtain a building extraction map with relatively high uncertainty. Second, in order to aggregate the useful information in the highest-level features, we design a Prior Information Guide Module to guide the highest-level features in learning the prior information from the conventional extraction map. Third, based on the uncertain extraction map, we introduce an Uncertainty Rank Algorithm to measure the uncertainty level of each pixel belonging to the foreground and the background. We further combine this algorithm with the proposed Uncertainty-Aware Fusion Module to facilitate level-by-level feature refinement and obtain the final refined extraction map with low uncertainty. To verify the performance of our proposed UANet, we conduct extensive experiments on three public building datasets, including the WHU building dataset, the Massachusetts building dataset, and the Inria aerial image dataset. Results demonstrate that the proposed UANet outperforms other state-of-the-art algorithms by a large margin. The source code of the proposed UANet is available at https://github.com/Henryjiepanli/Uncertainty-aware-Network.","<method>deep learning methods with encoder–decoder architectures</method>, <method>encoder–decoder network</method>, <method>Uncertainty-Aware Network (UANet)</method>, <method>Prior Information Guide Module</method>, <method>Uncertainty Rank Algorithm</method>, <method>Uncertainty-Aware Fusion Module</method>"
2024,https://openalex.org/W4391667625,Social Sciences,"How organizational readiness for green innovation, green innovation performance and knowledge integration affects sustainability performance of exporting firms","Purpose Consumers and businesses are becoming increasingly conscious of sustainable business practices and are often willing to pay a premium for responsibly sourced and manufactured products. Many countries and organizations have implemented regulations and standards for sustainability and companies face penalties or are barred from exporting for not meeting the requirements. Rooted in the resource-based view theory, this study aims to test a moderated mediation model to improve the sustainability performance of exporting firms. Design/methodology/approach Textile firms generating more than 25% of export revenues were targeted for this research. The data collected from 245 middle management-level employees were tested for reliability and validity. The structural equation modelling in AMOS 26 was used to test hypotheses. Findings Organizational readiness for green innovation (ORGI) has a direct positive effect on sustainability performance. The mediation analysis implies that ORGI translates into sustainability performance through improvement in green innovation performance. The moderating effect of knowledge integration highlights the importance of being prepared internally and actively seeking and incorporating external knowledge to improve green innovation performance. Originality/value The findings offer a solid foundation for informed decision-making, policy development and strategies to improve sustainability performance while aligning with the global nature of the textile industry and its inherent challenges. The proposed model and practical implications guide policymakers and managers of exporting firms to foster a culture of green innovation to leverage the effect of their readiness for green innovation on sustainability performance.",No methods found.
2024,https://openalex.org/W4392241969,Social Sciences,All models are wrong and yours are useless: making clinical prediction models impactful for patients,"All models are wrong and yours are useless: making clinical prediction models impactful for patients Florian MarkowetzCheck for updates Most published clinical prediction models are never used in clinical practice and there is a huge gap between academic research and clinical implementation.Here, I propose ways for academic researchers to be proactive partners in improving clinical practice and to design models in ways that ultimately benefit patients.""All models are wrong, but some are useful"" is an aphorism attributed to the statistician George Box.There is humility in claiming your model is wrong, but there is also bravado in implying your model might be useful.And, honestly, I don't think it is.I think your model is useless.How would I know?I don't even know who you are.Well, it is a bet.A bet I am willing to take because the odds are ridiculously in my favour.I will explain what I mean in the context of clinical prediction models.My points apply to a wide range of preclinical models, both computational and biological, but my own core expertise is with clinical prediction tools.These are computational models from statistics, machine learning or AI that try to predict clinically relevant variables and ultimately aim to help doctors to treat patients better.The papers describing them make claims like ""this model can be used in the clinic""; generally softened with words like ""might"", ""could"", ""potential"", ""promise"", or other techniques to reduce accountability.The Box quote offers a yardstick to measure the success of these models; not by how correctly they describe reality but by how useful they are in helping patients.And in general, almost none of these tools ever help anyone.There is a wealth of systematic reviews in different fields to show how many models have been proposed and how few have even been validated, let alone been adopted in the clinic.For example, 408(!) models for chronic obstructive pulmonary disease were systematically reviewed 1 and as a summary the authors bleakly note ""several methodological pitfalls in their development and a low rate of external validation"".And whatever biomedical area you work in, your experiences will mirror this resultmany novel prediction models, little help for patients.I believe that a model designed to be used for patients is useless unless it is actually used for patients.",No methods found.
2024,https://openalex.org/W4393380945,Social Sciences,One-Step Multi-View Clustering With Diverse Representation,"Multi-View clustering has attracted broad attention due to its capacity to utilize consistent and complementary information among views. Although tremendous progress has been made recently, most existing methods undergo high complexity, preventing them from being applied to large-scale tasks. Multi-View clustering via matrix factorization is a representative to address this issue. However, most of them map the data matrices into a fixed dimension, limiting the model's expressiveness. Moreover, a range of methods suffers from a two-step process, i.e., multimodal learning and the subsequent <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""> <tex-math notation=""LaTeX"">$k$</tex-math> </inline-formula> -means, inevitably causing a suboptimal clustering result. In light of this, we propose a one-step multi-view clustering with diverse representation (OMVCDR) method, which incorporates multi-view learning and <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""> <tex-math notation=""LaTeX"">$k$</tex-math> </inline-formula> -means into a unified framework. Specifically, we first project original data matrices into various latent spaces to attain comprehensive information and auto-weight them in a self-supervised manner. Then, we directly use the information matrices under diverse dimensions to obtain consensus discrete clustering labels. The unified work of representation learning and clustering boosts the quality of the final results. Furthermore, we develop an efficient optimization algorithm with proven convergence to solve the resultant problem. Comprehensive experiments on various datasets demonstrate the promising clustering performance of our proposed method. The code is publicly available at https://github.com/wanxinhang/OMVCDR.","<method>Multi-View clustering via matrix factorization</method>, <method>k-means</method>, <method>one-step multi-view clustering with diverse representation (OMVCDR)</method>, <method>multi-view learning</method>, <method>self-supervised auto-weighting</method>, <method>representation learning</method>"
2024,https://openalex.org/W4395010883,Social Sciences,Understanding the Role of Teacher-Student Relationships in Students’ Online Learning Engagement: Mediating Role of Academic Motivation,"Strengthening online learning outcomes requires the establishment of strong student-teacher relationships to engage students actively in learning activities. Teacher-student relationships are also pivotal factors for enhancing academic motivation for online learning engagement. Generally, however, research on online teaching remains underdeveloped. We aimed, in this study, to investigate the complex interplay in higher education in Pakistan between teacher-student relationships, academic motivation, and online learning engagement. We used Self-Determination Theory to frame an exploration of the impact of positive teacher-student relationships as mediated by intrinsic or extrinsic academic motivation on students' engagement in online learning activities. We administered a student self-report questionnaire to 437 participants from diverse universities in Sindh province. Using Structural Equation Modeling, we confirmed a model fit in which there were positive correlations between teacher-student relationships and students' online learning engagement; and between students' intrinsic and extrinsic academic motivations and their on line learning engagement. Our findings emphasized the need for communication, personalized support, and a sense of belonging in virtual education. Moreover, our findings revealed the mediating role of students' intrinsic and extrinsic academic motivation in teacher-student relationships, highlighting the nuanced dynamics of academic motivation in the virtual learning environment, with intrinsic motivation having the greatest mediating impact in the relationship between teacher-student relationships and on line learning engagement. Our study's practical implications include a need for professional educators to foster positive teacher-student relationships and integrate student motivational elements into online course design.",No methods found.
2024,https://openalex.org/W4401434014,Social Sciences,Crafting personalized learning paths with AI for lifelong learning: a systematic literature review,"The rapid evolution of knowledge requires constantly acquiring and updating skills, making lifelong learning crucial. Despite decades of artificial intelligence, recent advances promote new solutions to personalize learning in this context. The purpose of this article is to explore the current state of research on the development of artificial intelligence-mediated solutions for the design of personalized learning paths. To achieve this, a systematic literature review (SRL) of 78 articles published between 2019 and 2024 from the Scopus and Web or Science databases was conducted, answering seven questions grouped into three themes: characteristics of the published research, context of the research, and type of solution analyzed. This study identified that: (a) the greatest production of scientific research on the topic is developed in China, India and the United States, (b) the focus is mainly directed towards the educational context at the higher education level with areas of opportunity for application in the work context, and (c) the development of adaptive learning technologies predominates; however, there is a growing interest in the application of generative language models. This article contributes to the growing interest and literature related to personalized learning under artificial intelligence mediated solutions that will serve as a basis for academic institutions and organizations to design programs under this model.","<method>adaptive learning technologies</method>, <method>generative language models</method>"
2024,https://openalex.org/W2899297412,Social Sciences,The Philosophy of Money and Finance,"Abstract Most people use money almost every day, and financial assets have become so important to us that they determine our fate both as individuals and as societies. Yet we seldom stop to think about what all of this means, how it works, and how it ought to work. How can that small piece of paper in your wallet have value? How can so much power be vested in the numbers that roll across bankers’ computer screens? And what role should financial assets and financial institutions play in our lives and in society? The philosophy of money and finance is a field of inquiry that seeks to answer these types of questions and to take a look under the hood of money and finance. Although philosophical theorizing about money and finance dates back to antiquity, the topic has only recently emerged as a central research focus. Economic globalization, technological innovation, the events of the 2008 financial crisis, and the Covid pandemic, have brought new urgency to a broad array of questions in this field. The original chapters in this volume provide a comprehensive introduction to this exciting new field for scholars and the intellectually curious public. The book is divided into four parts: I. Metaphysics, II. Epistemology, III. Ethics, and IV. Political Philosophy. Within each part, questions that are central to the topic are presented and discussed by leading scholars. The chapters are written in a clear and straightforward manner and without presupposition of any background in either philosophy or finance.",No methods found.
2024,https://openalex.org/W4390918182,Social Sciences,How to engage and attract virtual influencers’ followers: a new non-human approach in the age of influencer marketing,"Purpose The purpose of this study is to identify the process of virtual influencer stickiness in the age of influencer marketing, which has received little attention in the literature. This is essential because the research creates a theoretical model of follower loyalty/stickiness to virtual influencer techniques from the standpoint of influencer marketing, which has a substantial effect on the evolution of the global marketing world. Design/methodology/approach In 2022, 302 people who currently follow an Instafamous virtual influencer took part in an Instagram self-administered online survey. Findings The findings show that both expertise and trustworthiness have a positive and significant influence on parasocial interaction, which in turn has a significant influence on virtual engagement and stickiness. Originality/value This research will specifically assist international readers in understanding how to harness and increase the efficiency and efficacy of interactive marketing strategies and methods to engage and retain followers of Instafamous virtual influencer. Moreover, the findings will be beneficial to opinion leaders, brand managers, company investors, entrepreneurs and service designers. Highlights The study pioneers a holistic virtual follower stickiness mechanism that comprises the role of source credibility, parasocial interaction, informational influence and virtual follower’s engagement and their interrelationship to each other. This study is based on parasocial interaction theory and source credibility theory to understand the relationship between virtual followers and influencers stickiness process at social media platforms. In addition, the study examined the subsequent effects of sources of credibility components on parasocial interaction; as well as, on virtual follower engagement and stickiness. This study also categorized and examined the moderating effects exerted by the genres of informative influence of virtual influencer.",No methods found.
2024,https://openalex.org/W4391071022,Social Sciences,Weaving a greener future: The impact of green human resources management and green supply chain management on sustainable performance in Bangladesh's textile industry,"The purpose of this study is to investigate the impact of Green Human Resource Management (GHRM) and Green Supply Chain Management (GSCM) on the sustainable performance of the Bangladeshi textile sector. Specifically, the study focuses on environmental and employee-related aspects. Additionally, we examine how environmental performance and employee performance mediate the relationship between GHRM and GSCM. This study draws upon data collected from 450 employees across various textile enterprises in Bangladesh. Structural Equation Modeling is employed using the Amos 24 software to analyze the relationships and interactions among these variables. These findings demonstrate that using environmentally sustainable practices in human resource management and supply chain management results in enhanced sustainability. The study indicates that environmental performance significantly influences the relationship between GHRM and GSCM regarding sustainable performance. The study findings indicate that firms operating in the textile industry should implement GHRM and GSCM practices to enhance their sustainability performance. Additionally, it is recommended that these organizations prioritize the well-being and engagement of their employees. Implementing such a strategy can bolster the organization's comprehensive sustainability initiatives and raise its standing among stakeholders. This study contributes to the expanding body of literature on textile sustainability by investigating the mediating role of employee and environmental performance. It emphasizes the significance of GHRM and GSCM techniques in improving sustainable performance. The findings provide valuable insights for firms seeking to develop more effective sustainability initiatives.",<method>Structural Equation Modeling</method>
2024,https://openalex.org/W4392124217,Social Sciences,A structural equation modeling framework for exploring the industry 5.0 and sustainable supply chain determinants,"Sustainable Supply Chain and Industry 5.0 are two important concepts reshaping how businesses operate in the modern world. Together, these two concepts drive the advancement of a highly sustainable and robust worldwide economy. Companies are now becoming more sustainable in supply chain management, using technologies like blockchain and co-bots to track the origin of goods, ensure ethical and sustainable sourcing, and work with humans safely and effectively. This study develops a theoretical model highlighting the determinants of Industry 5.0, Sustainable Supply Chain Practices, by combining theoretical frameworks from the manufacturing, supply chain, and information systems literature. The study's analytic sample comprises 342 responses collected from professionals working in the electronics industry's supply chain. Hypotheses were constructed employing deductive reasoning, leveraging insights gleaned from prior research. The study is conducted utilizing the Structural Equation Modeling (SEM) to substantiate the presumed connections among various constructs, namely, Industry 5.0 innovations, Sustainable Supply Chain Practices (SSCP), Sustainable Supply Chain Performance (SCP), and Supply Chain Risks (SCR). The Structural Equation Modeling analysis results show a direct impact of Industry 5.0 technologies through Sustainable Supply Chain Practices can enhance Supply Chain Performance and mitigate Supply Chain Risks. Combining the two paradigms can foster the development of new business models that prioritize sustainability and contribute to a more equitable and environmentally friendly economy that brings positive change for both businesses and society.",<method>Structural Equation Modeling (SEM)</method>
2024,https://openalex.org/W4392780082,Social Sciences,Navigating the double bind: Strategies for women leaders in overcoming stereotypes and leadership biases,"Women leaders often face a ""double bind,"" a phenomenon where they are expected to exhibit both stereotypically feminine traits (e.g., warmth, empathy) and stereotypically masculine traits (e.g., assertiveness, ambition) to be perceived as effective leaders. This abstract explores strategies for women leaders to navigate this double bind, challenging stereotypes and biases to achieve leadership success. The abstract begins by acknowledging the pervasive nature of gender stereotypes and biases in leadership, highlighting their impact on women's advancement in leadership roles. Research suggests that women who conform too closely to feminine stereotypes may be perceived as lacking in leadership qualities, while those who adopt more masculine traits may be viewed as lacking in warmth and likability. This double bind poses a significant challenge for women leaders, requiring them to navigate a narrow path to leadership success. To overcome the double bind, women leaders can employ a range of strategies. One approach is to adopt an ""androgynous"" leadership style, incorporating both stereotypically feminine and masculine traits as appropriate. By demonstrating a balance of warmth and assertiveness, women can challenge traditional gender stereotypes and expand perceptions of effective leadership. Additionally, women leaders can leverage their unique strengths and experiences to differentiate themselves in leadership roles. Emphasizing qualities such as empathy, collaboration, and emotional intelligence can help women leaders build trust and rapport with their teams, enhancing their effectiveness as leaders. Furthermore, women leaders can benefit from mentorship and networking opportunities to navigate the challenges of leadership. Building a strong support network of mentors, sponsors, and peers can provide women with valuable guidance, feedback, and advocacy in their leadership journey. In conclusion, navigating the double bind requires women leaders to challenge stereotypes and biases, adopt a balanced leadership approach, and leverage their unique strengths and experiences. By employing these strategies, women can overcome barriers to leadership success and contribute to creating more inclusive and diverse leadership cultures.",No methods found.
2024,https://openalex.org/W4390616591,Social Sciences,"To Earmark or to Nonearmark? The Role of Control, Transparency, and Warm-Glow","Problem definition: Charities face tension when deciding whether to earmark donations, that is, allow donors to restrict the use of their donations for a specific purpose. Research shows that earmarking decreases operational performance because it limits charities’ flexibility to use donations. However, there is also a common belief that earmarking increases donations. Earmarking is assumed to increase donations through three mechanisms: by (i) giving donors control over their donations, (ii) increasing operational transparency of donations, and (iii) changing donors’ levels of altruism and warm-glow. To resolve this tension, we study how, when, and why earmarking affects donors’ decisions. We consider three important decisions donors make that impact the fundraising outcome: (i) preference between earmarking and nonearmarking, (ii) decision to donate or not (i.e., donor activation), and (iii) the donation amount. Methodology/results: We design three online experiments that allow us to quantify the effect of earmarking on donors’ decisions and to investigate the role of the three aforementioned mechanisms in fundraising. Our results reveal, for example, that earmarking activates more donors but it does not always increase donation amounts. In addition, we determine the conditions under which the three mechanisms affect the outcome of fundraising campaigns. Managerial implications: Our findings provide actionable insights for how charities can design fundraising campaigns more effectively and suggest when to leverage earmarking and the three mechanisms depending on the charities’ fundraising goals. Funding: The authors gratefully acknowledge financial support provided by the Leeds School of Business at the University of Colorado Boulder. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0096 .",No methods found.
2024,https://openalex.org/W4390742710,Social Sciences,Machine Learning as a Tool for Hypothesis Generation,"Abstract While hypothesis testing is a highly formalized activity, hypothesis generation remains largely informal. We propose a systematic procedure to generate novel hypotheses about human behavior, which uses the capacity of machine learning algorithms to notice patterns people might not. We illustrate the procedure with a concrete application: judge decisions about whom to jail. We begin with a striking fact: the defendant’s face alone matters greatly for the judge’s jailing decision. In fact, an algorithm given only the pixels in the defendant’s mug shot accounts for up to half of the predictable variation. We develop a procedure that allows human subjects to interact with this black-box algorithm to produce hypotheses about what in the face influences judge decisions. The procedure generates hypotheses that are both interpretable and novel: they are not explained by demographics (e.g., race) or existing psychology research, nor are they already known (even if tacitly) to people or experts. Though these results are specific, our procedure is general. It provides a way to produce novel, interpretable hypotheses from any high-dimensional data set (e.g., cell phones, satellites, online behavior, news headlines, corporate filings, and high-frequency time series). A central tenet of our article is that hypothesis generation is a valuable activity, and we hope this encourages future work in this largely “prescientific” stage of science.",<method>machine learning algorithms</method>
2024,https://openalex.org/W4394961856,Social Sciences,Applications and challenges of neural networks in otolaryngology (Review),"Artificial Intelligence (AI) has become a topic of interest that is frequently debated in all research fields. The medical field is no exception, where several unanswered questions remain. When and how this field can benefit from AI support in daily routines are the most frequently asked questions. The present review aims to present the types of neural networks (NNs) available for development, discussing their advantages, disadvantages and how they can be applied practically. In addition, the present review summarizes how NNs (combined with various other features) have already been applied in studies in the ear nose throat research field, from assisting diagnosis to treatment management. Although the answer to this question regarding AI remains elusive, understanding the basics and types of applicable NNs can lead to future studies possibly using more than one type of NN. This approach may bypass the actual limitations in accuracy and relevance of information generated by AI. The proposed studies, the majority of which used convolutional NNs, obtained accuracies varying 70-98%, with a number of studies having the AI trained on a limited number of cases (<100 patients). The lack of standardization in AI protocols for research negatively affects data homogeneity and transparency of databases.","<method>neural networks (NNs)</method>, <method>convolutional neural networks (convolutional NNs)</method>"
2024,https://openalex.org/W4390913521,Social Sciences,A Review of Intraocular Lens Power Calculation Formulas Based on Artificial Intelligence,"Purpose: The proper selection of an intraocular lens power calculation formula is an essential aspect of cataract surgery. This study evaluated the accuracy of artificial intelligence-based formulas. Design: Systematic review. Methods: This review comprises articles evaluating the exactness of artificial intelligence-based formulas published from 2017 to July 2023. The papers were identified by a literature search of various databases (Pubmed/MEDLINE, Google Scholar, Crossref, Cochrane Library, Web of Science, and SciELO) using the terms “IOL formulas”, “FullMonte”, “Ladas”, “Hill-RBF”, “PEARL-DGS”, “Kane”, “Karmona”, “Hoffer QST”, and “Nallasamy”. In total, 25 peer-reviewed articles in English with the maximum sample and the largest number of compared formulas were examined. Results: The scores of the mean absolute error and percentage of patients within ±0.5 D and ±1.0 D were used to estimate the exactness of the formulas. In most studies the Kane formula obtained the smallest mean absolute error and the highest percentage of patients within ±0.5 D and ±1.0 D. Second place was typically achieved by the PEARL DGS formula. The limitations of the studies were also discussed. Conclusions: Kane seems to be the most accurate artificial intelligence-based formula. PEARL DGS also gives very good results. Hoffer QST, Karmona, and Nallasamy are the newest, and need further evaluation.","<method>FullMonte</method>, <method>Ladas</method>, <method>Hill-RBF</method>, <method>PEARL-DGS</method>, <method>Kane</method>, <method>Karmona</method>, <method>Hoffer QST</method>, <method>Nallasamy</method>"
2024,https://openalex.org/W4391754120,Social Sciences,"Good models borrow, great models steal: intellectual property rights and generative AI","Abstract Two critical policy questions will determine the impact of generative artificial intelligence (AI) on the knowledge economy and the creative sector. The first concerns how we think about the training of such models—in particular, whether the creators or owners of the data that are “scraped” (lawfully or unlawfully, with or without permission) should be compensated for that use. The second question revolves around the ownership of the output generated by AI, which is continually improving in quality and scale. These topics fall in the realm of intellectual property, a legal framework designed to incentivize and reward only human creativity and innovation. For some years, however, Britain has maintained a distinct category for “computer-generated” outputs; on the input issue, the EU and Singapore have recently introduced exceptions allowing for text and data mining or computational data analysis of existing works. This article explores the broader implications of these policy choices, weighing the advantages of reducing the cost of content creation and the value of expertise against the potential risk to various careers and sectors of the economy, which might be rendered unsustainable. Lessons may be found in the music industry, which also went through a period of unrestrained piracy in the early digital era, epitomized by the rise and fall of the file-sharing service Napster. Similar litigation and legislation may help navigate the present uncertainty, along with an emerging market for “legitimate” models that respect the copyright of humans and are clear about the provenance of their own creations.",No methods found.
2024,https://openalex.org/W4392516399,Social Sciences,Artificial intelligence in dermatology: advancements and challenges in skin of color,"Abstract Artificial intelligence (AI) uses algorithms and large language models in computers to simulate human‐like problem‐solving and decision‐making. AI programs have recently acquired widespread popularity in the field of dermatology through the application of online tools in the assessment, diagnosis, and treatment of skin conditions. A literature review was conducted using PubMed and Google Scholar analyzing recent literature (from the last 10 years through October 2023) to evaluate current AI programs in use for dermatologic purposes, identifying challenges in this technology when applied to skin of color (SOC), and proposing future steps to enhance the role of AI in dermatologic practice. Challenges surrounding AI and its application to SOC stem from the underrepresentation of SOC in datasets and issues with image quality and standardization. With these existing issues, current AI programs inevitably do worse at identifying lesions in SOC. Additionally, only 30% of the programs identified in this review had data reported on their use in dermatology, specifically in SOC. Significant development of these applications is required for the accurate depiction of darker skin tone images in datasets. More research is warranted in the future to better understand the efficacy of AI in aiding diagnosis and treatment options for SOC patients.","<method>algorithms</method>, <method>large language models</method>"
2024,https://openalex.org/W4395445841,Social Sciences,Building information modeling (BIM) adoption for enhanced legal and contractual management in construction projects,"Building information modeling (BIM) adoption offers significant benefits for construction projects, including enhanced collaboration and legal/contractual management. However, the impacts of BIM deployment on construction contracts and dispute resolution remain unclear. This research analyzed the regulatory and legal implications of BIM adoption in construction industry via a framework evaluating Contractual Frameworks (CF), Dispute Resolution (DR), Legal Awareness (LA), and Risk Management (RM). Fifteen qualitative interviews with construction industry experts provided key insights. The results showed CF, DR, and LA have a large positive impact on BIM adoption, while RM has a smaller effect. Specifically, clearly defined BIM protocols in contracts, efficient dispute resolution mechanisms, and legal knowledge of BIM are essential to ensure effective deployment. These findings imply construction project managers should proactively address contractual and legal factors to manage risks, encourage collaboration, and adhere to regulations when implementing BIM. This research makes a significant contribution by addressing the lack of empirical insights on the contractual and legal aspects of BIM adoption. The qualitative methodology provides in-depth perspectives from experienced industry practitioners. The findings offer valuable insights for stakeholders worldwide seeking to leverage BIM. Further comparative research across regions and industries would continue to advance comprehension of the legal implications of BIM adoption.",No methods found.
2024,https://openalex.org/W4400077472,Social Sciences,Exploring the factors that influence academic performance in Jordanian higher education institutions,"Since the coronavirus 2019 (COVID-19) pandemic hit the world, many universities have used digital asynchronous learning tools such as Digital Learning Management Systems (DLMS) to continue the educational process. Despite its global usage, only a few studies have investigated its quality in the Jordanian context during the COVID-19 period from a quantitative and qualitative approach perspective. Thus, the current study aims to explore the factors that influence academic performance in Jordanian higher education institutions during the COVID-19 pandemic. A mixed methods research approach was employed to evaluate the quality of the teaching-learning process for Jordanian students in higher education institutions. The triangulated data focused on three core pillars namely if students saw a difference in their grades prior to, during, and after the pandemic, the challenges faced and improvement suggestions. Accordingly, the quantitative approach with an online questionnaire and the qualitative approach with structured interviews were applied to collect the required data from Jordanian students in higher education institutions. The results of the current study revealed that the evaluation of the teaching-learning process quality during the pandemic period affected students' academic performance in different proportions based on their specialization area. In addition, the study results also identified the most important challenges that faced the students during this period and suggested procedures to overcome them and improve the distance learning process. The current study offers empirical evidence on critical success factors underlying digital learning management systems in the COVID-19 era, which can help policymakers in Jordanian universities and the ministry of higher education and scientific research to improve the quality of the teaching-learning process in the Jordanian context.",No methods found.
2024,https://openalex.org/W4390618081,Social Sciences,Making Sense of Machine Learning: A Review of Interpretation Techniques and Their Applications,"Transparency in AI models is essential for promoting human–AI collaboration and ensuring regulatory compliance. However, interpreting these models is a complex process influenced by various methods and datasets. This study presents a comprehensive overview of foundational interpretation techniques, meticulously referencing the original authors and emphasizing their pivotal contributions. Recognizing the seminal work of these pioneers is imperative for contextualizing the evolutionary trajectory of interpretation in the field of AI. Furthermore, this research offers a retrospective analysis of interpretation techniques, critically evaluating their inherent strengths and limitations. We categorize these techniques into model-based, representation-based, post hoc, and hybrid methods, delving into their diverse applications. Furthermore, we analyze publication trends over time to see how the adoption of advanced computational methods within various categories of interpretation techniques has shaped the development of AI interpretability over time. This analysis highlights a notable preference shift towards data-driven approaches in the field. Moreover, we consider crucial factors such as the suitability of these techniques for generating local or global insights and their compatibility with different data types, including images, text, and tabular data. This structured categorization serves as a guide for practitioners navigating the landscape of interpretation techniques in AI. In summary, this review not only synthesizes various interpretation techniques but also acknowledges the contributions of their original authors. By emphasizing the origins of these techniques, we aim to enhance AI model explainability and underscore the importance of recognizing biases, uncertainties, and limitations inherent in the methods and datasets. This approach promotes the ethical and practical use of interpretation insights, empowering AI practitioners, researchers, and professionals to make informed decisions when selecting techniques for responsible AI implementation in real-world scenarios.","<method>model-based methods</method>, <method>representation-based methods</method>, <method>post hoc methods</method>, <method>hybrid methods</method>, <method>data-driven approaches</method>"
2024,https://openalex.org/W4390688548,Social Sciences,Understanding the acceptance of business intelligence from healthcare professionals’ perspective: an empirical study of healthcare organizations,"Purpose Due to its ability to support well-informed decision-making, business intelligence (BI) has grown in popularity among executives across a range of industries. However, given the volume of data collected in health-care organizations, there is a lack of exploration concerning its implementation. Consequently, this research paper aims to investigate the key factors affecting the acceptance and use of BI in healthcare organizations. Design/methodology/approach Leveraging the theoretical lens of the “unified theory of acceptance and use of technology” (UTAUT), a study framework was proposed and integrated with three context-related factors, including “rational decision-making culture” (RDC), “perceived threat to professional autonomy” (PTA) and “medical–legal risk” (MLR). The variables in the study framework were categorized as follows: information systems (IS) perspective; organizational perspective; and user perspective. In Jordan, 434 healthcare professionals participated in a cross-sectional online survey that was used to collect data. Findings The findings of the “structural equation modeling” revealed that professionals’ behavioral intentions toward using BI systems were significantly affected by performance expectancy, social influence, facilitating conditions, MLR, RDC and PTA. Also, an insignificant effect of PTA on PE was found based on the results of statistical analysis. These variables explained 68% of the variance ( R 2 ) in the individuals’ intentions to use BI-based health-care systems. Practical implications To promote the acceptance and use of BI technology in health-care settings, developers, designers, service providers and decision-makers will find this study to have a number of practical implications. Additionally, it will support the development of effective strategies and BI-based health-care systems based on these study results, attracting the interest of many users. Originality/value To the best of the author’s knowledge, this is one of the first studies that integrates the UTAUT model with three contextual factors (RDC, PTA and MLR) in addition to examining the suggested framework in a developing nation (Jordan). This study is one of the few in which the users’ acceptance behavior of BI systems was investigated in a health-care setting. More specifically, to the best of the author’s knowledge, this is the first study that reveals the critical antecedents of individuals’ intention to accept BI for health-care purposes in the Jordanian context.",No methods found.
2024,https://openalex.org/W4391723759,Social Sciences,Artificial Intelligence to Automate Network Meta-Analyses: Four Case Studies to Evaluate the Potential Application of Large Language Models,"The emergence of artificial intelligence, capable of human-level performance on some tasks, presents an opportunity to revolutionise development of systematic reviews and network meta-analyses (NMAs). In this pilot study, we aim to assess use of a large-language model (LLM, Generative Pre-trained Transformer 4 [GPT-4]) to automatically extract data from publications, write an R script to conduct an NMA and interpret the results. We considered four case studies involving binary and time-to-event outcomes in two disease areas, for which an NMA had previously been conducted manually. For each case study, a Python script was developed that communicated with the LLM via application programming interface (API) calls. The LLM was prompted to extract relevant data from publications, to create an R script to be used to run the NMA and then to produce a small report describing the analysis. The LLM had a > 99% success rate of accurately extracting data across 20 runs for each case study and could generate R scripts that could be run end-to-end without human input. It also produced good quality reports describing the disease area, analysis conducted, results obtained and a correct interpretation of the results. This study provides a promising indication of the feasibility of using current generation LLMs to automate data extraction, code generation and NMA result interpretation, which could result in significant time savings and reduce human error. This is provided that routine technical checks are performed, as recommend for human-conducted analyses. Whilst not currently 100% consistent, LLMs are likely to improve with time.","<method>large-language model (LLM, Generative Pre-trained Transformer 4 [GPT-4])</method>"
2024,https://openalex.org/W4391731380,Social Sciences,What do we know about cryptocurrency investment? An empirical study of its adoption among Indian retail investors,"Purpose This study aims to examine the cryptocurrency adoption (CA) level among Indian retail investors who use cryptocurrency as an investment and mode of transaction. Design/methodology/approach Through self-administered survey questionnaires, data is collected from 397 retail investors of Haryana (India). This study adopted a quantitative method using partial least squares structural equation modeling (PLS-SEM). Findings This paper offered a robust model with a high explanatory value for CA in which four of the five proposed factors of diffusion of innovation theory (trialability, compatibility, complexity and observability) and one of the two proposed factors of consumer behavioral theory (perceived value) significantly influences CA. More specifically, the absence of regulatory support is a barrier to the broad adoption of cryptocurrencies, as its regulations are necessary to mitigate or minimize uncertain outcomes. Research limitations/implications This research primarily focuses on CA in India. Thus, it can be extended to cover diverse other countries for more precise results. Practical implications The results provide insights to the government to design the policies, better regulate and make investment strategies that can ultimately enhance CA. In addition, the study’s results also inform financial educators, policymakers, employers and academicians about the significance of several variables affecting CA in India. Social implications From a social standpoint, this study is an advance that directs central banks and governments to develop, regulate and manage digital currencies and implement a digital currency ecosystem. Moreover, the results assist in understanding investors’ perceptions and decision-making perspectives toward cryptocurrencies through the country’s digitalization. Originality/value This paper fills the study gap to assist policymakers and cryptocurrency experts in broadening their knowledge base and recognizing prioritized intentions. Additionally, this study provides a theoretical model with the latent variable for a present and pertinent matter.",<method>partial least squares structural equation modeling (PLS-SEM)</method>
2024,https://openalex.org/W4393281169,Social Sciences,Artificial intelligence techniques in financial trading: A systematic literature review,"Artificial Intelligence (AI) approaches have been increasingly used in financial markets as technology advances. In this research paper, we conduct a Systematic Literature Review (SLR) that studies financial trading approaches through AI techniques. It reviews 143 research articles that implemented AI techniques in financial trading markets. Accordingly, it presents several findings and observations after reviewing the papers from the following perspectives: the financial trading market and the asset type, the trading analysis type considered along with the AI technique, and the AI techniques utilized in the trading market, the estimation and performance metrics of the proposed models. The selected research articles were published between 2015 and 2023, and this review addresses four RQs. After analyzing the selected research articles, we observed 8 financial markets used in building predictive models. Moreover, we found that technical analysis is more adopted compared to fundamental analysis. Furthermore, 16% of the selected research articles entirely automate the trading process. In addition, we identified 40 different AI techniques that are used as standalone and hybrid models. Among these techniques, deep learning techniques are the most frequently used in financial trading markets. Building prediction models for financial markets using AI is a promising field of research, and academics have already deployed several machine learning models. As a result of this evaluation, we provide recommendations and guidance to researchers.","<method>deep learning techniques</method>, <method>machine learning models</method>"
2024,https://openalex.org/W4394620240,Social Sciences,Human-AI interaction in skin cancer diagnosis: a systematic review and meta-analysis,"Abstract The development of diagnostic tools for skin cancer based on artificial intelligence (AI) is increasing rapidly and will likely soon be widely implemented in clinical use. Even though the performance of these algorithms is promising in theory, there is limited evidence on the impact of AI assistance on human diagnostic decisions. Therefore, the aim of this systematic review and meta-analysis was to study the effect of AI assistance on the accuracy of skin cancer diagnosis. We searched PubMed, Embase, IEE Xplore, Scopus and conference proceedings for articles from 1/1/2017 to 11/8/2022. We included studies comparing the performance of clinicians diagnosing at least one skin cancer with and without deep learning-based AI assistance. Summary estimates of sensitivity and specificity of diagnostic accuracy with versus without AI assistance were computed using a bivariate random effects model. We identified 2983 studies, of which ten were eligible for meta-analysis. For clinicians without AI assistance, pooled sensitivity was 74.8% (95% CI 68.6–80.1) and specificity was 81.5% (95% CI 73.9–87.3). For AI-assisted clinicians, the overall sensitivity was 81.1% (95% CI 74.4–86.5) and specificity was 86.1% (95% CI 79.2–90.9). AI benefitted medical professionals of all experience levels in subgroup analyses, with the largest improvement among non-dermatologists. No publication bias was detected, and sensitivity analysis revealed that the findings were robust. AI in the hands of clinicians has the potential to improve diagnostic accuracy in skin cancer diagnosis. Given that most studies were conducted in experimental settings, we encourage future studies to further investigate these potential benefits in real-life settings.","<method>deep learning-based AI assistance</method>, <method>bivariate random effects model</method>"
2024,https://openalex.org/W4390506438,Social Sciences,Artificial intelligence for oral squamous cell carcinoma detection based on oral photographs: A comprehensive literature review,"Abstract Introduction Oral squamous cell carcinoma (OSCC) presents a significant global health challenge. The integration of artificial intelligence (AI) and computer vision holds promise for the early detection of OSCC through the analysis of digitized oral photographs. This literature review explores the landscape of AI‐driven OSCC automatic detection, assessing both the performance and limitations of the current state of the art. Materials and Methods An electronic search using several data base was conducted, and a systematic review performed in accordance with PRISMA guidelines (CRD42023441416). Results Several studies have demonstrated remarkable results for this task, consistently achieving sensitivity rates exceeding 85% and accuracy rates surpassing 90%, often encompassing around 1000 images. The review scrutinizes these studies, shedding light on their methodologies, including the use of recent machine learning and pattern recognition approaches coupled with different supervision strategies. However, comparing the results from different papers is challenging due to variations in the datasets used. Discussion Considering these findings, this review underscores the urgent need for more robust and reliable datasets in the field of OSCC detection. Furthermore, it highlights the potential of advanced techniques such as multi‐task learning, attention mechanisms, and ensemble learning as crucial tools in enhancing the accuracy and sensitivity of OSCC detection through oral photographs. Conclusion These insights collectively emphasize the transformative impact of AI‐driven approaches on early OSCC diagnosis, with the potential to significantly improve patient outcomes and healthcare practices.","<method>machine learning</method>, <method>pattern recognition</method>, <method>multi-task learning</method>, <method>attention mechanisms</method>, <method>ensemble learning</method>"
2024,https://openalex.org/W4390952671,Social Sciences,How service robots’ human-like appearance impacts consumer trust: a study across diverse cultures and service settings,"Purpose This study aims to compares the effects of different human-like appearances (low vs. medium vs. high) of service robots (SRs) on consumer trust in service robots (CTSR), examines the mediating role of perceived warmth (WA) and perceived competence (CO) and demonstrates the moderating role of culture and service setting. Design/methodology/approach The research design includes three scenario-based experiments (Chinese hotel setting, American hotel setting, Chinese hospital setting). Findings Study 1 found SR’s human-like appearance can arouse perceived anthropomorphism (PA), which positively affects CTSR through parallel mediators (WA and CO). Study 2 revealed consumers from Chinese (vs. American) culture had higher CTSR. Study 3 showed consumers had higher WA and CO for SRs in the credence (vs. experience) service setting. The authors also had an exploratory analysis of the uncanny valley phenomenon. Practical implications The findings have practical implications for promoting the diffusion of SRs in the hospitality industry. Managers can increase CTSR by augmenting the anthropomorphic design of SRs; however, they must consider the differences in this effect across all service recipients (consumers from different cultures) and service settings. Originality/value The authors introduce WA and CO as mediators between PA and CTSR and set the culture and service setting as moderators.",No methods found.
2024,https://openalex.org/W4391096835,Social Sciences,Diagnostic Performance Comparison between Generative AI and Physicians: A Systematic Review and Meta-Analysis,"Abstract Background The rapid advancement of generative artificial intelligence (AI) has led to the wide dissemination of models with exceptional understanding and generation of human language. Their integration into healthcare has shown potential for improving medical diagnostics, yet a comprehensive diagnostic performance evaluation of generative AI models and the comparison of their diagnostic performance with that of physicians has not been extensively explored. Methods In this systematic review and meta-analysis, a comprehensive search of Medline, Scopus, Web of Science, Cochrane Central, and MedRxiv was conducted for studies published from June 2018 through December 2023, focusing on those that validate generative AI models for diagnostic tasks. The risk of bias was assessed using the Prediction Model Study Risk of Bias Assessment Tool. Meta-regression was performed to summarize the performance of the models and to compare the accuracy of the models with that of physicians. Results The search resulted in 54 studies being included in the meta-analysis. Nine generative AI models were evaluated across 17 medical specialties. The quality assessment indicated a high risk of bias in the majority of studies, primarily due to small sample sizes. The overall accuracy for generative AI models across 54 studies was 56.9% (95% confidence interval [CI]: 51.0–62.7%). The meta-analysis demonstrated that, on average, physicians exceeded the accuracy of the models (difference in accuracy: 14.4% [95% CI: 4.9–23.8%], p-value =0.004). However, both Prometheus (Bing) and GPT-4 showed slightly better performance compared to non-experts (-2.3% [95% CI: -27.0–22.4%], p-value = 0.848 and -0.32% [95% CI: -14.4–13.7%], p-value = 0.962), but slightly underperformed when compared to experts (10.9% [95% CI: -13.1–35.0%], p-value = 0.356 and 12.9% [95% CI: 0.15–25.7%], p-value = 0.048). The sub-analysis revealed significantly improved accuracy in the fields of Gynecology, Pediatrics, Orthopedic surgery, Plastic surgery, and Otolaryngology, while showing reduced accuracy for Neurology, Psychiatry, Rheumatology, and Endocrinology compared to that of General Medicine. No significant heterogeneity was observed based on the risk of bias. Conclusions Generative AI exhibits promising diagnostic capabilities, with accuracy varying significantly by model and medical specialty. Although they have not reached the reliability of expert physicians, the findings suggest that generative AI models have the potential to enhance healthcare delivery and medical education, provided they are integrated with caution and their limitations are well-understood. Key Points Question: What is the diagnostic accuracy of generative AI models and how does this accuracy compare to that of physicians? Findings: This meta-analysis found that generative AI models have a pooled accuracy of 56.9% (95% confidence interval: 51.0–62.7%). The accuracy of expert physicians exceeds that of AI in all specialties, however, some generative AI models are comparable to non-expert physicians. Meaning: The diagnostic performance of generative AI models suggests that they do not match the level of experienced physicians but that they may have potential applications in healthcare delivery and medical education.",<method>generative AI models</method>
2024,https://openalex.org/W4391260165,Social Sciences,Artificial intelligence and corporate carbon neutrality: A qualitative exploration,"Abstract Many firms have established formal carbon neutrality (CN) targets in response to the increasing climate risk and related regulatory requirements. Subsequently, they have implemented various measures and adopted multiple approaches to attain these goals. Academic research has given due attention to firms' efforts in this direction. However, past studies have primarily focused on non‐digital and process‐oriented approaches to achieving CN, with the potential of digital technologies such as artificial intelligence (AI) remaining less explored. Our study aims to address this gap by qualitatively examining the use of AI for pursuing CN, drawing insights from firms with prior experience in the area. We analyzed the collected qualitative data to identify four key dimensions that capture different nuances of applying AI for achieving CN: (a) implementing AI for direct and indirect control of emissions, (b) accepting the strategic trade‐offs related to funding, data and systems concerns, and social priorities, (c) overcoming organizational and human‐related impediments, and (d) acknowledging the significant impact of AI in terms of gains in business model efficiency and measurable CN target attainment, which ultimately contribute to CN. Based on our findings, we propose a convergence–divergence model encompassing the positive aspects, inhibiting factors, synergies, and offsets necessary for firms to leverage AI to achieve net‐zero emissions effectively. Overall, our study contributes to the discourse on the utilization of AI for CN in a comprehensive manner.",No methods found.
2024,https://openalex.org/W4392714393,Social Sciences,Fostering a Safety Culture in Manufacturing Industry through Safety Behavior: A Structural Equation Modelling Approach,"Creating a robust safety management system is crucial for fostering a culture of safety in the workplace, particularly in industries like manufacturing where improvements are still needed. This study aimed to assess the impact of safety behavior on safety culture within the manufacturing sector. Employing a quantitative approach, questionnaires were distributed to 342 employees in manufacturing firms during data collection. The collected data underwent analysis using Structural Equation Modeling through IBM-SPSS-AMOS 24.0 to test the proposed model. The study findings revealed that components of safety behavior, specifically safety compliance and safety leadership, have a significant influence on safety culture. This implies that prioritizing safety behavior and culture is vital for occupational safety and health, aligning with guidelines set by responsible entities to ensure a secure work environment. The insights gained from this research can be instrumental in highlighting the importance of safety culture, the pivotal role of leadership, the complex nature of safety culture, and the potential for measuring and enhancing it. By understanding these implications, organizations can foster a safety-centric culture that not only protects employees but also enhances overall performance. Additionally, this research contributed to the existing literature by examining an integrated higher-order construct model using the SEM technique, predicting the model by 53 percent. The insights garnered from this study are applicable to various types of firms, emphasizing the integral role of safety culture in any organization.",<method>Structural Equation Modeling</method>
2024,https://openalex.org/W4393222088,Social Sciences,Methodological insights into ChatGPT’s screening performance in systematic reviews,"Abstract Background The screening process for systematic reviews and meta-analyses in medical research is a labor-intensive and time-consuming task. While machine learning and deep learning have been applied to facilitate this process, these methods often require training data and user annotation. This study aims to assess the efficacy of ChatGPT, a large language model based on the Generative Pretrained Transformers (GPT) architecture, in automating the screening process for systematic reviews in radiology without the need for training data. Methods A prospective simulation study was conducted between May 2nd and 24th, 2023, comparing ChatGPT’s performance in screening abstracts against that of general physicians (GPs). A total of 1198 abstracts across three subfields of radiology were evaluated. Metrics such as sensitivity, specificity, positive and negative predictive values (PPV and NPV), workload saving, and others were employed. Statistical analyses included the Kappa coefficient for inter-rater agreement, ROC curve plotting, AUC calculation, and bootstrapping for p-values and confidence intervals. Results ChatGPT completed the screening process within an hour, while GPs took an average of 7–10 days. The AI model achieved a sensitivity of 95% and an NPV of 99%, slightly outperforming the GPs’ sensitive consensus (i.e., including records if at least one person includes them). It also exhibited remarkably low false negative counts and high workload savings, ranging from 40 to 83%. However, ChatGPT had lower specificity and PPV compared to human raters. The average Kappa agreement between ChatGPT and other raters was 0.27. Conclusions ChatGPT shows promise in automating the article screening phase of systematic reviews, achieving high sensitivity and workload savings. While not entirely replacing human expertise, it could serve as an efficient first-line screening tool, particularly in reducing the burden on human resources. Further studies are needed to fine-tune its capabilities and validate its utility across different medical subfields.","<method>machine learning</method>, <method>deep learning</method>, <method>ChatGPT</method>, <method>Generative Pretrained Transformers (GPT) architecture</method>"
2024,https://openalex.org/W4394835724,Social Sciences,Machine Learning-Assisted Design of Advanced Polymeric Materials,"ConspectusPolymeric material research is encountering a new paradigm driven by machine learning (ML) and big data. The ML-assisted design has proven to be a successful approach for designing novel high-performance polymeric materials. This goal is mainly achieved through the following procedure: structure representation and database construction, establishment of a ML-based property prediction model, virtual design and high-throughput screening. The key to this approach lies in training ML models that delineate structure–property relationships based on available polymer data (e.g., structure, component, and property data), enabling the screening of promising polymers that satisfy the targeted property requirements. However, the relative scarcity of high-quality polymer data and the complex polymeric multiscale structure–property relationships pose challenges for this ML-assisted design method, such as data and modeling challenges.In this Account, we summarize the state-of-the-art advancements concerning the ML-assisted design of polymeric materials. Regarding structure representation and database construction, the digital representations of polymers are the predominant methods in cheminformatics along with some newly developed methods that integrate the polymeric multiscale structure characteristics. When establishing a ML-based property prediction model, the key is choosing and optimizing ML models to attain high-precision predictions across a vast chemical structure space. Advanced ML algorithms, such as transfer learning and multitask learning, have been utilized to address the data and modeling challenges. During the ML-assisted screening process, by defining and combining polymer genes, virtual polymer candidates are generated, and subsequently, their properties are predicted and high-throughput screened using ML property prediction models. Finally, the promising polymers identified through this approach are verified by computer simulations and experiments.We provide an overview of our recent efforts toward developing ML-assisted design approaches for discovering advanced polymeric materials and emphasize the intricate nature of polymer structural design. To well describe the multiscale structures of polymers, new structure representation methods, such as polymer fingerprint and cross-linking descriptors, were developed. Moreover, a multifidelity learning method was proposed to leverage the multisource isomerous polymer data from experiments and simulations. Additionally, graph neural networks and Bayesian optimization methods have been developed and applied for predicting polymer properties as well as designing polymer structures and compositions.Finally, we identify the current challenges and point out the development directions in this emerging field. It is highly desirable to establish new structure representation and advanced ML modeling methods for polymeric materials, particularly when constructing polymer large models based on chemical language. Through this Account, we seek to stimulate further interest and foster active collaborations for developing ML-assisted design approaches and realizing the innovation of advanced polymeric materials.","<method>transfer learning</method>, <method>multitask learning</method>, <method>multifidelity learning</method>, <method>graph neural networks</method>, <method>Bayesian optimization</method>"
2024,https://openalex.org/W4398169659,Social Sciences,The Sociodemographic Biases in Machine Learning Algorithms: A Biomedical Informatics Perspective,"Artificial intelligence models represented in machine learning algorithms are promising tools for risk assessment used to guide clinical and other health care decisions. Machine learning algorithms, however, may house biases that propagate stereotypes, inequities, and discrimination that contribute to socioeconomic health care disparities. The biases include those related to some sociodemographic characteristics such as race, ethnicity, gender, age, insurance, and socioeconomic status from the use of erroneous electronic health record data. Additionally, there is concern that training data and algorithmic biases in large language models pose potential drawbacks. These biases affect the lives and livelihoods of a significant percentage of the population in the United States and globally. The social and economic consequences of the associated backlash cannot be underestimated. Here, we outline some of the sociodemographic, training data, and algorithmic biases that undermine sound health care risk assessment and medical decision-making that should be addressed in the health care system. We present a perspective and overview of these biases by gender, race, ethnicity, age, historically marginalized communities, algorithmic bias, biased evaluations, implicit bias, selection/sampling bias, socioeconomic status biases, biased data distributions, cultural biases and insurance status bias, conformation bias, information bias and anchoring biases and make recommendations to improve large language model training data, including de-biasing techniques such as counterfactual role-reversed sentences during knowledge distillation, fine-tuning, prefix attachment at training time, the use of toxicity classifiers, retrieval augmented generation and algorithmic modification to mitigate the biases moving forward.","<method>knowledge distillation</method>, <method>fine-tuning</method>, <method>prefix attachment at training time</method>, <method>toxicity classifiers</method>, <method>retrieval augmented generation</method>, <method>algorithmic modification</method>"
2024,https://openalex.org/W4399244247,Social Sciences,Investigating influencing factors of learning satisfaction in AI ChatGPT for research: University students perspective,"This study investigates the determinants of ChatGPT adoption among university students and its impact on learning satisfaction. Utilizing the Technology Acceptance Model (TAM) and incorporating insights from interaction learning, collaborative learning, and information quality, a structural equation modeling approach was employed. This research collected valuable responses from 262 students at King Faisal University in Saudi Arabia through the use of self-report questionnaires. The data's reliability and validity were assessed using confirmation factor analysis, followed by path analysis to explore the hypotheses in the proposed model. The results indicate the pivotal roles of interaction learning and collaborative learning in fostering ChatGPT adoption. Social interaction played a significant role, as researchers engaging in conversations and knowledge-sharing expressed increased comfort with ChatGPT. Information quality was found to substantially influence researchers' decisions to continue using ChatGPT, emphasizing the need for ongoing improvement in the accuracy and relevance of content provided. Perceived ease of use and perceived usefulness played intermediary roles in linking ChatGPT engagement to learning satisfaction. User-friendly interfaces and perceived utility were identified as crucial factors affecting overall satisfaction levels. Notably, ChatGPT positively impacted learning motivation, indicating its potential to enhance student engagement and interest in learning. The study's findings have implications for educational practitioners seeking to improve the implementation of AI technologies in university students, emphasizing user-friendly design, collaborative learning, and factors influencing satisfaction. The study concludes with insights into the complex interplay between AI-powered tools, learning objectives, and motivation, highlighting the need for continued research to comprehensively understand these dynamics. This study investigates the determinants of ChatGPT adoption among university students and its impact on learning satisfaction. Utilizing the Technology Acceptance Model (TAM) and incorporating insights from interaction learning, collaborative learning, and information quality, a structural equation modeling approach was employed. This research collected valuable responses from 262 students at King Faisal University in Saudi Arabia through the use of self-report questionnaires. The data's reliability and validity were assessed using confirmation factor analysis, followed by path analysis to explore the hypotheses in the proposed model. The results indicate the pivotal roles of interaction learning and collaborative learning in fostering ChatGPT adoption. Social interaction played a significant role, as researchers engaging in conversations and knowledge-sharing expressed increased comfort with ChatGPT. Information quality was found to substantially influence researchers' decisions to continue using ChatGPT, emphasizing the need for ongoing improvement in the accuracy and relevance of content provided. Perceived ease of use and perceived usefulness played intermediary roles in linking ChatGPT engagement to learning satisfaction. User-friendly interfaces and perceived utility were identified as crucial factors affecting overall satisfaction levels. Notably, ChatGPT positively impacted learning motivation, indicating its potential to enhance student engagement and interest in learning. The study's findings have implications for educational practitioners seeking to improve the implementation of AI technologies in university students, emphasizing user-friendly design, collaborative learning, and factors influencing satisfaction. The study concludes with insights into the complex interplay between AI-powered tools, learning objectives, and motivation, highlighting the need for continued research to comprehensively understand these dynamics.","<method>Technology Acceptance Model (TAM)</method>, <method>structural equation modeling</method>, <method>confirmation factor analysis</method>, <method>path analysis</method>"
2024,https://openalex.org/W4399363436,Social Sciences,Collective Constitutional AI: Aligning a Language Model with Public Input,"There is growing consensus that language model (LM) developers should not be the sole deciders of LM behavior, creating a need for methods that enable the broader public to collectively shape the behavior of LM systems that affect them. To address this need, we present Collective Constitutional AI (CCAI): a multi-stage process for sourcing and integrating public input into LMs—from identifying a target population to sourcing principles to training and evaluating a model. We demonstrate the real-world practicality of this approach by creating what is, to our knowledge, the first LM fine-tuned with collectively sourced public input and evaluating this model against a baseline model trained with established principles from a LM developer. Our quantitative evaluations demonstrate several benefits of our approach: the CCAI-trained model shows lower bias across nine social dimensions compared to the baseline model, while maintaining equivalent performance on language, math, and helpful-harmless evaluations. Qualitative comparisons of the models suggest that the models differ on the basis of their respective constitutions, e.g., when prompted with contentious topics, the CCAI-trained model tends to generate responses that reframe the matter positively instead of a refusal. These results demonstrate a promising, tractable pathway toward publicly informed development of language models.","<method>Collective Constitutional AI (CCAI)</method>, <method>fine-tuning</method>"
2024,https://openalex.org/W4399715357,Social Sciences,AI-POWERED FRAUD DETECTION IN BANKING: SAFEGUARDING FINANCIAL TRANSACTIONS,"The banking industry's metamorphosis through digitalization has unquestionably revolutionized accessibility and convenience for customers worldwide. However, this paradigm shift has ushered in a new era of challenges, most notably in the realm of cybersecurity. Conventional rule-based fraud detection strategies have struggled to keep pace with the rapid evolution of cyber threats, prompting a surge of interest in more adaptive approaches like unsupervised learning. Furthermore, the COVID-19 pandemic has exacerbated the issue of bank fraud due to the widespread transition to online platforms and the proliferation of charitable funds, which present ripe opportunities for exploitation by cybercriminals. In response to these pressing concerns, this study delves into the realm of machine learning algorithms for the analysis and identification of fraudulent banking transactions. Notably, it contributes scientific novelty by developing models specifically tailored to this purpose and implementing innovative preprocessing techniques to enhance detection accuracy. Utilizing a diverse array of algorithms, including Random Forest, K-Nearest Neighbor (KNN), Naïve Bayes, Decision Trees, and Logistic Regression, the study showcases promising results. In particular, logistic regression and decision tree models exhibit impressive accuracy and Area Under the Curve (AUC) values of approximately 0.98, 0.97 and 0.95, 0.94, respectively. Given the pervasive nature of banking fraud in our digital society, the utilization of artificial intelligence algorithms for fraud detection stands as a critical and timely endeavor, promising enhanced security and trust in the financial ecosystem.","<method>unsupervised learning</method>, <method>Random Forest</method>, <method>K-Nearest Neighbor (KNN)</method>, <method>Naïve Bayes</method>, <method>Decision Trees</method>, <method>Logistic Regression</method>"
2024,https://openalex.org/W4401519440,Social Sciences,Blockchain technology adoption and its impact on SME performance: insights for entrepreneurs and policymakers,"Purpose This study aims to empirically examine and analyze the factors that influence the adoption of blockchain technology, particularly within small and medium-sized enterprises (SMEs). The study also predicts how adopting blockchain technology may affect SMEs’ market and financial performance. Design/methodology/approach The research is grounded in the theoretical frameworks of the “technology–organization–environment (TOE) framework” and the “resource-based view (RBV)” perspective. The researchers collected 407 responses from a survey conducted on SMEs in India. The statistical package for social science, followed by the “partial least square structural equation modeling (PLS-SEM)” technique, was applied for the data analysis. Findings This paper offered a robust research framework for blockchain technology adoption in which one of the two proposed technological factors (relative advantage), one organizational factor (top management support) and two environmental factors (competitive pressure and market dynamics) significantly influence blockchain technology adoption. Similarly, there is a substantial association between blockchain technology adoption and both market and financial performance. More specifically, the complexity and perceived investment cost have been recognized as barriers to SMEs adopting blockchain technology. Research limitations/implications The primary focus of this research lies in examining the adoption of blockchain technology among SMEs in India. Consequently, there exists an opportunity to broaden the scope of this study to include various other countries. Such an expansion holds the potential to yield more precise and comprehensive results, enabling a comparative analysis across diverse international contexts. Practical implications The outcomes have practical significance for SMEs as they navigate their strategies for adopting blockchain technology. Moreover, policymakers and practitioners can use these findings to enact specific measures targeting barriers, fostering the adoption of blockchain in Indian SMEs and creating a more supportive environment for technological integration and growth. Originality/value This study introduces a novel theoretical framework focusing on the impact of blockchain adoption on SMEs. Its distinctive contribution lies in investigating the mediating role of blockchain adoption in the relationship between market and financial performance, specifically within emerging economies. By addressing this gap, the study enhances the understanding of how blockchain adoption shapes SME performance in evolving economic landscapes.",No methods found.
2024,https://openalex.org/W4391560128,Social Sciences,Analyzing Preceding factors affecting behavioral intention on communicational artificial intelligence as an educational tool,"During the pandemic, artificial intelligence was employed and utilized by students around the globe. Students' conduct changed in a variety of ways when schooling returned to regular instruction. This study aimed to analyze the student's behavioral intention and actual academic use of communicational AI (CAI) as an educational tool. This study identified the variables by utilizing an integrated framework based on the Unified Theory of Acceptance and Use of Technology (UTAUT2) and self-determination theory. Through the use of an online survey and Structural Equation Modeling, data from 533 respondents were analyzed. The results showed that perceived relatedness has the most significant effect on the behavioral intention of students in using CAI as an educational tool, followed by perceived autonomy. It showed that students use CAI based on the objective and the possibility of increasing their productivity, rather than any other purpose in the education setting. Among the UTAUT2 domains, only facilitating conditions, habit, and performance expectancy provided a significant direct effect on behavioral intention and an indirect effect on actual academic use. Further implications were presented. Moreover, the methodology and framework of this study could be extended and applied to educational technology-related studies. Lastly, the outcome of this study may be considered in analyzing the behavioral intention of the students as the teaching-learning environment is still continuously expanding and developing.",<method>Structural Equation Modeling</method>
2024,https://openalex.org/W4391655051,Social Sciences,Do large language models show decision heuristics similar to humans? A case study using GPT-3.5.,"A Large Language Model (LLM) is an artificial intelligence system trained on vast amounts of natural language data, enabling it to generate human-like responses to written or spoken language input. Generative Pre-Trained Transformer (GPT)-3.5 is an example of an LLM that supports a conversational agent called ChatGPT. In this work, we used a series of novel prompts to determine whether ChatGPT shows heuristics and other context-sensitive responses. We also tested the same prompts on human participants. Across four studies, we found that ChatGPT was influenced by random anchors in making estimates (anchoring, Study 1); it judged the likelihood of two events occurring together to be higher than the likelihood of either event occurring alone, and it was influenced by anecdotal information (representativeness and availability heuristic, Study 2); it found an item to be more efficacious when its features were presented positively rather than negatively-even though both presentations contained statistically equivalent information (framing effect, Study 3); and it valued an owned item more than a newly found item even though the two items were objectively identical (endowment effect, Study 4). In each study, human participants showed similar effects. Heuristics and context-sensitive responses in humans are thought to be driven by cognitive and affective processes such as loss aversion and effort reduction. The fact that an LLM-which lacks these processes-also shows such responses invites consideration of the possibility that language is sufficiently rich to carry these effects and may play a role in generating these effects in humans. (PsycInfo Database Record (c) 2024 APA, all rights reserved).","<method>Large Language Model (LLM)</method>, <method>Generative Pre-Trained Transformer (GPT)-3.5</method>"
2024,https://openalex.org/W4400580797,Social Sciences,Promoting well-being through happiness at work: a systematic literature review and future research agenda,"Purpose Our study aims to understand what is known about happiness at work (HAW) in terms of publication, citations, dimensions and characteristics, as well as how knowledge about HAW is generated regarding theoretical frameworks, context and methods. Additionally, it explores future directions for HAW research. Design/methodology/approach This paper conducts a systematic literature review of 56 empirical articles published between 2000 and 2022 to comprehensively explore HAW. It examines publication trends, citation patterns, dimensions, characteristics, theoretical frameworks, contextual factors and research methodologies employed in HAW studies. Findings Our findings suggest that while HAW research has gained momentum, there is still a need for exploration, particularly in developing countries. Various theoretical frameworks such as the job demand-resources model, social exchange theory and broaden-and-build theory are identified, with suggestions for the adoption of less popular theories like the positive emotion, engagement, relationships, meaning and accomplishment (PERMA) model and flow theory for future investigations. The review contributes to workplace happiness literature by offering a comprehensive analysis spanning two decades and provides valuable insights for guiding future research toward exploring factors influencing employee well-being. Originality/value Our article offers a structured analysis of HAW literature, emphasizing the necessity for more extensive research, especially in developing nations. It provides valuable insights into the theories and dimensions associated with HAW, guiding future research and assisting organizations in formulating strategies to enhance employee happiness and overall well-being.",No methods found.
2024,https://openalex.org/W4401941326,Social Sciences,Predicting ground vibration during rock blasting using relevance vector machine improved with dual kernels and metaheuristic algorithms,"The ground vibration caused by rock blasting is an extremely hazardous outcome of the blasting operation. Blasting activity has detrimental effects on both the ecology and the human population living in proximity to the area. Evaluating the magnitude of blasting vibrations requires careful evaluation of the peak particle velocity (PPV) as a fundamental and essential parameter for quantifying vibration velocity. Therefore, this study employs models using the relevance vector machine (RVM) approach for predicting the PPV resulting from quarry blasting. This investigation utilized the conventional and optimized RVM models for the first time in ground vibration prediction. This work compares thirty-three RVM models to choose the most efficient performance model. The following conclusions have been mapped from the outcomes of the several analyses. The performance evaluation of each RVM model demonstrates each model achieved a performance of more than 0.85 during the testing phase, there was a strong correlation observed between the actual ground vibrations and the predicted ones. The analysis of performance metrics (RMSE = 21.2999 mm/s, 16.2272 mm/s, R = 0.9175, PI = 1.59, IOA = 0.8239, IOS = 0.2541), score analysis (= 93), REC curve (= 6.85E-03, close to the actual, i.e., 0), curve fitting (= 1.05 close to best fit, i.e., 1), AD test (= 11.607 close to the actual, i.e., 9.790), Wilcoxon test (= 95%), Uncertainty analysis (WCB = 0.0134), and computational cost (= 0.0180) demonstrate that PSO_DRVM model MD29 outperformed better than other RVM models in the testing phase. This study will help mining and civil engineers and blasting experts to select the best kernel function and its hyperparameters in estimating ground vibration during rock blasting project. In the context of the mining and civil industry, the application of this study offers significant potential for enhancing safety protocols and optimizing operational efficiency.","<method>relevance vector machine (RVM)</method>, <method>conventional RVM</method>, <method>optimized RVM</method>, <method>PSO_DRVM</method>"
2024,https://openalex.org/W4402305045,Social Sciences,Triplet Contrastive Representation Learning for Unsupervised Vehicle Re-identification,"Part feature learning plays a crucial role in achieving fine-grained semantic understanding in unsupervised vehicle re-identification. However, existing approaches directly model part and global features, which can easily lead to severe gradient vanishing issues due to their unequal feature information and unreliable pseudo-labels. To address this problem, in this paper, we propose a triplet contrastive representation learning (TCRL) framework, which leverages cluster features to bridge the part features and global features for unsupervised vehicle re-identification. Specifically, TCRL devises three memory banks to store the instance/cluster features and proposes a proxy contrastive loss (PCL) to make contrastive learning between adjacent memory banks, thus presenting the associations between the part and global features as a transition of the part-cluster and cluster-global associations. Since the cluster memory bank copes with all the vehicle features, it can summarize them into a discriminative feature representation. To deeply exploit the instance/cluster information, TCRL proposes two additional loss functions. For the instance-level feature, a hybrid contrastive loss (HCL) re-defines the sample correlations by approaching the positive instance features and pushing all negative instance features away. For the cluster-level feature, a weighted regularization cluster contrastive loss (WRCCL) refines the pseudo labels by penalizing the mislabeled images according to the instance similarity. Extensive experiments show that TCRL outperforms many state-of-the-art unsupervised vehicle re-identification approaches.","<method>triplet contrastive representation learning (TCRL)</method>, <method>contrastive learning</method>, <method>proxy contrastive loss (PCL)</method>, <method>hybrid contrastive loss (HCL)</method>, <method>weighted regularization cluster contrastive loss (WRCCL)</method>"
2024,https://openalex.org/W4391618879,Social Sciences,Likelihood-based feature representation learning combined with neighborhood information for predicting circRNA–miRNA associations,"Connections between circular RNAs (circRNAs) and microRNAs (miRNAs) assume a pivotal position in the onset, evolution, diagnosis and treatment of diseases and tumors. Selecting the most potential circRNA-related miRNAs and taking advantage of them as the biological markers or drug targets could be conducive to dealing with complex human diseases through preventive strategies, diagnostic procedures and therapeutic approaches. Compared to traditional biological experiments, leveraging computational models to integrate diverse biological data in order to infer potential associations proves to be a more efficient and cost-effective approach. This paper developed a model of Convolutional Autoencoder for CircRNA-MiRNA Associations (CA-CMA) prediction. Initially, this model merged the natural language characteristics of the circRNA and miRNA sequence with the features of circRNA-miRNA interactions. Subsequently, it utilized all circRNA-miRNA pairs to construct a molecular association network, which was then fine-tuned by labeled samples to optimize the network parameters. Finally, the prediction outcome is obtained by utilizing the deep neural networks classifier. This model innovatively combines the likelihood objective that preserves the neighborhood through optimization, to learn the continuous feature representation of words and preserve the spatial information of two-dimensional signals. During the process of 5-fold cross-validation, CA-CMA exhibited exceptional performance compared to numerous prior computational approaches, as evidenced by its mean area under the receiver operating characteristic curve of 0.9138 and a minimal SD of 0.0024. Furthermore, recent literature has confirmed the accuracy of 25 out of the top 30 circRNA-miRNA pairs identified with the highest CA-CMA scores during case studies. The results of these experiments highlight the robustness and versatility of our model.","<method>Convolutional Autoencoder</method>, <method>deep neural networks classifier</method>, <method>5-fold cross-validation</method>"
2024,https://openalex.org/W4392515337,Social Sciences,A scoping review of fair machine learning techniques when using real-world data,"The integration of artificial intelligence (AI) and machine learning (ML) in health care to aid clinical decisions is widespread. However, as AI and ML take important roles in health care, there are concerns about AI and ML associated fairness and bias. That is, an AI tool may have a disparate impact, with its benefits and drawbacks unevenly distributed across societal strata and subpopulations, potentially exacerbating existing health inequities. Thus, the objectives of this scoping review were to summarize existing literature and identify gaps in the topic of tackling algorithmic bias and optimizing fairness in AI/ML models using real-world data (RWD) in health care domains. We conducted a thorough review of techniques for assessing and optimizing AI/ML model fairness in health care when using RWD in health care domains. The focus lies on appraising different quantification metrics for accessing fairness, publicly accessible datasets for ML fairness research, and bias mitigation approaches. We identified 11 papers that are focused on optimizing model fairness in health care applications. The current research on mitigating bias issues in RWD is limited, both in terms of disease variety and health care applications, as well as the accessibility of public datasets for ML fairness research. Existing studies often indicate positive outcomes when using pre-processing techniques to address algorithmic bias. There remain unresolved questions within the field that require further research, which includes pinpointing the root causes of bias in ML models, broadening fairness research in AI/ML with the use of RWD and exploring its implications in healthcare settings, and evaluating and addressing bias in multi-modal data. This paper provides useful reference material and insights to researchers regarding AI/ML fairness in real-world health care data and reveals the gaps in the field. Fair AI/ML in health care is a burgeoning field that requires a heightened research focus to cover diverse applications and different types of RWD.",No methods found.
2024,https://openalex.org/W4401567811,Social Sciences,DisenSemi: Semi-Supervised Graph Classification via Disentangled Representation Learning,"Graph classification is a critical task in numerous multimedia applications, where graphs are employed to represent diverse types of multimedia data, including images, videos, and social networks. Nevertheless, in the real world, labeled graph data are always limited or scarce. To address this issue, we focus on the semi-supervised graph classification task, which involves both supervised and unsupervised models learning from labeled and unlabeled data. In contrast to recent approaches that transfer the entire knowledge from the unsupervised model to the supervised one, we argue that an effective transfer should only retain the relevant semantics that align well with the supervised task. We introduce a novel framework termed in this article, which learns disentangled representation for semi-supervised graph classification. Specifically, a disentangled graph encoder is proposed to generate factorwise graph representations for both supervised and unsupervised models. Then, we train two models via supervised objective and mutual information (MI)-based constraints, respectively. To ensure the meaningful transfer of knowledge from the unsupervised encoder to the supervised one, we further define an MI-based disentangled consistency regularization between two models and identify the corresponding rationale that aligns well with the current graph classification task. Experiments conducted on various publicly available datasets demonstrate the effectiveness of our .","<method>semi-supervised graph classification</method>, <method>disentangled graph encoder</method>, <method>supervised objective</method>, <method>mutual information (MI)-based constraints</method>, <method>MI-based disentangled consistency regularization</method>"
2024,https://openalex.org/W4391029001,Social Sciences,Smart energy management in residential buildings: the impact of knowledge and behavior,"Abstract A new technology called smart energy management makes use of IoT concepts to enhance energy efficiency and lower waste in structures. The goal of this study is to comprehend how household energy management knowledge affects energy usage, user behavior, related expenses, and environmental effect. Through a survey of 100 valid replies in Palestine, the research model assessed the knowledge and consumption habits of building occupants. Smart PLS software was used to analyze the research model using partial least squares structural equation modeling (PLS-SEM). Using path coefficients and behavior as a mediating variable, the structural model connected the latent variables. The mediation hypotheses were tested using the Preacher and Hayes method, and the indirect effect and confidence intervals were estimated and calculated using bootstrapping. The findings demonstrated that by lowering energy use and enhancing overall building performance, residential buildings that implement smart energy consumption management systems may move toward a more sustainable future. Furthermore, the study found that education and awareness campaigns are necessary to increase residents’ knowledge of these systems to promote energy savings. The results also indicated statistically significant indirect effects, supporting the existence of mediation of the behavior construct. Path coefficient values and P -values were presented to further support the study’s hypotheses. Such smart energy management systems represent an important innovation in building management and can help create more sustainable and efficient buildings.","<method>partial least squares structural equation modeling (PLS-SEM)</method>, <method>Preacher and Hayes method</method>, <method>bootstrapping</method>"
2024,https://openalex.org/W4391062514,Social Sciences,Translating a value-based framework for resilient e-learning impact in post COVID-19 times: Research-based Evidence from Higher Education in Kuwait,"The covid-19 pandemic has changed people's daily lives and behaviors all across the world and has impacted practically every element of human existence. The introduction of remote education systems and the move toward online learning have had some of the most significant effects. The on-site operations of educational institutions, such as schools, colleges, and universities, have had to be suspended in order to stop the virus' spread. In order to effectively disseminate instructional material and guarantee the unbroken progression of students' academic endeavors, educators have been forced to look for novel approaches. The study used the Value-Based Adoption Model (VAM) as a conceptual framework to look into the factors that affected Kuwait's e-learning outcomes in the midst of the covid-19 pandemic. 382 students at Kuwaiti universities and colleges were the source of quantitative data collection. The findings revealed that peer interaction emerged as the most influential factor in shaping outcomes within the educational context of Kuwait, while instructors and course design factors were not significant. Using the VAM, this study investigated the impact of several factors on students' e-learning results during times of crisis. The research expands the existing knowledge base in the field on this subject and suggests developing a well-organized online learning crisis approach. The main contribution of this work is summarized on (i) An integrated framework for the quality of the e-learning experience in universities in post-covid-19 times and (ii) A resilient higher education institutional learning strategy model in post-covid-19 times. The findings of this paper can be generalizable to other Gulf Corporation Council (GCC) countries such as Kingdom of Saudi Arabia, Qatar, United Arab Emirates (UAE), Bahrain and Oman. This is due to the shared cultural traditions and values, along with similar educational systems among these nations.",No methods found.
2024,https://openalex.org/W4391346549,Social Sciences,CSR and employee outcomes: a systematic literature review,"Abstract The purpose of this research is to consolidate and extend the current literature on employee outcomes of CSR (referred to as micro-level outcomes). The authors use a systematic review of the literature as a method to summarize and synthesise the different effects of CSR activities on employees based on 270 journal articles. The contribution of this paper is that it provides a comprehensive list of employee outcomes classified into different categories and a conceptual framework that maps desirable and undesirable outcomes of CSR activities on employees. The results show that various dimensions of CSR have different effects on employee outcomes. In addition, we explain mediators of CSR-employee outcomes relationships and moderators that could strengthen or weaken this relationship. The review reveals important gaps and offers a research agenda for the future. We have found only a few studies dealing with the negative impacts of CSR on employees as well as only a few studies that explain how different dimensions of CSR affect employees differently. The study has also practical implications for companies, as understanding different effects of CSR on employees helps organizations to design and implement CSR strategies and policies that foster employees’ positive attitudes and behaviours as well as prevent or reduce the negative effects, and hence create a business value and sustainable growth for the company.",No methods found.
2024,https://openalex.org/W4391755073,Social Sciences,Specifying cross-system collaboration strategies for implementation: a multi-site qualitative study with child welfare and behavioral health organizations,"Abstract Background Cross-system interventions that integrate health, behavioral health, and social services can improve client outcomes and expand community impact. Successful implementation of these interventions depends on the extent to which service partners can align frontline services and organizational operations. However, collaboration strategies linking multiple implementation contexts have received limited empirical attention. This study identifies, describes, and specifies multi-level collaboration strategies used during the implementation of Ohio Sobriety Treatment and Reducing Trauma (Ohio START), a cross-system intervention that integrates services across two systems (child welfare and evidence-based behavioral health services) for families that are affected by co-occurring child maltreatment and parental substance use disorders. Methods In phase 1, we used a multi-site qualitative design with 17 counties that implemented Ohio START. Qualitative data were gathered from 104 staff from child welfare agencies, behavioral health treatment organizations, and regional behavioral health boards involved in implementation via 48 small group interviews about collaborative approaches to implementation. To examine cross-system collaboration strategies, qualitative data were analyzed using an iterative template approach and content analysis. In phase 2, a 16-member expert panel met to validate and specify the cross-system collaboration strategies identified in the interviews. The panel was comprised of key child welfare and behavioral health partners and scholars. Results In phase 1, we identified seven cross-system collaboration strategies used for implementation. Three strategies were used to staff the program: (1) contract for expertise, (2) provide joint supervision, and (3) co-locate staff. Two strategies were used to promote service access: (4) referral protocols and (5) expedited access agreements. Two strategies were used to align case plans: (6) shared decision-making meetings, and (7) sharing data. In phase 2, expert panelists specified operational details of the cross-system collaboration strategies, and explained the processes by which strategies were perceived to improve implementation and service system outcomes. Conclusions We identified a range of cross-system collaboration strategies that show promise for improving staffing, service access, and case planning. Leaders, supervisors, and frontline staff used these strategies during all phases of implementation. These findings lay the foundation for future experimental and quasi-experimental studies that test the effectiveness of cross-system collaboration strategies.",No methods found.
2024,https://openalex.org/W4391838804,Social Sciences,Simulating institutional heterogeneity in sustainability science,"Sustainability outcomes are influenced by the laws and configurations of natural and engineered systems as well as activities in socio-economic systems. An important subset of human activity is the creation and implementation of institutions, formal and informal rules shaping a wide range of human behavior. Understanding these rules and codifying them in computational models can provide important missing insights into why systems function the way they do (static) as well as the pace and structure of transitions required to improve sustainability (dynamic). Here, we conduct a comparative synthesis of three modeling approaches— integrated assessment modeling, engineering–economic optimization, and agent-based modeling—with underexplored potential to represent institutions. We first perform modeling experiments on climate mitigation systems that represent specific aspects of heterogeneous institutions, including formal policies and institutional coordination, and informal attitudes and norms. We find measurable but uneven aggregate impacts, while more politically meaningful distributional impacts are large across various actors. Our results show that omitting institutions can influence the costs of climate mitigation and miss opportunities to leverage institutional forces to speed up emissions reduction. These experiments allow us to explore the capacity of each modeling approach to represent insitutions and to lay out a vision for the next frontier of endogenizing institutional change in sustainability science models. To bridge the gap between modeling, theories, and empirical evidence on social institutions, this research agenda calls for joint efforts between sustainability modelers who wish to explore and incorporate institutional detail, and social scientists studying the socio-political and economic foundations for sustainability transitions.",No methods found.
2024,https://openalex.org/W4392156210,Social Sciences,Servant leadership style and socially responsible leadership in university context: moderation of promoting sense of community,"Purpose The purpose of this study is to examine the extent to which promoting sense of community moderates the relationship between servant leadership style and socially responsible leadership (SRL) of public universities in Uganda. Design/methodology/approach The study adopted cross-sectional survey design to collect data at one point in time using self-administered questionnaires from 214 respondents to examine the relationship between servant leadership and socially responsible leadership with promoting sense of community as a moderator. The study used statistical package for social scientists (SPSS) PROCESS MACRO to establish clusters among the surveyed public universities and later a model was derived. Findings The study found a significant moderating effect of promoting sense of community on servant leadership and socially responsible leadership. Implying that investment in promoting sense of community creates awareness about the socially responsible leadership in public universities. Practical implications Managers of public universities need to pay keen interest in promoting sense of community to boost socially responsible leadership by building a strong servant leadership style through promoting sense of community for senior managers and leaders especially heads of departments, faculty deans and principals in public universities. Originality/value This study contributes to socially responsible leadership literature by advancing the idea that SRL is an important resource that enhances through instituting servant leadership and promoting sense of community in a complex environment. Ideally, servant leadership and promoting sense of community is one of the drivers of customer value, efficiency and effectiveness of public universities.",No methods found.
2024,https://openalex.org/W4392955615,Social Sciences,Variation in social media sensitivity across people and contexts,"Abstract Social media impacts people’s wellbeing in different ways, but relatively little is known about why this is the case. Here we introduce the construct of “social media sensitivity” to understand how social media and wellbeing associations differ across people and the contexts in which these platforms are used. In a month-long large-scale intensive longitudinal study (total n = 1632; total number of observations = 120,599), we examined for whom and under which circumstances social media was associated with positive and negative changes in social and affective wellbeing. Applying a combination of frequentist and Bayesian multilevel models, we found a small negative average association between social media use AND subsequent wellbeing, but the associations were heterogenous across people. People with psychologically vulnerable dispositions (e.g., those who were depressed, lonely, not satisfied with life) tended to experience heightened negative social media sensitivity in comparison to people who were not psychologically vulnerable. People also experienced heightened negative social media sensitivity when in certain types of places (e.g., in social places, in nature) and while around certain types of people (e.g., around family members, close ties), as compared to using social media in other contexts. Our results suggest that an understanding of the effects of social media on wellbeing should account for the psychological dispositions of social media users, and the physical and social contexts surrounding their use. We discuss theoretical and practical implications of social media sensitivity for scholars, policymakers, and those in the technology industry.","<method>frequentist multilevel models</method>, <method>Bayesian multilevel models</method>"
2024,https://openalex.org/W4395074044,Social Sciences,"The Value of Political Connections of Developers in Residential Land Leasing: Case of Chengdu, China","The graduate approach applied in China for the economic transition poses the risk of continued government influence on the market. The land reform and the following adjustment in China have introduced a seemingly complete market for residential land. However, a widely practiced coalition between the local developmental states and developers might impact residential land leasing in a more hidden way. Taking central Chengdu as the study area, this study takes the enterprise ownership and affiliations as two explanatory factors that impact the land leasing prices and builds an MGWR model to evaluate the premium of political connections for the developers to obtain the land. The result gives a clue to the local protectionism and preference for state-owned enterprises that might exist in land leasing in Chengdu. It is proved in this study that the average purchase price by state-owned enterprises is 8.9% lower than the prices that private enterprises could enjoy, and the average land leasing price by local enterprises is 14.2% lower than that enjoyed by non-local enterprises. The preceding conceptual and empirical discussion in this study advocates for a review and rethinking of the public sector’s intervention in China’s land market. In-depth analyses of the factors that define the land leasing behaviors of the local government are needed.",<method>MGWR model</method>
2024,https://openalex.org/W4390506124,Social Sciences,Machine learning models for predicting preeclampsia: a systematic review,"Abstract Background This systematic review provides an overview of machine learning (ML) approaches for predicting preeclampsia. Method This review adhered to the Preferred Reporting Items for Systematic Reviews and Meta-Analyzes (PRISMA) guidelines. We searched the Cochrane Central Register, PubMed, EMBASE, ProQuest, Scopus, and Google Scholar up to February 2023. Search terms were limited to “preeclampsia” AND “artificial intelligence” OR “machine learning” OR “deep learning.” All studies that used ML-based analysis for predicting preeclampsia in pregnant women were considered. Non-English articles and those that are unrelated to the topic were excluded. The PROBAST was used to assess the risk of bias and applicability of each included study. Results The search strategy yielded 128 citations; after duplicates were removed and title and abstract screening was completed, 18 full-text articles were evaluated for eligibility. Four studies were included in this review. Two studies were at low risk of bias, and two had low to moderate risk. All of the study designs included were retrospective cohort studies. Nine distinct models were chosen as ML models from the four studies. Maternal characteristics, medical history, medication intake, obstetrical history, and laboratory and ultrasound findings obtained during prenatal care visits were candidate predictors to train the ML model. Elastic net, stochastic gradient boosting, extreme gradient boosting, and Random forest were among the best models to predict preeclampsia. All four studies used metrics such as the area under the curve, true positive rate, negative positive rate, accuracy, precision, recall, and F1 score. The AUC of ML models varied from 0.860 to 0.973 in four studies. Conclusion The results of studies yielded high prediction performance of ML models for preeclampsia risk from routine early pregnancy information.","<method>Elastic net</method>, <method>stochastic gradient boosting</method>, <method>extreme gradient boosting</method>, <method>Random forest</method>"
2024,https://openalex.org/W4390644295,Social Sciences,Creating valuable outcomes: An exploration of value creation pathways in the business models of energy communities,"Energy communities (ECs) have emerged as new, collective market actors within European energy systems. Their innovative business models are thought to result in a multiple types of value creation that transcend mere economic considerations. Yet, despite widespread acknowledgement of this, how multiple types of value are created in ECs remains inadequately explored and understood. We engage with this challenge by (1) situating ECs within the longer history and body of knowledge on community participation in energy systems before (2) mobilising insights on value creation from the literature on business models, and (3) distinguishing between value creation as a business model process and types of value outcomes derived from business activities. We apply this approach to three paradigmatic European cases and through comparative analysis identify two distinct value creation pathways. The first pathway links business activities and how they are assembled to functional value outcomes such as cost savings or CO2 reduction, which are quantifiable. The second pathway links how ECs are governed to normative value outcomes such as increased agency or social cohesion, which is a matter of individuals' perception. These results, we argue, call for greater attention and support to questions of organisation and process to ensure the transformational potential of ECs is realised.",No methods found.
2024,https://openalex.org/W4390811856,Social Sciences,Who is the next China? Comparative advantage analysis from top ten apparel exporting nations,"Purpose This study aims to measure the competitiveness of top apparel exporting nations competing with China in different apparel product categories across the global environment. Design/methodology/approach Compound annual growth rate, trade competitiveness, market share percentages, revealed comparative advantage and its variant normalized revealed comparative advantage using two-, four- and six-digit harmonized system codes for the period of 2016–2021 were used to understand the comparative advantage of competing apparel exporting nations. Findings The findings revealed that China still holds a more decisive comparative advantage than its competitors over the majority of the product categories within the knitted or not knitted apparel and clothing accessories. The other competing nations hold better export competitiveness over China in specific categories. However, that is not sufficient to be the “Next China.” Research limitations/implications The study has important implications for different stakeholders of the global apparel industry, such as governments, industry officials, policymakers, investors, researchers and students. The study’s limitations arise from using product categories as competitiveness indicators, notably relying on a macro level approach for measurement while the micro level perspective is not analyzed, which constitutes a significant limitation of the study. Originality/value This research thoroughly analyzes the competitive position of the top ten apparel-exporting countries in the global market.",No methods found.
2024,https://openalex.org/W4391260578,Social Sciences,"The effect of teacher self-efficacy, online pedagogical and content knowledge, and emotion regulation on teacher digital burnout: a mediation model","Abstract Background With the increasing prevalence of online teaching, understanding the dynamics that impact educators' well-being and effectiveness is paramount. This study addresses the interconnected relationships among online teaching competence, self-efficacy, emotion regulation, and digital burnout among teachers in the digital learning environment. Objectives The primary objectives of this research are to investigate the direct and mediated effects of online teaching competence and self-efficacy on emotion regulation and digital burnout among teachers. Additionally, the study aims to explore the mediating role of emotion regulation in the relationship between self-efficacy and digital burnout. The overarching goal is to provide comprehensive insights into the factors influencing teacher well-being in the online teaching context. Methodology A cross-sectional survey design was employed, involving a convenience sample of educators from a specific university. Participants responded to validated self-report measures assessing online teaching competence, self-efficacy, emotion regulation, and digital burnout. Statistical analyses, including regression and mediation analyses, were conducted to examine the relationships among the key variables. Results The findings reveal significant relationships and effects among the investigated variables. Online teaching competence is a substantial predictor of emotion regulation and digital burnout. Similarly, self-efficacy significantly impacts emotion regulation and digital burnout. Emotion regulation mediates the relationship between online teaching competence, self-efficacy, and digital burnout. These results highlight the intricate connections shaping teachers' experiences in the digital teaching environment. Conclusions and implications In conclusion, this study provides robust evidence supporting the interconnectedness of online teaching competence, self-efficacy, emotion regulation, and digital burnout among teachers. The implications underscore the importance of fostering these competencies through targeted professional development. Educational institutions and policymakers can use these insights to implement strategies that enhance teacher well-being, ultimately promoting a more effective and sustainable online teaching environment.",No methods found.
2024,https://openalex.org/W4391558438,Social Sciences,UniLog: Automatic Logging via LLM and In-Context Learning,"Logging, which aims to determine the position of logging statements, the verbosity levels, and the log messages, is a crucial process for software reliability enhancement. In recent years, numerous automatic logging tools have been designed to assist developers in one of the logging tasks (e.g., providing suggestions on whether to log in try-catch blocks). These tools are useful in certain situations yet cannot provide a comprehensive logging solution in general. Moreover, although recent research has started to explore end-to-end logging, it is still largely constrained by the high cost of fine-tuning, hindering its practical usefulness in software development. To address these problems, this paper proposes UniLog, an automatic logging framework based on the in-context learning (ICL) paradigm of large language models (LLMs). Specifically, UniLog can generate an appropriate logging statement with only a prompt containing five demonstration examples without any model tuning. In addition, UniLog can further enhance its logging ability after warmup with only a few hundred random samples. We evaluated UniLog on a large dataset containing 12,012 code snippets extracted from 1,465 GitHub repositories. The results show that UniLog achieved the state-of-the-art performance in automatic logging: (1) 76.9% accuracy in selecting logging positions, (2) 72.3% accuracy in predicting verbosity levels, and (3) 27.1 BLEU-4 score in generating log messages. Meanwhile, UniLog requires less than 4% of the parameter tuning time needed by fine-tuning the same LLM.","<method>in-context learning (ICL) paradigm of large language models (LLMs)</method>, <method>fine-tuning</method>"
2024,https://openalex.org/W4391824636,Social Sciences,"Stakeholders collaborations, challenges and emerging concepts in digital twin ecosystems","Digital twin (DT) ecosystems are rapidly evolving, connecting many stakeholders, such as manufacturers, customers, and application platform providers. These ecosystems require collaboration and interaction between diverse actors to create value. This study delves into the collaboration of such stakeholders within DT-focused ecosystems. This research aims to understand stakeholder collaboration within DT ecosystems, identify potential challenges, and provide insights for managing these stakeholders. It also seeks to define the DT ecosystem and its implications for both research and practice. A systematic literature review was conducted, supplemented by empirical evidence gathered from interviews with DT experts who were knowledgeable about the DT ecosystem. The study also analyzed DT systems, stakeholder roles, and the challenges with ecosystem-focused DT development. The study identified various stakeholders and their roles in adding value to a DT ecosystem. It highlighted the benefits of stakeholder collaboration, such as knowledge gain during DT system development. The research also revealed the technical and non-technical challenges encountered in ecosystem-focused DTs, emphasizing the importance of standardization as a solution. A new definition of the DT ecosystem was proposed, emphasizing its data-driven nature, interconnected DTs, stakeholder value creation, and technology enablement. Stakeholder collaboration is pivotal in DT ecosystems, with each actor playing a distinct role. Addressing challenges, especially through standardization (OPC UA and ISO 23247), can lead to more efficient and coherent DT ecosystems. The insights provided by this study can guide industries in designing, developing, and maintaining their DT ecosystems, ensuring value creation and stakeholder satisfaction. Future research avenues that emphasize the importance of understanding the challenges involved and deploy appropriate solutions were suggested.",No methods found.
2024,https://openalex.org/W4392358105,Social Sciences,"Driving frugal innovation in SMEs: how sustainable leadership, knowledge sources and information credibility make a difference","This study investigates the driving factors behind frugal innovation in Small and Medium-sized Enterprises (SMEs). It specifically examines sustainable leadership as an independent variable, considering its impact on frugal innovation, with sources of knowledge mediating this relationship and information credibility moderating the effects. Employing a Partial Least Squares Structural Equation Modeling (PLS-SEM) approach, data were gathered from 325 employees of SMEs in Pakistan. This methodology was chosen for its ability to handle complex relationships between multiple variables simultaneously, offering robust insights into the interplay among sustainable leadership, sources of knowledge, information credibility, and frugal innovation. The results reveal significant associations between sustainable leadership, sources of knowledge, information credibility, and frugal innovation. Sustainable leadership demonstrates a substantial influence on both sources of knowledge and frugal innovation. Furthermore, sources of knowledge play a vital role in mediating the relationship between sustainable leadership and frugal innovation. Information credibility emerges as a significant moderator, affecting the pathways between sustainable leadership, sources of knowledge, and frugal innovation. The findings underscore the importance of sustainable leadership and credible information sources in driving frugal innovation within SMEs. They highlight the intricate interdependencies among these variables and emphasize the pivotal role of information credibility in shaping these dynamics. These results carry significant implications for SMEs in Pakistan, shedding light on the mechanisms through which sustainable leadership and reliable knowledge sources can stimulate frugal innovation in emerging economies.",No methods found.
2024,https://openalex.org/W4392726867,Social Sciences,Dropout in online higher education: a systematic literature review,"Abstract The increased availability of technology in higher education has led to the growth of online learning platforms. However, a significant concern exists regarding dropout rates in online higher education (OHE). In this ever-evolving landscape, student attrition poses a complex challenge that demands careful investigation. This systematic literature review presents a comprehensive analysis of the literature to uncover the reasons behind dropout rates in virtual learning environments. Following the PRISMA guidelines, this study systematically identifies and elucidates the risk factors associated with dropout in online higher education. The selection process encompassed articles published between 2013 and June 2023, resulting in the inclusion of 110 relevant articles that significantly contribute to the discourse in this field. We examine demographic, course-related, technology-related, motivational, and support-related aspects that shape students’ decisions in online learning programs. The review highlights key contributors to dropout like the quality of the course, academic preparation, student satisfaction, learner motivation, system attributes, and support services. Conversely, health concerns, financial limitations, technological issues, screen fatigue, isolation, and academic workload, emerge as significant limitations reported by online learners. These insights offer a holistic understanding of dropout dynamics, guiding the development of targeted interventions and strategies to enhance the quality and effectiveness of online education.",No methods found.
2024,https://openalex.org/W4393068914,Social Sciences,Understanding the Sustainable Development of Community (Social) Disaster Resilience in Serbia: Demographic and Socio-Economic Impacts,"This paper presents the results of quantitative research examining the impacts of demographic and socioeconomic factors on the sustainable development of community disaster resilience. The survey was carried out utilizing a questionnaire distributed to, and subsequently collected online from, 321 participants during January 2024. The study employed an adapted version of the ‘5S’ social resilience framework (62 indicators), encompassing five sub-dimensions—social structure, social capital, social mechanisms, social equity and diversity, and social belief. To explore the relationship between predictors and the sustainable development of community disaster resilience in Serbia, various statistical methods, such as t-tests, one-way ANOVA, Pearson’s correlation, and multivariate linear regression, were used. The results of the multivariate regressions across various community disaster resilience subscales indicate that age emerged as the most significant predictor for the social structure subscale. At the same time, education stood out as the primary predictor for the social capital subscale. Additionally, employment status proved to be the most influential predictor for both social mechanisms and social equity-diversity subscales, with property ownership being the key predictor for the social beliefs subscale. The findings can be used to create strategies and interventions aimed at enhancing the sustainable development of resilience in communities in Serbia by addressing the intricate interplay between demographic characteristics, socio-economic factors, and their ability to withstand, adapt to, and recover from different disasters.",No methods found.
2024,https://openalex.org/W4396560710,Social Sciences,Experience management in hospitality and tourism: reflections and implications for future research,"Purpose This paper aims to provide a critical reflection on the management of experiences in hospitality and tourism (H&amp;T). The paper investigates the evolution of experience research, while discussing the emerging challenges and opportunities for management. Design/methodology/approach The study adopts a critical and reflective approach for providing future directions of experience research. Three major fields are identified to discuss advances, challenges and opportunities in experience research: conceptualization and dimensions of experiences; relational network for experience management; and theoretical and methodological approaches. Findings The paper proposes a mindset shift to guide experience research, but also to redirect and research thinking and managerial practices about the role of experiences in the economy and society. This proposed humanized perspective to experience research and management is deemed important given the contemporary socio-economic, environmental and technological challenges of the environment. Research limitations/implications This paper identifies a set of theoretical and managerial implications to help scholars and professionals alike to implement the humanized perspective to experience research. Implications relate to conceptualization, relational network and theoretical and methodological approaches in experience research. Originality/value This study critically assesses research challenges and opportunities around customer experience management (CEM) in H&amp;T contexts. This reflective and critical look at customer experiences not only informs future research for advancing knowledge and practice but also proposes a mindset shift about the role and nature of CEM in the society and economy.",No methods found.
2024,https://openalex.org/W4398183551,Social Sciences,A Survey on Malware Detection with Graph Representation Learning,"Malware detection has become a major concern due to the increasing number and complexity of malware. Traditional detection methods based on signatures and heuristics are used for malware detection, but unfortunately, they suffer from poor generalization to unknown attacks and can be easily circumvented using obfuscation techniques. In recent years, Machine Learning (ML) and notably Deep Learning (DL) achieved impressive results in malware detection by learning useful representations from data and have become a solution preferred over traditional methods. Recently, the application of Graph Representation Learning (GRL) techniques on graph-structured data has demonstrated impressive capabilities in malware detection. This success benefits notably from the robust structure of graphs, which are challenging for attackers to alter, and their intrinsic explainability capabilities. In this survey, we provide an in-depth literature review to summarize and unify existing works under the common approaches and architectures. We notably demonstrate that Graph Neural Networks (GNNs) reach competitive results in learning robust embeddings from malware represented as expressive graph structures such as Function Call Graphs (FCGs) and Control Flow Graphs (CFGs). This study also discusses the robustness of GRL-based methods to adversarial attacks, contrasts their effectiveness with other ML/DL approaches, and outlines future research for practical deployment.","<method>Machine Learning (ML)</method>, <method>Deep Learning (DL)</method>, <method>Graph Representation Learning (GRL)</method>, <method>Graph Neural Networks (GNNs)</method>"
2024,https://openalex.org/W4400225515,Social Sciences,Impact of heuristic–systematic cues on the purchase intention of the electronic commerce consumer through the perception of product quality,"For electronic commerce (e-commerce) consumers, it is impossible to evaluate the quality of the products on offer as they are unable to physically test them before purchase. Therefore, sellers must convey quality cues that are readily identifiable to such consumers. However, thanks to major technological advances and the development of social interaction systems on e-commerce platforms, individuals can now access large volumes of information (reviews, opinions, ratings) posted directly by fellow consumers, which can also provide cues by which to judge product quality, pre-purchase. Based on dual-process theory and signaling theory, the objective of this study is to analyze the impact of the heuristic or systematic cues related to electronic word-of-mouth on consumer perceived product quality, to determine how this quality affects perceived product performance risk and consumer purchase intention. A quantitative approach was taken using a structured online questionnaire. Data were collected from 835 consumers of e-commerce platforms and analyzed using maximum likelihood structural equation modeling and LISREL software. The results show that the quantity of reviews, source credibility, review usefulness, and brand experience all exert a positive and significant effect on perceived product quality, which, in turn, positively and significantly influences purchase intention. A negative and significant relationship between product performance risk and purchase intention is also found. The findings contribute to the literature by improving our understanding of those determinants of perceived product quality in e-commerce that motivate consumers to make a purchase and can help sellers improve their use of integrated social interaction tools to adequately reflect the quality of their products.",No methods found.
2024,https://openalex.org/W4390588030,Social Sciences,Enterprise social media as enablers of employees' agility: the impact of work stress and enterprise social media visibility,"Purpose According to extensive analysis, employee agility is influenced by teamwork, coordination and the organizational environment. However, less consideration has been given to the role of work stressors (challenge, hindrance) in influencing employee agility. To address this research gap, this study sheds light on how the use of enterprise social media (ESM) for social and work purposes influences employee agility through work stressors. Design/methodology/approach This research also explores how ESM visibility enhances the interaction between work stressors and employee agility by using primary data obtained from Chinese workers. A total of 377 entries were analyzed using AMOS 24.10 tools. All the hypotheses were tested using structural equation modeling (SEM). Findings The findings revealed that ESM use (social and work) negatively impacts challenge and hindrance work stressors. The results also reflect that challenge stressors have a significant impact on employee agility, whereas hindrance stressors are negatively related to it. Furthermore, the outcome also indicated that increased ESM visibility reinforces the connection between challenge stressors and employee agility. However, ESM visibility did not indicate a significant moderating impact on the link between hindrance stressors and employee agility. Originality/value This study describes how ESM usage effects agility of stressed employees. This research also explores how ESM visibility improves the interaction between work stressors and employee agility. The study results contribute to growing research on social media and employee agility and suggest several points of guidance for managers.",No methods found.
2024,https://openalex.org/W4390662926,Social Sciences,Artificial intelligence performance in detecting lymphoma from medical imaging: a systematic review and meta-analysis,"Abstract Background Accurate diagnosis and early treatment are essential in the fight against lymphatic cancer. The application of artificial intelligence (AI) in the field of medical imaging shows great potential, but the diagnostic accuracy of lymphoma is unclear. This study was done to systematically review and meta-analyse researches concerning the diagnostic performance of AI in detecting lymphoma using medical imaging for the first time. Methods Searches were conducted in Medline, Embase, IEEE and Cochrane up to December 2023. Data extraction and assessment of the included study quality were independently conducted by two investigators. Studies that reported the diagnostic performance of an AI model/s for the early detection of lymphoma using medical imaging were included in the systemic review. We extracted the binary diagnostic accuracy data to obtain the outcomes of interest: sensitivity (SE), specificity (SP), and Area Under the Curve (AUC). The study was registered with the PROSPERO, CRD42022383386. Results Thirty studies were included in the systematic review, sixteen of which were meta-analyzed with a pooled sensitivity of 87% (95%CI 83–91%), specificity of 94% (92–96%), and AUC of 97% (95–98%). Satisfactory diagnostic performance was observed in subgroup analyses based on algorithms types (machine learning versus deep learning, and whether transfer learning was applied), sample size (≤ 200 or &gt; 200), clinicians versus AI models and geographical distribution of institutions (Asia versus non-Asia). Conclusions Even if possible overestimation and further studies with a better standards for application of AI algorithms in lymphoma detection are needed, we suggest the AI may be useful in lymphoma diagnosis.","<method>machine learning</method>, <method>deep learning</method>, <method>transfer learning</method>"
2024,https://openalex.org/W4390796562,Social Sciences,"If we build it together, will they use it? A mixed-methods study evaluating the implementation of Prep-to-Play PRO: an injury prevention programme for women’s elite Australian Football","Objectives We evaluated the implementation of Prep-to-Play PRO, an injury prevention programme for women’s elite Australian Football League (AFLW). Methods The Reach, Effectiveness, Adoption, Implementation and Maintenance (RE-AIM) of Prep-to-Play PRO were assessed based on the proportion of AFLW players and/or staff who: were aware of the programme (R), believed it may reduce anterior cruciate ligament injury (E), attempted to implement any/all programme components (A), implemented all intended components as practically as possible (I) and intended future programme implementation (M). Quantitative and qualitative data were triangulated to assess 58 RE-AIM items (evidence of yes/no/unsure/no evidence) and the 5 RE-AIM dimensions (fully achieved=evidence of yes on &gt;50% dimension items, partially achieved=50% of items evidence of yes and 50% unsure or 50% mix of unsure and unanswered, or not met=evidence of yes on &lt;50% dimension items). Results Multiple sources including AFLW training observations (n=7 total), post-implementation surveys (141 players, 25 staff), semistructured interviews (19 players, 13 staff) and internal programme records (9 staff) contributed to the RE-AIM assessment. After the 2019 season, 8 of 10 (80%) AFLW clubs fully met all five RE-AIM dimensions. All 10 clubs participating in the AFLW fully achieved the reach (R) dimension. One club partially achieved the implementation (I) dimension, and one club partially achieved the effectiveness (E) and adoption (A) dimensions. Conclusion The Prep-to-Play PRO injury prevention programme for the AFLW achieved high implementation, possibly due to the programme’s deliberately flexible approach coupled with our pragmatic definition of implementation. Engaging key stakeholders at multiple ecological levels (organisation, coaches, athletes) throughout programme development and implementation likely enhanced programme implementation.",No methods found.
2024,https://openalex.org/W4390939347,Social Sciences,Adaptive Attention-Based Graph Representation Learning to Detect Phishing Accounts on the Ethereum Blockchain,"With Ethereum blockchain advancement, the Ethereum platform gathers numerous users. In this context, traditional phishing appears new fraud methods, resulting in significant losses. Currently, network embedding methods are considered effective solutions in the field of phishing detection. However, investigating existing Ethereum phishing node detection algorithms finds they are not optimal and still face two issues. Firstly, the Ethereum network's topology is unsatisfactory, with nodes exhibiting a long-tail distribution in their degree. Current technologies typically allow high-degree nodes to acquire high-quality embeddings, while low-degree nodes, constrained by limited structure, obtain embeddings of lower quality, significantly impacting the detection accuracy of downstream tasks. Secondly, different features of nodes will suffer losses during the fusion process, resulting in the final learned feature embedding being suboptimal. This paper presents an attention-based graphical representation learning approach (ABGRL) to address these problems. ABGRL extracts different feature information by means of multiple channels, and fuses the different feature information using adaptive attention convolution to select the feature information that has the greatest impact on the downstream task. Then the tail node feature information is enhanced by a self-supervised regression model with robust tail node embedding. Finally, the effectiveness of the proposed model was validated through extensive experiments.","<method>network embedding methods</method>, <method>attention-based graphical representation learning approach (ABGRL)</method>, <method>adaptive attention convolution</method>, <method>self-supervised regression model</method>"
2024,https://openalex.org/W4391065516,Social Sciences,Prefeasibility techno-economic analysis of hybrid renewable energy system,"The feasibility of a hybrid renewable energy system for long-term rural electrification in Billerahalli village, Karnataka, India, is investigated in this paper. This paper presents a systematic methodology for planning and designing of an efficient hybrid system that incorporates techno-economic optimization analysis. Using HOMER PRO software, several hybridization situations are modelled and analyzed. In this design, electrolyzer and a hydrogen tank are included to reduce the need for batteries. The results shown that the optimal configuration system is a wind turbine, electrolyzer, hydrogen tank, converter, and LL1500-W battery (4 units of 800kW/ 100kW/ 100kg/ 734kW/ 500 units), with a low net present cost, cost of energy, a significant return on investment, and reductions in carbon emissions. This system offers a steady power supply by utilizing 100% renewable sources, with a net present cost of $5.21 million and a relatively low energy cost of $0.25 per kilowatt-hour. The optimized hybrid system outperforms other system configurations in terms of cost-effectiveness and environmental performance. Concurrently, the sensitivity analysis emphasizes the system costs majorly depends on the unpredictability of solar radiation and wind speed, as well as changes in interest rates impacting future investment decisions. Finally, the study expects to provide the awareness of renewable energy potential usage in Billerahalli village and significant information for the efficient design and exploitation of hybrid renewable energy systems within the larger energy sector.",No methods found.
2024,https://openalex.org/W4391127198,Social Sciences,Risk predictions of surgical wound complications based on a machine learning algorithm: A systematic review,"Abstract Surgical wounds may arise due to harm inflicted upon soft tissue during surgical intervention, and many complications and injuries may accompany them. These complications can lead to prolonged hospitalization and poorer clinical outcomes. Also, Machine learning (ML) is a Section of artificial intelligence (AI) that has emerged in medical care and is increasingly used for diagnosis, complications, prognosis and recurrence prediction. This study aims to investigate surgical wound risk predictions and management using a ML algorithm by R programming language analysis. The systematic review, following PRISMA guidelines, spanned electronic databases using search terms like ‘machine learning’, ‘surgical’ and ‘wound’. Inclusion criteria covered experimental studies from 1990 to the present on ML's application in surgical wound evaluation. Exclusion criteria included studies lacking full text, focusing on ML in all surgeries, neglecting wound assessment and duplications. Two authors rigorously assessed titles, abstracts and full texts, excluding reviews and guidelines. Ultimately, relevant articles were then analysed. The present study identified nine articles employing ML for surgical wound management. The analysis encompassed various surgical procedures, including Cardiothoracic, Caesarean total abdominal colectomy, Burn plastic surgery, facial plastic surgery, laparotomy, minimal invasive surgery, hernia repair and unspecified surgeries. ML was skillful in evaluating surgical site infections (SSI) in seven studies, while two extended its use to burn‐grade diagnosis and wound classification. Support Vector Machine (SVM) and Convolutional Neural Network (CNN) were the most utilized algorithms. ANN achieved a 96% accuracy in facial plastic surgery wound management. CNN demonstrated commendable accuracies in various surgeries, and SVM exhibited high accuracy in multiple surgeries and burn plastic surgery. In sum, these findings underscore ML's potential for significant improvements in postoperative management and the development of enhanced care techniques, particularly in surgical wound management.","<method>Support Vector Machine (SVM)</method>, <method>Convolutional Neural Network (CNN)</method>, <method>Artificial Neural Network (ANN)</method>"
2024,https://openalex.org/W4391749761,Social Sciences,"Green process innovation, green product innovation, leverage, and corporate financial performance; evidence from system GMM","Natural resource usage has produced various environmental challenges. Green process innovation has been considered a viable option that can help both industry and society. This study investigates the impact of green process innovation and green product innovation on corporate financial performance. We based our findings on a sample of 280 listed non-financial firms operating in South Asia. Information was gathered from firms' annual and CSR reports from 2012 to 2022. This study's data was analyzed using a two-step dynamic panel system GMM, correlation analysis, multicollinearity diagnostic tests, and descriptive statistics. Corporate financial performance is measured with ROA, ROE and Tobin's Q. Overall findings of the study show that green innovation has a significant positive impact on all measures of financial performance. Investing in the innovation of green products and green process can assist businesses in avoiding environmental concerns and regulatory penalties, while also assisting them in establishing new market prospects and achieving new levels of success with their green products. In addition, developing products that are friendly to the environment is tightly connected to expanding green competencies, promoting a company's green image, and improving the company's financial performance. Particularly useful for policymakers in developing countries, the study's findings can be used to introduce paradigm-shifting legislation and penalties that speed up business adoption of green process innovation.",No methods found.
2024,https://openalex.org/W4392449443,Social Sciences,RGBT Tracking via Challenge-Based Appearance Disentanglement and Interaction,"RGB and thermal source data suffer from both shared and specific challenges, and how to explore and exploit them plays a critical role in representing the target appearance in RGBT tracking. In this paper, we propose a novel approach, which performs target appearance representation disentanglement and interaction via both modality-shared and modality-specific challenge attributes, for robust RGBT tracking. In particular, we disentangle the target appearance representations via five challenge-based branches with different structures according to their properties, including three parameter-shared branches to model modality-shared challenges and two parameter-independent branches to model modality-specific challenges. Considering the complementary advantages between modality-specific cues, we propose a guidance interaction module to transfer discriminative features from one modality to another one to enhance the discriminative ability of weak modality. Moreover, we design an aggregation interaction module to combine all challenge-based target representations, which could form more discriminative target representations and fit the challenge-agnostic tracking process. These challenge-based branches are able to model the target appearance under certain challenges so that the target representations can be learned by a few parameters even in the situation of insufficient training data. In addition, to relieve labor costs and avoid label ambiguity, we design a generation strategy to generate training data with different challenge attributes. Comprehensive experiments demonstrate the superiority of the proposed tracker against the state-of-the-art methods on four benchmark datasets.","<method>target appearance representation disentanglement</method>, <method>guidance interaction module</method>, <method>aggregation interaction module</method>, <method>generation strategy to generate training data with different challenge attributes</method>"
2024,https://openalex.org/W4392858515,Social Sciences,Key factors influencing the e-government adoption: a systematic literature review,"Purpose This study aimed to identify and analyse the key factors influencing the adoption of e-government services and to discern their implications for various stakeholders, from policymakers to platform developers. Design/methodology/approach Through a comprehensive review of existing literature and detailed analysis of multiple studies, this research organised the influential factors based on their effect: highest, direct and indirect. The study also integrated findings to present a consolidated view of e-government adoption drivers. Findings The research found that users' behaviour, attitude, optimism bias and subjective norms significantly shape their approach to e-government platforms. Trust in e-Government (TEG) emerged as a critical determinant, with security perceptions being of paramount importance. Additionally, non-technical factors, such as cultural, religious and social influences, play a substantial role in e-government adoption decisions. The study also highlighted the importance of performance expectancy, effect expectancy and other determinants influencing e-government adoption. Originality/value While numerous studies have explored e-government adoption, this research offers a novel classification based on the relative effects of each determinant. Integrating findings from diverse studies and emphasising non-technical factors introduce an interdisciplinary approach, bridging the gap between information technology and fields like sociology, anthropology and behavioural sciences. This integrative lens provides a fresh perspective on the topic, encouraging more holistic strategies for enhancing e-government adoption globally.",No methods found.
2024,https://openalex.org/W4394948580,Social Sciences,Inductive Cognitive Diagnosis for Fast Student Learning in Web-Based Intelligent Education Systems,"Cognitive diagnosis aims to gauge students' mastery levels based on their response logs.Serving as a pivotal module in web-based online intelligent education systems (WOIESs), it plays an upstream and fundamental role in downstream tasks like learning item recommendation and computerized adaptive testing.WOIESs are open learning environment where numerous new students constantly register and complete exercises.In WOIESs, efficient cognitive diagnosis is crucial to fast feedback and accelerating student learning.However, the existing cognitive diagnosis methods always employ intrinsically transductive student-specific embeddings, which become slow and costly due to retraining when dealing with new students who are unseen during training.To this end, this paper proposes an inductive cognitive diagnosis model (ICDM) for fast new students' mastery levels inference in WOIESs.Specifically, in ICDM, we propose a novel student-centered graph (SCG).Rather than inferring mastery levels through updating student-specific embedding, we derive the inductive mastery levels as the aggregated outcomes of students' neighbors in SCG.Namely, SCG enables to shift the task from finding the most suitable student-specific embedding that fits the response logs to finding the most suitable representations for different node types in SCG, and the latter is more efficient since it no longer requires retraining.To obtain this representation, ICDM consists of a construction-aggregation-generationtransformation process to learn the final representation of students, exercises and concepts.Extensive experiments across real-world datasets show that, compared with the existing cognitive diagnosis methods that are always transductive, ICDM is much more faster while maintains the competitive inference performance for new students.","<method>inductive cognitive diagnosis model (ICDM)</method>, <method>student-centered graph (SCG)</method>"
2024,https://openalex.org/W4400617823,Social Sciences,Wind farm site selection using geographic information system and fuzzy decision making model,"As the demand for renewable energy sources increases, finding the right places to install wind turbines becomes more and more important. The goal of this research is to create and implement a technique that uses geographic information system (GIS) technology to discover appropriate wind farm locations utilizing multi-criteria decision-making (MCDM) approaches. The complexity of this decision-making process, which includes multiple criteria and uncertainty, requires the use of advanced techniques. Fuzzy MCDM methods provide a framework for evaluating and prioritizing potential wind farm sites, taking into account subjective judgments and linguistic terms. In this article, Fuzzy Stepwise Weight Evaluation Ratio Analysis (F-SWARA) is preferred for prioritizing and ranking the criteria in the wind farm installation, while Fuzzy Measurement Alternatives and Ranking by Compromise Solution (F-MARCOS) are used to determine the most suitable location for the wind farm. A database of alternatives and criteria was created using GIS, which was converted into a fuzzy decision matrix via triangular fuzzy numbers. In order to make this evaluation, Sivas province, located in the middle of Turkey, was chosen as the study area. Results obtained show that 36,5% of the whole study area is very suitable for wind farm, and Gürün and Kangal districts are suitable for wind farm. According to the result of F-SWARA method used to evaluate the criteria, wind speed is the most important criteria with a weight of 0,45039. According to the F-MARCOS method used for wind farm site selection, Ulaş district was determined the most suitable location. Furthermore, a sensitivity analysis was performed to test the robustness of the proposed methodology and the results revealed that the proposed integrated MCDM framework is feasible.","<method>Fuzzy Stepwise Weight Evaluation Ratio Analysis (F-SWARA)</method>, <method>Fuzzy Measurement Alternatives and Ranking by Compromise Solution (F-MARCOS)</method>"
2024,https://openalex.org/W4401153291,Social Sciences,Evaluating user engagement via Metaverse environment through immersive experience for travel and tourism websites,"Purpose The purpose of this paper is to explore user engagement (UE) within the Metaverse (MV) environment, emphasising the crucial role of immersive experiences (IEs). This study aims to understand how IEs influence UE and the mediating effects of hedonic value (HV) and utilitarian value (UV) on this relationship. Additionally, the authors examine the moderating impacts of user perceptions (UPs) such as headset comfort, simulation sickness, prior knowledge and ease of use on the utilisation of the MV. This study seeks to elucidate the dynamics of virtual travel at a pre-experience stage, enhancing the comprehension of how digital platforms can revolutionise UE in travel and tourism. Design/methodology/approach This study used a triangulation methodology to provide a thorough investigation into the factors influencing UE in the MV. A systematic literature review (SLR) was conducted to frame the research context and identify relevant variables. To gather empirical data, 25 interviews were performed with active MV users, supplemented by a survey distributed to 118 participants. The data collected was analysed using structural equation modelling (SEM) to test the hypothesised relationships between IEs, UPs, HV and UV and their combined effect on UE within the MV. Findings The findings from the SEM indicate that engaging in the MV leads to a positive IE, which significantly enhances UE. Additionally, it was discovered that HV and UV play a mediating role in strengthening the link between IEs and UE. Furthermore, UPs, including headset comfort, simulation sickness, prior knowledge and ease of use, are significant moderators in the relationship between IEs and MV usage. These insights provide a nuanced understanding of the variables that contribute to and enhance UE in virtual environments. Originality/value This research contributes original insights into the burgeoning field of digital tourism by focusing on the MV, a rapidly evolving platform. It addresses the gap in the existing literature by delineating the complex interplay between IEs, UPs and value constructs within the MV. By using a mixed-method approach and advanced statistical analysis, this study provides a comprehensive model of UE specific to virtual travel platforms. The findings are particularly valuable for developers and marketers in the hospitality and tourism sectors seeking to capitalise on digital transformation and enhance UE through immersive technologies.",No methods found.
2024,https://openalex.org/W4401487891,Social Sciences,CARLA: Self-supervised contrastive representation learning for time series anomaly detection,"One main challenge in time series anomaly detection (TSAD) is the lack of labelled data in many real-life scenarios. Most of the existing anomaly detection methods focus on learning the normal behaviour of unlabelled time series in an unsupervised manner. The normal boundary is often defined tightly, resulting in slight deviations being classified as anomalies, consequently leading to a high false positive rate and a limited ability to generalise normal patterns. To address this, we introduce a novel end-to-end self-supervised ContrAstive Representation Learning approach for time series Anomaly detection (CARLA). While existing contrastive learning methods assume that augmented time series windows are positive samples and temporally distant windows are negative samples, we argue that these assumptions are limited as augmentation of time series can transform them to negative samples, and a temporally distant window can represent a positive sample. Existing approaches to contrastive learning for time series have directly copied methods developed for image analysis. We argue that these methods do not transfer well. Instead, our contrastive approach leverages existing generic knowledge about time series anomalies and injects various types of anomalies as negative samples. Therefore, CARLA not only learns normal behaviour but also learns deviations indicating anomalies. It creates similar representations for temporally close windows and distinct ones for anomalies. Additionally, it leverages the information about representations' neighbours through a self-supervised approach to classify windows based on their nearest/furthest neighbours to further enhance the performance of anomaly detection. In extensive tests on seven major real-world TSAD datasets, CARLA shows superior performance (F1 and AU-PR) over state-of-the-art self-supervised, semi-supervised, and unsupervised TSAD methods for univariate time series and multivariate time series. Our research highlights the immense potential of contrastive representation learning in advancing the TSAD field, thus paving the way for novel applications and in-depth exploration.","<method>self-supervised ContrAstive Representation Learning</method>, <method>contrastive learning</method>, <method>self-supervised approach</method>, <method>semi-supervised methods</method>, <method>unsupervised methods</method>"
2024,https://openalex.org/W4390947155,Social Sciences,Identifying entrepreneurial opportunities during crises: a qualitative study of Italian firms,"Purpose Recognizing novel entrepreneurial opportunities arising from a crisis is of paramount importance for firms. Hence, understanding the pivotal factors that facilitate firms in this endeavor holds significant value. This study delves into such factors within a representative empirical context impacted by a crisis, drawing insights from existing literature on opportunity recognition during such tumultuous periods. Design/methodology/approach The authors conducted a qualitative inspection of 14 Italian firms during the COVID-19 pandemic crisis. The authors collected a rich body of multi-source qualitative data, including 34 interviews (with senior managers and entrepreneurs) and secondary data (press releases, videos, web interviews, newspapers, reports and academic articles) in two phases (March–August 2020 and September–December 2020). Findings The results suggest the existence of a process model of opportunity recognition during crises based on five entrepreneurial influencing factors (entrepreneurial knowledge, entrepreneurial alertness, entrepreneurial proclivity, entrepreneurial personality and entrepreneurial purpose). Originality/value Various scholars have highlighted that, in times of crises, it is not easy and indeed very challenging for entrepreneurs to identify novel entrepreneurial opportunities. However, recent research has shown that crises can also positively impact entrepreneurs and their capacity to identify new entrepreneurial opportunities. Given these findings, not much research has analyzed the process by which entrepreneurs identify novel entrepreneurial opportunities during crises. This study shows that some entrepreneurial influencing factors are very important to identify new entrepreneurial opportunities during crises.",No methods found.
2024,https://openalex.org/W4391026819,Social Sciences,Predicting sustainable fashion consumption intentions and practices,"Abstract The fashion industry has a significant impact on the environment, and sustainable fashion consumption (SFC) has become a pressing concern. This study aimed to investigate the factors influencing sustainable fashion consumption behavior (SCB) among Chinese adults, specifically the role of values, attitudes, and norms in shaping such behavior, using the value-belief-norm framework. The study used an online cross-sectional survey design to collect data from 350 participants recruited through a convenience sampling method using social media platforms and email invitations, and the obtained data were analyzed using partial least squares structural equation modelling. The results of the study showed that biospheric (BV), altruistic (AV), and egoistic (EV) values significantly influenced the New ecological paradigm (EP), which, in turn, positively affected awareness of consequences (AC). Personal norms (PN) were positively influenced by EP, AC, and ascription of responsibility (AR). Social norms (SN) and trust in recycling (TR) were also found to positively influence sustainable fashion consumption intentions (SCI). Finally, the study found that SCI and TR were significant predictors of SCB, whereas the moderating effect of TR not statistically significant. The study’s originality lies in its comprehensive investigation of the interplay between various factors (particularly using norms in two facets; PN and SN) in shaping SCB, using a structural equation modeling approach, and exploring the moderating effect of TR. The findings of this study suggest that interventions aimed at promoting SFC should focus on fostering values and beliefs that prioritize the environment, encouraging individuals to take responsibility for their actions, creating an environment in which SFC is normalized, and increasing TR.",No methods found.
2024,https://openalex.org/W4391097175,Social Sciences,Toward Improving Breast Cancer Classification Using an Adaptive Voting Ensemble Learning Algorithm,"Over the past decade, breast cancer has been the most common type of cancer in women. Different methods were proposed for breast cancer detection. These methods mainly classify and categorize malignant and Benign tumors. Machine learning is a practical approach for breast cancer classification. Data mining and classification are effective methods to predict and categorize breast cancer. The optimum classification for detecting Breast Cancer (BC) is ensemble-based. The ensemble approach involves using multiple ways to find the best possible solution. This study used the Wisconsin Breast Cancer Diagnostic (WBCD) dataset. We created a voting ensemble classifier that combines four different machine learning models: Extra Trees Classifier (ETC), Light Gradient Boosting Machine (LightGBM), Ridge Classifier (RC), and Linear Discriminant Analysis (LDA). The proposed ELRL-E approach achieved an accuracy of 97.6%, a precision of 96.4%, a recall of 100%, and an F1 score of 98.1%. Various output evaluations are used to evaluate the performance and efficiency of the proposed model and other classifiers. Overall, the recommended strategy performed better. Results are directly compared with the individual classifier and different recognized state-of-the-art classifiers. The primary objective of this study is to identify the most influential ensemble machine learning classifier for breast cancer detection and diagnosis in terms of accuracy and AUC score.","<method>Machine learning</method>, <method>Data mining</method>, <method>classification</method>, <method>ensemble-based classification</method>, <method>voting ensemble classifier</method>, <method>Extra Trees Classifier (ETC)</method>, <method>Light Gradient Boosting Machine (LightGBM)</method>, <method>Ridge Classifier (RC)</method>, <method>Linear Discriminant Analysis (LDA)</method>"
2024,https://openalex.org/W4391604911,Social Sciences,Halal cosmetics: a technology-empowered systematic literature review,"Purpose Globally, the halal cosmetics market is experiencing rapid growth and is considered a key economic driver in shaping economy development and growth. However, the extant research on halal cosmetics is fragmented, potentially impeding the field’s advancement when challenged with conflicting viewpoints and limited replications. Therefore, this paper aims to address the knowledge gap by conducting a rigorous and technology-enabled systematic review by leveraging appropriate software to comprehensively evaluate the state of the halal cosmetics literature. Design/methodology/approach A domain-based review using a hybrid approach that incorporates both bibliometric and interpretive analyses are used to comprehensively assess the current progress of halal cosmetics, identify research gaps and suggest potential directions for future research. Findings Through a comprehensive review of 66 articles, this review provides a holistic and comprehensive overview of halal cosmetics that both academic scholars and market practitioners can rely upon in strategizing and positioning for future development of halal cosmetics. The study provides a holistic and comprehensive overview of halal cosmetics that both academic scholars and market practitioners can reply upon in strategizing and positioning for future development of halal cosmetics. Originality/value The fragmented knowledge of extant research on halal cosmetics across various disciplines limits a comprehensive understanding of the field. It is opportune to conduct a comprehensive and systematic review of the field, providing insight into both its current and future progress. In this regard, this review serves as a “one-stop reference” in providing a state-of-the-art understanding of the field, and enables industry practitioners to reveal the full potential and bridge the theory-practice gap in the halal cosmetics industry.",No methods found.
2024,https://openalex.org/W4391643149,Social Sciences,Development and psychometric validation of a novel scale for measuring ‘psychedelic preparedness’,"Abstract Preparing participants for psychedelic experiences is crucial for ensuring these experiences are safe and, potentially beneficial. However, there is currently no validated measure to assess the extent to which participants are well-prepared for such experiences. Our study aimed to address this gap by developing, validating, and testing the Psychedelic Preparedness Scale (PPS). Using a novel iterative Delphi-focus group methodology (‘DelFo’), followed by qualitative pre-test interviews, we incorporated the perspectives of expert clinicians/researchers and of psychedelic users to generate items for the scale. Psychometric validation of the PPS was carried out in two large online samples of psychedelic users (N = 516; N = 716), and the scale was also administered to a group of participants before and after a 5–7-day psilocybin retreat (N = 46). Exploratory and confirmatory factor analysis identified four factors from the 20-item PPS: Knowledge-Expectations, Intention-Preparation, Psychophysical-Readiness, and Support-Planning. The PPS demonstrated excellent reliability (ω = 0.954) and evidence supporting convergent, divergent and discriminant validity was also obtained. Significant differences between those scoring high and low (on psychedelic preparedness) before the psychedelic experience were found on measures of mental health/wellbeing outcomes assessed after the experience, suggesting that the scale has predictive utility. By prospectively measuring modifiable pre-treatment preparatory behaviours and attitudes using the PPS, it may be possible to determine whether a participant has generated the appropriate mental ‘set’ and is therefore likely to benefit from a psychedelic experience, or at least, less likely to be harmed.",No methods found.
2024,https://openalex.org/W4391992420,Social Sciences,Shifts at the margin of European welfare states: How important is food aid in complementing inadequate minimum incomes?,"In recent decades, disappointing poverty trends and welfare state limitations in many European countries – including constraints on minimum income benefits – have paved the way for a larger role of the third sector. An interesting but controversial form of third-sector in-kind support is food aid provision. In Europe, food aid is, so far, a non-rights-based practice displaying worrisome discretionary and stigmatizing characteristics. Yet, the phenomenon of food aid in Europe has spread, professionalized, and penetrated the institutions of the welfare state. This raises the question if, how and to what extent food aid plays a role in bypassing structural constraints on minimum income protection. This article applies an exploratory case study approach to estimate the monetary value of food aid in relation to statutory minimum incomes in four EU-countries. We use cross-nationally comparable food reference budgets to price food aid packages in Belgium, Finland, Hungary and Spain. The results show that food aid, although not sufficient to close the at-risk-of-poverty gap, is non-trivial for some European households. In Spain and Belgium food aid packages can reach up to €100 a month (expressing 7% to 11% of respective minimum income benefit levels). Importantly, we perceive (formalized) cooperation and interaction between local welfare agencies and food charities in all countries, suggesting that welfare state actors use non-rights-based food aid for filling gaps in the social safety net. The large between- and within-country variation of the monetary values of food aid packages points, however, to food aid as a problematic discretionary practice.",No methods found.
2024,https://openalex.org/W4392127827,Social Sciences,The role of global health partnerships in vaccine equity: A scoping review,"The emergence of global health partnerships (GHPs) towards the end of the twentieth century reflected concerns about slow progress in access to essential medicines, including vaccines. These partnerships bring together governments, private philanthropic foundations, NGOs, and international agencies. Those in the vaccine field seek to incentivise the development and manufacture of new vaccines, raise funds to pay for them and develop and support systems to deliver them to those in need. These activities became more critical during the COVID-19 pandemic, with the COVAX Facility Initiative promoting global vaccine equity. This review identifies lessons from previous experiences with GHPs. Findings contribute to understanding the emergence of GHPs, the mechanisms they leverage to support global access to vaccines, and the inherent challenges associated with their implementation. Using Arksey and O’Malley’s method, we conducted a scoping review to identify and synthesise relevant articles. We analysed data thematically to identify barriers and opportunities for success. We included 68 eligible articles of 3,215 screened. Most (65 [95%]) were discussion or review articles describing partnerships or programmes they supported, and three (5%) were commentaries. Emerging themes included policy responses (e.g., immunisation mandates), different forms of partnerships arising in vaccine innovation (e.g., product development partnerships, public-private partnerships for access), and influence on global governance decision-making processes (e.g., the rising influence of foundations, diminishing authority of WHO, lack of accountability and transparency, creation of disease silos). If global health partnerships are to maximise their contributions, they should: (1) increase transparency, especially regarding their impacts; (2) address the need for health systems strengthening; and (3) address disincentives for cooperative vaccine research and development partnerships and encourage expansion of manufacturing capacity in low and middle-income countries.",No methods found.
2024,https://openalex.org/W4392195616,Social Sciences,Adopting the metaverse in the luxury hotel business: a cost–benefit perspective,"Purpose This study aims to examine the perceived values of the metaverse when adopting it in the luxury hospitality business. Based on the cost–benefit perspective, this research provides solid theoretical contributions and actionable managerial recommendations. Design/methodology/approach An exploratory sequential mixed-method design was used. For the qualitative phase, 21 hotel managers and 24 hotel guests (who often stay in four-star and five-star hotels and resorts) were interviewed after showing them a series of videos about using the metaverse in the hotel business. Based on the results of the qualitative phase, the analytic hierarchy process method was used, and 476 valid questionnaires were analyzed. Findings The results highlight the perceived benefits (personalized services, immersive experience and positive brand image) and costs (lack of human touch, time and effort and security and privacy) of metaverse adoption for hotel managers and their guests. In addition, the study determines the weight of each value attribute of metaverse adoption for each travel stage (pre-travel, during travel and post-travel). Practical implications Regarding metaverse adoption, the research offers practical suggestions for luxury hotels. For instance, the cost of equipment and the time and effort required are perceived costs of metaverse adoption. To address these challenges, hotels may offer free equipment (e.g. VR headsets) and training to their guests to stimulate the use of the metaverse. Originality/value This study addresses a gap in the literature by presenting a conceptual framework for examining metaverse adoption in the luxury hotel scenario. Unlike using conventional models like the technology acceptance model or the unified theory of acceptance and use of technology to investigate a technology’s adoption, this study stands out by unraveling the topic through the lens of value proposition. The latter often comes from an efficient value co-creation process, which is indeed shaped by an adequate appreciation of the congruence of perceived values (i.e. perceived benefits and costs) of metaverse from hotel manager and guest perspectives.",No methods found.
2024,https://openalex.org/W4392514027,Social Sciences,Introducing machine‐learning‐based data fusion methods for analyzing multimodal data: An application of measuring trustworthiness of microenterprises,"Abstract Research Summary Multimodal data, comprising interdependent unstructured text, image, and audio data that collectively characterize the same source, with video being a prominent example, offer a wealth of information for strategy researchers. We emphasize the theoretical importance of capturing the interdependencies between different modalities when evaluating multimodal data. To automate the analysis of video data, we introduce advanced deep machine learning and data fusion methods that comprehensively account for all intra‐ and inter‐modality interdependencies. Through an empirical demonstration focused on measuring the trustworthiness of grassroots sellers in live streaming commerce on Tik Tok, we highlight the crucial role of interpersonal interactions in the business success of microenterprises. We provide access to our data and algorithms to facilitate data fusion in strategy research that relies on multimodal data. Managerial Summary Our study highlights the vital role of both verbal and nonverbal communication in attaining strategic objectives. Through the analysis of multimodal data—incorporating text, images, and audio—we demonstrate the essential nature of interpersonal interactions in bolstering trustworthiness, thus facilitating the success of microenterprises. Leveraging advanced machine learning techniques, such as data fusion for multimodal data and explainable artificial intelligence, we notably enhance predictive accuracy and theoretical interpretability in assessing trustworthiness. By bridging strategic research with cutting‐edge computational techniques, we provide practitioners with actionable strategies for enhancing communication effectiveness and fostering trust‐based relationships. Access our data and code for further exploration.","<method>deep machine learning</method>, <method>data fusion</method>, <method>advanced machine learning techniques</method>, <method>explainable artificial intelligence</method>"
2024,https://openalex.org/W4392672219,Social Sciences,"Cultivating the digital citizen: trust, digital literacy and e-government adoption","Purpose This study aims to examine the role of trust and digital literacy in influencing citizens’ adoption of e-government services. Design/methodology/approach Grounded in the technology acceptance model (TAM), a research model was developed focusing on e-filing services adoption. Hypotheses were formulated to assess the moderating effect of digital literacy on the relationship between trust and the key TAM determinants of perceived usefulness and perceived ease of use. A questionnaire-based survey of 876 citizens who have used e-filing using the snow-ball sampling technique was adopted to generate data. The data was analyzed using PLS-SEM through the aid of SmartPLS 4 to assess the measurement model and structural relationships. Findings Trust positively influences perceived usefulness and ease of use, which in turn drive adoption. Additionally, digital literacy significantly moderates the impact of trust on usefulness and ease of use perceptions – the effect is stronger for higher digital literacy. Research limitations/implications The study adopted a single country developing economy context limiting cross-cultural applicability. Second, the focus on e-filing adoption precludes insights across other e-government services. Third, the reliance on perceptual measures risks respondent biases and fourth, the study is a cross-sectional survey design. Practical implications The findings emphasize multifaceted strategies to accelerate e-government adoption. Nurturing citizen trust in e-government systems through enhanced reliability, security and transparency remains vital. Simultaneously, initiatives to cultivate digital access, skills and proficiencies across population segments need to be undertaken. Originality/value This study integrates trust and digital literacy within the theoretical model to provide a more holistic understanding of adoption determinants. It highlights the need for balanced technology-enabled and social interventions to foster acceptance of e-government services.",No methods found.
2024,https://openalex.org/W4396767636,Social Sciences,From explainable to interpretable deep learning for natural language processing in healthcare: How far from reality?,"Deep learning (DL) has substantially enhanced natural language processing (NLP) in healthcare research. However, the increasing complexity of DL-based NLP necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review of explainable and interpretable DL in healthcare NLP. The term ""eXplainable and Interpretable Artificial Intelligence"" (XIAI) is introduced to distinguish XAI from IAI. Different models are further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms are the most prevalent emerging IAI technique. The use of IAI is growing, distinguishing it from XAI. The major challenges identified are that most XIAI does not explore ""global"" modelling processes, the lack of best practices, and the lack of systematic evaluation and benchmarks. One important opportunity is to use attention mechanisms to enhance multi-modal XIAI for personalized medicine. Additionally, combining DL with causal logic holds promise. Our discussion encourages the integration of XIAI in Large Language Models (LLMs) and domain-specific smaller models. In conclusion, XIAI adoption in healthcare requires dedicated in-house expertise. Collaboration with domain experts, end-users, and policymakers can lead to ready-to-use XIAI methods across NLP and medical tasks. While challenges exist, XIAI techniques offer a valuable foundation for interpretable NLP algorithms in healthcare.","<method>deep learning (DL)</method>, <method>attention mechanisms</method>, <method>combining DL with causal logic</method>, <method>Large Language Models (LLMs)</method>"
2024,https://openalex.org/W4400191428,Social Sciences,"Impact of leader support on open innovation: The mediating role of organizational culture, intellectual property, and collaboration","This study investigates how open innovation (OI) functions in the Thai food industry, focusing on the direct and indirect effects of leader support (LS) on OI through organizational culture (OC), intellectual property (IP), and collaboration (CO). The study employs a sample size of 380 units and utilizes path analysis and bootstrapping techniques to assess the relationships among the variables. The results indicate that LS has a significant direct impact on OI, with OC, IP, and CO serving as crucial mediators. Specifically, OC acts as a mediator, with IP and CO following, emphasizing their substantial indirect impacts on OI. OC, IP, and CO act as mediators, emphasizing their substantial indirect impacts on OI. These findings underscore the importance of effective LS in fostering a culture of innovation within the Thai food industry, where traditional recipes intersect with modern culinary trends. By elucidating the mechanisms through which LS influences OI, this study offers practical insights for organizational leaders seeking to stimulate creativity, encourage risk-taking, and foster collaboration in order to ultimately drive innovation and maintain competitiveness in the dynamic landscape of the Thai food industry. This research contributes to the body of knowledge by identifying collaboration as a critical mediator, integrating constructs from leadership, collaboration, and innovation literatures, and validating the mediation model in the context of the Thai food industry.",No methods found.
2024,https://openalex.org/W4400903160,Social Sciences,Deep Learning for Pneumonia Detection in Chest X-ray Images: A Comprehensive Survey,"This paper addresses the significant problem of identifying the relevant background and contextual literature related to deep learning (DL) as an evolving technology in order to provide a comprehensive analysis of the application of DL to the specific problem of pneumonia detection via chest X-ray (CXR) imaging, which is the most common and cost-effective imaging technique available worldwide for pneumonia diagnosis. This paper in particular addresses the key period associated with COVID-19, 2020–2023, to explain, analyze, and systematically evaluate the limitations of approaches and determine their relative levels of effectiveness. The context in which DL is applied as both an aid to and an automated substitute for existing expert radiography professionals, who often have limited availability, is elaborated in detail. The rationale for the undertaken research is provided, along with a justification of the resources adopted and their relevance. This explanatory text and the subsequent analyses are intended to provide sufficient detail of the problem being addressed, existing solutions, and the limitations of these, ranging in detail from the specific to the more general. Indeed, our analysis and evaluation agree with the generally held view that the use of transformers, specifically, vision transformers (ViTs), is the most promising technique for obtaining further effective results in the area of pneumonia detection using CXR images. However, ViTs require extensive further research to address several limitations, specifically the following: biased CXR datasets, data and code availability, the ease with which a model can be explained, systematic methods of accurate model comparison, the notion of class imbalance in CXR datasets, and the possibility of adversarial attacks, the latter of which remains an area of fundamental research.","<method>deep learning (DL)</method>, <method>transformers</method>, <method>vision transformers (ViTs)</method>"
2024,https://openalex.org/W4402270574,Social Sciences,OLIVES: A Go̅-like Model for Stabilizing Protein Structure via Hydrogen Bonding Native Contacts in the Martini 3 Coarse-Grained Force Field,"Coarse-grained molecular dynamics simulations enable the modeling of increasingly complex systems at millisecond timescales. The transferable coarse-grained force field Martini 3 has shown great promise in modeling a wide range of biochemical processes, yet folded proteins in Martini 3 are not stable without the application of external bias potentials, such as elastic networks or Go̅-like models. We herein develop an algorithm, called OLIVES, which identifies native contacts with hydrogen bond capabilities in coarse-grained proteins and use it to implement a novel Go̅-like model for Martini 3. We show that the protein structure instability originates in part from the lack of hydrogen bond energy in the coarse-grained force field representation. By using realistic hydrogen bond energies obtained from literature ab initio calculations, it is demonstrated that protein stability can be recovered by the reintroduction of a coarse-grained hydrogen bond network and that OLIVES removes the need for secondary structure restraints. OLIVES is validated against known protein complexes and at the same time addresses the open question of whether there is a need for protein quaternary structure bias in Martini 3 simulations. It is shown that OLIVES can reduce the number of bias terms, hereby speeding up Martini 3 simulations of proteins by up to ≈30% on a GPU architecture compared to the established Go̅MARTINI Go̅-like model.",No methods found.
2024,https://openalex.org/W4405289793,Social Sciences,The capital budgeting decision,"In September 2011, Amitoj Singh, the proprietor of New Horizon Knitwears, found himself reflecting on the current state of his business as he gazed out of his office window, captivated by an eagle soaring high in the sunlit sky. The decline in sales and a diminishing customer base demanded his attention, prompting him to call a meeting with his company officials that morning. The discussions revealed a crucial insight into the evolving landscape of consumer preferences due to technological advancements. Recognizing that competitors were leveraging new technologies and investing in cutting-edge machinery to enhance efficiency and product offerings, Singh acknowledged the necessity for the company to evolve with the times, particularly in modernization activities, aimed at reducing costs while simultaneously improving color options, patterns and overall product quality. With a history of financial success and a strong reputation, the leadership team also identified future expansion as a strategic priority. Singh envisioned expansion not only as a means to boost profitability but also as an opportunity to leverage existing client relationships and explore new markets. Recognizing the pivotal role that state-of-the-art machinery played in this ambitious endeavor, Singh approached Ashutosh Rana, a senior management executive, to formulate a comprehensive strategy for the company's future success in an evolving market. The ensuing dialogue between Singh and Rana laid the groundwork for a detailed plan to address the challenges of meeting customer demand through significant capital investment in a new machine. Rana was entrusted with aligning the machine with technological upgrades and the company's expansion goals. Singh outlined his responsibilities, including identifying a suitable machine, projecting future cash flows and estimating the necessary cost of capital. Given the strategic importance and substantial investment of the expansion project, Singh also sought Rana's assistance in the capital appraisal decision. As Singh contemplated the decision-making process, he considered appropriate capital budgeting techniques, crucial decision inputs and whether the investment in the new machine would genuinely enhance the company's value. Deliberations also focused on the approach to the capital budgeting decision and techniques suitable for making a sound investment choice, alongside factors such as necessary inputs for the decision-making process and potential value creation resulting from the investment. At this crucial juncture, Singh was faced with a dilemma, with several questions demanding redressal: how to go ahead with capital budgeting and which techniques should be used to make a prudent decision? What inputs would be required to make the decision, and, most importantly, will the investment in the new machine create value for the company?",No methods found.
2024,https://openalex.org/W4390658537,Social Sciences,Types of learning and varieties of innovation: how does policy learning enable policy innovation?,"Policy innovation is considered important for addressing major challenges such as climate change and the sustainable energy transition. Although policy learning is likely to play a key role in enabling policy innovation, the link between them remains unclear despite much research on both topics. To address this gap, we move beyond a binary treatment of policy innovation and differentiate policy problem innovation from policy instrument innovation and policy process innovation. Subsequently, we synthesise the literature on policy learning with the research on the multiple streams framework (MSF), a well-known lens for explaining policy innovation. Like earlier policy learning studies, we distinguish several types of learning by posing the key questions of learning, but in the context of each stream of the MSF: who learns (actors), what (beliefs), how (modes), and to what effect (ripening). This new conceptualisation clarifies the relationship of each type of policy learning to the varieties of policy innovation. Further, it indicates that policy learning is likely to result in policy innovation if and only if it influences the coupling among the three streams during a window of opportunity – through policy entrepreneurship – and not otherwise. We conclude with the implications of this study for future research on policy innovation, policy learning, and the MSF.",No methods found.
2024,https://openalex.org/W4390659208,Social Sciences,GraphCL-DTA: A Graph Contrastive Learning With Molecular Semantics for Drug-Target Binding Affinity Prediction,"Drug-target binding affinity prediction plays an important role in the early stages of drug discovery, which can infer the strength of interactions between new drugs and new targets. However, the performance of previous computational models is limited by the following drawbacks. The learning of drug representation relies only on supervised data without considering the information in the molecular graph itself. Moreover, most previous studies tended to design complicated representation learning modules, while uniformity used to measure representation quality is ignored. In this study, we propose GraphCL-DTA, a graph contrastive learning with molecular semantics for drug-target binding affinity prediction. This graph contrastive learning framework replaces the dropout-based data augmentation strategy by performing data augmentation in the embedding space, thereby better preserving the semantic information of the molecular graph. A more essential and effective drug representation can be learned through this graph contrastive framework without additional supervised data. Next, we design a new loss function that can be directly used to adjust the uniformity of drug and target representations. By directly optimizing the uniformity of representations, the representation quality of drugs and targets can be improved. The effectiveness of the above innovative elements is verified on two real datasets, KIBA and Davis. Compared with the GraphDTA model, the relative improvement of the GraphCL-DTA model on the two datasets is 2.7% and 4.5%. The graph contrastive learning framework and uniformity function in the GraphCL-DTA model can be embedded into other computational models as independent modules to improve their generalization capability.","<method>graph contrastive learning</method>, <method>data augmentation in the embedding space</method>, <method>loss function to adjust uniformity of representations</method>"
2024,https://openalex.org/W4390738651,Social Sciences,Enhancing Multi-UAV Reconnaissance and Search Through Double Critic DDPG With Belief Probability Maps,"Unmanned Aerial Vehicles (UAVs) have recently attracted significant attention due to their potential applications in reconnaissance and search. This paper aims to investigate the issue of multi-UAV cooperative reconnaissance and search (MCRS) to ensure ample coverage of the mission area and precise localization of static targets. The MCRS problem is modeled as a multi-objective optimization problem, taking into account the credibility of search results. To achieve this, we design a belief probability map based on the Dempster-Shafer (DS) evidence theory, comprising an uncertainty map and two target maps. This representation enables a clear depiction of both the presence of the target and the uncertainty within the map. Subsequently, we reformulate this multi-objective optimization problem within the framework of Decentralized Partially Observable Markov Decision Process (Dec-POMDP). To address this reformulation, a new deep reinforcement learning approach called Double Critic Deep Deterministic Policy Gradient (DCDDPG) is proposed. Specifically, we introduce both a centralized critic and a local critic for each UAV agent to estimate the action-value function. This approach helps balance the bias in the action-value function estimation and the variance in the policy updates, thereby improving the coordination effect. Extensive simulation results demonstrate that DCDDPG outperforms existing techniques in terms of search efficiency and coverage.","<method>Dempster-Shafer (DS) evidence theory</method>, <method>Decentralized Partially Observable Markov Decision Process (Dec-POMDP)</method>, <method>Double Critic Deep Deterministic Policy Gradient (DCDDPG)</method>"
2024,https://openalex.org/W4390825338,Social Sciences,PEGA: A Privacy-Preserving Genetic Algorithm for Combinatorial Optimization,"Evolutionary algorithms (EAs), such as the genetic algorithm (GA), offer an elegant way to handle combinatorial optimization problems (COPs). However, limited by expertise and resources, most users lack the capability to implement EAs for solving COPs. An intuitive and promising solution is to outsource evolutionary operations to a cloud server, however, it poses privacy concerns. To this end, this article proposes a novel computing paradigm called evolutionary computation as a service (ECaaS), where a cloud server renders evolutionary computation services for users while ensuring their privacy. Following the concept of ECaaS, this article presents privacy-preserving genetic algorithm (PEGA), a privacy-preserving GA designed specifically for COPs. PEGA enables users, regardless of their domain expertise or resource availability, to outsource COPs to the cloud server that holds a competitive GA and approximates the optimal solution while safeguarding privacy. Notably, PEGA features the following characteristics. First, PEGA empowers users without domain expertise or sufficient resources to solve COPs effectively. Second, PEGA protects the privacy of users by preventing the leakage of optimization problem details. Third, PEGA performs comparably to the conventional GA when approximating the optimal solution. To realize its functionality, we implement PEGA falling in a twin-server architecture and evaluate it on two widely known COPs: 1) the traveling Salesman problem (TSP) and 2) the 0/1 knapsack problem (KP). Particularly, we utilize encryption cryptography to protect users' privacy and carefully design a suite of secure computing protocols to support evolutionary operators of GA on encrypted chromosomes. Privacy analysis demonstrates that PEGA successfully preserves the confidentiality of COP contents. Experimental evaluation results on several TSP datasets and KP datasets reveal that PEGA performs equivalently to the conventional GA in approximating the optimal solution.","<method>evolutionary algorithms (EAs)</method>, <method>genetic algorithm (GA)</method>, <method>privacy-preserving genetic algorithm (PEGA)</method>"
2024,https://openalex.org/W4390961691,Social Sciences,"E-government quality from the citizen's perspective: the role of perceived factors, demographic variables and the digital divide","Purpose Governments globally are adopting e-Government services to streamline administrative processes and meet citizens' expectations. This study investigates e-Government service quality from citizens' perspectives in 50 Greek municipalities, using the technology acceptance model (TAM) and cognitive theory. Design/methodology/approach The data from 707 respondents across 50 Greek municipalities are analyzed using structural equation modeling (SEM), ANOVA and moderation analysis. The study assesses the relationships between key factors and citizens' intentions to use e-Government services, examining the impact of demographics and the digital divide. Findings The study reveals that perceived attractiveness (PA), perceived usefulness (PU), perceived ease of use (PEOU) and awareness (AWA) significantly influence citizens' behavioral intentions (BINTs) toward municipal e-Government services. Interestingly, PEOU negatively impacts users' intentions, suggesting dissatisfaction with portal attractiveness and utility. The study explores the influence of demographic variables and the digital divide on citizens' BINTs, highlighting economic activity and income as crucial determinants. Practical implications The study emphasizes the significance of user-friendly design, PU, PEOU and AWA campaigns for the development of effective e-Government platforms. Strategies to address the digital divide and promote citizen engagement are essential for enhancing user experience, service utility and AWA, ultimately fostering a positive attitude toward e-Government. Social implications Addressing demographic differences ensures inclusive e-Government systems, while bridging the digital divide promotes equitable service delivery and citizen engagement. Originality/value This research provides insights into factors influencing citizens' BINTs toward e-Government services. The study's examination of demographic attributes and the digital divide enhances understanding, contributing to the development of citizen-centric e-Government services and supporting inclusive digital transformations.","<method>structural equation modeling (SEM)</method>, <method>ANOVA</method>, <method>moderation analysis</method>"
2024,https://openalex.org/W4391430106,Social Sciences,A soft voting ensemble learning approach for credit card fraud detection,"With the advancement of e-commerce and modern technological development, credit cards are widely used for both online and offline purchases, which has increased the number of daily fraudulent transactions. Many organizations and financial institutions worldwide lose billions of dollars annually because of credit card fraud. Due to the global distribution of both legitimate and fraudulent transactions, it is difficult to discern between the two. Furthermore, because only a small proportion of transactions are fraudulent, there is a problem of class imbalance. Hence, an effective fraud-detection methodology is required to sustain the reliability of the payment system. Machine learning has recently emerged as a viable substitute for identifying this type of fraud. However, ML approaches have difficulty identifying fraud with high prediction accuracy, while also decreasing misclassification costs due to the size of the imbalanced data. In this research, a soft voting ensemble learning approach for detecting credit card fraud on imbalanced data is proposed. To do this, the proposed approach is evaluated and compared with numerous sophisticated sampling techniques (i.e., oversampling, undersampling, and hybrid sampling) to overcome the class imbalance problem. We develop several credit card fraud classifiers, including ensemble classifiers, with and without sampling techniques. According to the experimental results, the proposed soft-voting approach outperforms individual classifiers. With a false negative rate (FNR) of 0.0306, it achieves a precision of 0.9870, recall of 0.9694, f1-score of 0.8764, and AUROC of 0.9936.","<method>soft voting ensemble learning</method>, <method>oversampling</method>, <method>undersampling</method>, <method>hybrid sampling</method>, <method>ensemble classifiers</method>"
2024,https://openalex.org/W4392117982,Social Sciences,"Unveiling ChatGPT in tourism education: exploring perceptions, advantages and recommendations from educators","Purpose Following the remarkable debut of ChatGPT and its rapid adoption by a global audience since its launch in November 2022, this study delves into educators' perceptions of ChatGPT within the specialized domains of tourism and hospitality education. While acknowledging ChatGPT’s swift rise and advanced capabilities, this research aims to comprehensively explore educators' perspectives, advantages and concerns regarding its integration into academic settings. Design/methodology/approach A qualitative approach was utilized to reveal dominant themes from in-depth, semi-structured face-to-face interviews with twenty lecturers in tourism faculties in North Cyprus. Collected responses from respondents were subjected to analysis using Leximancer software. Findings Our analysis unearthed seven significant themes encapsulating educators' perceptions of ChatGPT: 1 – “reliability and accuracy concerns”; 2 – “dependency”; 3 – “student engagement”; 4 – “ethical considerations”; 5 – “time efficiency and productivity”; 6 – “teacher-student interaction” and 7 – “learning flexibility”. Noteworthy positive perceptions emerged regarding “student engagement,” “time efficiency and productivity,” and “learning flexibility.” Originality/value This study contributes to the originality of research by addressing the underexplored aspect of educators' perceptions of ChatGPT within the domains of tourism and hospitality education, shedding light on its potential implications, advantages and drawbacks in a specialized educational context. Furthermore, it aims to offer insights into educators' recommendations for the effective incorporation of ChatGPT technologies into this specific educational setting, filling a crucial gap in understanding the integration of artificial intelligence (AI) in specialized fields of study.",No methods found.
2024,https://openalex.org/W4392245149,Social Sciences,Towards development of functional climate-driven early warning systems for climate-sensitive infectious diseases: Statistical models and recommendations,"Climate, weather and environmental change have significantly influenced patterns of infectious disease transmission, necessitating the development of early warning systems to anticipate potential impacts and respond in a timely and effective way. Statistical modelling plays a pivotal role in understanding the intricate relationships between climatic factors and infectious disease transmission. For example, time series regression modelling and spatial cluster analysis have been employed to identify risk factors and predict spatial and temporal patterns of infectious diseases. Recently advanced spatio-temporal models and machine learning offer an increasingly robust framework for modelling uncertainty, which is essential in climate-driven disease surveillance due to the dynamic and multifaceted nature of the data. Moreover, Artificial Intelligence (AI) techniques, including deep learning and neural networks, excel in capturing intricate patterns and hidden relationships within climate and environmental data sets. Web-based data has emerged as a powerful complement to other datasets encompassing climate variables and disease occurrences. However, given the complexity and non-linearity of climate-disease interactions, advanced techniques are required to integrate and analyse these diverse data to obtain more accurate predictions of impending outbreaks, epidemics or pandemics. This article presents an overview of an approach to creating climate-driven early warning systems with a focus on statistical model suitability and selection, along with recommendations for utilizing spatio-temporal and machine learning techniques. By addressing the limitations and embracing the recommendations for future research, we could enhance preparedness and response strategies, ultimately contributing to the safeguarding of public health in the face of evolving climate challenges.","<method>time series regression modelling</method>, <method>spatial cluster analysis</method>, <method>spatio-temporal models</method>, <method>machine learning</method>, <method>Artificial Intelligence (AI) techniques</method>, <method>deep learning</method>, <method>neural networks</method>"
2024,https://openalex.org/W4392791479,Social Sciences,Health equity assessment of machine learning performance (HEAL): a framework and dermatology AI model case study,"BackgroundArtificial intelligence (AI) has repeatedly been shown to encode historical inequities in healthcare. We aimed to develop a framework to quantitatively assess the performance equity of health AI technologies and to illustrate its utility via a case study.MethodsHere, we propose a methodology to assess whether health AI technologies prioritise performance for patient populations experiencing worse outcomes, that is complementary to existing fairness metrics. We developed the Health Equity Assessment of machine Learning performance (HEAL) framework designed to quantitatively assess the performance equity of health AI technologies via a four-step interdisciplinary process to understand and quantify domain-specific criteria, and the resulting HEAL metric. As an illustrative case study (analysis conducted between October 2022 and January 2023), we applied the HEAL framework to a dermatology AI model. A set of 5420 teledermatology cases (store-and-forward cases from patients of 20 years or older, submitted from primary care providers in the USA and skin cancer clinics in Australia), enriched for diversity in age, sex and race/ethnicity, was used to retrospectively evaluate the AI model's HEAL metric, defined as the likelihood that the AI model performs better for subpopulations with worse average health outcomes as compared to others. The likelihood that AI performance was anticorrelated to pre-existing health outcomes was estimated using bootstrap methods as the probability that the negated Spearman's rank correlation coefficient (i.e., ""R"") was greater than zero. Positive values of R suggest that subpopulations with poorer health outcomes have better AI model performance. Thus, the HEAL metric, defined as p (R >0), measures how likely the AI technology is to prioritise performance for subpopulations with worse average health outcomes as compared to others (presented as a percentage below). Health outcomes were quantified as disability-adjusted life years (DALYs) when grouping by sex and age, and years of life lost (YLLs) when grouping by race/ethnicity. AI performance was measured as top-3 agreement with the reference diagnosis from a panel of 3 dermatologists per case.FindingsAcross all dermatologic conditions, the HEAL metric was 80.5% for prioritizing AI performance of racial/ethnic subpopulations based on YLLs, and 92.1% and 0.0% respectively for prioritizing AI performance of sex and age subpopulations based on DALYs. Certain dermatologic conditions were significantly associated with greater AI model performance compared to a reference category of less common conditions. For skin cancer conditions, the HEAL metric was 73.8% for prioritizing AI performance of age subpopulations based on DALYs.InterpretationAnalysis using the proposed HEAL framework showed that the dermatology AI model prioritised performance for race/ethnicity, sex (all conditions) and age (cancer conditions) subpopulations with respect to pre-existing health disparities. More work is needed to investigate ways of promoting equitable AI performance across age for non-cancer conditions and to better understand how AI models can contribute towards improving equity in health outcomes.FundingGoogle LLC.","<method>Health Equity Assessment of machine Learning performance (HEAL) framework</method>, <method>bootstrap methods</method>"
2024,https://openalex.org/W4392960540,Social Sciences,A sentiment analysis approach for understanding users’ perception of metaverse marketplace,"This research explores the user perceptions of the Metaverse Marketplace, analyzing a substantial dataset of over 860,000 Twitter posts through sentiment analysis and topic modeling techniques. The study aims to uncover the driving factors behind user engagement and sentiment in this novel digital trading space. Key findings highlight a predominantly positive user sentiment, with significant enthusiasm for the marketplace's revenue generation and entertainment potential, particularly within the gaming sector. Users express appreciation for the innovative opportunities the Metaverse Marketplace offers for artists, designers, and traders in handling and trading digital assets. This positive outlook is tempered by notable concerns regarding security and privacy within the Metaverse, pointing to a critical area for development and assurance. The study also reveals a substantial neutral sentiment, reflecting users' cautious but interested stance, particularly regarding the marketplace's role in investment and passive income opportunities. This balanced view underscores the evolving nature of user perceptions in this emerging field. Theoretically, the research enriches the discourse on technology adoption, particularly in virtual environments, by highlighting perceived benefits and enjoyment as significant adoption drivers. These insights are invaluable for stakeholders in the Metaverse Marketplace, guiding the development of more secure, engaging, and user-friendly platforms. While providing a pioneering perspective on Metaverse user perceptions, the study acknowledges its limitation to Twitter data, suggesting the need for broader research methodologies for a more holistic understanding.","<method>sentiment analysis</method>, <method>topic modeling</method>"
2024,https://openalex.org/W4392979783,Social Sciences,BEVSOC: Self-Supervised Contrastive Learning for Calibration-Free BEV 3-D Object Detection,"3D object detection based on multi-view cameras and bird's-eye view (BEV) representation is a key task for autonomous driving, as it enables the perception systems to understand the surrounding scenes. However, most existing BEV representation methods rely on the projection matrix of camera intrinsic and extrinsic parameters, which requires a complex and time-consuming calibration process that may introduce errors and degrade the detection performance. Moreover, the calibration results may vary due to environmental changes and affect the stability of the detection system. To address this problem, we propose a calibration-free 3D object detection method that leverages a group-equivariant convolutional network to extract features from multi-view images and a projection network module to learn the implicit 3D-to-2D projection relationship for obtaining BEV representation. Furthermore, we employ contrastive learning to pre-train the projection network module without using manually annotated data. By exploiting the multi-view camera data through contrastive learning, our proposed method eliminates the need for tedious calibration, avoids calibration errors, and reduces the dependence on a large amount of annotated data for calibration-free 3D object detection. We evaluate our method on the nuScenes dataset and demonstrate its competitive performance. Our method improves the stability and reliability of 3D object detection in long-term autonomous driving.","<method>group-equivariant convolutional network</method>, <method>projection network module</method>, <method>contrastive learning</method>"
2024,https://openalex.org/W4394838135,Social Sciences,The paradox of immersive artificial intelligence (AI) in luxury hospitality: how immersive AI shapes consumer differentiation and luxury value,"Purpose This paper aims to bridge the extended reality framework and the luxury hospitality literature by providing insights into how immersive technologies using artificial intelligence (AI) can shape luxury value and consumer differentiation. Design/methodology/approach The authors conducted three experimental studies comparing immersive AI versus traditional hospitality across luxury contexts (hotels, restaurants and spas). Study 1 investigates the effect of immersive AI (vs traditional hospitality) on customers’ behavioral intentions and the need for differentiation using virtual-assisted reality. Study 2 tests the underlying mechanism of the need for differentiation and luxury value in an augmented reality context. Study 3 provides additional support for the proposed underlying mechanism using virtual-assisted reality in luxury hospitality. Findings The findings reveal that immersive AI (vs traditional) luxury hospitality reduces customers’ behavioral intentions of using such services and perceived luxury value. Moreover, the findings indicate that the intention to use immersive AI (vs traditional) luxury hospitality services is contingent upon customers’ need for differentiation. Originality/value The findings have important theoretical and managerial implications for immersive technologies in luxury hospitality. They shed light on the dynamics between integrating immersive AI into luxury hospitality and its impact on customers’ differentiation motives and perceived luxury value. The findings reveal the detrimental effect of using immersive AI (vs traditional hospitality) within this context.",No methods found.
2024,https://openalex.org/W4395110469,Social Sciences,ac4C-AFL: A high-precision identification of human mRNA N4-acetylcytidine sites based on adaptive feature representation learning,"RNA N4-acetylcytidine (ac4C) is a highly conserved RNA modification that plays a crucial role in controlling mRNA stability, processing, and translation. Consequently, accurate identification of ac4C sites across the genome is critical for understanding gene expression regulation mechanisms. In this study, we have developed ac4C-AFL, a bioinformatics tool that precisely identifies ac4C sites from primary RNA sequences. In ac4C-AFL, we identified the optimal sequence length for model building and implemented an adaptive feature representation strategy that is capable of extracting the most representative features from RNA. To identify the most relevant features, we proposed a novel ensemble feature importance scoring strategy to rank features effectively. We then used this information to conduct the sequential forward search, which individually determine the optimal feature set from the 16 sequence-derived feature descriptors. Utilizing these optimal feature descriptors, we constructed 176 baseline models using 11 popular classifiers. The most efficient baseline models were identified using the two-step feature selection approach, whose predicted scores were integrated and trained with the appropriate classifier to develop the final prediction model. Our rigorous cross-validations and independent tests demonstrate that ac4C-AFL surpasses contemporary tools in predicting ac4C sites. Moreover, we have developed a publicly accessible web server at https://balalab-skku.org/ac4C-AFL/.","<method>ensemble feature importance scoring strategy</method>, <method>sequential forward search</method>, <method>classifiers</method>, <method>two-step feature selection approach</method>"
2024,https://openalex.org/W4396902020,Social Sciences,Sustainability and environmental impact in the LNG value chain: Current trends and future opportunities,"The liquefied natural gas (LNG) industry plays a crucial role in the global energy landscape, offering a cleaner alternative to traditional fossil fuels. However, the LNG value chain presents environmental challenges that must be addressed to ensure long-term sustainability. This paper examines current trends and future opportunities for enhancing sustainability and reducing environmental impact across the LNG value chain. The LNG value chain comprises several stages, including natural gas extraction, liquefaction, transportation, regasification, and distribution. Each stage presents unique sustainability challenges, such as methane emissions during extraction and transportation, energy-intensive liquefaction processes, and the carbon footprint of regasification and distribution. Current trends in the LNG industry focus on mitigating these challenges through various strategies. These include the adoption of advanced technologies for methane detection and reduction, the use of renewable energy sources for liquefaction, and the implementation of efficient regasification and distribution practices. Additionally, there is a growing emphasis on stakeholder engagement, transparency, and reporting to enhance sustainability performance across the value chain. Future opportunities for improving sustainability in the LNG value chain lie in the continued development and deployment of innovative technologies. These include carbon capture and storage (CCS) technologies to reduce emissions, the use of renewable natural gas (RNG) as a feedstock, and the integration of LNG with renewable energy sources to create hybrid energy systems. Addressing sustainability and environmental impact in the LNG value chain requires collaboration among industry stakeholders, governments, and regulatory bodies. By implementing best practices, embracing innovation, and prioritizing sustainability, the LNG industry can continue to play a vital role in the transition to a cleaner and more sustainable energy future.",No methods found.
2024,https://openalex.org/W4396919944,Social Sciences,CrossHAR: Generalizing Cross-dataset Human Activity Recognition via Hierarchical Self-Supervised Pretraining,"The increasing availability of low-cost wearable devices and smartphones has significantly advanced the field of sensor-based human activity recognition (HAR), attracting considerable research interest. One of the major challenges in HAR is the domain shift problem in cross-dataset activity recognition, which occurs due to variations in users, device types, and sensor placements between the source dataset and the target dataset. Although domain adaptation methods have shown promise, they typically require access to the target dataset during the training process, which might not be practical in some scenarios. To address these issues, we introduce CrossHAR, a new HAR model designed to improve model performance on unseen target datasets. CrossHAR involves three main steps: (i) CrossHAR explores the sensor data generation principle to diversify the data distribution and augment the raw sensor data. (ii) CrossHAR then employs a hierarchical self-supervised pretraining approach with the augmented data to develop a generalizable representation. (iii) Finally, CrossHAR fine-tunes the pretrained model with a small set of labeled data in the source dataset, enhancing its performance in cross-dataset HAR. Our extensive experiments across multiple real-world HAR datasets demonstrate that CrossHAR outperforms current state-of-the-art methods by 10.83% in accuracy, demonstrating its effectiveness in generalizing to unseen target datasets.","<method>domain adaptation methods</method>, <method>data augmentation</method>, <method>hierarchical self-supervised pretraining</method>, <method>fine-tuning</method>"
2024,https://openalex.org/W4396965624,Social Sciences,Recognizing ecosystem service's contribution to SDGs: Ecological foundation of sustainable development,"There is less than half the time left to achieve the United Nations Sustainable Development Goals (SDGs), and progress toward SDGs is obviously insufficient. The contribution of ecosystem services (ES) to SDGs realization has received extensive attention, but systematic generalization and recognition are still lacking. Based on a review of the progress and challenge of sustainable development, this study summarized ES's potential contribution to 17 SDGs, and systematically reviewed empirical researches focused on the ES's contribution to SDGs based on the RepOrting standards for Systematic Evidence Syntheses (ROSES). The results found that from the 1960s to the 2020s, the ES's contribution has gradually become more important in sustainable development. ES has potential contribution to all SDGs, but the contribution to different SDGs varies. In the empirical study, ES's contribution to SDG2, SDG6, SDG13, and SDG15 were strongly focused. ES's contribution to SDG4, SDG5, SDG10, SDG16, and SDG17 were weakly focused. Most researches explored the ES's contribution to SDGs based on ES supply at a single scale, with lacked attention to ES demand and scale differences, and insufficient attention to intervention factors affecting the ES's contribution to SDGs. Faced with the above deficiencies, future research could deepen the exploration of ES's contribution to SDGs from the following four perspectives: clarifying true contributions, exploring leverage point, integrating multi-scale differences, and focusing on intervention factors.",No methods found.
2024,https://openalex.org/W4398257517,Social Sciences,Transferable deep generative modeling of intrinsically disordered protein conformations,"Intrinsically disordered proteins have dynamic structures through which they play key biological roles. The elucidation of their conformational ensembles is a challenging problem requiring an integrated use of computational and experimental methods. Molecular simulations are a valuable computational strategy for constructing structural ensembles of disordered proteins but are highly resource-intensive. Recently, machine learning approaches based on deep generative models that learn from simulation data have emerged as an efficient alternative for generating structural ensembles. However, such methods currently suffer from limited transferability when modeling sequences and conformations absent in the training data. Here, we develop a novel generative model that achieves high levels of transferability for intrinsically disordered protein ensembles. The approach, named idpSAM, is a latent diffusion model based on transformer neural networks. It combines an autoencoder to learn a representation of protein geometry and a diffusion model to sample novel conformations in the encoded space. IdpSAM was trained on a large dataset of simulations of disordered protein regions performed with the ABSINTH implicit solvent model. Thanks to the expressiveness of its neural networks and its training stability, idpSAM faithfully captures 3D structural ensembles of test sequences with no similarity in the training set. Our study also demonstrates the potential for generating full conformational ensembles from datasets with limited sampling and underscores the importance of training set size for generalization. We believe that idpSAM represents a significant progress in transferable protein ensemble modeling through machine learning.","<method>deep generative models</method>, <method>latent diffusion model</method>, <method>transformer neural networks</method>, <method>autoencoder</method>, <method>diffusion model</method>"
2024,https://openalex.org/W4399411615,Social Sciences,Innovative community-based strategies to combat adolescent substance use in urban areas of the US and Africa,"Adolescent substance use remains a pervasive challenge in urban areas across the United States and Africa, posing significant threats to the health, well-being, and future potential of young populations. Addressing this issue necessitates innovative, community-based strategies that are culturally relevant and adaptable to the unique socio-economic contexts of different regions. This comparative analysis explores various innovative approaches employed in urban settings of the US and Africa to combat adolescent substance use, highlighting the efficacy and adaptability of these strategies. In the US, community-based initiatives often leverage multi-sectoral collaboration involving schools, healthcare providers, local government, and non-profit organizations. Programs such as the Drug-Free Communities (DFC) Support Program emphasize community mobilization, youth engagement, and the creation of local coalitions to drive prevention efforts. Techniques like peer mentoring, family-based interventions, and the integration of technology through mobile apps and social media campaigns are pivotal in these efforts. Evidence shows that these strategies not only reduce substance use but also foster resilience and healthy lifestyle choices among adolescents. Conversely, in urban areas of Africa, innovative strategies are often rooted in cultural practices and community structures. Initiatives such as peer education programs, which harness the influence of respected community members and elders, are instrumental. Additionally, partnerships with local organizations and religious institutions play a crucial role in disseminating preventive messages and providing support services. Programs like ""U-Turn"" in South Africa utilize a holistic approach, incorporating vocational training, counseling, and community service to rehabilitate and reintegrate affected youth. Mobile health (mHealth) solutions, leveraging the widespread use of mobile phones, have also shown promise in delivering prevention and intervention services. This analysis underscores the importance of tailoring strategies to local contexts and the potential benefits of cross-continental learning and collaboration. By sharing best practices and adapting successful elements from each region, urban communities in both the US and Africa can enhance their efforts to combat adolescent substance use. The study concludes with policy recommendations and practical guidelines for implementing and sustaining effective community-based interventions, ultimately aiming to foster healthier, drug-free environments for adolescents worldwide. Keywords: Innovative, Community-Based Strategies, Adolescent Substance Use, Urban Areas, Combat.",No methods found.
2024,https://openalex.org/W4400009952,Social Sciences,Trust beyond Technology Algorithms: A Theoretical Exploration of Consumer Trust and Behavior in Technological Consumption and AI Projects,"In an era dominated by artificial intelligence (AI), establishing customer confidence is crucial for the integration and acceptance of AI technologies. This interdisciplinary study examines factors influencing customer trust in AI systems through a mixed-methods approach, blending quantitative analysis with qualitative insights to create a comprehensive conceptual framework. Quantitatively, the study analyzes responses from 1248 participants using structural equation modeling (SEM), exploring interactions between technological factors like perceived usefulness and transparency, psychological factors including perceived risk and domain expertise, and organizational factors such as leadership support and ethical accountability. The results confirm the model, showing significant impacts of these factors on consumer trust and AI adoption attitudes. Qualitatively, the study includes 35 semi-structured interviews and five case studies, providing deeper insight into the dynamics shaping trust. Key themes identified include the necessity of explainability, domain competence, corporate culture, and stakeholder engagement in fostering trust. The qualitative findings complement the quantitative data, highlighting the complex interplay between technology capabilities, human perceptions, and organizational practices in establishing trust in AI. By integrating these findings, the study proposes a novel conceptual model that elucidates how various elements collectively influence consumer trust in AI. This model not only advances theoretical understanding but also offers practical implications for businesses and policymakers. The research contributes to the discourse on trust creation and decision-making in technology, emphasizing the need for interdisciplinary efforts to address societal challenges associated with technological advancements. It lays the groundwork for future research, including longitudinal, cross-cultural, and industry-specific studies, to further explore consumer trust in AI.",<method>structural equation modeling (SEM)</method>
2024,https://openalex.org/W653837439,Social Sciences,Crafts and Creative Media in Therapy,"For more than 20 years, Crafts and Creative Media in Therapy, Fifth Edition has been an illuminating reference for the use of creative approaches in helping clients achieve their therapeutic goals. Carol Crellin Tubbs has included a range of craft and creative activity categories, from paper crafts, to cooking, to the use of recycled materials, and everything in between. Each chapter includes a brief history of the craft, several projects along with suggestions for grading or adapting, examples of related documentation, and a short case study. The text also features chapters on activity analysis, general strategies for implementation of creative activities, and documentation, as well as a chapter describing the relevance of this media from both historical and current occupation-based perspectives. In this updated Fifth Edition, the craft projects have been updated and numerous resources and links for more ideas have been added. There are new chapters on making therapy tools and crafting with a purpose, and the recycled and found materials chapter has been expanded in keeping with cultural trends. A flow chart has been added to each case study to help students better understand the process and rationale for tailoring activities for individual client needs, and project suggestions for working on specific performance skills or client factors are scattered throughout the chapters. Other additions include a behavioral observation checklist as an aid in evaluation and documentation, and several illustrations to help students distinguish between the use of occupation as means and occupation as end. This Fifth Edition also includes an updated instructors' manual with additional resources and suggestions for lesson planning. Crafts and Creative Media in Therapy, Fifth Edition not only provides a wide assortment of craft ideas and instructions, but also provides multiple suggestions for therapeutic uses for activities in each category. It includes ways to grade activities to best achieve therapy objectives, and examples of documentation for reimbursement. For each craft category, there is discussion on precautions for use with certain populations, contextual limitations, and safety considerations. Information is presented in several different formats such as examples, tables, illustrations, and other formats to promote student understanding. Included with the text are online supplemental materials for faculty use in the classroom. . Crafts and Creative Media in Therapy, Fifth Edition is the foremost resource for using creative approaches in helping clients achieve their therapeutic goals and should be used by all occupational therapists, occupational therapy assistants, and recreational therapists.",No methods found.
2024,https://openalex.org/W4390496539,Social Sciences,A machine learning-based classification model to support university students with dyslexia with personalized tools and strategies,"Abstract Dyslexia is a specific learning disorder that causes issues related to reading, which affects around 10% of the worldwide population. This can compromise comprehension and memorization skills, and result in anxiety and lack of self-esteem, if no support is provided. Moreover, this support should be highly personalized, to be actually helpful. In this paper, a model to classify the most useful methodologies to support students with dyslexia has been created, with a focus on university alumni. The prediction algorithm is based on supervised machine learning techniques; starting from the issues that dyslexic students experience during their career, it is capable of suggesting customized support digital tools and learning strategies for each of them. The algorithm was trained and tested on data acquired through a self-evaluation questionnaire, which was designed and then spread to more than 1200 university students. It allowed 17 useful tools and 22 useful strategies to be detected. The results of the testing showed an average prediction accuracy higher than 90%, which rises to 94% by renouncing to guess the less-predictable 8 tools/strategies. In the light of this, it is possible to state that the implemented algorithm can achieve the set goal and, thus, reduce the gap between dyslexic and non-dyslexic students. This achievement paves the way for a new modality of facing the problem of dyslexia by university institutions, which aims at modifying teaching activities toward students’ needs, instead of simply reducing their study load or duties. This complies with the definition and the aims of inclusivity.",<method>supervised machine learning techniques</method>
2024,https://openalex.org/W4390500011,Social Sciences,Offshore wind farm site selection in Norway: Using a fuzzy trigonometric weighted assessment model,"Maximising the energy potential of offshore wind farms requires an in-depth assessment of technological, economic, sociopolitical, and environmental aspects. Given the large economic impact of large-scale projects, a robust site selection procedure is critical for limiting financial risks while supporting informed investments. This research uncovers a novel and multidisciplinary approach for boosting the efficacy of Norwegian and global offshore wind farm siting investments. The proposed method uses a two-stage fuzzy mathematical model that considers technical, economic, logistical, and environmental factors. It combines the Ordinal Priority Approach (F-OPA) and Trigonometric Weighted Assessment (TRWA) technique by using an in-depth techno-economic assessment. An alternative reactive power compensation model, power loss calculations, and associated techno-economic analysis were performed for the investigated offshore wind farm locations. Furthermore, the energy economic calculations are carried out to provide support for the proposed decision-making framework. The proposed methodology was tested through a case study, focusing on ranking Norwegian offshore wind farm sites selected from potential locations announced by The Norwegian Water Resources and Energy Directorate (NVE). Within the Norwegian offshore wind farm sites, the approach demonstrated a versatile and efficient decision-making process at both individual and collective levels, identifying the Sandskallen-Sørøya Nord project as a pivotal investment priority and providing valuable managerial insights to enhance Norway's offshore wind initiatives. The model's stability was affirmed through a sensitivity analysis, underscoring its potential to enhance renewable energy policy and decision-making globally.","<method>two-stage fuzzy mathematical model</method>, <method>Ordinal Priority Approach (F-OPA)</method>, <method>Trigonometric Weighted Assessment (TRWA) technique</method>"
2024,https://openalex.org/W4390571563,Social Sciences,Employee work engagement in the digital transformation of enterprises: a fuzzy-set qualitative comparative analysis,"Abstract Information technology has brought about significant changes in enterprises, and new work situations have led to new problems. Employee resistance to new technologies, their ability to learn, and their ability to utilize personal resources to improve work engagement in the face of technological pressure are important factors that companies need to consider when undergoing digital transformation. The influence mechanism of configuration effects on factors around employee work engagement has not been explored, and technostress creators have rarely been included in the configuration as influencing factors in previous studies. On the basis of the job demands-resources (JD-R) model and trait activation theory, this study explored the factors that affect employees’ work engagement at the level of job demands and personal resources. The fuzzy-set qualitative comparative analysis (fsQCA) method was used to investigate the influence of technical stressors, self-efficacy, and the Big Five personality traits on employees’ work engagement. Through a survey of 225 employees in the context of enterprise digital transformation, the results show three driving paths that promote employees’ work engagement: openness to experience conscientiousness, self-efficacy driven, and inhibition to technical stressors. The study also analyzed employees’ low work engagement state, which is driven by an inhibition of agreeableness and extraversion. This research enriches the study of factors influencing work engagement in the digital transformation of enterprises.",<method>fuzzy-set qualitative comparative analysis (fsQCA)</method>
2024,https://openalex.org/W4390954565,Social Sciences,Understanding influences on entrepreneurship educator role identity,"Purpose Despite the considerable increase in research on entrepreneurship education, few studies examine the role of entrepreneurship educators. Similarly, most frameworks from entrepreneurship education recognize the educator’s importance in facilitating instruction and assessment, but the factors influencing the educator role are not well understood. According to the identity theory, personal factors including self-efficacy, job satisfaction and personal values influence the perspective of self, significance and anticipations that an individual in this role associates with it, determining their planning and actions. The stronger the role identity the more likely entrepreneurship educators will be in effectively developing their entrepreneurial skills as well as the overall learning experience of their students. The objective of this study is to pinpoint the factors that affect entrepreneurial role identity. Design/methodology/approach Drawing upon the identity theory, this study developed a theoretical framework and carried out an empirical investigation involving a survey of 289 entrepreneurship educators across the globe. Structural equation modeling (SEM) technique was applied to analyze and explore the factors that impact the identity of the educators in their role as entrepreneurship teachers. Findings The findings show that the role identity of entrepreneurship educators is significantly influenced by their self-efficacy, job satisfaction and personal values. Among these factors, self-efficacy and job satisfaction have the most significant impacts on how educators perceive their role. The implications of these results and directions for future research are also discussed. Originality/value The novelty of the current study is derived from its conceptualization of the antecedents of role perception among entrepreneurship educators. This study stands out as one of the earliest attempts to investigate the factors that shape an individual’s scene of self and professional identity as an entrepreneurship educator. The significance of comprehending the antecedents of role perception lies in the insights it can offer into how educators undertake and execute their role, and consequently, their effectiveness in teaching entrepreneurship.",No methods found.
2024,https://openalex.org/W4391002310,Social Sciences,Multi-risk assessment in transboundary areas: A framework for harmonized evaluation considering seismic and flood risks,"Effective disaster risk management would require the analysis and the comparison of relevant risks potentially affecting a territory, also considering possible interactions among them (e.g., cascading effects). Modelling such interactions may be complex, increasing the challenge to perform exhaustive multi-risk assessment. Independent analyses are often performed for the analysis of multiple risks in a given area. However, as usually for the analysis of different risks, different methodologies and different impact metrics are adopted, results of single-risk analyses may be not comparable. The problem of comparability is exacerbated in cross-border areas, where additional challenges in risk analysis arise due to the diversity of databases and risk analysis models in neighboring countries. This paper presents the approach for multi-risk assessment proposed within BORIS project (Cross BOrder RISk assessment for increased prevention and preparedness in Europe). Focusing on seismic and flood risks, this project aims to develop a shared methodology for cross-border multi-risk assessment in Europe. Adopting a single-layer multi-risk approach, each risk is assessed through independent analysis, but the procedures for risk assessment are harmonized in order to ensure their comparability. Also, the issue of transboundary assessment of single risks is addressed. The proposed methodology is applied in two regions in the cross-border area between Italy, Slovenia and Austria, demonstrating the usefulness of the BORIS approach for multi-risk comparison and ranking. Acknowledging the need to improve coordination between neighboring countries, the methodology of the BORIS project also reflects certain compromise solutions that allow its ease application and exportability to other countries.",No methods found.
2024,https://openalex.org/W4391131339,Social Sciences,Supervised Machine Learning Approaches for Predicting Key Pollutants and for the Sustainable Enhancement of Urban Air Quality: A Systematic Review,"Urban air pollution is a pressing global issue driven by factors such as swift urbanization, population expansion, and heightened industrial activities. To address this challenge, the integration of Machine Learning (ML) into smart cities presents a promising avenue. Our article offers comprehensive insights into recent advancements in air quality research, employing the PRISMA method as a cornerstone for the reviewing process, while simultaneously exploring the application of frequently employed ML methodologies. Focusing on supervised learning algorithms, the study meticulously analyzes air quality data, elucidating their unique benefits and challenges. These frequently employed ML techniques, including LSTM (Long Short-Term Memory), RF (Random Forest), ANN (Artificial Neural Networks), and SVR (Support Vector Regression), are instrumental in our quest for cleaner, healthier urban environments. By accurately predicting key pollutants such as particulate matter (PM), nitrogen oxides (NOx), carbon monoxide (CO), and ozone (O3), these methods offer tangible solutions for society. They enable informed decision-making for urban planners and policymakers, leading to proactive, sustainable strategies to combat urban air pollution. As a result, the well-being and health of urban populations are significantly improved. In this revised abstract, the importance of frequently employed ML methods in the context of air quality is explicitly emphasized, underlining their role in improving urban environments and enhancing the well-being of urban populations.","<method>LSTM (Long Short-Term Memory)</method>, <method>RF (Random Forest)</method>, <method>ANN (Artificial Neural Networks)</method>, <method>SVR (Support Vector Regression)</method>"
2024,https://openalex.org/W4391255815,Social Sciences,"Real Estate Industry Sustainable Solution (Environmental, Social, and Governance) Significance Assessment—AI-Powered Algorithm Implementation","As the global imperative for sustainable development intensifies, the real estate industry stands at the intersection of environmental responsibility and economic viability. This paper presents a comprehensive exploration of the significance of sustainable solutions within the real estate sector, employing advanced artificial intelligence (AI) algorithms to assess their impact. This study focuses on the integration of AI-powered tools in a decision-making process analysis. The research methodology involves the development and implementation of AI algorithms capable of analyzing vast datasets related to real estate attributes. By leveraging machine learning techniques, the algorithm assesses the significance of energy efficiency solutions along with other intrinsic and extrinsic attributes. This paper examines the effectiveness of these solutions in relation to the influence on property prices with a framework based on an AI-driven algorithm. The findings aim to inform real estate professionals and investors about the tangible advantages of integrating AI technologies into sustainable solutions, promoting a more informed and responsible approach to industry practices. This research contributes to the growing interest in the connection of the real estate sector, sustainability, and AI, offering insights that can guide strategic decision making. By implementing the random forest method in the real estate feature significance assessment original methodology, it has been shown that AI-powered algorithms can be a useful tool from the perspective of real estate price prediction. The methodology’s ability to handle non-linear relationships and provide insights into feature importance proved advantageous in comparison to the multiple regression analysis.","<method>artificial intelligence (AI) algorithms</method>, <method>machine learning techniques</method>, <method>random forest method</method>, <method>multiple regression analysis</method>"
2024,https://openalex.org/W4391542194,Social Sciences,"Digitale Gesundheitskompetenz der Bevölkerung in Deutschland: Aktueller Stand, Konzepte und Herausforderungen","Zusammenfassung Eine wesentliche Voraussetzung für eine erfolgreiche digitale Transformation des Gesundheitswesens ist eine gut ausgeprägte digitale Gesundheitskompetenz (DGK) der Bevölkerung. DGK ist die Fähigkeit zum Umgang mit gesundheitsbezogenen digitalen Informationen und Informationsmöglichkeiten mit dem Ziel, Gesundheit und Wohlbefinden für sich selbst und sein Umfeld zu fördern und zu erhalten. Der Artikel beleuchtet die Diskussion über DGK, vorhandene Studien und die darin verwendeten Messinstrumente sowie die Datenlage in Deutschland und erörtert aktuelle Herausforderungen. DGK besteht aus verschiedenen Teilkompetenzen, die aktuelle digitale Informationsverhalten, -möglichkeiten und -risiken widerspiegeln. Die Datenlage ist, aufgrund unterschiedlicher Studiendesigns und -instrumente, sehr heterogen, was die Aussagekraft limitiert. Zwei repräsentative Studien, HLS-GER 2 der Universität Bielefeld sowie die Studie der AOK Rheinland/Hamburg und des Leibniz-WissenschaftsCampus, weisen trotz unterschiedlicher Methoden auf einen hohen Anteil von Menschen mit geringer DGK hin. National wie international zeigt sich, dass die DGK einem sozialen Gradienten unterliegt und mit Bildungsniveau, Sozialstatus, finanzieller Deprivation und Alter assoziiert ist. Die DGK ist in Deutschland den vorliegenden Daten zufolge noch unzureichend; somit besteht ein großer Handlungsbedarf. Erforderliche gesetzliche Rahmenbedingungen sind gegeben, dennoch fehlt es an verlässlichen finanziellen Ressourcen ebenso an einer soliden Datengrundlage auf Bevölkerungsebene zu DGK. Damit ließen sich Vulnerabilitätsfaktoren identifizieren und die Implementation von Maßnahmen vorbereiten und evaluieren. Zudem bedarf es einer vertiefenden konzeptionellen Diskussion zur DGK, die an das etablierte Gesundheitskompetenzkonzept anknüpft und auch die gesundheitsbezogene Infodemie und ihre Folgen für die DGK aufgreift.",No methods found.
2024,https://openalex.org/W4391655925,Social Sciences,Governance in the exploration of global and regional determinants of ICT development,"The present study assesses how governance affects information and communication technology (ICT) at the global level contingent on macroeconomic policy factors such as trade, foreign investment (FDI), manufacturing value added, and agricultural value added. The study focuses on 183 countries from 2003 to 2021, and the empirical evidence is based on the generalized method of moments (GMM). The following main findings are established. For the full sample, governance unconditionally promotes ICT development, while trade openness (industrial added value) moderates governance to promote (dampen) ICT development. In sub-Saharan Africa, only trade openness effectively moderates governance to induce an overall positive effect on ICT, while in the Middle East and North Africa (MENA) region, all policy variables moderate governance for an overall positive incidence on ICT sector development. The findings for the MENA region are confirmed in the Europe and Central Asia (ECA) region, with the exception of the moderating role of industrial added value, which engenders an overall negative effect. In the East and South Asia and the Pacific (ESAP) countries, one overall positive incidence is apparent in the role of trade openness, while net negative effects are established from the moderating roles of industrial added value and agricultural added value. In the American sub-sample, a positive (negative) net effect is apparent from the role of industrial added value (trade) in moderating the incidence of governance on ICT sector development. Finally, policy implications are discussed.",<method>generalized method of moments (GMM)</method>
2024,https://openalex.org/W4391691888,Social Sciences,Highly Accurate Prediction of NMR Chemical Shifts from Low-Level Quantum Mechanics Calculations Using Machine Learning,"Theoretical predictions of NMR chemical shifts from first-principles can greatly facilitate experimental interpretation and structure identification of molecules in gas, solution, and solid-state phases. However, accurate prediction of chemical shifts using the gold-standard coupled cluster with singles, doubles, and perturbative triple excitations [CCSD(T)] method with a complete basis set (CBS) can be prohibitively expensive. By contrast, machine learning (ML) methods offer inexpensive alternatives for chemical shift predictions but are hampered by generalization to molecules outside the original training set. Here, we propose several new ideas in ML of the chemical shift prediction for H, C, N, and O that first introduce a novel feature representation, based on the atomic chemical shielding tensors within a molecular environment using an inexpensive quantum mechanics (QM) method, and train it to predict NMR chemical shieldings of a high-level composite theory that approaches the accuracy of CCSD(T)/CBS. In addition, we train the ML model through a new progressive active learning workflow that reduces the total number of expensive high-level composite calculations required while allowing the model to continuously improve on unseen data. Furthermore, the algorithm provides an error estimation, signaling potential unreliability in predictions if the error is large. Finally, we introduce a novel approach to keep the rotational invariance of the features using tensor environment vectors (TEVs) that yields a ML model with the highest accuracy compared to a similar model using data augmentation. We illustrate the predictive capacity of the resulting inexpensive shift machine learning (iShiftML) models across several benchmarks, including unseen molecules in the NS372 data set, gas-phase experimental chemical shifts for small organic molecules, and much larger and more complex natural products in which we can accurately differentiate between subtle diastereomers based on chemical shift assignments.","<method>machine learning (ML) methods</method>, <method>progressive active learning workflow</method>"
2024,https://openalex.org/W4391853581,Social Sciences,A Survey on Information Bottleneck,"This survey is for the remembrance of one of the creators of the information bottleneck theory, Prof. Naftali Tishby, passing away at the age of 68 on August, 2021. Information bottleneck (IB), a novel information theoretic approach for pattern analysis and representation learning, has gained widespread popularity since its birth in 1999. It provides an elegant balance between data compression and information preservation, and improves its prediction or representation ability accordingly. This survey summarizes both the theoretical progress and practical applications on IB over the past 20-plus years, where its basic theory, optimization, extensive models and task-oriented algorithms are systematically explored. Existing IB methods are roughly divided into two parts: traditional and deep IB, where the former contains the IBs optimized by traditional machine learning analysis techniques without involving any neural networks, and the latter includes the IBs involving the interpretation, optimization and improvement of deep neural works (DNNs). Specifically, based on the technique taxonomy, traditional IBs are further classified into three categories: Basic, Informative and Propagating IB; While the deep IBs, based on the taxonomy of problem settings, contain Debate: Understanding DNNs with IB, Optimizing DNNs Using IB, and DNN-based IB methods. Furthermore, some potential issues deserving future research are discussed. This survey attempts to draw a more complete picture of IB, from which the subsequent studies can benefit.","<method>Information bottleneck (IB)</method>, <method>traditional IBs</method>, <method>Basic IB</method>, <method>Informative IB</method>, <method>Propagating IB</method>, <method>deep IBs</method>, <method>Debate: Understanding DNNs with IB</method>, <method>Optimizing DNNs Using IB</method>, <method>DNN-based IB methods</method>"
2024,https://openalex.org/W4392242086,Social Sciences,"Geographic and Demographic Differences in the Proportion of Individuals Living in Households With a Firearm, 1990-2018","Importance Measures of the proportion of individuals living in households with a firearm (HFR), over time, across states, and by demographic groups are needed to evaluate disparities in firearm violence and the effects of firearm policies. Objective To estimate HFR across states, years, and demographic groups in the US. Design, Setting, and Participants In this survey study, substate HFR totals from 1990 to 2018 were estimated using bayesian multilevel regression with poststratification to analyze survey data on HFR from the Behavioral Risk Factor Surveillance System and the General Social Survey. HFR was estimated for 16 substate demographic groups defined by gender, race, marital status, and urbanicity in each state and year. Exposures Survey responses indicating household firearm ownership were analyzed and compared with a common proxy for firearm ownership, the fraction of suicides completed with a firearm (FSS). Main Outcome and Measure HFR, FSS, and their correlations and differences. Results Among US adults in 2018, HFR was significantly higher among married, nonurban, non-Hispanic White and American Indian male individuals (65.0%; 95% credible interval [CI], 61.5%-68.7%) compared with their unmarried, urban, female counterparts from other racial and ethnic groups (7.3%; 95% CIs, 6.0%-9.2%). Marginal HFR rates for larger demographic groups also revealed important differences, with racial minority groups and urban dwellers having less than half the HFR of either White and American Indian (39.5%; 95% CI, 37.4%-42.9% vs 17.2%; 95% CI, 15.5%-19.9%) or nonurban populations (46.0%; 95% CI, 43.8%-49.5% vs 23.1%; 95% CI, 21.3%-26.2%). Population growth among groups less likely to own firearms, rather than changes in ownership within demographic groups, explains 30% of the 7 percentage point decline in HFR nationally from 1990 to 2018. Comparing HFR estimates with FSS revealed the expected high overall correlation across states (r = 0.84), but scaled FSS differed from HFR by as many as 20 percentage points for some states and demographic groups. Conclusions and Relevance This survey study of HFR providing detailed, publicly available HFR estimates highlights key disparities among individuals in households with firearms across states and demographic groups; it also identifies potential biases in the use of FSS as a proxy for firearm ownership rates. These findings are essential for researchers, policymakers, and public health experts looking to address geographic and demographic disparities in firearm violence.",<method>bayesian multilevel regression with poststratification</method>
2024,https://openalex.org/W4392449656,Social Sciences,Augmenting Reinforcement Learning With Transformer-Based Scene Representation Learning for Decision-Making of Autonomous Driving,"Decision-making for urban autonomous driving is challenging due to the stochastic nature of interactive traffic participants and the complexity of road structures. Although reinforcement learning (RL)-based decision-making schemes are promising to handle urban driving scenarios, they suffer from low sample efficiency and poor adaptability. In this paper, we propose the Scene-Rep Transformer to enhance RL decision-making capabilities through improved scene representation encoding and sequential predictive latent distillation. Specifically, a multi-stage Transformer (MST) encoder is constructed to model not only the interaction awareness between the ego vehicle and its neighbors but also intention awareness between the agents and their candidate routes. A sequential latent Transformer (SLT) with self-supervised learning objectives is employed to distill future predictive information into the latent scene representation, in order to reduce the exploration space and speed up training. The final decision-making module based on soft actor-critic (SAC) takes as input the refined latent scene representation from the Scene-Rep Transformer and generates decisions. The framework is validated in five challenging simulated urban scenarios with dense traffic, and its performance is manifested quantitatively by substantial improvements in data efficiency and performance in terms of success rate, safety, and efficiency. Qualitative results reveal that our framework is able to extract the intentions of neighbor agents, enabling better decision-making and more diversified driving behaviors.","<method>reinforcement learning (RL)</method>, <method>Transformer</method>, <method>multi-stage Transformer (MST) encoder</method>, <method>sequential latent Transformer (SLT)</method>, <method>self-supervised learning</method>, <method>soft actor-critic (SAC)</method>"
2024,https://openalex.org/W4392714571,Social Sciences,Machine learning study using 2020 SDHS data to determine poverty determinants in Somalia,"Abstract Extensive research has been conducted on poverty in developing countries using conventional regression analysis, which has limited prediction capability. This study aims to address this gap by applying advanced machine learning (ML) methods to predict poverty in Somalia. Utilizing data from the first-ever 2020 Somalia Demographic and Health Survey (SDHS), a cross-sectional study design is considered. ML methods, including random forest (RF), decision tree (DT), support vector machine (SVM), and logistic regression, are tested and applied using R software version 4.1.2, while conventional methods are analyzed using STATA version 17. Evaluation metrics, such as confusion matrix, accuracy, precision, sensitivity, specificity, recall, F1 score, and area under the receiver operating characteristic (AUROC), are employed to assess the performance of predictive models. The prevalence of poverty in Somalia is notable, with approximately seven out of ten Somalis living in poverty, making it one of the highest rates in the region. Among nomadic pastoralists, agro-pastoralists, and internally displaced persons (IDPs), the poverty average stands at 69%, while urban areas have a lower poverty rate of 60%. The accuracy of prediction ranged between 67.21% and 98.36% for the advanced ML methods, with the RF model demonstrating the best performance. The results reveal geographical region, household size, respondent age group, husband employment status, age of household head, and place of residence as the top six predictors of poverty in Somalia. The findings highlight the potential of ML methods to predict poverty and uncover hidden information that traditional statistical methods cannot detect, with the RF model identified as the best classifier for predicting poverty in Somalia.","<method>random forest (RF)</method>, <method>decision tree (DT)</method>, <method>support vector machine (SVM)</method>, <method>logistic regression</method>"
2024,https://openalex.org/W4392851982,Social Sciences,Understanding the Barriers to Consumer Purchasing of Electric Vehicles: The Innovation Resistance Theory,"In the context of sustainable transition, the factors that impact the decision to purchase electric vehicles (EVs) have garnered significant interest. However, existing research predominantly concentrates on the promotional factors while disregarding an examination of the resistance effects. Drawing on the innovation resistance theory (IRT), this study aims to investigate the influence of three functional barriers (usage, value, and risk) and two psychological barriers (tradition and image) on consumers’ intention to purchase EVs. Additionally, we also analyze the moderating effect of environmental concern and incentive policy. Based on a survey of 297 respondents in China, we used SPSS 26.0 and AMOS 24.0 to verify our hypothesis. Our findings indicate that usage, value, risk, and tradition barriers negatively affect EV purchase intentions. Moreover, the negative relationship between functional barriers and EV purchase intentions is weaker for a strong incentive policy. Furthermore, we found that Gen Y and households with private car consumers are more willing to purchase EVs. These findings contribute to extending the applicability of IRT to the sustainable transportation field. They also offer practical guidance for EV enterprises with regard to marketing strategies that effectively mitigate the functional and psychological barriers to enhance profits, and for policymakers to better stimulate the development of the EV market.",No methods found.
2024,https://openalex.org/W4393142165,Social Sciences,TAM-Based Study of Farmers’ Live Streaming E-Commerce Adoption Intentions,"Amidst the digital economy surge, live streaming e-commerce of agricultural products has significantly boosted agricultural prosperity. Investigating farmers’ behavioral intentions toward adopting live streaming e-commerce holds critical importance for fostering agricultural healthy and swift growth. Utilizing the Technology Acceptance Model (TAM) as a foundation, this study incorporates three additional variables—government support, platform support, and social learning—to devise a theoretical model. It takes the agriculture-related live streaming e-commerce platform as an example, with 424 Chinese farmers as the sample, to quantitatively assess the factors that impact the intentions to adopt live streaming e-commerce behaviors. The findings indicate that, firstly, the TAM is applicable to the assessment of farmers’ intentions to adopt live streaming e-commerce. Secondly, government support positively impacts perceived usefulness, social learning enhances perceived ease of use, and platform support positively impacts both perceived ease of use and usefulness. Lastly, the technology acceptance extension model applicability varies among farmer groups: government support influence on perceived ease of use is more significant among traditional farmers, social learning impact on perceived ease of use is higher in farmers with higher education levels, and platform support effect on perceived usefulness is stronger among farmers experienced in e-commerce. Therefore, differentiated promotion strategies by the government are necessary, and e-commerce platforms should leverage their technology to offer efficient services and encourage farmer education. A multi-party collaboration model involving the government, platforms, and farmers is essential to collectively foster the healthy development of rural live streaming e-commerce.",No methods found.
2024,https://openalex.org/W4393153153,Social Sciences,TimesURL: Self-Supervised Contrastive Learning for Universal Time Series Representation Learning,"Learning universal time series representations applicable to various types of downstream tasks is challenging but valuable in real applications. Recently, researchers have attempted to leverage the success of self-supervised contrastive learning (SSCL) in Computer Vision(CV) and Natural Language Processing(NLP) to tackle time series representation. Nevertheless, due to the special temporal characteristics, relying solely on empirical guidance from other domains may be ineffective for time series and difficult to adapt to multiple downstream tasks. To this end, we review three parts involved in SSCL including 1) designing augmentation methods for positive pairs, 2) constructing (hard) negative pairs, and 3) designing SSCL loss. For 1) and 2), we find that unsuitable positive and negative pair construction may introduce inappropriate inductive biases, which neither preserve temporal properties nor provide sufficient discriminative features. For 3), just exploring segment- or instance-level semantics information is not enough for learning universal representation. To remedy the above issues, we propose a novel self-supervised framework named TimesURL. Specifically, we first introduce a frequency-temporal-based augmentation to keep the temporal property unchanged. And then, we construct double Universums as a special kind of hard negative to guide better contrastive learning. Additionally, we introduce time reconstruction as a joint optimization objective with contrastive learning to capture both segment-level and instance-level information. As a result, TimesURL can learn high-quality universal representations and achieve state-of-the-art performance in 6 different downstream tasks, including short- and long-term forecasting, imputation, classification, anomaly detection and transfer learning.","<method>self-supervised contrastive learning (SSCL)</method>, <method>frequency-temporal-based augmentation</method>, <method>double Universums as a special kind of hard negative</method>, <method>time reconstruction as a joint optimization objective with contrastive learning</method>"
2024,https://openalex.org/W4394749617,Social Sciences,MS-BACL: enhancing metabolic stability prediction through bond graph augmentation and contrastive learning,"Abstract Motivation Accurately predicting molecular metabolic stability is of great significance to drug research and development, ensuring drug safety and effectiveness. Existing deep learning methods, especially graph neural networks, can reveal the molecular structure of drugs and thus efficiently predict the metabolic stability of molecules. However, most of these methods focus on the message passing between adjacent atoms in the molecular graph, ignoring the relationship between bonds. This makes it difficult for these methods to estimate accurate molecular representations, thereby being limited in molecular metabolic stability prediction tasks. Results We propose the MS-BACL model based on bond graph augmentation technology and contrastive learning strategy, which can efficiently and reliably predict the metabolic stability of molecules. To our knowledge, this is the first time that bond-to-bond relationships in molecular graph structures have been considered in the task of metabolic stability prediction. We build a bond graph based on ‘atom-bond-atom’, and the model can simultaneously capture the information of atoms and bonds during the message propagation process. This enhances the model’s ability to reveal the internal structure of the molecule, thereby improving the structural representation of the molecule. Furthermore, we perform contrastive learning training based on the molecular graph and its bond graph to learn the final molecular representation. Multiple sets of experimental results on public datasets show that the proposed MS-BACL model outperforms the state-of-the-art model. Availability and Implementation The code and data are publicly available at https://github.com/taowang11/MS.","<method>deep learning</method>, <method>graph neural networks</method>, <method>bond graph augmentation technology</method>, <method>contrastive learning strategy</method>"
2024,https://openalex.org/W4394893217,Social Sciences,"Advancements in machine visions for fruit sorting and grading: A bibliometric analysis, systematic review, and future research directions","This research conducted a bibliometric analysis of scholarly literature on fruit sorting and grading using machine vision, identifying primary themes, sources, most-cited publications, and countries. The literature and bibliometric analysis were thoroughly evaluated to consolidate knowledge, identify research trends, and propose specific research opportunities within the context of machine vision for fruit sorting and grading. Research articles from 2011 to 2023, indexed in the main collections of the Dimensions, Web-of-science, and Scopus databases, were examined. Findings were presented quantitatively, using tables and graphs to emphasize the key performance factors for article writing and citation. Upon applying inclusion and exclusion criteria, 129 out of 1,812 discovered articles were included for examination, while 1,683 studies were excluded due to non-compliance with the requirements and duplicates. Thirty-four (34) case study publications on machine vision applications for fruit sorting and grading were comprehensively examined to identify the adopted methodologies and future research opportunities. Covered methodologies include fruit varieties, data volumes, data collection, classification methods, and accuracy metrics. The study's findings indicate a significant increase in deep learning applications for fruit recognition in the recent five years (2019-2023), with excellent results achieved either by utilizing new models or with pre-trained networks for transfer learning. The research also identifies gaps and future directions for machine vision in fruit sorting and grading, such as enhancing system robustness, scalability, and adaptability, integrating multiple sensors and technological methods, and developing evaluation and comparison standards and criteria. The paper concludes that machine vision holds promise as a potent tool for fruit quality assessment, but further research and development are needed to address existing challenges and meet the growing demands of the fruit industry.","<method>deep learning</method>, <method>pre-trained networks for transfer learning</method>"
2024,https://openalex.org/W4395027859,Social Sciences,"Digital Economy, Entrepreneurship, and High-Quality Development of the Manufacturing Industry","The digital economy has emerged as a significant catalyst for economic expansion on a global scale, and this trend is evident in China.China is emerging as a dominant force in the digital economy and manufacturing industry due to its burgeoning entrepreneurial culture and emphasis on top-notch development.The present study investigates the interplay among the digital economy, entrepreneurship, and the high-quality development of the manufacturing industry in China.This paper comprehensively analyzes the current digital economy and entrepreneurship status in China.It examines the policies and initiatives to foster high-quality development in the manufacturing sector.This paper investigates the influence of digital technologies on productivity and efficiency within the manufacturing industry.It presents an analysis of data and case studies to underscore the notable advancements achieved by China in the digital economy and manufacturing sector while acknowledging the obstacles that remain to be overcome.The results indicate that prioritizing digital technologies, innovation, and entrepreneurship can bolster the manufacturing sector's competitiveness, resulting in sustainable economic expansion and progress.The investigation recognizes the obstacles that necessitate resolution to completely actualize the possibilities of the digital economy and entrepreneurship within the manufacturing sector, utilizing the Multi-Criteria Decision Making (MCDM) approach.The paper advocates for increased cooperation among government, industry, and academia to cultivate a conducive environment for digital entrepreneurship.This collaboration facilitates the manufacturing industry's ability to leverage the digital era's opportunities.",<method>Multi-Criteria Decision Making (MCDM)</method>
2024,https://openalex.org/W4396240923,Social Sciences,Sequential predictive learning is a unifying theory for hippocampal representation and replay,"Abstract The mammalian hippocampus contains a cognitive map that represents an animal’s position in the environment 1 and generates offline “replay” 2,3 for the purposes of recall 4 , planning 5,6 , and forming long term memories 7 . Recently, it’s been found that artificial neural networks trained to predict sensory inputs develop spatially tuned cells 8 , aligning with predictive theories of hippocampal function 9–11 . However, whether predictive learning can also account for the ability to produce offline replay is unknown. Here, we find that spatially-tuned cells, which robustly emerge from all forms of predictive learning, do not guarantee the presence of a cognitive map with the ability to generate replay. Offline simulations only emerged in networks that used recurrent connections and head-direction information to predict multi-step observation sequences, which promoted the formation of a continuous attractor reflecting the geometry of the environment. These offline trajectories were able to show wake-like statistics, autonomously replay recently experienced locations, and could be directed by a virtual head direction signal. Further, we found that networks trained to make cyclical predictions of future observation sequences were able to rapidly learn a cognitive map and produced sweeping representations of future positions reminiscent of hippocampal theta sweeps 12 . These results demonstrate how hippocampal-like representation and replay can emerge in neural networks engaged in predictive learning, and suggest that hippocampal theta sequences reflect a circuit that implements a data-efficient algorithm for sequential predictive learning. Together, this framework provides a unifying theory for hippocampal functions and hippocampal-inspired approaches to artificial intelligence.","<method>artificial neural networks</method>, <method>predictive learning</method>, <method>recurrent connections</method>, <method>cyclical predictions of future observation sequences</method>"
2024,https://openalex.org/W4399177791,Social Sciences,"Mental health, positive psychology and leadership: a positive autoethnographic case study of Claude-Hélène Mayer","Purpose The purpose of this paper is to meet Dr Claude-Hélène Mayer, Professor in Industrial and Organisational Psychology at the University of Johannesburg, Johannesburg, South Africa. Design/methodology/approach This is a qualitative positive autoethnographic case study. Mayer uses positive autoethnography to reflect on mental health, positive psychology and leadership across the life span. The first author also responds to ten questions. Findings Leadership can be developed and appreciated in others. Leadership can be a positive and transformative force that can assist and contribute to mental health and well-being. Positive leadership can be supported by developing wisdom, creativity, critical thinking and conflict and emotional management. Research limitations/implications Qualitative research cannot be generalised. Positive autoethnography reflects the views and experiences of the author, in this case a transdisciplinary and transcultural researcher and practitioner. Practical implications Readers will find a range of practical recommendations as to how to develop mental health and leadership and stay positive in challenging times. Academic literature relating to practical recommendations is also shared. Social implications Positive leadership has many social implications. It can be a constructive influence that impacts our lives and benefits our personal development, but also one that impacts the lives and benefits the personal development of other people. Originality/value In this original paper, Dr Claude-Hélène Mayer shares mental health, positive psychology and leadership experiences and recommendations. Leadership skills are of particular value in times of complex change.",No methods found.
2024,https://openalex.org/W4399501912,Social Sciences,Combating the Challenges of False Positives in AI-Driven Anomaly Detection Systems and Enhancing Data Security in the Cloud,"Anomaly detection is critical for network security, fraud detection, and system health monitoring applications. Traditional methods like statistical approaches and distance-based techniques often struggle with high-dimensional and complex data, leading to high false positive rates. This study addresses the challenge by investigating advanced AI-driven techniques to reduce false positives and enhance data security within cloud computing environments. This study employs deep learning models, integrates contextual data, and incorporates comprehensive security measures to enhance anomaly detection performance. Data from synthetic sources, such as the NSL-KDD dataset and real-world cloud environments, were utilized to capture user behavior logs, system states, and network traffic. Over 50 academic journals were reviewed, and 21 were selected based on inclusion criteria, such as relevance to AI-driven anomaly detection, empirical performance metrics, and the focus on cloud environments, and exclusion criteria that filtered out studies lacking empirical data or not specific to cloud-based systems. Methodologically, the research involves a comparative analysis of different AI techniques and their impact on false positive rates, accuracy, precision, and recall. The findings demonstrate that deep learning techniques significantly outperform traditional methods, achieving a lower false positive rate and higher accuracy. The results underscore the importance of contextual data and robust security protocols in reliable anomaly detection. This research fills a gap by thoroughly evaluating advanced AI techniques for reducing false positives in cloud environments. The study's significance lies in guiding the development of more effective anomaly detection systems, thereby enhancing security and reliability across various applications. Additionally, organizations should invest in continuously developing and integrating AI-driven anomaly detection systems with comprehensive security measures to improve their effectiveness the study suggests that further study be conducted with large datasets to evaluate the effectiveness of Hybrid anomaly detection systems in detecting and addressing false positives.","<method>deep learning models</method>, <method>deep learning techniques</method>, <method>AI-driven techniques</method>, <method>Hybrid anomaly detection systems</method>"
2024,https://openalex.org/W4399926471,Social Sciences,"Literature review: Industry 5.0. Leveraging technologies for environmental, social and governance advancement in corporate settings","Purpose This research paper explores the transformative potential of Industry 5.0 for environmental, social and governance (ESG) factors within corporate settings. This study aims to elucidate the role of Industry 5.0 and its related technologies in influencing ESG factors, explore potential risks linked to ESG and present strategies for mitigation through Industry 5.0. Design/methodology/approach This paper is the literature review that introduces Industry 5.0 as a pivotal factor in implementing and mitigating ESG and its related risks. It outlines Industry 5.0's characteristics, driven by advanced technologies. Findings Literature reviews suggest that Industry 5.0 has the potential to significantly influence ESG factors within corporate settings. It can promote sustainability, enhance working conditions and offer operational advantages. Practical implications The practical implications of this research paper are twofold. First, it provides valuable insights to policymakers, organizations and regulatory bodies, guiding them in adapting their frameworks to embrace Industry 5.0. This adaptation is essential for achieving ESG goals and facilitating sustainable development. Second, it highlights the critical role of Industry 5.0 in mitigating ESG-related risks, offering a robust structure for sustainable development. Originality/value This research paper contributes to the existing body of knowledge by highlighting the transformative potential of Industry 5.0 in the context of ESG. It offers a comprehensive exploration of the historical evolution of corporate governance, the integration of sustainability and the growing focus on ESG. It also highlights the originality and value of Industry 5.0 as a critical mitigating factor for ESG-related risks, presenting a holistic approach to sustainable corporate practices.",No methods found.
2024,https://openalex.org/W4400046623,Social Sciences,Methodological and practical guidance for designing and conducting online qualitative surveys in public health,"Abstract Online qualitative surveys—those surveys that prioritise qualitative questions and interpretivist values—have rich potential for researchers, particularly in new or emerging areas of public health. However, there is limited discussion about the practical development and methodological implications of such surveys, particularly for public health researchers. This poses challenges for researchers, funders, ethics committees, and peer reviewers in assessing the rigour and robustness of such research, and in deciding the appropriateness of the method for answering different research questions. Drawing and extending on the work of other researchers, as well as our own experiences of conducting online qualitative surveys with young people and adults, we describe the processes associated with developing and implementing online qualitative surveys and writing up online qualitative survey data. We provide practical examples and lessons learned about question development, the importance of rigorous piloting strategies, use of novel techniques to prompt detailed responses from participants, and decisions that are made about data preparation and interpretation. We consider reviewer comments, and some ethical considerations of this type of qualitative research for both participants and researchers. We provide a range of practical strategies to improve trustworthiness in decision-making and data interpretation—including the importance of using theory. Rigorous online qualitative surveys that are grounded in qualitative interpretivist values offer a range of unique benefits for public health researchers, knowledge users, and research participants.",No methods found.
2024,https://openalex.org/W4400127350,Social Sciences,Teaching Sustainability through Traditional Sporting Games,"Traditional sports games (TSGs), deeply rooted in local culture, serve as valuable resources for educating in sustainable development, aligning with guidelines set forth by international resolutions such as the Agenda 2030. This study investigated how ethnomotor variables in the educational use of TSGs with objects influenced emotional well-being and the acquisition of significant and contextualised physical and social sustainability learning involving various educational agents. The study adopted a qualitative and inductive approach centred on an ethnomotor intervention to promote sustainable learning within and beyond the educational setting. A total of 226 primary school students aged between 11 and 12 from seven primary education centres in the Canary Islands, Spain participated. Seven intervention sessions were conducted, including out-of-school activities involving family members and in-school activities with the collaboration of teaching staff and a specialised researcher. Various data collection instruments were employed (field notes, questionnaires, and interviews). A content analysis of qualitative data was conducted and subsequently transformed into quantitative data. For statistical analyses of these data, multidimensional frequency areas, crosstab (Pearson’s chi-square), associated effect size (Cramer’s V), and decision trees were utilised. This research discusses the relevance of TSGs as tools for promoting physically and socially sustainable learning. Furthermore, the role of various educational agents, including family members, teachers, and teacher-researchers, in developing these experiences is highlighted. The findings also highlight emotional well-being (psychosocial and socio-emotional affectivity) as a key aspect in the process of meaningful and contextualised sustainable learning.",No methods found.
2024,https://openalex.org/W4390615924,Social Sciences,Board Gender Diversity and Firm Performance: Recent Evidence from Japan,"Gender diversity is increasingly recognized as a critical element in corporate management. However, existing research on its impact on firm performance demonstrates inconsistency in a global context. This study employs 1990 publicly listed Japanese companies from 2006 to 2023 and examines the effect of board gender diversity on firm performance in Japan. Findings from the fixed-effects regression model revealed a significant negative impact of board gender diversity on firm performance. This adverse correlation is more pronounced in smaller firms, those with greater leverage and reduced institutional ownership, and regulated and consumer-focused industries, particularly pre-COVID-19. The detrimental impact of board gender diversity on firm performance is transmitted via corporate social responsibility and firm innovation instead of board independence or CEO duality. Notably, the two-stage least squares estimation addresses potential endogeneity, employing an equal opportunity policy as an instrumental variable. Moreover, the robustness of our results is affirmed via the substitution of return on equity for return on assets as an indicator of firm performance. Lastly, our analysis does not reveal a U-shaped nonlinear relationship between board gender diversity and corporate performance. As Japan progressively promotes women’s participation in corporate governance, this research bears significant implications for corporate leaders, investors, and policymakers in Japan.",No methods found.
2024,https://openalex.org/W4390628414,Social Sciences,"Police legitimacy in urban, suburban and rural settings: a study in Slovenia","Purpose Police legitimacy presents a social value of the institution based on citizens' normative, moral and ethical feelings that they should voluntarily comply with and support the authority of the police. The present study focuses on residents' perceptions of police legitimacy in different settings in Slovenia. Design/methodology/approach Drawing on data from a survey of 1,022 citizens in Slovenia, this study examined the correlates of police legitimacy and differences in citizen perceptions of police legitimacy in urban, suburban and rural settings. Findings Multivariate statistical analyses showed that feelings of obligation to obey, trust in police, procedural justice, police effectiveness, relations with police officers and gender influence perceptions of police legitimacy. Significant differences between residents' perceptions of police legitimacy, obligation to obey, trust in police, procedural justice, police effectiveness and legal cynicism in urban, suburban and rural settings were also observed. In general, residents of rural areas were found to have more positive attitudes towards the police than those in urban and suburban settings. Practical implications The article is useful for police leaders and practitioners planning policies and training of police officers for democratic policing. Social implications Police legitimacy reflects the legitimacy of governance, as the police are the most visible representatives of the state authority. Therefore, police legitimacy is crucial for policing in urban, suburban and rural settings. Originality/value The study presents the first test of police legitimacy in a non-Western cultural environment based on a national sample of citizens, which enables the generalisation of concepts of legitimacy, and its correlates in a different cultural setting. The study also presents the first attempt to test and compare the effect of the settings (i.e. rural, suburban and urban) on variables influencing residents' perceptions of police legitimacy.",No methods found.
2024,https://openalex.org/W4390905130,Social Sciences,Long-Term Preference Mining With Temporal and Spatial Fusion for Point-of-Interest Recommendation,"The growth of the tourism industry has greatly boosted the Point-of-Interest (POI) recommendation tasks using Location-based Social Networks (LBSNs). The ever-evolving nature of user preferences poses a major problem. To address this, we propose a Long-Term Preference Mining (LTPM) approach that utilizes the Temporal Recency (TR) measure in the visits along with the location-aware recommendation based on Spatial Proximity (SP) to the user's location. The temporal dynamics and changing preferences are exploited based on the modified Long Short-term Memory (LSTM) that utilizes the time decay. The spatial considerations are modeled in two aspects: geographical proximity based on enhanced representation learning using orthogonal mapping. Second, the Region-of-Interest (ROI) is based on spatial griding and metric learning to capture the spatial relationships between POIs to enhance the metric space representation. The final recommendations are based on a multi-head attention mechanism that allocates the weights to different features. The combination of three models, called, LTPM-TRSP approach captures the user-POI, POI-POI, and POI-time relationships by focusing on the informative representation of sequential and spatial data. The category-aware final recommendations based on comprehensive historical behavior and geographical context are quite efficacious. The experimentation on three real-world datasets, Gowalla, Foursquare, and Weeplaces, also suggests the potency compared to other state-of-the-art approaches.","<method>Long Short-term Memory (LSTM)</method>, <method>enhanced representation learning using orthogonal mapping</method>, <method>metric learning</method>, <method>multi-head attention mechanism</method>"
2024,https://openalex.org/W4391024493,Social Sciences,Bridging the Gap: Traditional vs. Modern Education (A Value-Based Approach for Multiculturalism),"The dynamic landscape of education has witnessed a profound shift from traditional to modern pedagogical paradigms over the years. The discussion of results delves into the intriguing debate between traditional and modern educational systems (TES and MES), examining them through the lens of a value-based approach. This exploration is crucial in understanding how these two approaches shape the educational experiences of learners, faculty, and impact society at large. Drawing from the literature review and insights from a survey involving 179 students and 28 faculty staff, the work advocates a balanced integration of traditional and modern educational approaches. It underscores the pressing need for a value-based model that harmonizes age-old wisdom with contemporary innovations. The survey reveals student aspirations for a holistic, value-driven education, while the faculty acknowledges challenges and opportunities inherent in bridging this educational gap. In conclusion, the data reinforce the value-based approach, emphasizing its importance in curricula and pedagogy to promote ethical values, critical thinking, and empathy. Furthermore, the findings shed light on practical implementation challenges and offer valuable guidance to educators and policymakers. In an era of transformative education, bridging theory and practice will resonate with both students and faculty who recognize the societal benefits of a balanced synthesis between tradition and modernity.",No methods found.
2024,https://openalex.org/W4391222558,Social Sciences,Impact of stakeholder engagement strategies on managerial cognitive decision-making: the context of CSP and CSR,"Purpose This study aims to explore the strategies corporations use in engaging stakeholders to sustain healthy corporate partnerships and create value for the corporate entity and the society in which they operate and their influence on the corporate manager’s cognitive abilities and decision-making. Design/methodology/approach The authors used an interpretive research approach leveraging the strengths of qualitative method of content analysis and comparative and critical analyses to report the results. Interpretive methods incorporate social theories and standpoints that view reality as the social construction of understandable events in the context of organizational communication. Findings The findings of this study suggest that corporations are assumed to follow and execute the principles of engaging stakeholders to achieve corporate social responsibility (CSR) claiming to manage a sustainable and responsible business practices that recognize local cultures, human rights and protect the environment. However, little attention has been paid to the cognitive reasoning of the individuals responsible for CSR and corporate sustainability (CS) as opposed to the growing concerns about strategies corporations use in engaging stakeholders to sustain healthy corporate partnerships and create value – especially the processes that take place during engagement and decision-making including cognitive offloading. Practical implications Stakeholder engagement requires practical approaches that enable corporations and individuals charged with decision-making responsibilities to understand, respond and fulfill their CSRs. To achieve CSRs, corporations and managers responsible for relevant decision-making would need to involve stakeholders in social performance planning, as social reporting/auditing has long been advocating for preventing managerial biasness, groupthink and increased information dissemination via detailed reporting practices toward more collaborative stakeholder relationships. Thus, it is crucial for corporations to implement enhanced stakeholder and managerial decision-making strategies such as integrative approaches to achieve balance in the trio elements of sustainability as well as the growing use of paradox perspective to understand the nature of the tensions being sought to balance and, in the process, provide opportunity for a better evaluation of complex sustainability issues for innovative approach to resolving them. While cognitive decision-making is at play, in practice, managers tasked with making decisions must ensure the most effective stakeholder engagement strategies that are transparent and inclusive are used. Originality/value The main contribution of this study is its argument regarding the tools corporations use in engaging key stakeholders and the cognitive reasoning of the individuals responsible for CSR and CS. The study further contributes to interpreting the integrative approach to achieving balance in the trio elements of sustainability as well as the growing use of paradox perspective to understand the nature of the tensions being sought to balance and, in the process, provide an opportunity for a better evaluation of complex sustainability issues for an innovative approach to resolving them.",No methods found.
2024,https://openalex.org/W4391228781,Social Sciences,Enhancing digital literacy in education: educational directions,"Purpose The significance of digital literacy in online social capital accumulation and surviving the contemporary society is widely recognised. Despite that the current generation is regarded as “digital natives”, their levels and nature of digital literacy vary. To generate educational insights, this study investigates the type(s) of digital literacy which are mostly related to the online social capital accumulation, and how one’s socio-economic background affects the connection between digital literacy and online social capital. Design/methodology/approach A total of 1,747 participants aged 13–30 were invited to take part in a quantitative study. Spearman's rank correlation analysis, hierarchical regression analysis and mediation analyses were performed to investigate the relationship between participants' demographic background, engagement in the online platforms, digital literacy and online social capital. Findings The results showed that the creative dimension of digital literacy was mostly significantly predictive of online social capital accumulation. Also, education significantly affected the relationship between the creative dimension of digital literacy and online social capital more than demographic background. Originality/value Results suggest that education helps enhance digital literacy, offsetting the influence of socio-demographic background. The author examines the implications of how to implement training programmes in youth settings to enhance students' digital literacy and benefit those who are marginalised.",No methods found.
2024,https://openalex.org/W4391256320,Social Sciences,Gas adsorption meets deep learning: voxelizing the potential energy surface of metal-organic frameworks,"Abstract Intrinsic properties of metal-organic frameworks (MOFs), such as their ultra porosity and high surface area, deem them promising solutions for problems involving gas adsorption. Nevertheless, due to their combinatorial nature, a huge number of structures is feasible which renders cumbersome the selection of the best candidates with traditional techniques. Recently, machine learning approaches have emerged as efficient tools to deal with this challenge, by allowing researchers to rapidly screen large databases of MOFs via predictive models. The performance of the latter is tightly tied to the mathematical representation of a material, thus necessitating the use of informative descriptors. In this work, a generalized framework to predict gaseous adsorption properties is presented, using as one and only descriptor the capstone of chemical information: the potential energy surface (PES). In order to be machine understandable, the PES is voxelized and subsequently a 3D convolutional neural network (CNN) is exploited to process this 3D energy image. As a proof of concept, the proposed pipeline is applied on predicting $${\hbox {CO}_{2}}$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:msub> <mml:mtext>CO</mml:mtext> <mml:mn>2</mml:mn> </mml:msub> </mml:math> uptake in MOFs. The resulting model outperforms a conventional model built with geometric descriptors and requires two orders of magnitude less training data to reach a given level of performance. Moreover, the transferability of the approach to different host-guest systems is demonstrated, examining $${\hbox {CH}_4}$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:msub> <mml:mtext>CH</mml:mtext> <mml:mn>4</mml:mn> </mml:msub> </mml:math> uptake in COFs. The generic character of the proposed methodology, inherited from the PES, renders it applicable to fields other than reticular chemistry.","<method>machine learning approaches</method>, <method>3D convolutional neural network (CNN)</method>"
2024,https://openalex.org/W4391449759,Social Sciences,Nature and Mental Health in Urban Texas: A NatureScore-Based Study,"In this cross-sectional study, we examined the impact of access to nature on mental health utilization in urban neighborhoods using Texas outpatient encounters data merged with NatureScoreTM (0–100; low to high nature levels) and US census data (household income, education, employment, poverty, and insurance coverage) at the zipcode level. Our sample size included 61 million outpatient encounters across 1169 zipcodes, with 63% women and 30% elderly. A total of 369,344 mental health encounters were identified, with anxiety/stress and depression encounters representing 68.3% and 23.6%, respectively. We found that neighborhoods with a NatureScore of 60+ had lower overall mental health utilization than those below 40 (RR 0.51, 95%CI 0.38–0.69). This relationship persisted for depression, bipolar disorder, and anxiety/stress and in neighborhoods with a NatureScore above 80 (p &lt; 0.001). Compared to neighborhoods with a NatureScore below 40, those above 80 had significantly lower depression (aRR 0.68, 95%CI 0.49–0.95) and bipolar (aRR 0.59, 95%CI 0.36–0.99) health encounters after adjusting for demographic and socioeconomic factors. This novel approach, utilizing NatureScore as a proxy for urban greenness, demonstrates the correlation between a higher NatureScore and reduced mental health utilization. Our findings highlight the importance of integrating nature into our healthcare strategies to promote well-being and mental health.",No methods found.
2024,https://openalex.org/W4391013663,Social Sciences,Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model,"Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation & memory efficiency. For example, Vim is 2.8$\times$ faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248$\times$1248. The results demonstrate that Vim is capable of overcoming the computation & memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models. Code is available at https://github.com/hustvl/Vim.","<method>state space models (SSMs)</method>, <method>Mamba deep learning model</method>, <method>bidirectional state space models</method>, <method>vision transformers</method>, <method>DeiT</method>"
2024,https://openalex.org/W4391135337,Social Sciences,ACCORD (ACcurate COnsensus Reporting Document): A reporting guideline for consensus methods in biomedicine developed via a modified Delphi,"Background In biomedical research, it is often desirable to seek consensus among individuals who have differing perspectives and experience. This is important when evidence is emerging, inconsistent, limited, or absent. Even when research evidence is abundant, clinical recommendations, policy decisions, and priority-setting may still require agreement from multiple, sometimes ideologically opposed parties. Despite their prominence and influence on key decisions, consensus methods are often poorly reported. Our aim was to develop the first reporting guideline dedicated to and applicable to all consensus methods used in biomedical research regardless of the objective of the consensus process, called ACCORD (ACcurate COnsensus Reporting Document). Methods and findings We followed methodology recommended by the EQUATOR Network for the development of reporting guidelines: a systematic review was followed by a Delphi process and meetings to finalize the ACCORD checklist. The preliminary checklist was drawn from the systematic review of existing literature on the quality of reporting of consensus methods and suggestions from the Steering Committee. A Delphi panel ( n = 72) was recruited with representation from 6 continents and a broad range of experience, including clinical, research, policy, and patient perspectives. The 3 rounds of the Delphi process were completed by 58, 54, and 51 panelists. The preliminary checklist of 56 items was refined to a final checklist of 35 items relating to the article title ( n = 1), introduction ( n = 3), methods ( n = 21), results ( n = 5), discussion ( n = 2), and other information ( n = 3). Conclusions The ACCORD checklist is the first reporting guideline applicable to all consensus-based studies. It will support authors in writing accurate, detailed manuscripts, thereby improving the completeness and transparency of reporting and providing readers with clarity regarding the methods used to reach agreement. Furthermore, the checklist will make the rigor of the consensus methods used to guide the recommendations clear for readers. Reporting consensus studies with greater clarity and transparency may enhance trust in the recommendations made by consensus panels.",No methods found.
2024,https://openalex.org/W4391069573,Social Sciences,ChatGPT in healthcare: A taxonomy and systematic review,"The recent release of ChatGPT, a chat bot research project/product of natural language processing (NLP) by OpenAI, stirs up a sensation among both the general public and medical professionals, amassing a phenomenally large user base in a short time. This is a typical example of the 'productization' of cutting-edge technologies, which allows the general public without a technical background to gain firsthand experience in artificial intelligence (AI), similar to the AI hype created by AlphaGo (DeepMind Technologies, UK) and self-driving cars (Google, Tesla, etc.). However, it is crucial, especially for healthcare researchers, to remain prudent amidst the hype. This work provides a systematic review of existing publications on the use of ChatGPT in healthcare, elucidating the 'status quo' of ChatGPT in medical applications, for general readers, healthcare professionals as well as NLP scientists. The large biomedical literature database PubMed is used to retrieve published works on this topic using the keyword 'ChatGPT'. An inclusion criterion and a taxonomy are further proposed to filter the search results and categorize the selected publications, respectively. It is found through the review that the current release of ChatGPT has achieved only moderate or 'passing' performance in a variety of tests, and is unreliable for actual clinical deployment, since it is not intended for clinical applications by design. We conclude that specialized NLP models trained on (bio)medical datasets still represent the right direction to pursue for critical clinical applications.",No methods found.
2024,https://openalex.org/W4390608362,Social Sciences,"Transformative Potential of AI in Healthcare: Definitions, Applications, and Navigating the Ethical Landscape and Public Perspectives","Artificial intelligence (AI) has emerged as a crucial tool in healthcare with the primary aim of improving patient outcomes and optimizing healthcare delivery. By harnessing machine learning algorithms, natural language processing, and computer vision, AI enables the analysis of complex medical data. The integration of AI into healthcare systems aims to support clinicians, personalize patient care, and enhance population health, all while addressing the challenges posed by rising costs and limited resources. As a subdivision of computer science, AI focuses on the development of advanced algorithms capable of performing complex tasks that were once reliant on human intelligence. The ultimate goal is to achieve human-level performance with improved efficiency and accuracy in problem-solving and task execution, thereby reducing the need for human intervention. Various industries, including engineering, media/entertainment, finance, and education, have already reaped significant benefits by incorporating AI systems into their operations. Notably, the healthcare sector has witnessed rapid growth in the utilization of AI technology. Nevertheless, there remains untapped potential for AI to truly revolutionize the industry. It is important to note that despite concerns about job displacement, AI in healthcare should not be viewed as a threat to human workers. Instead, AI systems are designed to augment and support healthcare professionals, freeing up their time to focus on more complex and critical tasks. By automating routine and repetitive tasks, AI can alleviate the burden on healthcare professionals, allowing them to dedicate more attention to patient care and meaningful interactions. However, legal and ethical challenges must be addressed when embracing AI technology in medicine, alongside comprehensive public education to ensure widespread acceptance.","<method>machine learning algorithms</method>, <method>natural language processing</method>, <method>computer vision</method>"
2024,https://openalex.org/W4392851477,Social Sciences,"Generative AI in healthcare: an implementation science informed translational path on application, integration and governance","Abstract Background Artificial intelligence (AI), particularly generative AI, has emerged as a transformative tool in healthcare, with the potential to revolutionize clinical decision-making and improve health outcomes. Generative AI, capable of generating new data such as text and images, holds promise in enhancing patient care, revolutionizing disease diagnosis and expanding treatment options. However, the utility and impact of generative AI in healthcare remain poorly understood, with concerns around ethical and medico-legal implications, integration into healthcare service delivery and workforce utilisation. Also, there is not a clear pathway to implement and integrate generative AI in healthcare delivery. Methods This article aims to provide a comprehensive overview of the use of generative AI in healthcare, focusing on the utility of the technology in healthcare and its translational application highlighting the need for careful planning, execution and management of expectations in adopting generative AI in clinical medicine. Key considerations include factors such as data privacy, security and the irreplaceable role of clinicians’ expertise. Frameworks like the technology acceptance model (TAM) and the Non-Adoption, Abandonment, Scale-up, Spread and Sustainability (NASSS) model are considered to promote responsible integration. These frameworks allow anticipating and proactively addressing barriers to adoption, facilitating stakeholder participation and responsibly transitioning care systems to harness generative AI’s potential. Results Generative AI has the potential to transform healthcare through automated systems, enhanced clinical decision-making and democratization of expertise with diagnostic support tools providing timely, personalized suggestions. Generative AI applications across billing, diagnosis, treatment and research can also make healthcare delivery more efficient, equitable and effective. However, integration of generative AI necessitates meticulous change management and risk mitigation strategies. Technological capabilities alone cannot shift complex care ecosystems overnight; rather, structured adoption programs grounded in implementation science are imperative. Conclusions It is strongly argued in this article that generative AI can usher in tremendous healthcare progress, if introduced responsibly. Strategic adoption based on implementation science, incremental deployment and balanced messaging around opportunities versus limitations helps promote safe, ethical generative AI integration. Extensive real-world piloting and iteration aligned to clinical priorities should drive development. With conscientious governance centred on human wellbeing over technological novelty, generative AI can enhance accessibility, affordability and quality of care. As these models continue advancing rapidly, ongoing reassessment and transparent communication around their strengths and weaknesses remain vital to restoring trust, realizing positive potential and, most importantly, improving patient outcomes.","<method>generative AI</method>, <method>technology acceptance model (TAM)</method>, <method>Non-Adoption, Abandonment, Scale-up, Spread and Sustainability (NASSS) model</method>, <method>implementation science</method>"
2024,https://openalex.org/W4392783116,Social Sciences,Is it harmful or helpful? Examining the causes and consequences of generative AI usage among university students,"Abstract While the discussion on generative artificial intelligence, such as ChatGPT, is making waves in academia and the popular press, there is a need for more insight into the use of ChatGPT among students and the potential harmful or beneficial consequences associated with its usage. Using samples from two studies, the current research examined the causes and consequences of ChatGPT usage among university students. Study 1 developed and validated an eight-item scale to measure ChatGPT usage by conducting a survey among university students (N = 165). Study 2 used a three-wave time-lagged design to collect data from university students (N = 494) to further validate the scale and test the study’s hypotheses. Study 2 also examined the effects of academic workload, academic time pressure, sensitivity to rewards, and sensitivity to quality on ChatGPT usage. Study 2 further examined the effects of ChatGPT usage on students’ levels of procrastination, memory loss, and academic performance. Study 1 provided evidence for the validity and reliability of the ChatGPT usage scale. Furthermore, study 2 revealed that when students faced higher academic workload and time pressure, they were more likely to use ChatGPT. In contrast, students who were sensitive to rewards were less likely to use ChatGPT. Not surprisingly, use of ChatGPT was likely to develop tendencies for procrastination and memory loss and dampen the students’ academic performance. Finally, academic workload, time pressure, and sensitivity to rewards had indirect effects on students’ outcomes through ChatGPT usage.",No methods found.
2024,https://openalex.org/W4392599656,Social Sciences,Generative AI in Medical Practice: In-Depth Exploration of Privacy and Security Challenges,"As advances in artificial intelligence (AI) continue to transform and revolutionize the field of medicine, understanding the potential uses of generative AI in health care becomes increasingly important. Generative AI, including models such as generative adversarial networks and large language models, shows promise in transforming medical diagnostics, research, treatment planning, and patient care. However, these data-intensive systems pose new threats to protected health information. This Viewpoint paper aims to explore various categories of generative AI in health care, including medical diagnostics, drug discovery, virtual health assistants, medical research, and clinical decision support, while identifying security and privacy threats within each phase of the life cycle of such systems (ie, data collection, model development, and implementation phases). The objectives of this study were to analyze the current state of generative AI in health care, identify opportunities and privacy and security challenges posed by integrating these technologies into existing health care infrastructure, and propose strategies for mitigating security and privacy risks. This study highlights the importance of addressing the security and privacy threats associated with generative AI in health care to ensure the safe and effective use of these systems. The findings of this study can inform the development of future generative AI systems in health care and help health care organizations better understand the potential benefits and risks associated with these systems. By examining the use cases and benefits of generative AI across diverse domains within health care, this paper contributes to theoretical discussions surrounding AI ethics, security vulnerabilities, and data privacy regulations. In addition, this study provides practical insights for stakeholders looking to adopt generative AI solutions within their organizations.","<method>generative adversarial networks</method>, <method>large language models</method>"
2024,https://openalex.org/W4390587679,Social Sciences,"A Systematic Review and Meta-Analysis of Artificial Intelligence Tools in Medicine and Healthcare: Applications, Considerations, Limitations, Motivation and Challenges","Artificial intelligence (AI) has emerged as a transformative force in various sectors, including medicine and healthcare. Large language models like ChatGPT showcase AI’s potential by generating human-like text through prompts. ChatGPT’s adaptability holds promise for reshaping medical practices, improving patient care, and enhancing interactions among healthcare professionals, patients, and data. In pandemic management, ChatGPT rapidly disseminates vital information. It serves as a virtual assistant in surgical consultations, aids dental practices, simplifies medical education, and aids in disease diagnosis. A total of 82 papers were categorised into eight major areas, which are G1: treatment and medicine, G2: buildings and equipment, G3: parts of the human body and areas of the disease, G4: patients, G5: citizens, G6: cellular imaging, radiology, pulse and medical images, G7: doctors and nurses, and G8: tools, devices and administration. Balancing AI’s role with human judgment remains a challenge. A systematic literature review using the PRISMA approach explored AI’s transformative potential in healthcare, highlighting ChatGPT’s versatile applications, limitations, motivation, and challenges. In conclusion, ChatGPT’s diverse medical applications demonstrate its potential for innovation, serving as a valuable resource for students, academics, and researchers in healthcare. Additionally, this study serves as a guide, assisting students, academics, and researchers in the field of medicine and healthcare alike.","<method>Large language models</method>, <method>ChatGPT</method>, <method>systematic literature review using the PRISMA approach</method>"
2024,https://openalex.org/W4392193191,Social Sciences,Challenges and barriers of using large language models (LLM) such as ChatGPT for diagnostic medicine with a focus on digital pathology – a recent scoping review,"Abstract Background The integration of large language models (LLMs) like ChatGPT in diagnostic medicine, with a focus on digital pathology, has garnered significant attention. However, understanding the challenges and barriers associated with the use of LLMs in this context is crucial for their successful implementation. Methods A scoping review was conducted to explore the challenges and barriers of using LLMs, in diagnostic medicine with a focus on digital pathology. A comprehensive search was conducted using electronic databases, including PubMed and Google Scholar, for relevant articles published within the past four years. The selected articles were critically analyzed to identify and summarize the challenges and barriers reported in the literature. Results The scoping review identified several challenges and barriers associated with the use of LLMs in diagnostic medicine. These included limitations in contextual understanding and interpretability, biases in training data, ethical considerations, impact on healthcare professionals, and regulatory concerns. Contextual understanding and interpretability challenges arise due to the lack of true understanding of medical concepts and lack of these models being explicitly trained on medical records selected by trained professionals, and the black-box nature of LLMs. Biases in training data pose a risk of perpetuating disparities and inaccuracies in diagnoses. Ethical considerations include patient privacy, data security, and responsible AI use. The integration of LLMs may impact healthcare professionals’ autonomy and decision-making abilities. Regulatory concerns surround the need for guidelines and frameworks to ensure safe and ethical implementation. Conclusion The scoping review highlights the challenges and barriers of using LLMs in diagnostic medicine with a focus on digital pathology. Understanding these challenges is essential for addressing the limitations and developing strategies to overcome barriers. It is critical for health professionals to be involved in the selection of data and fine tuning of the models. Further research, validation, and collaboration between AI developers, healthcare professionals, and regulatory bodies are necessary to ensure the responsible and effective integration of LLMs in diagnostic medicine.",<method>large language models (LLMs)</method>
2024,https://openalex.org/W4392014897,Social Sciences,Comprehensive systematic review of information fusion methods in smart cities and urban environments,"Smart cities result from integrating advanced technologies and intelligent sensors into modern urban infrastructure. The Internet of Things (IoT) and data integration are pivotal in creating interconnected and intelligent urban spaces. In this literature review, we explore the different methods of information fusion used in smart cities, along with their advantages and challenges. However, there are notable challenges in managing diverse data sources, handling large data volumes, and meeting the near-real-time demands of various smart city applications. The review aims to examine smart city applications in detail, incorporating quality evaluation and information fusion techniques and identifying critical issues while outlining promising research directions. In order to accomplish our goal, we conducted a comprehensive search of literature and applied selective criteria. We identified 59 recent studies addressing machine learning (ML) and deep learning (DL) techniques in smart city applications. These studies were obtained from various databases such as ScienceDirect (SD), Scopus, Web of Science (WoS), and IEEE Xplore. The main objective of this study is to provide more detailed insights into smart cities by supplementing existing research. The word cloud visualisation of machine learning/deep learning and information fusion in smart cities papers shows a diverse landscape, covering both technical aspects of artificial intelligence and practical applications in urban settings. Apart from technical exploration, the study also delves into the ethical and privacy implications arising in smart cities. Moreover, it thoroughly examines the challenges that must be addressed to realise this urban revolution's potential fully.","<method>machine learning (ML)</method>, <method>deep learning (DL)</method>"
2024,https://openalex.org/W4391776447,Social Sciences,Theories of motivation: A comprehensive analysis of human behavior drivers,"This paper explores theories of motivation, including instinct theory, arousal theory, incentive theory, intrinsic theory, extrinsic theory, the ARCS model, self-determination theory, expectancy-value theory, and goal-orientation theory. Each theory is described in detail, along with its key concepts, assumptions, and implications for behavior. Intrinsic theory suggests that individuals are motivated by internal factors like enjoyment and satisfaction, while extrinsic theory suggests that external factors like rewards and social pressure drive behavior. Arousal theory says that to feel motivated, people try to keep an optimal level of activation or excitement. Incentive theory suggests that behavior is driven by the promise of rewards or the threat of punishment. The ARCS model, designed to motivate learners, incorporates elements of attention, relevance, confidence, and satisfaction. Self-determination theory proposes that individuals are motivated by their needs for autonomy, competence, and relatedness. The expectation-value theory suggests that behavior is influenced by individuals' beliefs about their ability to succeed and the value they place on the task. The goal-orientation theory suggests that individuals have different goals for engaging in a behavior. By understanding these different theories of motivation, educators, coaches, managers, and individuals may analyze what drives behavior and how to harness it to achieve their goals. In essence, a nuanced comprehension of these diverse motivation theories equips individuals across varied domains with a strategic toolkit to navigate the complex landscape of human behavior, fostering a more profound understanding of what propels actions and how to channel these insights toward the attainment of overarching goals.",No methods found.
2024,https://openalex.org/W4391243055,Social Sciences,Systematic literature review: Quantum machine learning and its applications,"Quantum physics has changed the way we understand our environment, and one of its branches, quantum mechanics, has demonstrated accurate and consistent theoretical results. Quantum computing is the process of performing calculations using quantum mechanics. This field studies the quantum behavior of certain subatomic particles (photons, electrons, etc.) for subsequent use in performing calculations, as well as for large-scale information processing. These advantages are achieved through the use of quantum features, such as entanglement or superposition. These capabilities can give quantum computers an advantage in terms of computational time and cost over classical computers. Nowadays, scientific challenges are impossible to perform by classical computation due to computational complexity (more bytes than atoms in the observable universe) or the time it would take (thousands of years), and quantum computation is the only known answer. However, current quantum devices do not have yet the necessary qubits and are not fault-tolerant enough to achieve these goals. Nonetheless, there are other fields like machine learning, finance, or chemistry where quantum computation could be useful with current quantum devices. This manuscript aims to present a review of the literature published between 2017 and 2023 to identify, analyze, and classify the different types of algorithms used in quantum machine learning and their applications. The methodology follows the guidelines related to Systematic Literature Review methods, such as the one proposed by Kitchenham and other authors in the software engineering field. Consequently, this study identified 94 articles that used quantum machine learning techniques and algorithms and shows their implementation using computational quantum circuits or ansatzs. The main types of found algorithms are quantum implementations of classical machine learning algorithms, such as support vector machines or the k-nearest neighbor model, and classical deep learning algorithms, like quantum neural networks. One of the most relevant applications in the machine learning field is image classification. Many articles, especially within the classification, try to solve problems currently answered by classical machine learning but using quantum devices and algorithms. Even though results are promising, quantum machine learning is far from achieving its full potential. An improvement in quantum hardware is required for this potential to be achieved since the existing quantum computers lack enough quality, speed, and scale to allow quantum computing to achieve its full potential.","<method>support vector machines</method>, <method>k-nearest neighbor model</method>, <method>quantum neural networks</method>"
2024,https://openalex.org/W4391755293,Social Sciences,Education reform and change driven by digital technology: a bibliometric study from a global perspective,"Abstract Amidst the global digital transformation of educational institutions, digital technology has emerged as a significant area of interest among scholars. Such technologies have played an instrumental role in enhancing learner performance and improving the effectiveness of teaching and learning. These digital technologies also ensure the sustainability and stability of education during the epidemic. Despite this, a dearth of systematic reviews exists regarding the current state of digital technology application in education. To address this gap, this study utilized the Web of Science Core Collection as a data source (specifically selecting the high-quality SSCI and SCIE) and implemented a topic search by setting keywords, yielding 1849 initial publications. Furthermore, following the PRISMA guidelines, we refined the selection to 588 high-quality articles. Using software tools such as CiteSpace, VOSviewer, and Charticulator, we reviewed these 588 publications to identify core authors (such as Selwyn, Henderson, Edwards), highly productive countries/regions (England, Australia, USA), key institutions (Monash University, Australian Catholic University), and crucial journals in the field ( Education and Information Technologies , Computers &amp; Education , British Journal of Educational Technology ). Evolutionary analysis reveals four developmental periods in the research field of digital technology education application: the embryonic period, the preliminary development period, the key exploration, and the acceleration period of change. The study highlights the dual influence of technological factors and historical context on the research topic. Technology is a key factor in enabling education to transform and upgrade, and the context of the times is an important driving force in promoting the adoption of new technologies in the education system and the transformation and upgrading of education. Additionally, the study identifies three frontier hotspots in the field: physical education, digital transformation, and professional development under the promotion of digital technology. This study presents a clear framework for digital technology application in education, which can serve as a valuable reference for researchers and educational practitioners concerned with digital technology education application in theory and practice.",No methods found.
2024,https://openalex.org/W4391974599,Social Sciences,"Generative AI for Transformative Healthcare: A Comprehensive Study of Emerging Models, Applications, Case Studies, and Limitations","Generative artificial intelligence (GAI) can be broadly described as an artificial intelligence system capable of generating images, text, and other media types with human prompts. GAI models like ChatGPT, DALL-E, and Bard have recently caught the attention of industry and academia equally. GAI applications span various industries like art, gaming, fashion, and healthcare. In healthcare, GAI shows promise in medical research, diagnosis, treatment, and patient care and is already making strides in real-world deployments. There has yet to be any detailed study concerning the applications and scope of GAI in healthcare. Addressing this research gap, we explore several applications, real-world scenarios, and limitations of GAI in healthcare. We examine how GAI models like ChatGPT and DALL-E can be leveraged to aid in the applications of medical imaging, drug discovery, personalized patient treatment, medical simulation and training, clinical trial optimization, mental health support, healthcare operations and research, medical chatbots, human movement simulation, and a few more applications. Along with applications, we cover four real-world healthcare scenarios that employ GAI: visual snow syndrome diagnosis, molecular drug optimization, medical education, and dentistry. We also provide an elaborate discussion on seven healthcare-customized LLMs like Med-PaLM, BioGPT, DeepHealth, etc.,Since GAI is still evolving, it poses challenges like the lack of professional expertise in decision making, risk of patient data privacy, issues in integrating with existing healthcare systems, and the problem of data bias which are elaborated on in this work along with several other challenges. We also put forward multiple directions for future research in GAI for healthcare.","<method>Generative artificial intelligence (GAI)</method>, <method>GAI models like ChatGPT</method>, <method>GAI models like DALL-E</method>, <method>healthcare-customized LLMs like Med-PaLM</method>, <method>healthcare-customized LLMs like BioGPT</method>, <method>healthcare-customized LLMs like DeepHealth</method>"
2024,https://openalex.org/W4391572037,Social Sciences,Machine Learning Applications in Healthcare: Current Trends and Future Prospects,"The integration of machine learning (ML) in healthcare has witnessed remarkable advancements, transforming the landscape of medical diagnosis, treatment, and overall patient care. This article provides a comprehensive review of the current trends and future prospects of machine learning applications in the healthcare domain.The current landscape is characterized by the utilization of ML algorithms for disease diagnosis and risk prediction, personalized treatment plans, and efficient healthcare resource management. Notable applications include image recognition for radiology and pathology, predictive analytics for disease prognosis, and the development of precision medicine tailored to individual patient profiles.This review explores the evolving role of ML in improving patient outcomes, enhancing clinical decision-making, and optimizing healthcare workflows. It delves into the challenges faced in integrating ML into existing healthcare systems, such as data privacy concerns, interpretability of complex models, and the need for robust validation processes.Additionally, the article discusses future prospects and emerging trends in ML healthcare applications, including the potential for predictive analytics to preemptively identify health issues, the integration of wearable devices and remote monitoring for continuous patient care, and the intersection of ML with genomics for personalized medicine.The overarching goal of this article is to provide healthcare professionals, researchers, and policymakers with insights into the current state of ML applications in healthcare, along with an outlook on the transformative potential that machine learning holds for the future of healthcare delivery and patient outcomes.","<method>machine learning (ML) algorithms</method>, <method>image recognition</method>, <method>predictive analytics</method>"
2024,https://openalex.org/W4392239564,Social Sciences,Human-AI collaboration patterns in AI-assisted academic writing,"Artificial Intelligence (AI) has increasingly influenced higher education, notably in academic writing where AI-powered assisting tools offer both opportunities and challenges. Recently, the rapid growth of generative AI (GAI) has brought its impacts into sharper focus, yet the dynamics of its utilisation in academic writing remain largely unexplored. This paper focuses on examining the nature of human-AI interactions in academic writing, specifically investigating the strategies doctoral students employ when collaborating with a GAI-powered assisting tool. This study involves 626 recorded activities on how ten doctoral students interact with GAI-powered assisting tool during academic writing. AI-driven learning analytics approach was adopted for three layered analyses: (1) data pre-processing and analysis with quantitative content analysis, (2) sequence analysis with Hidden Markov Model (HMM) and hierarchical sequence clustering, and (3) pattern analysis with process mining. Findings indicate that doctoral students engaging in iterative, highly interactive processes with the GAI-powered assisting tool generally achieve better performance in the writing task. In contrast, those who use GAI merely as a supplementary information source, maintaining a linear writing approach, tend to get lower writing performance. This study points to the need for further investigations into human-AI collaboration in learning in higher education, with implications for tailored educational strategies and solutions.","<method>AI-driven learning analytics</method>, <method>quantitative content analysis</method>, <method>Hidden Markov Model (HMM)</method>, <method>hierarchical sequence clustering</method>, <method>process mining</method>"
2024,https://openalex.org/W4391031493,Social Sciences,Board gender diversity and ESG performance: The mediating role of temporal orientation in South Africa context,"Prevailing research on the interaction between board gender diversity (BGD) and Environmental, Social, and Governance (ESG) performance presents equivocal findings, particularly in the context of developing countries. This study ventures into an exploratory examination of this association, situated in the socio-cultural milieu of South Africa, a region where the lower social status of women often leads to a bias towards short-term perspectives. Drawing on the role congruity theory of prejudice toward female leaders, this study aims to investigate the mediating role of short-term orientation (SHRT) in the BGD-ESG relationship. We further explore how the preference of female directors toward SHRT varies depending on their tenure on the board and across family and non-family firms. The empirical findings, drawn from an examination of publicly listed non-financial firms on the Johannesburg Stock Exchange (JSE) from 2015 to 2020, indicate a negative relationship between BGD and ESG, with SHRT predominantly mediating this association. Additionally, the tenure of female directors attenuates their preference for SHRT. Notably, we found the effect of BGD on SHRT is less pronounced in family firms, where the choices of female directors are more aligned with the family firm's long-term orientation. Our findings contribute to both theory and practice by advancing our understanding of the BGD-ESG relationship and providing practical implications for organizations, leaders, and policymakers.",No methods found.
2024,https://openalex.org/W4391505626,Social Sciences,Professionalizing Legal Translator Training: Prospects and Opportunities,"Legal transactions have permeated every aspect of our life. Much of this is accomplished through legal translators who, by their outputs, impact our personal and professional future. That said, this article seeks to tackle the challenges and opportunities in preparing legal translators for professional practice. The article is a quality review in its nature which adopts the descriptive approach. The interactionist perspective is adopted in this present article to examine the challenges faced by and the opportunities offered to legal translators under training. This examination is placed within the context of the rapidly evolving translation industry and its related interdisciplinary research, which covers the technology and legal translation, quality in legal translation, and training pathways for legal translators. The subjective perspective is acknowledged as the human experience is involved to explain the individual phenomena within broader context of legal translation profession. The article draws that there is a need to make changes in the legal translation status because we need to improve the translator’s perception of their role. Moreover, training models adopted to prepare legal translators have to be updated by revising the outdated practices of legal translation, and integrating the social role to face the new challenges as the translators are the intercultural mediators who facilitate the international legal communication.",No methods found.
2024,https://openalex.org/W638137987,Social Sciences,State Formation and Political Legitimacy,"evolution of the state from earlier forms of political organization is associated with revolutionary changes in the structure of inequality. These magnify distinctions in rank and power that outweigh anything previously known in so-called primitive societies. This volume explains how and why people came to accept and even identify themselves with this new form of authority. introduction provides a new theory of legitimacy by synthesizing and uniting earlier theories from psychological, cultural-materialist, rational choice, and Marxist approaches. case studies which follow present a wide range of materials on cultures in both Western and non-Western settings, and across a number of different historical periods. Included are examples from Africa, Asia, Europe, and the New World. Older states such as Ur, Inca, and medieval France are examined along with more contemporary states including Indonesia, Tanzania, and the revolutionary beginnings of the United States. Using a variety of approaches the contributors show in each instance how the state obtained and used its power, then attempted to have its power accepted as the natural order under the protection of supra-naturally ordained authority. No matter how tyrannical or benign, the cases show that state power must be justified by faith and experience that demonstrates its value to the participants. Through such analysis, the book demonstrates that states must be capable of enforcing their rule, but that they cannot deceive populations into accepting state domination. Indeed, the book suggests that social evolution moves toward less coercive rule and increased democratization. Ronald Cohen is a political anthropologist who has taught at the Universities of Toronto, McGill, Northwestern, and Ahmadu Bello, and is on the faculty of the University of Florida. He has carried out field research in Africa, the Arctic and Washington. His major works include The Kanuri of Borno, Dominance and Defiance, Origins of the State,  and a book in preparation on food policy and agricultural transformation in Africa. Judith D. Toland is a lecturer at University College, Northwestern University, and the College of Arts and Sciences, Loyola University of Chicago. She is the director of her own corporate and non-profit consulting firm. She has done fieldwork in Ayacucho, Peru and has written widely on the Inca State.",No methods found.
2024,https://openalex.org/W4391071215,Social Sciences,Automatic assessment of text-based responses in post-secondary education: A systematic review,"Text-based open-ended questions in academic formative and summative assessments help students become deep learners and prepare them to understand concepts for a subsequent conceptual assessment. However, grading text-based questions, especially in large (>50 enrolled students) courses, is tedious and time-consuming for instructors. Text processing models continue progressing with the rapid development of Artificial Intelligence (AI) tools and Natural Language Processing (NLP) algorithms. Especially after breakthroughs in Large Language Models (LLM), there is immense potential to automate rapid assessment and feedback of text-based responses in education. This systematic review adopts a scientific and reproducible literature search strategy based on the PRISMA process using explicit inclusion and exclusion criteria to study text-based automatic assessment systems in post-secondary education, screening 838 papers and synthesizing 93 studies. To understand how text-based automatic assessment systems have been developed and applied in education in recent years, three research questions are considered: 1) What types of automated assessment systems can be identified using input, output, and processing framework? 2) What are the educational focus and research motivations of studies with automated assessment systems? 3) What are the reported research outcomes in automated assessment systems and the next steps for educational applications? All included studies are summarized and categorized according to a proposed comprehensive framework, including the input and output of the system, research motivation, and research outcomes, aiming to answer the research questions accordingly. Additionally, the typical studies of automated assessment systems, research methods, and application domains in these studies are investigated and summarized. This systematic review provides an overview of recent educational applications of text-based assessment systems for understanding the latest AI/NLP developments assisting in text-based assessments in higher education. Findings will particularly benefit researchers and educators incorporating LLMs such as ChatGPT into their educational activities.","<method>Natural Language Processing (NLP) algorithms</method>, <method>Large Language Models (LLM)</method>"
2024,https://openalex.org/W4392695348,Social Sciences,Big AI: Cloud infrastructure dependence and the industrialisation of artificial intelligence,"Critical scholars contend that ‘There is no AI without Big Tech’. This study delves into the substantial role played by major technology conglomerates, including Amazon, Microsoft, and Google (Alphabet), in the ‘industrialisation of artificial intelligence’. This concept encapsulates the shift of AI technologies from the research and development stage to practical, real-world applications across diverse industry sectors, resulting in new dependencies and associated investments. We employ the term ‘Big AI’ to encapsulate the structural convergence of AI and Big Tech, characterised by the profound interdependence of AI with the infrastructure, resources, and investments of these major technology companies. Using a ‘technographic’ approach, our study scrutinises the infrastructural support and investments of Big Tech in the AI sector, focussing on corporate partnerships, acquisitions, and financial investments. Additionally, we conduct a detailed examination of the complete spectrum of cloud platform products and services offered by Amazon, Microsoft, and Google. We demonstrate that AI is not merely an abstract idea but an actual technology stack encompassing infrastructure, models, applications, and an ecosystem of applications and companies relying on this stack. Significantly, these tech giants have seamlessly integrated all three components of the stack into their cloud offerings. Furthermore, they have developed industry-focussed solutions and marketplaces aimed at attracting third-party developers and businesses, fostering the growth of a broader AI ecosystem. This analysis underscores the intricate interdependence between AI and cloud infrastructure, emphasising the industry-specific aspects of cloud AI.",No methods found.
2024,https://openalex.org/W4393072609,Social Sciences,"A comprehensive survey of research towards AI-enabled unmanned aerial systems in pre-, active-, and post-wildfire management","Wildfires have emerged as one of the most destructive natural disasters worldwide, causing catastrophic losses. These losses have underscored the urgent need to improve public knowledge and advance existing techniques in wildfire management. Recently, the use of Artificial Intelligence (AI) in wildfires, propelled by the integration of Unmanned Aerial Vehicles (UAVs) and deep learning models, has created an unprecedented momentum to implement and develop more effective wildfire management. Although existing survey papers have explored learning-based approaches in wildfire, drone use in disaster management, and wildfire risk assessment, a comprehensive review emphasizing the application of AI-enabled UAV systems and investigating the role of learning-based methods throughout the overall workflow of multi-stage wildfire management, including pre-fire (e.g., vision-based vegetation fuel measurement), active-fire (e.g., fire growth modeling), and post-fire tasks (e.g., evacuation planning) is notably lacking. This survey synthesizes and integrates state-of-the-science reviews and research at the nexus of wildfire observations and modeling, AI, and UAVs - topics at the forefront of advances in wildfire management, elucidating the role of AI in performing monitoring and actuation tasks from pre-fire, through the active-fire stage, to post-fire management. To this aim, we provide an extensive analysis of the existing remote sensing systems with a particular focus on the UAV advancements, device specifications, and sensor technologies relevant to wildfire management. We also examine the pre-fire and post-fire management approaches, including fuel monitoring, prevention strategies, as well as evacuation planning, damage assessment, and operation strategies. Additionally, we review and summarize a wide range of computer vision techniques in active-fire management, with an emphasis on Machine Learning (ML), Reinforcement Learning (RL), and Deep Learning (DL) algorithms for wildfire classification, segmentation, detection, and monitoring tasks. Ultimately, we underscore the substantial advancement in wildfire modeling through the integration of cutting-edge AI techniques and UAV-based data, providing novel insights and enhanced predictive capabilities to understand dynamic wildfire behavior.","<method>Machine Learning (ML)</method>, <method>Reinforcement Learning (RL)</method>, <method>Deep Learning (DL)</method>"
2024,https://openalex.org/W4394009485,Social Sciences,AI-Driven Clinical Decision Support Systems: An Ongoing Pursuit of Potential,"Clinical Decision Support Systems (CDSS) are essential tools in contemporary healthcare, enhancing clinicians' decisions and patient outcomes. The integration of artificial intelligence (AI) is now revolutionizing CDSS even further. This review delves into AI technologies transforming CDSS, their applications in healthcare decision-making, associated challenges, and the potential trajectory toward fully realizing AI-CDSS's potential. The review begins by laying the groundwork with a definition of CDSS and its function within the healthcare field. It then highlights the increasingly significant role that AI is playing in enhancing CDSS effectiveness and efficiency, underlining its evolving prominence in shaping healthcare practices. It examines the integration of AI technologies into CDSS, including machine learning algorithms like neural networks and decision trees, natural language processing, and deep learning. It also addresses the challenges associated with AI integration, such as interpretability and bias. We then shift to AI applications within CDSS, with real-life examples of AI-driven diagnostics, personalized treatment recommendations, risk prediction, early intervention, and AI-assisted clinical documentation. The review emphasizes user-centered design in AI-CDSS integration, addressing usability, trust, workflow, and ethical and legal considerations. It acknowledges prevailing obstacles and suggests strategies for successful AI-CDSS adoption, highlighting the need for workflow alignment and interdisciplinary collaboration. The review concludes by summarizing key findings, underscoring AI's transformative potential in CDSS, and advocating for continued research and innovation. It emphasizes the need for collaborative efforts to realize a future where AI-powered CDSS optimizes healthcare delivery and improves patient outcomes.","<method>neural networks</method>, <method>decision trees</method>, <method>natural language processing</method>, <method>deep learning</method>"
2024,https://openalex.org/W4400145965,Social Sciences,A systematic review of trustworthy artificial intelligence applications in natural disasters,"Artificial intelligence (AI) holds significant promise for advancing natural disaster management through the use of predictive models that analyze extensive datasets, identify patterns, and forecast potential disasters. These models facilitate proactive measures such as early warning systems (EWSs), evacuation planning, and resource allocation, addressing the substantial challenges associated with natural disasters. This study offers a comprehensive exploration of trustworthy AI applications in natural disasters, encompassing disaster management, risk assessment, and disaster prediction. This research is underpinned by an extensive review of reputable sources, including Science Direct (SD), Scopus, IEEE Xplore (IEEE), and Web of Science (WoS). Three queries were formulated to retrieve 981 papers from the earliest documented scientific production until February 2024. After meticulous screening, deduplication, and application of the inclusion and exclusion criteria, 108 studies were included in the quantitative synthesis. This study provides a specific taxonomy of AI applications in natural disasters and explores the motivations, challenges, recommendations, and limitations of recent advancements. It also offers an overview of recent techniques and developments in disaster management using explainable artificial intelligence (XAI), data fusion, data mining, machine learning (ML), deep learning (DL), fuzzy logic, and multicriteria decision-making (MCDM). This systematic contribution addresses seven open issues and provides critical solutions through essential insights, laying the groundwork for various future works in trustworthiness AI-based natural disaster management. Despite the potential benefits, challenges persist in the application of AI to natural disaster management. In these contexts, this study identifies several unused and used areas in natural disaster-based AI theory, collects the disaster datasets, ML, and DL techniques, and offers a valuable XAI approach to unravel the complex relationships and dynamics involved and the utilization of data fusion techniques in decision-making processes related to natural disasters. Finally, the study extensively analyzed ethical considerations, bias, and consequences in natural disaster-based AI.","<method>explainable artificial intelligence (XAI)</method>, <method>data fusion</method>, <method>data mining</method>, <method>machine learning (ML)</method>, <method>deep learning (DL)</method>, <method>fuzzy logic</method>, <method>multicriteria decision-making (MCDM)</method>"
2024,https://openalex.org/W4390667862,Social Sciences,Integration of Generative AI Techniques and Applications in Student Behavior and Cognitive Achievement in Arab Higher Education,"The integration of Artificial Intelligence (AI) in higher education has the power to revolutionize the learning experience by fostering engagement, personalization, efficiency, and innovation. AI offers a wide range of exciting possibilities where AI-powered tools enable students to receive tailored feedback and guidance, enabling them to learn at their own pace and excel academically. This research aims to investigate the effects of generative AI techniques and applications on students' cognitive achievement through student behavior. Data was collected through surveys in three Arab countries including Oman, Jordan and Yemen. 768 students from these Arab country's universities were participated in completing surveys randomly. Structure Equation Modeling SEM-PLS was adopted to analysis data. Results reveal that generative AI techniques and applications have positive and significant effects on students' cognitive achievement in Arab higher education institutions. Results also reveal that student behavior enhances the relationship among AI techniques, applications and cognitive achievement. These results highlight the crucial role of AI applications among students in higher education while the integration of this emerging technology is still at the first stage, students' interaction with and utility of these applications show high satisfactory level of their impact on students' behavior and cognitive achievement. This research contributes to literature of generative AI applications giving evidence from Arab region and filling the gap regarding usage of these applications in higher education.","<method>generative AI techniques</method>, <method>Structure Equation Modeling SEM-PLS</method>"
2024,https://openalex.org/W4391527655,Social Sciences,"Micro(nano)plastics in the Human Body: Sources, Occurrences, Fates, and Health Risks","The increasing global attention on micro(nano)plastics (MNPs) is a result of their ubiquity in the water, air, soil, and biosphere, exposing humans to MNPs on a daily basis and threatening human health. However, crucial data on MNPs in the human body, including the sources, occurrences, behaviors, and health risks, are limited, which greatly impedes any systematic assessment of their impact on the human body. To further understand the effects of MNPs on the human body, we must identify existing knowledge gaps that need to be immediately addressed and provide potential solutions to these issues. Herein, we examined the current literature on the sources, occurrences, and behaviors of MNPs in the human body as well as their potential health risks. Furthermore, we identified key knowledge gaps that must be resolved to comprehensively assess the effects of MNPs on human health. Additionally, we addressed that the complexity of MNPs and the lack of efficient analytical methods are the main barriers impeding current investigations on MNPs in the human body, necessitating the development of a standard and unified analytical method. Finally, we highlighted the need for interdisciplinary studies from environmental, biological, medical, chemical, computer, and material scientists to fill these knowledge gaps and drive further research. Considering the inevitability and daily occurrence of human exposure to MNPs, more studies are urgently required to enhance our understanding of their potential negative effects on human health.",No methods found.
2024,https://openalex.org/W4398203672,Social Sciences,Hallucination Rates and Reference Accuracy of ChatGPT and Bard for Systematic Reviews: Comparative Analysis,"Background Large language models (LLMs) have raised both interest and concern in the academic community. They offer the potential for automating literature search and synthesis for systematic reviews but raise concerns regarding their reliability, as the tendency to generate unsupported (hallucinated) content persist. Objective The aim of the study is to assess the performance of LLMs such as ChatGPT and Bard (subsequently rebranded Gemini) to produce references in the context of scientific writing. Methods The performance of ChatGPT and Bard in replicating the results of human-conducted systematic reviews was assessed. Using systematic reviews pertaining to shoulder rotator cuff pathology, these LLMs were tested by providing the same inclusion criteria and comparing the results with original systematic review references, serving as gold standards. The study used 3 key performance metrics: recall, precision, and F1-score, alongside the hallucination rate. Papers were considered “hallucinated” if any 2 of the following information were wrong: title, first author, or year of publication. Results In total, 11 systematic reviews across 4 fields yielded 33 prompts to LLMs (3 LLMs×11 reviews), with 471 references analyzed. Precision rates for GPT-3.5, GPT-4, and Bard were 9.4% (13/139), 13.4% (16/119), and 0% (0/104) respectively (P&lt;.001). Recall rates were 11.9% (13/109) for GPT-3.5 and 13.7% (15/109) for GPT-4, with Bard failing to retrieve any relevant papers (P&lt;.001). Hallucination rates stood at 39.6% (55/139) for GPT-3.5, 28.6% (34/119) for GPT-4, and 91.4% (95/104) for Bard (P&lt;.001). Further analysis of nonhallucinated papers retrieved by GPT models revealed significant differences in identifying various criteria, such as randomized studies, participant criteria, and intervention criteria. The study also noted the geographical and open-access biases in the papers retrieved by the LLMs. Conclusions Given their current performance, it is not recommended for LLMs to be deployed as the primary or exclusive tool for conducting systematic reviews. Any references generated by such models warrant thorough validation by researchers. The high occurrence of hallucinations in LLMs highlights the necessity for refining their training and functionality before confidently using them for rigorous academic purposes.","<method>Large language models (LLMs)</method>, <method>ChatGPT</method>, <method>Bard (Gemini)</method>, <method>GPT-3.5</method>, <method>GPT-4</method>"
2024,https://openalex.org/W4392627614,Social Sciences,How to conduct a bibliometric content analysis: Guidelines and contributions of content co‐occurrence or co‐word literature reviews,"Abstract Literature reviews summarize existing literature, uncover research gaps, and offer future research directions, thus aiding in theoretical and methodological development. Informetric research including bibliometric, scientometric, webometric, cybermetric, patentometric, and altmetric methods are becoming increasingly prevalent in conducting literature review studies. Looking at the common informetric literature review methods—citation, co‐citation, co‐author, bibliographic coupling, and content co‐occurrence analyses, this study aims to serve as a guide in using content co‐occurrence also known as co‐word analysis to conduct literature reviews. This study outlines a variety of informetric research methods and how they are utilized to conduct review and evidence‐based conceptual studies. In addition to the analyses, the study highlights different informetric software packages like Bibliometrix, Biblioshiny, Leximancer, NVivo, and CiteSpace including their comparison. The study further discusses contributions of algorithm‐based content analyses including offering taxonomies, definitions, classifications, typologies, comparisons, and theoretical development to constitute integrative literature reviews. Finally, this study offers step‐by‐step guidelines for conducting a review study using VOSviewer content co‐occurrence analysis while providing a systems view of informetric research in social science. The study also notes the emergence of generative artificial intelligence (AI) like Open AI's ChatGPT, Google's Bard, Elicit, Scite, Research Rabbit, and ChatPDF among others, and its potential in contributing to the literature review methods and, as such, being an interesting direction for future research.","<method>content co‐occurrence analysis (co‐word analysis)</method>, <method>algorithm‐based content analyses</method>, <method>generative artificial intelligence (AI)</method>"
2024,https://openalex.org/W4390674594,Social Sciences,Open access research outputs receive more diverse citations,"Abstract The goal of open access is to allow more people to read and use research outputs. An observed association between highly cited research outputs and open access has been claimed as evidence of increased usage of the research, but this remains controversial. A higher citation count also does not necessarily imply wider usage such as citations by authors from more places. A knowledge gap exists in our understanding of who gets to use open access research outputs and where users are located. Here we address this gap by examining the association between an output’s open access status and the diversity of research outputs that cite it. By analysing large-scale bibliographic data from 2010 to 2019, we found a robust association between open access and increased diversity of citation sources by institutions, countries, subregions, regions, and fields of research, across outputs with both high and medium–low citation counts. Open access through disciplinary or institutional repositories showed a stronger effect than open access via publisher platforms. This study adds a new perspective to our understanding of how citations can be used to explore the effects of open access. It also provides new evidence at global scale of the benefits of open access as a mechanism for widening the use of research and increasing the diversity of the communities that benefit from it.",No methods found.
2024,https://openalex.org/W4390584313,Social Sciences,A Conceptual Model for Inclusive Technology: Advancing Disability Inclusion through Artificial Intelligence,"Artificial intelligence (AI) has ushered in transformative changes, championing inclusion and accessibility for individuals with disabilities. This article delves into the remarkable AI-driven solutions that have revolutionized their lives across various domains. From assistive technologies such as voice recognition and AI-powered smart glasses catering to diverse needs, to healthcare benefiting from early disease detection algorithms and wearable devices that monitor vital signs and alert caregivers in emergencies, AI has steered in significant enhancements. Moreover, AI-driven prosthetics and exoskeletons have substantially improved mobility for those with limb impairments. The realm of education has not been left untouched, with AI tools creating inclusive learning environments that adapt to individual learning styles, paving the way for academic success among students with disabilities. However, the boundless potential of AI also presents ethical concerns and challenges. Issues like safeguarding data privacy, mitigating algorithmic bias, and bridging the digital divide must be thoughtfully addressed to fully harness AI’s potential in empowering individuals with disabilities. To complement these achievements, a robust conceptual model for AI disability inclusion serves as the theoretical framework, guiding the development of tailored AI solutions. By striking a harmonious balance between innovation and ethics, AI has the power to significantly enhance the overall quality of life for individuals with disabilities across a spectrum of vital areas.","<method>voice recognition</method>, <method>early disease detection algorithms</method>"
2024,https://openalex.org/W4390665705,Social Sciences,The ethical implications of using generative chatbots in higher education,"Incorporating artificial intelligence (AI) into education, specifically through generative chatbots, can transform teaching and learning for education professionals in both administrative and pedagogical ways. However, the ethical implications of using generative chatbots in education must be carefully considered. Ethical concerns about advanced chatbots have yet to be explored in the education sector. This short article introduces the ethical concerns associated with introducing platforms such as ChatGPT in education. The article outlines how handling sensitive student data by chatbots presents significant privacy challenges, thus requiring adherence to data protection regulations, which may not always be possible. It highlights the risk of algorithmic bias in chatbots, which could perpetuate societal biases, which can be problematic. The article also examines the balance between fostering student autonomy in learning and the potential impact on academic self-efficacy, noting the risk of over-reliance on AI for educational purposes. Plagiarism continues to emerge as a critical ethical concern, with AI-generated content threatening academic integrity. The article advocates for comprehensive measures to address these ethical issues, including clear policies, advanced plagiarism detection techniques, and innovative assessment methods. By addressing these ethical challenges, the article argues that educators, AI developers, policymakers, and students can fully harness the potential of chatbots in education, creating a more inclusive, empowering, and ethically sound educational future.",No methods found.
2024,https://openalex.org/W4391528827,Social Sciences,Deep learning-aided decision support for diagnosis of skin disease across skin tones,"Abstract Although advances in deep learning systems for image-based medical diagnosis demonstrate their potential to augment clinical decision-making, the effectiveness of physician–machine partnerships remains an open question, in part because physicians and algorithms are both susceptible to systematic errors, especially for diagnosis of underrepresented populations. Here we present results from a large-scale digital experiment involving board-certified dermatologists ( n = 389) and primary-care physicians ( n = 459) from 39 countries to evaluate the accuracy of diagnoses submitted by physicians in a store-and-forward teledermatology simulation. In this experiment, physicians were presented with 364 images spanning 46 skin diseases and asked to submit up to four differential diagnoses. Specialists and generalists achieved diagnostic accuracies of 38% and 19%, respectively, but both specialists and generalists were four percentage points less accurate for the diagnosis of images of dark skin as compared to light skin. Fair deep learning system decision support improved the diagnostic accuracy of both specialists and generalists by more than 33%, but exacerbated the gap in the diagnostic accuracy of generalists across skin tones. These results demonstrate that well-designed physician–machine partnerships can enhance the diagnostic accuracy of physicians, illustrating that success in improving overall diagnostic accuracy does not necessarily address bias.",<method>deep learning system decision support</method>
2024,https://openalex.org/W4394762924,Social Sciences,Exploring the Impact of Artificial Intelligence on Global Health and Enhancing Healthcare in Developing Nations,"Background: Artificial intelligence (AI), which combines computer science with extensive datasets, seeks to mimic human-like intelligence. Subsets of AI are being applied in almost all fields of medicine and surgery. Aim: This review focuses on the applications of AI in healthcare settings in developing countries, designed to underscore its significance by comprehensively outlining the advancements made thus far, the shortcomings encountered in AI applications, the present status of AI integration, persistent challenges, and innovative strategies to surmount them. Methodology: Articles from PubMed, Google Scholar, and Cochrane were searched from 2000 to 2023 with keywords including AI and healthcare, focusing on multiple medical specialties. Results: The increasing role of AI in diagnosis, prognosis prediction, and patient management, as well as hospital management and community healthcare, has made the overall healthcare system more efficient, especially in the high patient load setups and resource-limited areas of developing countries where patient care is often compromised. However, challenges, including low adoption rates and the absence of standardized guidelines, high installation and maintenance costs of equipment, poor transportation and connectivvity issues hinder AI’s full use in healthcare. Conclusion: Despite these challenges, AI holds a promising future in healthcare. Adequate knowledge and expertise of healthcare professionals for the use of AI technology in healthcare is imperative in developing nations.",No methods found.
2024,https://openalex.org/W4394806371,Social Sciences,The TESCREAL bundle: Eugenics and the promise of utopia through artificial general intelligence,"The stated goal of many organizations in the field of artificial intelligence (AI) is to develop artificial general intelligence (AGI), an imagined system with more intelligence than anything we have ever seen. Without seriously questioning whether such a system can and should be built, researchers are working to create “safe AGI” that is “beneficial for all of humanity.” We argue that, unlike systems with specific applications which can be evaluated following standard engineering principles, undefined systems like “AGI” cannot be appropriately tested for safety. Why, then, is building AGI often framed as an unquestioned goal in the field of AI? In this paper, we argue that the normative framework that motivates much of this goal is rooted in the Anglo-American eugenics tradition of the twentieth century. As a result, many of the very same discriminatory attitudes that animated eugenicists in the past (e.g., racism, xenophobia, classism, ableism, and sexism) remain widespread within the movement to build AGI, resulting in systems that harm marginalized groups and centralize power, while using the language of “safety” and “benefiting humanity” to evade accountability. We conclude by urging researchers to work on defined tasks for which we can develop safety protocols, rather than attempting to build a presumably all-knowing system such as AGI.",No methods found.
2024,https://openalex.org/W4390978664,Social Sciences,Twenty years of network meta‐analysis: Continuing controversies and recent developments,"Abstract Network meta‐analysis (NMA) is an extension of pairwise meta‐analysis (PMA) which combines evidence from trials on multiple treatments in connected networks. NMA delivers internally consistent estimates of relative treatment efficacy, needed for rational decision making. Over its first 20 years NMA's use has grown exponentially, with applications in both health technology assessment (HTA), primarily re‐imbursement decisions and clinical guideline development, and clinical research publications. This has been a period of transition in meta‐analysis, first from its roots in educational and social psychology, where large heterogeneous datasets could be explored to find effect modifiers, to smaller pairwise meta‐analyses in clinical medicine on average with less than six studies. This has been followed by narrowly‐focused estimation of the effects of specific treatments at specific doses in specific populations in sparse networks, where direct comparisons are unavailable or informed by only one or two studies. NMA is a powerful and well‐established technique but, in spite of the exponential increase in applications, doubts about the reliability and validity of NMA persist. Here we outline the continuing controversies, and review some recent developments. We suggest that heterogeneity should be minimized, as it poses a threat to the reliability of NMA which has not been fully appreciated, perhaps because it has not been seen as a problem in PMA. More research is needed on the extent of heterogeneity and inconsistency in datasets used for decision making, on formal methods for making recommendations based on NMA, and on the further development of multi‐level network meta‐regression.",No methods found.
2024,https://openalex.org/W4394681533,Social Sciences,REVIEWING THE IMPACT OF HEALTH INFORMATION TECHNOLOGY ON HEALTHCARE MANAGEMENT EFFICIENCY,"This research paper explores the intricate relationship between Health Information Technology (HIT) and healthcare management efficiency, investigating current trends, emerging technologies, and their potential implications. The study encompasses a thorough literature review, highlighting the impact of HIT on operational and clinical aspects of healthcare delivery. Key findings reveal the transformative role of technology in streamlining administrative processes, improving communication, and enhancing overall patient care. Ethical considerations, patient privacy, and regulation compliance are crucial factors in successfully implementing HIT. Looking towards the future, the paper anticipates the integration of emerging technologies such as Artificial Intelligence, Blockchain, and the Internet of Things, signalling a paradigm shift in healthcare management. While acknowledging the potential benefits, the research also underscores the importance of ethical frameworks, transparency, and user-centred design in adopting these technologies. The study concludes with reflections on the limitations of the research, suggesting avenues for future exploration. Recommendations emphasize the need for ongoing research, longitudinal studies, and a global perspective to ensure healthcare organizations effectively leverage technology while maintaining ethical standards. The findings of this research carry implications for healthcare practitioners, policymakers, and technology innovators, encouraging a strategic and ethical approach to the ever-evolving landscape of health information technology.&#x0D; Keywords: Health Information Technology, Healthcare Management Efficiency, Emerging Technologies, Ethical Considerations, Patient Privacy.",<method>Artificial Intelligence</method>
2024,https://openalex.org/W4390739507,Social Sciences,AI in Decision Making: Transforming Business Strategies,"This paper delves into the transformative impact of Artificial Intelligence (AI) on strategic business decision-making, offering a nuanced perspective on how AI is reshaping the corporate world. The primary purpose of this study is to explore the emergence and evolution of AI within the realm of business strategy, examining its role in disrupting traditional decision models and enhancing business agility. This study systematically analyzes academic and industry sources through a meticulous literature review, providing a comprehensive understanding of AI’s multifaceted role in business. The methodology adopted is a systematic literature review, which serves as a robust framework for evaluating source credibility and synthesizing insights. This approach enables a thorough examination of AI’s integration into business management, its influence on corporate performance metrics, and its potential in fostering inclusive business practices. The study also addresses the unique challenges and opportunities presented by AI in the business context. Key findings reveal that AI is not merely a technological tool but a strategic asset that significantly redefines business decision-making. The integration of AI into business strategies demonstrates substantial potential in enhancing corporate performance and promoting sustainable business practices. The study concludes that AI is a cornerstone in business evolution, offering unparalleled opportunities for innovation and efficiency. Recommendations advocate for a balanced approach to AI integration, emphasizing the need for businesses to align AI with their core values and strategic objectives. As AI continues to evolve, its role in business decision-making is expected to shape the corporate landscape significantly.",No methods found.
2024,https://openalex.org/W4392764062,Social Sciences,"When artificial intelligence substitutes humans in higher education: the cost of loneliness, student success, and retention","Artificial intelligence (AI) may be the new-new-norm in a post-pandemic learning environment. There is a growing number of university students using AI like ChatGPT and Bard to support their academic experience. Much of the AI in higher education research to date has focused on academic integrity and matters of authorship; yet, there may be unintended consequences beyond these concerns for students. That is, there may be people who reduce their formal social interactions while using these tools. This study evaluates 387 university students and their relationship to – and with – artificial intelligence large-language model-based tools. Using structural equation modelling, the study finds evidence that while AI chatbots designed for information provision may be associated with student performance, when social support, psychological wellbeing, loneliness, and sense of belonging are considered it has a net negative effect on achievement. This study tests an AI-specific form of social support, and the cost it may pose to student success, wellbeing, and retention. Indeed, while AI chatbot usage may be associated with poorer social outcomes, human-substitution activity that may be occurring when a student chooses to seek support from an AI rather than a human (e.g. a librarian, professor, or student advisor) may pose interesting learning and teaching policy implications. We explore the implications of this from the lens of student success and belonging.",<method>structural equation modelling</method>
2024,https://openalex.org/W4398183308,Social Sciences,The applications of nature‐inspired algorithms in Internet of Things‐based healthcare service: A systematic literature review,"Abstract Nature‐inspired algorithms revolve around the intersection of nature‐inspired algorithms and the IoT within the healthcare domain. This domain addresses the emerging trends and potential synergies between nature‐inspired computational approaches and IoT technologies for advancing healthcare services. Our research aims to fill gaps in addressing algorithmic integration challenges, real‐world implementation issues, and the efficacy of nature‐inspired algorithms in IoT‐based healthcare. We provide insights into the practical aspects and limitations of such applications through a systematic literature review. Specifically, we address the need for a comprehensive understanding of the applications of nature‐inspired algorithms in IoT‐based healthcare, identifying gaps such as the lack of standardized evaluation metrics and studies on integration challenges and security considerations. By bridging these gaps, our paper offers insights and directions for future research in this domain, exploring the diverse landscape of nature‐inspired algorithms in healthcare. Our chosen methodology is a Systematic Literature Review (SLR) to investigate related papers rigorously. Categorizing these algorithms into groups such as genetic algorithms, particle swarm optimization, cuckoo algorithms, ant colony optimization, other approaches, and hybrid methods, we employ meticulous classification based on critical criteria. MATLAB emerges as the predominant programming language, constituting 37.9% of cases, showcasing a prevalent choice among researchers. Our evaluation emphasizes adaptability as the paramount parameter, accounting for 18.4% of considerations. By shedding light on attributes, limitations, and potential directions for future research and development, this review aims to contribute to a comprehensive understanding of nature‐inspired algorithms in the dynamic landscape of IoT‐based healthcare services.","<method>genetic algorithms</method>, <method>particle swarm optimization</method>, <method>cuckoo algorithms</method>, <method>ant colony optimization</method>, <method>hybrid methods</method>"
2024,https://openalex.org/W4390637043,Social Sciences,Blockchain meets machine learning: a survey,"Abstract Blockchain and machine learning are two rapidly growing technologies that are increasingly being used in various industries. Blockchain technology provides a secure and transparent method for recording transactions, while machine learning enables data-driven decision-making by analyzing large amounts of data. In recent years, researchers and practitioners have been exploring the potential benefits of combining these two technologies. In this study, we cover the fundamentals of blockchain and machine learning and then discuss their integrated use in finance, medicine, supply chain, and security, including a literature review and their contribution to the field such as increased security, privacy, and decentralization. Blockchain technology enables secure and transparent decentralized record-keeping, while machine learning algorithms can analyze vast amounts of data to derive valuable insights. Together, they have the potential to revolutionize industries by enhancing efficiency through automated and trustworthy processes, enabling data-driven decision-making, and strengthening security measures by reducing vulnerabilities and ensuring the integrity of information. However, there are still some important challenges to be handled prior to the common use of blockchain and machine learning such as security issues, strategic planning, information processing, and scalable workflows. Nevertheless, until the difficulties that have been identified are resolved, their full potential will not be achieved.",<method>machine learning algorithms</method>
2024,https://openalex.org/W4392106982,Social Sciences,AI hype as a cyber security risk: the moral responsibility of implementing generative AI in business,"Abstract This paper examines the ethical obligations companies have when implementing generative Artificial Intelligence (AI). We point to the potential cyber security risks companies are exposed to when rushing to adopt generative AI solutions or buying into “AI hype”. While the benefits of implementing generative AI solutions for business have been widely touted, the inherent risks associated have been less well publicised. There are growing concerns that the race to integrate generative AI is not being accompanied by adequate safety measures. The rush to buy into the hype of generative AI and not fall behind the competition is potentially exposing companies to broad and possibly catastrophic cyber-attacks or breaches. In this paper, we outline significant cyber security threats generative AI models pose, including potential ‘backdoors’ in AI models that could compromise user data or the risk of ‘poisoned’ AI models producing false results. In light of these the cyber security concerns, we discuss the moral obligations of implementing generative AI into business by considering the ethical principles of beneficence, non-maleficence, autonomy, justice, and explicability. We identify two examples of ethical concern, overreliance and over-trust in generative AI, both of which can negatively influence business decisions, leaving companies vulnerable to cyber security threats. This paper concludes by recommending a set of checklists for ethical implementation of generative AI in business environment to minimise cyber security risk based on the discussed moral responsibilities and ethical concern.",No methods found.
2024,https://openalex.org/W4393119757,Social Sciences,Unmasking bias in artificial intelligence: a systematic review of bias detection and mitigation strategies in electronic health record-based models,"Abstract Objectives Leveraging artificial intelligence (AI) in conjunction with electronic health records (EHRs) holds transformative potential to improve healthcare. However, addressing bias in AI, which risks worsening healthcare disparities, cannot be overlooked. This study reviews methods to handle various biases in AI models developed using EHR data. Materials and Methods We conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and Meta-analyses guidelines, analyzing articles from PubMed, Web of Science, and IEEE published between January 01, 2010 and December 17, 2023. The review identified key biases, outlined strategies for detecting and mitigating bias throughout the AI model development, and analyzed metrics for bias assessment. Results Of the 450 articles retrieved, 20 met our criteria, revealing 6 major bias types: algorithmic, confounding, implicit, measurement, selection, and temporal. The AI models were primarily developed for predictive tasks, yet none have been deployed in real-world healthcare settings. Five studies concentrated on the detection of implicit and algorithmic biases employing fairness metrics like statistical parity, equal opportunity, and predictive equity. Fifteen studies proposed strategies for mitigating biases, especially targeting implicit and selection biases. These strategies, evaluated through both performance and fairness metrics, predominantly involved data collection and preprocessing techniques like resampling and reweighting. Discussion This review highlights evolving strategies to mitigate bias in EHR-based AI models, emphasizing the urgent need for both standardized and detailed reporting of the methodologies and systematic real-world testing and evaluation. Such measures are essential for gauging models’ practical impact and fostering ethical AI that ensures fairness and equity in healthcare.","<method>resampling</method>, <method>reweighting</method>"
2024,https://openalex.org/W4396707866,Social Sciences,Strategies for Integrating Generative AI into Higher Education: Navigating Challenges and Leveraging Opportunities,"The recent emergence of generative AI (GenAI) tools such as ChatGPT, Midjourney, and Gemini have introduced revolutionary capabilities that are predicted to transform numerous facets of society fundamentally. In higher education (HE), the advent of GenAI presents a pivotal moment that may profoundly alter learning and teaching practices in aspects such as inaccuracy, bias, overreliance on technology and algorithms, and limited access to educational AI resources that require in-depth investigation. To evaluate the implications of adopting GenAI in HE, a team of academics and field experts have co-authored this paper, which analyzes the potential for the responsible integration of GenAI into HE and provides recommendations about this integration. This paper recommends strategies for integrating GenAI into HE to create the following positive outcomes: raise awareness about disruptive change, train faculty, change teaching and assessment practices, partner with students, impart AI learning literacies, bridge the digital divide, and conduct applied research. Finally, we propose four preliminary scale levels of a GenAI adoption for faculty. At each level, we suggest courses of action to facilitate progress to the next stage in the adoption of GenAI. This study offers a valuable set of recommendations to decision-makers and faculty, enabling them to prepare for the responsible and judicious integration of GenAI into HE.",No methods found.
2024,https://openalex.org/W4391147961,Social Sciences,An Incentive Mechanism-Based Minimum Adjustment Consensus Model Under Dynamic Trust Relationship,"In traditional group decision making, the inconsistent experts are usually forced to make compromises toward the group opinion to increase the group consensus level. However, the strategy of reaching group consensus via an incentive mechanism encouraging adjustment of preferences is more effective than forcing, which is the aim of this article. Specifically, this article establishes a novel incentive mechanism to support group consensus under dynamic trust relationship. First, the supremum and infimum incentives-based rule driven by trust relationship is defined. Based on the assumption that if incentive conditions are met, then experts will be willing to adjust their preferences, the incentive behavior-driven minimum adjustment consensus model is developed to generate optimal incentive-based recommendation preferences. Thus, the proposed incentive mechanism can effectively reduce the preference adjustment cost and promote group consensus reaching. Third, the updated trust relationships between experts are shown to be strengthen by the proposed incentive-driven preference revision. Consequently, the optimization model based on trust interaction relationship is constructed to obtain the final group preference matrix. Finally, a supplier selection case of high-end medical equipment is provided to illustrate the proposed method and show the rationality and advantages of the proposed methodology with both a sensitivity analysis and a comparison analysis.",No methods found.
2024,https://openalex.org/W4393359157,Social Sciences,"A systematic review of AI literacy conceptualization, constructs, and implementation and assessment efforts (2019–2023)","The explosion of AI across all facets of society has given rise to the need for AI education across domains and levels. AI literacy has become an important concept in the current technological landscape, emphasizing the need for individuals to acquire the necessary knowledge and skills to engage with AI systems. This systematic review examined 47 articles published between 2019 to 2023, focusing on recent work to capture new insights and initiatives given the burgeoning of the literature on this topic. In the initial stage, we explored the dataset to identify the themes covered by the selected papers and the target population for AI literacy efforts. We identified that the articles broadly contributed to one of the following themes: a) conceptualizing AI literacy, b) prompting AI literacy efforts, and c) developing AI literacy assessment instruments. We also found that a range of populations, from pre-K students to adults in the workforce, were targeted. In the second stage, we conducted a thorough content analysis to synthesize six key constructs of AI literacy: Recognize, Know and Understand, Use and Apply, Evaluate, Create, and Navigate Ethically. We then applied this framework to categorize a range of empirical studies and identify the prevalence of each construct across the studies. We subsequently review assessment instruments developed for AI literacy and discuss them. The findings of this systematic review are relevant for formal education and workforce preparation and advancement, empowering individuals to leverage AI and drive innovation.",No methods found.
2024,https://openalex.org/W4396723652,Social Sciences,"Analysis of college students' attitudes toward the use of ChatGPT in their academic activities: effect of intent to use, verification of information and responsible use","Abstract Background In recent years, the use of artificial intelligence (AI) in education has increased worldwide. The launch of the ChatGPT-3 posed great challenges for higher education, given its popularity among university students. The present study aimed to analyze the attitudes of university students toward the use of ChatGPTs in their academic activities. Method This study was oriented toward a quantitative approach and had a nonexperimental design. An online survey was administered to the 499 participants. Results The findings of this study revealed a significant association between various factors and attitudes toward the use of the ChatGPT. The higher beta coefficients for responsible use (β=0.806***), the intention to use frequently (β=0.509***), and acceptance (β=0.441***) suggested that these are the strongest predictors of a positive attitude toward ChatGPT. The presence of positive emotions (β=0.418***) also plays a significant role. Conversely, risk (β=-0.104**) and boredom (β=-0.145**) demonstrate a negative yet less decisive influence. These results provide an enhanced understanding of how students perceive and utilize ChatGPTs, supporting a unified theory of user behavior in educational technology contexts. Conclusion Ease of use, intention to use frequently, acceptance, and intention to verify information influenced the behavioral intention to use ChatGPT responsibly. On the one hand, this study provides suggestions for HEIs to improve their educational curricula to take advantage of the potential benefits of AI and contribute to AI literacy.",No methods found.
2024,https://openalex.org/W4391268329,Social Sciences,"Entrepreneurial innovations and trends: A global review: Examining emerging trends, challenges, and opportunities in the field of entrepreneurship, with a focus on how technology and globalization are shaping new business ventures","This scholarly inquiry delves into the intricate dynamics of entrepreneurship in the global economy, focusing on the pivotal roles of technological advancements and globalization. The study's primary aim is to unravel the complexities and evolving nature of entrepreneurial success in the 21st century, examining how technology and global integration shape new business ventures. Employing a qualitative and theoretical approach, the research methodically synthesizes a wide array of literature, offering a comprehensive analysis of the current entrepreneurial landscape. The findings of the study reveal a multifaceted entrepreneurial ecosystem, significantly influenced by digital transformation and global market integration. It highlights the emergence of social entrepreneurship and the critical role of technological innovations in developing new business models. The research also uncovers the dual nature of globalization, presenting both opportunities and challenges for entrepreneurs. A sector-specific analysis further elucidates the variations in entrepreneurial growth and decline, emphasizing the significance of geographic factors in entrepreneurial outcomes. Conclusively, the study underscores the necessity for entrepreneurs to adapt to rapidly changing business environments, advocating for strategic innovation and adaptability as essential tools for sustainability and growth. It posits that the future of entrepreneurship will be characterized by increased digitalization, innovation, and a focus on sustainable and inclusive growth. Policy implications suggest the need for inclusive, technology-integrated strategies to support entrepreneurial ventures. Recommendations for future entrepreneurs centre on embracing digitalization, fostering an innovative mindset, and leveraging global opportunities while navigating associated challenges.",No methods found.
2024,https://openalex.org/W4391750864,Social Sciences,Efficacy of virtual reality-based training programs and games on the improvement of cognitive disorders in patients: a systematic review and meta-analysis,"Abstract Introduction Cognitive impairments present challenges for patients, impacting memory, attention, and problem-solving abilities. Virtual reality (VR) offers innovative ways to enhance cognitive function and well-being. This study explores the effects of VR-based training programs and games on improving cognitive disorders. Methods PubMed, Scopus, and Web of Science were systematically searched until May 20, 2023. Two researchers selected and extracted data based on inclusion and exclusion criteria, resolving disagreements through consultation with two other authors. Inclusion criteria required studies of individuals with any cognitive disorder engaged in at least one VR-based training session, reporting cognitive impairment data via scales like the MMSE. Only English-published RCTs were considered, while exclusion criteria included materials not primarily focused on the intersection of VR and cognitive disorders. The risk of bias in the included studies was assessed using the MMAT tool. Publication bias was assessed using funnel plots and Egger’s test. The collected data were utilized to calculate the standardized mean differences (Hedges’s g) between the treatment and control groups. The heterogeneity variance was estimated using the Q test and I2 statistic. The analysis was conducted using Stata version 17.0. Results Ten studies were included in the analysis out of a total of 3,157 retrieved articles. VR had a statistically significant improvement in cognitive impairments among patients (Hedges’s g = 0.42, 95% CI: 0.15, 0.68; p _value = 0.05). games (Hedges’s g = 0.61, 95% CI: 0.30, 0.39; p _value = 0.20) had a more significant impact on cognitive impairment improvement compared to cognitive training programs (Hedges’s g = 0.29, 95% CI: -0.11, 0.69; p _value = 0.24). The type of VR intervention was a significant moderator of the heterogeneity between studies. Conclusion VR-based interventions have demonstrated promise in enhancing cognitive function and addressing cognitive impairment, highlighting their potential as valuable tools in improving care for individuals with cognitive disorders. The findings underscore the relevance of incorporating virtual reality into therapeutic approaches for cognitive disorders.",No methods found.
2024,https://openalex.org/W4396652595,Social Sciences,Geological survey techniques and carbon storage: Optimizing renewable energy site selection and carbon sequestration,"Geological survey techniques play a crucial role in optimizing site selection for renewable energy projects and identifying suitable locations for carbon storage to mitigate climate change. This abstract provides an overview of how geological survey techniques can be used to achieve these objectives. Renewable energy development, particularly solar and wind power, requires careful site selection to maximize energy generation efficiency and minimize environmental impacts. Geological surveys are instrumental in assessing factors such as subsurface geology, topography, soil composition, and hydrological conditions. These surveys help identify suitable locations with optimal wind or solar resources and geologic conditions for infrastructure development. Additionally, geological surveys are essential for identifying suitable sites for carbon storage, a critical component of carbon capture and storage (CCS) technologies aimed at reducing greenhouse gas emissions. Geological formations, such as deep saline aquifers, depleted oil and gas reservoirs, and unmineable coal seams, can serve as storage reservoirs for captured carbon dioxide (CO2). Geological surveys help characterize these formations to assess their suitability for long-term CO2 storage, considering factors such as porosity, permeability, and sealing integrity. Optimizing site selection for renewable energy projects and carbon storage requires a comprehensive understanding of subsurface geology and environmental conditions. Advanced geological survey techniques, such as seismic imaging, remote sensing, and geophysical surveys, are essential for acquiring detailed subsurface data. These techniques enable scientists and engineers to assess site suitability, evaluate risks, and design effective mitigation measures. In conclusion, geological survey techniques are invaluable tools for optimizing site selection for renewable energy projects and identifying suitable locations for carbon storage. By leveraging these techniques, stakeholders can make informed decisions that promote sustainable energy development and mitigate the impacts of climate change.",No methods found.
2024,https://openalex.org/W4399387113,Social Sciences,ChatGPT prompts for generating multiple-choice questions in medical education and evidence on their validity: a literature review,"Abstract ChatGPT’s role in creating multiple-choice questions (MCQs) is growing but the validity of these artificial-intelligence-generated questions is unclear. This literature review was conducted to address the urgent need for understanding the application of ChatGPT in generating MCQs for medical education. Following the database search and screening of 1920 studies, we found 23 relevant studies. We extracted the prompts for MCQ generation and assessed the validity evidence of MCQs. The findings showed that prompts varied, including referencing specific exam styles and adopting specific personas, which align with recommended prompt engineering tactics. The validity evidence covered various domains, showing mixed accuracy rates, with some studies indicating comparable quality to human-written questions, and others highlighting differences in difficulty and discrimination levels, alongside a significant reduction in question creation time. Despite its efficiency, we highlight the necessity of careful review and suggest a need for further research to optimize the use of ChatGPT in question generation. Main messages Ensure high-quality outputs by utilizing well-designed prompts; medical educators should prioritize the use of detailed, clear ChatGPT prompts when generating MCQs. Avoid using ChatGPT-generated MCQs directly in examinations without thorough review to prevent inaccuracies and ensure relevance. Leverage ChatGPT’s potential to streamline the test development process, enhancing efficiency without compromising quality.",No methods found.
2024,https://openalex.org/W4401667275,Social Sciences,Artificial intelligence for literature reviews: opportunities and challenges,"Abstract This paper presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous and organised methodology that assesses and integrates prior research on a given topic. Numerous tools have been developed to assist and partially automate the SLR process. The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews. Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases. We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features. We also analyse 11 recent tools that leverage large language models for searching the literature and assisting academic writing. Finally, the paper discusses current trends in the field, outlines key research challenges, and suggests directions for future research. We highlight three primary research challenges: integrating advanced AI solutions, such as large language models and knowledge graphs, improving usability, and developing a standardised evaluation framework. We also propose best practices to ensure more robust evaluations in terms of performance, usability, and transparency. Overall, this review offers a detailed overview of AI-enhanced SLR tools for researchers and practitioners, providing a foundation for the development of next-generation AI solutions in this field.","<method>large language models</method>, <method>knowledge graphs</method>"
2024,https://openalex.org/W357964437,Social Sciences,Women's Medical Work in Early Modern France,"For a majority of the French population during the period known as the Renaissance, most medical care would come at the hands of women. Women's medical work, like that of other providers, needs to be situated in specific historical and social contexts. This book adopts a number of methodological approaches which will help to highlight and understand women's medical practices, and may provide new ways to perceive their contribution to the history of medicine more generally. It focuses on women because, as practitioners, they cut across most sectors of medical practice. The book is structured in such a way as to demonstrate how different contexts and communities responded to women's medical work in varied and sometimes contrasting ways. It explores religious understandings of female healing work as lay and religious women. The book presents the study of women's domestic and charitable medical labour, by exploring the impact of print in the context of women as readers and patrons of medical literature, with a focus on the publication of manuals contributing to the domestic care discourse. It examines the role of women in the municipally organised systems of poor relief and child care for foundlings and orphans. The book also follows women's gynaecological and reproductive knowledge, particularly in the contexts of elite and royal court life.",No methods found.
2024,https://openalex.org/W4391062409,Social Sciences,"Critical analysis of the technological affordances, challenges and future directions of Generative AI in education: a systematic review","Generative artificial intelligence has been regarded as a transformative tool. While responsible and ethical applications could bring opportunities to education, their misuse could pose demanding challenges. It is necessary to clarify the technological affordances and challenges in a normative way to lay the foundation for future development. This study addressed the dearth of literature by performing a systematic review, aiming to (i) explore the utility and availability from the technological affordances perspective; (ii) summarize the current challenges in risks prevention; and (iii) propose possible directions for future research and practice. A total of 27 academic articles published in core journals between 2020 and 2023 were analyzed, and the inductive grounded approach was used to categorize the coding schemes. The findings revealed four technological affordances: accessibility, personalization, automation, and interactivity; and five challenges: academic integrity risk, response errors and bias, over-dependence risk, the widening digital divide, and privacy and security. We propose future directions, encourage educational organizations to formulate guidelines for the ethical use of AI in education, call on educators to embrace future trends in AI education instead of shunning its use, and guide students to treat it as a thought aid and reference, rather than relying on it entirely.",No methods found.
2024,https://openalex.org/W4392304778,Social Sciences,Ethical use of data in the metaverse for corporate social responsibility,"The study examined ethical use of data in the metaverse for corporate social responsibility (CSR). The study noted that Metaverse, is an emerging technology with vast potential for businesses, social interactions, and entertainment. Nevertheless, with the increasing use of data in this digital realm, it is crucial to ensure the ethical use of data and prioritize Corporate Social Responsibility (CSR). Thus, metaverse has garnered a lot of interest recently. It describes a group virtual shared environment that is produced through the combination of virtual, augmented, and physical reality settings. The metaverse can be conceptualized as an immersive, persistent, and networked virtual environment where users can explore different virtual environments, interact with one another, and take part in a variety of activities. Consequently, in the metaverses, CSR plays a crucial role. These elements consist of safeguarding user privacy and data security, battling false and misleading information, encouraging diversity and inclusion, addressing the impact on the environment, supporting moral business conduct, and enhancing stakeholder trust. The study concluded that Corporate social responsibility is crucial when it comes to the moral use of data in the metaverse. Companies can foster trust with their users and help the metaverse succeed as a whole by making sure that user data is gathered, stored, and used ethically. It was recommended that firms should consider the environmental impact of the metaverse and adopt sustainable practices to minimize negative consequences on the environment.",No methods found.
2024,https://openalex.org/W4390665799,Social Sciences,Artificial intelligence for skin cancer detection and classification for clinical environment: a systematic review,"Background Skin cancer is one of the most common forms worldwide, with a significant increase in incidence over the last few decades. Early and accurate detection of this type of cancer can result in better prognoses and less invasive treatments for patients. With advances in Artificial Intelligence (AI), tools have emerged that can facilitate diagnosis and classify dermatological images, complementing traditional clinical assessments and being applicable where there is a shortage of specialists. Its adoption requires analysis of efficacy, safety, and ethical considerations, as well as considering the genetic and ethnic diversity of patients. Objective The systematic review aims to examine research on the detection, classification, and assessment of skin cancer images in clinical settings. Methods We conducted a systematic literature search on PubMed, Scopus, Embase, and Web of Science, encompassing studies published until April 4th, 2023. Study selection, data extraction, and critical appraisal were carried out by two independent reviewers. Results were subsequently presented through a narrative synthesis. Results Through the search, 760 studies were identified in four databases, from which only 18 studies were selected, focusing on developing, implementing, and validating systems to detect, diagnose, and classify skin cancer in clinical settings. This review covers descriptive analysis, data scenarios, data processing and techniques, study results and perspectives, and physician diversity, accessibility, and participation. Conclusion The application of artificial intelligence in dermatology has the potential to revolutionize early detection of skin cancer. However, it is imperative to validate and collaborate with healthcare professionals to ensure its clinical effectiveness and safety.",No methods found.
2024,https://openalex.org/W4393308240,Social Sciences,Generative AI and deepfakes: a human rights approach to tackling harmful content,"The EU's Artificial Intelligence Act (AIA) introduces necessary deepfake regulations. However, these could infringe on the rights of AI providers and deployers or users, potentially conflicting with privacy and free expression under Articles 8 and 10 of the European Convention on Human Rights, and the General Data Protection Regulation (EU) 2016/679 (GDPR). This paper critically examines how an unmodified AIA could enable voter manipulation, blackmail, and the generation of sexual abusive content, facilitating misinformation and potentially harming millions, both emotionally and financially. Through analysis of the AIA's provisions, GDPR's regulations, relevant case law, and academic literature, the paper identifies risks for both AI providers and users. While the AIA's yearly review cycle is important, the immediacy of these threats demands swifter action. This paper proposes two key amendments: 1) mandate structured synthetic data for deepfake detection, and 2) classify AI intended for malicious deepfakes as 'high-risk'. These amendments, alongside clear definitions and robust safeguards would ensure effective deepfake regulation while protecting fundamental rights. The paper urges policymakers to adopt these amendments during the next review cycle to protect democracy, individual safety, and children. Only then will the AIA fully achieve its aims while safeguarding the freedoms it seeks to uphold.",No methods found.
2024,https://openalex.org/W4396636942,Social Sciences,Artificial intelligence in digital pathology: a systematic review and meta-analysis of diagnostic test accuracy,"Abstract Ensuring diagnostic performance of artificial intelligence (AI) before introduction into clinical practice is essential. Growing numbers of studies using AI for digital pathology have been reported over recent years. The aim of this work is to examine the diagnostic accuracy of AI in digital pathology images for any disease. This systematic review and meta-analysis included diagnostic accuracy studies using any type of AI applied to whole slide images (WSIs) for any disease. The reference standard was diagnosis by histopathological assessment and/or immunohistochemistry. Searches were conducted in PubMed, EMBASE and CENTRAL in June 2022. Risk of bias and concerns of applicability were assessed using the QUADAS-2 tool. Data extraction was conducted by two investigators and meta-analysis was performed using a bivariate random effects model, with additional subgroup analyses also performed. Of 2976 identified studies, 100 were included in the review and 48 in the meta-analysis. Studies were from a range of countries, including over 152,000 whole slide images (WSIs), representing many diseases. These studies reported a mean sensitivity of 96.3% (CI 94.1–97.7) and mean specificity of 93.3% (CI 90.5–95.4). There was heterogeneity in study design and 99% of studies identified for inclusion had at least one area at high or unclear risk of bias or applicability concerns. Details on selection of cases, division of model development and validation data and raw performance data were frequently ambiguous or missing. AI is reported as having high diagnostic accuracy in the reported areas but requires more rigorous evaluation of its performance.",No methods found.
2024,https://openalex.org/W4402827393,Social Sciences,Larger and more instructable language models become less reliable,"Abstract The prevailing methods to make large language models more powerful and amenable have been based on continuous scaling up (that is, increasing their size, data volume and computational resources 1 ) and bespoke shaping up (including post-filtering 2,3 , fine tuning or use of human feedback 4,5 ). However, larger and more instructable large language models may have become less reliable. By studying the relationship between difficulty concordance, task avoidance and prompting stability of several language model families, here we show that easy instances for human participants are also easy for the models, but scaled-up, shaped-up models do not secure areas of low difficulty in which either the model does not err or human supervision can spot the errors. We also find that early models often avoid user questions but scaled-up, shaped-up models tend to give an apparently sensible yet wrong answer much more often, including errors on difficult questions that human supervisors frequently overlook. Moreover, we observe that stability to different natural phrasings of the same question is improved by scaling-up and shaping-up interventions, but pockets of variability persist across difficulty levels. These findings highlight the need for a fundamental shift in the design and development of general-purpose artificial intelligence, particularly in high-stakes areas for which a predictable distribution of errors is paramount.","<method>continuous scaling up</method>, <method>post-filtering</method>, <method>fine tuning</method>, <method>use of human feedback</method>"
2024,https://openalex.org/W4390533386,Social Sciences,"A Systematic Literature Review of Digital Twin Research for Healthcare Systems: Research Trends, Gaps, and Realization Challenges","Using the PRISMA approach, we present the first systematic literature review of digital twin (DT) research in healthcare systems (HSs). This endeavor stems from the pressing need for a thorough analysis of this emerging yet fragmented research area, with the goal of consolidating knowledge to catalyze its growth. Our findings are structured around three research questions aimed at identifying: (i) current research trends, (ii) gaps, and (iii) realization challenges. Current trends indicate global interest and interdisciplinary collaborations to address complex HS challenges. However, existing research predominantly focuses on conceptualization; research on integration, verification, and implementation is nascent. Additionally, we document that a substantial body of papers mislabel their work, often disregarding modeling and twinning methods that are necessary elements of a DT. Furthermore, we provide a non-exhaustive classification of the literature based on two axes: <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">the object</i> (i.e., product or process) and <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">the context</i> (i.e., patient's body, medical procedures, healthcare facilities, and public health). While this is a testament to the diversity of the field, it implies a specific pattern that could be reimagined. We also identify two gaps: (i) considering the human-in-the-loop nature of HSs with a focus on provider decision-making and (ii) implementation research. Lastly, we discuss two challenges for broad-scale implementation of DTs in HSs: improving virtual-to-physical connectivity and data-related issues. In conclusion, this study suggests that DT research could potentially help alleviate the acute shortcomings of HSs that are often manifested in the inability to concurrently improve the quality of care, provider wellbeing, and cost efficiency.",No methods found.
2024,https://openalex.org/W4390751384,Social Sciences,Robotics in Healthcare: A Survey,"Abstract Research and innovation in the area of robotics in healthcare has seen significant growth in recent years. Global trends indicate that patients are getting older and sicker, while demands in healthcare workers are increasing their chance of injury. Robotic technology has the potential to enable high levels of patient care, clinical productivity and safety for both patients and healthcare workers. This paper surveys the state-of-the-art in robotics in healthcare and well-being, with particular attention to the key barriers and enablers to the implementation of this technology in real-world settings. Desktop research was used to identify available and emerging robotic technology currently in use (or with potential use) in healthcare settings. Primary sources of information included: academic publications, international organisations, commercial websites and online news agencies. In this paper, applications of robots in healthcare were divided into five main areas: service, assistive, socially-assistive, teleoperated and interventional robots. The maturity and readiness of different products is still an open challenge, with service and interventional robots leading the way. Wide-spread adoption of robots is likely to happen as the cost of the technology reduces, and wide evidence of beneficial long-term impact is available. This manuscript identified the main drivers, challenges, opportunities and considerations for implementing robots in healthcare. We hope this manuscript will raise awareness about robotics in healthcare among a wider audience to maximise availability, quality, and acceptability this technology.",No methods found.
2024,https://openalex.org/W4391484346,Social Sciences,Sensors for Emerging Water Contaminants: Overcoming Roadblocks to Innovation,"Ensuring water quality and safety requires the effective detection of emerging contaminants, which present significant risks to both human health and the environment. Field deployable low-cost sensors provide solutions to detect contaminants at their source and enable large-scale water quality monitoring and management. Unfortunately, the availability and utilization of such sensors remain limited. This Perspective examines current sensing technologies for detecting emerging contaminants and analyzes critical barriers, such as high costs, lack of reliability, difficulties in implementation in real-world settings, and lack of stakeholder involvement in sensor design. These technical and nontechnical barriers severely hinder progression from proof-of-concepts and negatively impact user experience factors such as ease-of-use and actionability using sensing data, ultimately affecting successful translation and widespread adoption of these technologies. We provide examples of specific sensing systems and explore key strategies to address the remaining scientific challenges that must be overcome to translate these technologies into the field such as improving sensitivity, selectivity, robustness, and performance in real-world water environments. Other critical aspects such as tailoring research to meet end-users' requirements, integrating cost considerations and consumer needs into the early prototype design, establishing standardized evaluation and validation protocols, fostering academia-industry collaborations, maximizing data value by establishing data sharing initiatives, and promoting workforce development are also discussed. The Perspective describes a set of guidelines for the development, translation, and implementation of water quality sensors to swiftly and accurately detect, analyze, track, and manage contamination.",No methods found.
2024,https://openalex.org/W4391265035,Social Sciences,"An integrated framework for sustainable and efficient building maintenance operations aligning with climate change, SDGs, and emerging technology","Improving the operation and maintenance of buildings can significantly reduce carbon emissions, energy consumption, and other environmental challenges while promoting sustainability. While existing literature offers various frameworks, they primarily focus on traditional building maintenance procedures and overlook the importance of integrating sustainability, climate change, environmental factors, and emerging technologies. To address this gap, this research has developed a comprehensive framework that caters to current needs, challenges, and future priorities. The integrated framework for building maintenance operations aligns with the Sustainable Development Goals (SDGs), climate change mitigation and adaptation, the adoption of emerging technology, energy conservation, as well as safety, resilience, and effectiveness. The development of the framework encompassed four phases: pre-development phases 1 and 2, development phase 3, and validation phase 4. During this process, current issues and challenges were identified, impacts were assessed, and strategies were developed. The framework serves as a roadmap to address these challenges and requirements in future building maintenance operations, making significant contributions to all three dimensions of sustainability: environmental, social, and economic. In summary, this study offers a comprehensive and in-depth analysis of the current issues, challenges, and potential improvements and benefits in building maintenance operations, providing a practical guide for industry stakeholders and making a significant contribution to the existing body of knowledge.",No methods found.
2024,https://openalex.org/W4391737478,Social Sciences,"Ethical considerations in implementing generative AI for healthcare supply chain optimization: A cross-country analysis across India, the United Kingdom, and the United States of America","This review paper critically examines the ethical considerations involved in implementing generative Artificial Intelligence (AI) in healthcare supply chain optimization across three distinct regions: India, the United Kingdom, and the United States of America. The study synthesizes findings from various case studies and academic research to highlight both common and unique ethical challenges faced in these countries. Key themes such as data privacy, algorithmic transparency, and equitable access to AI-driven healthcare solutions are explored, alongside the unique socio-cultural, legal, and regulatory challenges specific to each region. The paper proposes a set of best practices for incorporating ethical considerations into the deployment of generative AI in healthcare. These include the development of inclusive ethical frameworks, regular ethical audits, comprehensive training and education programs, public engagement initiatives, and interdisciplinary collaboration. The paper also delves into future research directions and policy development, emphasizing the need to address healthcare disparities, adapt legal and regulatory frameworks, enhance generative AI explainability, and evaluate long-term outcomes.The study concludes by underscoring the importance of ethical design and deployment of generative AI systems in healthcare, advocating for a balanced approach that aligns technological advancements with ethical standards and global healthcare needs. This comprehensive review aims to contribute to the discourse on ethical generative AI implementation, offering insights and recommendations for policymakers, healthcare professionals, and generative AI developers to foster responsible and beneficial use of generative AI in healthcare globally.",No methods found.
2024,https://openalex.org/W4392343921,Social Sciences,Data extraction for evidence synthesis using a large language model: A proof‐of‐concept study,"Abstract Data extraction is a crucial, yet labor‐intensive and error‐prone part of evidence synthesis. To date, efforts to harness machine learning for enhancing efficiency of the data extraction process have fallen short of achieving sufficient accuracy and usability. With the release of large language models (LLMs), new possibilities have emerged to increase efficiency and accuracy of data extraction for evidence synthesis. The objective of this proof‐of‐concept study was to assess the performance of an LLM (Claude 2) in extracting data elements from published studies, compared with human data extraction as employed in systematic reviews. Our analysis utilized a convenience sample of 10 English‐language, open‐access publications of randomized controlled trials included in a single systematic review. We selected 16 distinct types of data, posing varying degrees of difficulty (160 data elements across 10 studies). We used the browser version of Claude 2 to upload the portable document format of each publication and then prompted the model for each data element. Across 160 data elements, Claude 2 demonstrated an overall accuracy of 96.3% with a high test–retest reliability (replication 1: 96.9%; replication 2: 95.0% accuracy). Overall, Claude 2 made 6 errors on 160 data items. The most common errors ( n = 4) were missed data items. Importantly, Claude 2's ease of use was high; it required no technical expertise or labeled training data for effective operation (i.e., zero‐shot learning). Based on findings of our proof‐of‐concept study, leveraging LLMs has the potential to substantially enhance the efficiency and accuracy of data extraction for evidence syntheses.","<method>large language models (LLMs)</method>, <method>zero-shot learning</method>"
2024,https://openalex.org/W4392769397,Social Sciences,Human-centred learning analytics and AI in education: A systematic literature review,"The rapid expansion of Learning Analytics (LA) and Artificial Intelligence in Education (AIED) offers new scalable, data-intensive systems but raises concerns about data privacy and agency. Excluding stakeholders—like students and teachers—from the design process can potentially lead to mistrust and inadequately aligned tools. Despite a shift towards human-centred design in recent LA and AIED research, there remain gaps in our understanding of the importance of human control, safety, reliability, and trustworthiness in the design and implementation of these systems. We conducted a systematic literature review to explore these concerns and gaps. We analysed 108 papers to provide insights about i) the current state of human-centred LA/AIED research; ii) the extent to which educational stakeholders have contributed to the design process of human-centred LA/AIED systems; iii) the current balance between human control and computer automation of such systems; and iv) the extent to which safety, reliability and trustworthiness have been considered in the literature. Results indicate some consideration of human control in LA/AIED system design, but limited end-user involvement in actual design. Based on these findings, we recommend: 1) carefully balancing stakeholders' involvement in designing and deploying LA/AIED systems throughout all design phases 2) actively involving target end-users, especially students, to delineate the balance between human control and automation, and 3) exploring safety, reliability, and trustworthiness as principles in future human-centred LA/AIED systems.",No methods found.
2024,https://openalex.org/W4399857583,Social Sciences,Integrating artificial intelligence to assess emotions in learning environments: a systematic literature review,"Introduction Artificial Intelligence (AI) is transforming multiple sectors within our society, including education. In this context, emotions play a fundamental role in the teaching-learning process given that they influence academic performance, motivation, information retention, and student well-being. Thus, the integration of AI in emotional assessment within educational environments offers several advantages that can transform how we understand and address the socio-emotional development of students. However, there remains a lack of comprehensive approach that systematizes advancements, challenges, and opportunities in this field. Aim This systematic literature review aims to explore how artificial intelligence (AI) is used to evaluate emotions within educational settings. We provide a comprehensive overview of the current state of research, focusing on advancements, challenges, and opportunities in the domain of AI-driven emotional assessment within educational settings. Method The review involved a search across the following academic databases: Pubmed, Web of Science, PsycINFO and Scopus. Forty-one articles were selected that meet the established inclusion criteria. These articles were analyzed to extract key insights related to the integration of AI and emotional assessment within educational environments. Results The findings reveal a variety of AI-driven approaches that were developed to capture and analyze students’ emotional states during learning activities. The findings are summarized in four fundamental topics: (1) emotion recognition in education, (2) technology integration and learning outcomes, (3) special education and assistive technology, (4) affective computing. Among the key AI techniques employed are machine learning and facial recognition, which are used to assess emotions. These approaches demonstrate promising potential in enhancing pedagogical strategies and creating adaptive learning environments that cater to individual emotional needs. The review identified emerging factors that, while important, require further investigation to understand their relationships and implications fully. These elements could significantly enhance the use of AI in assessing emotions within educational settings. Specifically, we are referring to: (1) federated learning, (2) convolutional neural network (CNN), (3) recurrent neural network (RNN), (4) facial expression databases, and (5) ethics in the development of intelligent systems. Conclusion This systematic literature review showcases the significance of AI in revolutionizing educational practices through emotion assessment. While advancements are evident, challenges related to accuracy, privacy, and cross-cultural validity were also identified. The synthesis of existing research highlights the need for further research into refining AI models for emotion recognition and emphasizes the importance of ethical considerations in implementing AI technologies within educational contexts.","<method>machine learning</method>, <method>facial recognition</method>, <method>federated learning</method>, <method>convolutional neural network (CNN)</method>, <method>recurrent neural network (RNN)</method>"
2024,https://openalex.org/W4400916341,Social Sciences,Reviewing the current state of virtual reality integration in medical education - a scoping review,"Abstract Background In medical education, new technologies like Virtual Reality (VR) are increasingly integrated to enhance digital learning. Originally used to train surgical procedures, now use cases also cover emergency scenarios and non-technical skills like clinical decision-making. This scoping review aims to provide an overview of VR in medical education, including requirements, advantages, disadvantages, as well as evaluation methods and respective study results to establish a foundation for future VR integration into medical curricula. Methods This review follows the updated JBI methodology for scoping reviews and adheres to the respective PRISMA extension. We included reviews in English or German language from 2012 to March 2022 that examine the use of VR in education for medical and nursing students, registered nurses, and qualified physicians. Data extraction focused on medical specialties, subjects, curricula, technical/didactic requirements, evaluation methods and study outcomes as well as advantages and disadvantages of VR. Results A total of 763 records were identified. After eligibility assessment, 69 studies were included. Nearly half of them were published between 2021 and 2022, predominantly from high-income countries. Most reviews focused on surgical training in laparoscopic and minimally invasive procedures (43.5%) and included studies with qualified physicians as participants (43.5%). Technical, didactic and organisational requirements were highlighted and evaluations covering performance time and quality, skills acquisition and validity, often showed positive outcomes. Accessibility, repeatability, cost-effectiveness, and improved skill development were reported as advantages, while financial challenges, technical limitations, lack of scientific evidence, and potential user discomfort were cited as disadvantages. Discussion Despite a high potential of VR in medical education, there are mandatory requirements for its integration into medical curricula addressing challenges related to finances, technical limitations, and didactic aspects. The reported lack of standardised and validated guidelines for evaluating VR training must be overcome to enable high-quality evidence for VR usage in medical education. Interdisciplinary teams of software developers, AI experts, designers, medical didactics experts and end users are required to design useful VR courses. Technical issues and compromised realism can be mitigated by further technological advancements.",No methods found.
2024,https://openalex.org/W4391070180,Social Sciences,AI in medical diagnosis: AI prediction &amp; human judgment,"AI has long been regarded as a panacea for decision-making and many other aspects of knowledge work; as something that will help humans get rid of their shortcomings. We believe that AI can be a useful asset to support decision-makers, but not that it should replace decision-makers. Decision-making uses algorithmic analysis, but it is not solely algorithmic analysis; it also involves other factors, many of which are very human, such as creativity, intuition, emotions, feelings, and value judgments. We have conducted semi-structured open-ended research interviews with 17 dermatologists to understand what they expect from an AI application to deliver to medical diagnosis. We have found four aggregate dimensions along which the thinking of dermatologists can be described: the ways in which our participants chose to interact with AI, responsibility, 'explainability', and the new way of thinking (mindset) needed for working with AI. We believe that our findings will help physicians who might consider using AI in their diagnosis to understand how to use AI beneficially. It will also be useful for AI vendors in improving their understanding of how medics want to use AI in diagnosis. Further research will be needed to examine if our findings have relevance in the wider medical field and beyond.",No methods found.
2024,https://openalex.org/W4390664389,Social Sciences,A knowledge-guided visualization framework of disaster scenes for helping the public cognize risk information,"As an important application of virtual geographic environments (VGEs), virtual disaster scenes are essential in enhancing the public's risk awareness. However, existing virtual disaster scene visualization methods lack expert guidance and fail to meet the public's requirements, resulting in an ineffective public understanding. Therefore, this paper proposes a knowledge-guided disaster scene 3D visualization framework. First, the public's demand for disaster scene visualization is analyzed, and a geographic knowledge graph of disaster scenes is constructed. Second, through the guidance of the knowledge graph, the virtual disaster scenes are fusion modeled and suitability represented. Third, a diverse organization and adaptive scheduling method of disaster scene data for multi-computing devices is established. Finally, we developed a prototype system for disaster scene visualization, selected a typical disaster, and conducted cognitive experiments with eye-tracking technology. The results show that the proposed method can effectively support the adaptive visualization of virtual disaster scenes for four computing devices and maintain an efficient frame rate. In addition, compared with other disaster scene visualization methods, our framework incorporates semantic knowledge of scene, user, demand, and space. It can effectively convey disaster information and help the public cognize disaster risks and has significant advantages in modeling standardization, personalization, and adaptability.",No methods found.
2024,https://openalex.org/W4391508432,Social Sciences,Artificial intelligence-driven virtual rehabilitation for people living in the community: A scoping review,"Abstract Virtual Rehabilitation (VRehab) is a promising approach to improving the physical and mental functioning of patients living in the community. The use of VRehab technology results in the generation of multi-modal datasets collected through various devices. This presents opportunities for the development of Artificial Intelligence (AI) techniques in VRehab, namely the measurement, detection, and prediction of various patients’ health outcomes. The objective of this scoping review was to explore the applications and effectiveness of incorporating AI into home-based VRehab programs. PubMed/MEDLINE, Embase, IEEE Xplore, Web of Science databases, and Google Scholar were searched from inception until June 2023 for studies that applied AI for the delivery of VRehab programs to the homes of adult patients. After screening 2172 unique titles and abstracts and 51 full-text studies, 13 studies were included in the review. A variety of AI algorithms were applied to analyze data collected from various sensors and make inferences about patients’ health outcomes, most involving evaluating patients’ exercise quality and providing feedback to patients. The AI algorithms used in the studies were mostly fuzzy rule-based methods, template matching, and deep neural networks. Despite the growing body of literature on the use of AI in VRehab, very few studies have examined its use in patients’ homes. Current research suggests that integrating AI with home-based VRehab can lead to improved rehabilitation outcomes for patients. However, further research is required to fully assess the effectiveness of various forms of AI-driven home-based VRehab, taking into account its unique challenges and using standardized metrics.","<method>fuzzy rule-based methods</method>, <method>template matching</method>, <method>deep neural networks</method>"
2024,https://openalex.org/W4392658193,Social Sciences,Beyond boundaries: exploring the Metaverse in tourism,"Purpose This study aims to investigate the engagement gap between Metaverse and in-person travel, the influence of Metaverse tourism on tourists and the industry and the challenges and responses associated with Metaverse technology. The study presents practical cases and highlights the implications of this research for practice, society and future research. Design/methodology/approach This study uses a literature review to explore concerns about Metaverse technology in tourism. It analyzes the difference between in-person travel and Metaverse tourism, the impact on tourists and the industry and challenges and responses to Metaverse. The review shows a rising trend in Metaverse tourism research. Findings These findings suggest differences between Metaverse tourism and in-person travel. By providing personalized travel options, social interaction, immersive experiences and soliciting visitor feedback, it is possible to enhance the tourist experience. Additionally, the study highlights the opportunities and challenges that Metaverse tourism presents to the tourism industry. The study provides practical cases in the tourism industry and implications for practice, society and future research. Practical implications The study’s implications for Metaverse tourism are practical, societal and future research-related. Metaverse technology can enhance the tourist experience through personalized options, social interaction, immersive experiences and feedback. This inclusivity can promote social equity and cultural exchange. Further research is needed to explore the social effects of Metaverse tourism and its long-term impacts on local communities, economies and the environment. Originality/value This study contributes by exploring the impact of Metaverse tourism, supporting academic research and practice. It fills a knowledge gap by analyzing the application of Metaverse technology in tourism, providing insights for researchers and practitioners. It offers practical guidance by identifying opportunities and challenges in Metaverse tourism, fostering industry innovation. Additionally, it informs policymakers about the impact of Metaverse tourism on development.",No methods found.
2024,https://openalex.org/W4396908686,Social Sciences,Firefighter Skill Advancement through IoT-Enabled Virtual Reality and CNN-Based Training,"To maintain the safety and efficacy of firefighters in various circumstances, modern firefighting necessitates constantly improving skills and training techniques. Utilizing the Internet of Things (IoT), virtual reality (VR), and convolutional neural networks (CNN), this paper details a novel method for training firefighters. The proposed system collects real-time data on ambient variables, equipment state, and firefighter biometrics via integrating IoT sensors into firefighting equipment and training settings. Using this information, it can develop lifelike VR training simulations of difficult and potentially dangerous scenarios. To make the training settings more realistic and malleable, CNN-based algorithms are used to assess the data. The capacity to simulate a wide variety of firefighting situations, customize training difficulty depending on individual and team performance, and provide instant feedback and performance metrics to trainees are all major benefits of this method. The method also allows teachers to check in and evaluate their learners remotely, improving instruction quality. An IoT-enabled VR and CNN-based training technique has shown promising preliminary results in pilot trials, suggesting it might greatly enhance firefighter competence, situational awareness, and decision-making ability. Because of this, it has the potential to completely alter the way firefighters are informed and prepared for the ever-changing dangers users may encounter on the job.",<method>convolutional neural networks (CNN)</method>
2024,https://openalex.org/W4391753097,Social Sciences,Machine Vision—Moving from Industry 4.0 to Industry 5.0,"The Fourth Industrial Revolution combined with the advent of artificial intelligence brought significant changes to humans’ daily lives. Extended research in the field has aided in both documenting and presenting these changes, giving a more general picture of this new era. This work reviews the application field of the scientific research literature on the presence of machine vision in the Fourth Industrial Revolution and the changes it brought to each sector to which it contributed, determining the exact extent of its influence. Accordingly, an attempt is made to present an overview of its use in the Fifth Industrial Revolution to identify and present the changes between the two consequent periods. This work uses the PRISMA methodology and follows the form of a Scoping Review using sources from Scopus and Google Scholar. Most publications reveal the emergence of machine vision in almost every field of human life with significant influence and performance results. Undoubtedly, this review highlights the great influence and offer of machine vision in many sectors, establishing its use and searching for more ways to use it. It is also proven that machine vision systems can help industries to gain competitive advantage in terms of better product quality, higher customer satisfaction, and improved productivity.",<method>PRISMA methodology</method>
2024,https://openalex.org/W4391936064,Social Sciences,Adaptive Segmentation Enhanced Asynchronous Federated Learning for Sustainable Intelligent Transportation Systems,"The proliferation of advanced embedded and communication technologies has facilitated the possibility of modern Intelligent Transportation System (ITS). The hierarchical nature of such large-scale and distributed systems brings obvious challenges in creating a scalable and sustainable computing environment, and hence the development and application of edge intelligence become critical. Federated learning (FL), as an emerging distributed machine learning paradigm, aims to offer secure knowledge sharing and effective learning across multiple devices. However, conventional FL may fall into trouble when facing large-scale and network-agnostic systems with fast moving devices and changing network attributes. In this study, we propose an Adaptive Segmentation enhanced Asynchronous Federated Learning (AS-AFL) model, aiming to improve the learning efficiency and reliability in sustainable ITS via a decentralized fashion. Specifically, a meta-learning based adaptive segmentation scheme is designed to automatically separate the client nodes (e.g., vehicles) into multiple edge groups according to their homogeneous attributes. An integrated aggregation mechanism is then developed to realize the horizontal FL among a group of similar client nodes via the so-called intra-group synchronous aggregation, while allowing the vertical FL across different groups via the so-called inter-group asynchronous aggregation. Experiment and evaluation results based on an open-source dataset demonstrate the outstanding learning and communication performance of our proposed model, compared with several conventional FL schemes in a distributed ITS application scenario.","<method>Federated learning (FL)</method>, <method>Adaptive Segmentation enhanced Asynchronous Federated Learning (AS-AFL)</method>, <method>meta-learning based adaptive segmentation</method>"
2024,https://openalex.org/W4393092671,Social Sciences,CFSSynergy: Combining Feature-Based and Similarity-Based Methods for Drug Synergy Prediction,"Drug synergy prediction plays a vital role in cancer treatment. Because experimental approaches are labor-intensive and expensive, computational-based approaches get more attention. There are two types of computational methods for drug synergy prediction: feature-based and similarity-based. In feature-based methods, the main focus is to extract more discriminative features from drug pairs and cell lines to pass to the task predictor. In similarity-based methods, the similarities among all drugs and cell lines are utilized as features and fed into the task predictor. In this work, a novel approach, called CFSSynergy, that combines these two viewpoints is proposed. First, a discriminative representation is extracted for paired drugs and cell lines as input. We have utilized transformer-based architecture for drugs. For cell lines, we have created a similarity matrix between proteins using the Node2Vec algorithm. Then, the new cell line representation is computed by multiplying the protein–protein similarity matrix and the initial cell line representation. Next, we compute the similarity between unique drugs and unique cells using the learned representation for paired drugs and cell lines. Then, we compute a new representation for paired drugs and cell lines based on the similarity-based features and the learned features. Finally, these features are fed to XGBoost as a task predictor. Two well-known data sets were used to evaluate the performance of our proposed method: DrugCombDB and OncologyScreen. The CFSSynergy approach consistently outperformed existing methods in comparative evaluations. This substantiates the efficacy of our approach in capturing complex synergistic interactions between drugs and cell lines, setting it apart from conventional similarity-based or feature-based methods.","<method>transformer-based architecture</method>, <method>Node2Vec algorithm</method>, <method>XGBoost</method>"
2024,https://openalex.org/W4396622564,Social Sciences,Unveiling the shadows: Beyond the hype of AI in education,"Despite the wave of enthusiasm for the role of Artificial Intelligence (AI) in reshaping education, critical voices urge a more tempered approach. This study investigates the less-discussed 'shadows' of AI implementation in educational settings, focusing on potential negatives that may accompany its integration. Through a multi-phased exploration consisting of content analysis and survey research, the study develops and validates a theoretical model that pinpoints several areas of concern. The initial phase, a systematic literature review, yielded 56 relevant studies from which the model was crafted. The subsequent survey with 260 participants from a Saudi Arabian university aimed to validate the model. Findings confirm concerns about human connection, data privacy and security, algorithmic bias, transparency, critical thinking, access equity, ethical issues, teacher development, reliability, and the consequences of AI-generated content. They also highlight correlations between various AI-associated concerns, suggesting intertwined consequences rather than isolated issues. For instance, enhancements in AI transparency could simultaneously support teacher professional development and foster better student outcomes. Furthermore, the study acknowledges the transformative potential of AI but cautions against its unexamined adoption in education. It advocates for comprehensive strategies to maintain human connections, ensure data privacy and security, mitigate biases, enhance system transparency, foster creativity, reduce access disparities, emphasize ethics, prepare teachers, ensure system reliability, and regulate AI-generated content. Such strategies underscore the need for holistic policymaking to leverage AI's benefits while safeguarding against its disadvantages.",No methods found.
2024,https://openalex.org/W4390905132,Social Sciences,LinK3D: Linear Keypoints Representation for 3D LiDAR Point Cloud,"Feature extraction and matching are the basic parts of many robotic vision tasks, such as 2D or 3D object detection, recognition, and registration. As is known, 2D feature extraction and matching have already achieved great success. Unfortunately, in the field of 3D, the current methods may fail to support the extensive application of 3D LiDAR sensors in robotic vision tasks due to their poor descriptiveness and inefficiency. To address this limitation, we propose a novel 3D feature representation method: <underline xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Lin</u> ear <underline xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">K</u> eypoints representation for <underline xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">3D</u> LiDAR point cloud, called LinK3D. The novelty of LinK3D lies in that it fully considers the characteristics (such as the sparsity and complexity) of LiDAR point clouds and represents the keypoint with its robust neighbor keypoints, which provide strong constraints in the description of the keypoint. The proposed LinK3D has been evaluated on three public datasets, and the experimental results show that our method achieves great matching performance. More importantly, LinK3D also shows excellent real-time performance, faster than the sensor frame rate at 10 Hz of a typical rotating LiDAR sensor. LinK3D only takes an average of 30 milliseconds to extract features from the point cloud collected by a 64-beam LiDAR and takes merely about 20 milliseconds to match two LiDAR scans when executed on a computer with an Intel Core i7 processor. Moreover, our method can be extended to LiDAR odometry task, and shows good scalability.",<method>Linear Keypoints representation for 3D LiDAR point cloud (LinK3D)</method>
2024,https://openalex.org/W4390984444,Social Sciences,"Trends and challenges of fruit by-products utilization: insights into safety, sensory, and benefits of the use for the development of innovative healthy food: a review","Abstract A significant portion of the human diet is comprised of fruits, which are consumed globally either raw or after being processed. A huge amount of waste and by-products such as skins, seeds, cores, rags, rinds, pomace, etc. are being generated in our homes and agro-processing industries every day. According to previous statistics, nearly half of the fruits are lost or discarded during the entire processing chain. The concern arises when those wastes and by-products damage the environment and simultaneously cause economic losses. There is a lot of potential in these by-products for reuse in a variety of applications, including the isolation of valuable bioactive ingredients and their application in developing healthy and functional foods. The development of novel techniques for the transformation of these materials into marketable commodities may offer a workable solution to this waste issue while also promoting sustainable economic growth from the bio-economic viewpoint. This approach can manage waste as well as add value to enterprises. The goal of this study is twofold based on this scenario. The first is to present a brief overview of the most significant bioactive substances found in those by-products. The second is to review the current status of their valorization including the trends and techniques, safety assessments, sensory attributes, and challenges. Moreover, specific attention is drawn to the future perspective, and some solutions are discussed in this report.",No methods found.
2024,https://openalex.org/W4391352096,Social Sciences,"Here, There and Everywhere: On the Responsible Use of Artificial Intelligence (AI) in Management Research and the Peer‐Review Process","Abstract This editorial introduces and explains the Journal of Management Studies’ (JMS) new policy on artificial intelligence (AI) . We reflect on the use of AI in conducting research and generating journal submissions and what this means for the wider JMS community, including our authors, reviewers, editors, and readers. Specifically, we consider how AI‐generated research and text could both assist and augment the publication process, as well as harm it. Consequentially, our policy acknowledges the need for careful oversight regarding the use of AI to assist in the authoring of texts and in data analyses, while also noting the importance of requiring authors to be transparent about how, when and where they have utilized AI in their submissions or underlying research. Additionally, we examine how and in what ways AI's use may be antithetical to the spirit of a quality journal like JMS that values both human voice and research transparency. Our editorial explains why we require author teams to oversee all aspects of AI use within their projects, and to take personal responsibility for accuracy in all aspects of their research. We also explain our prohibition of AI's use in peer‐reviewers’ evaluations of submissions, and regarding editors’ handling of manuscripts.",No methods found.
2024,https://openalex.org/W4391531696,Social Sciences,Artificial intelligence in the risk prediction models of cardiovascular disease and development of an independent validation screening tool: a systematic review,"Abstract Background A comprehensive overview of artificial intelligence (AI) for cardiovascular disease (CVD) prediction and a screening tool of AI models (AI-Ms) for independent external validation are lacking. This systematic review aims to identify, describe, and appraise AI-Ms of CVD prediction in the general and special populations and develop a new independent validation score (IVS) for AI-Ms replicability evaluation. Methods PubMed, Web of Science, Embase, and IEEE library were searched up to July 2021. Data extraction and analysis were performed for the populations, distribution, predictors, algorithms, etc. The risk of bias was evaluated with the prediction risk of bias assessment tool (PROBAST). Subsequently, we designed IVS for model replicability evaluation with five steps in five items, including transparency of algorithms, performance of models, feasibility of reproduction, risk of reproduction, and clinical implication, respectively. The review is registered in PROSPERO (No. CRD42021271789). Results In 20,887 screened references, 79 articles (82.5% in 2017–2021) were included, which contained 114 datasets (67 in Europe and North America, but 0 in Africa). We identified 486 AI-Ms, of which the majority were in development ( n = 380), but none of them had undergone independent external validation. A total of 66 idiographic algorithms were found; however, 36.4% were used only once and only 39.4% over three times. A large number of different predictors (range 5–52,000, median 21) and large-span sample size (range 80–3,660,000, median 4466) were observed. All models were at high risk of bias according to PROBAST, primarily due to the incorrect use of statistical methods. IVS analysis confirmed only 10 models as “recommended”; however, 281 and 187 were “not recommended” and “warning,” respectively. Conclusion AI has led the digital revolution in the field of CVD prediction, but is still in the early stage of development as the defects of research design, report, and evaluation systems. The IVS we developed may contribute to independent external validation and the development of this field.",No methods found.
2024,https://openalex.org/W4392169182,Social Sciences,Object Detection in Autonomous Vehicles under Adverse Weather: A Review of Traditional and Deep Learning Approaches,"Enhancing the environmental perception of autonomous vehicles (AVs) in intelligent transportation systems requires computer vision technology to be effective in detecting objects and obstacles, particularly in adverse weather conditions. Adverse weather circumstances present serious difficulties for object-detecting systems, which are essential to contemporary safety procedures, infrastructure for monitoring, and intelligent transportation. AVs primarily depend on image processing algorithms that utilize a wide range of onboard visual sensors for guidance and decisionmaking. Ensuring the consistent identification of critical elements such as vehicles, pedestrians, and road lanes, even in adverse weather, is a paramount objective. This paper not only provides a comprehensive review of the literature on object detection (OD) under adverse weather conditions but also delves into the ever-evolving realm of the architecture of AVs, challenges for automated vehicles in adverse weather, the basic structure of OD, and explores the landscape of traditional and deep learning (DL) approaches for OD within the realm of AVs. These approaches are essential for advancing the capabilities of AVs in recognizing and responding to objects in their surroundings. This paper further investigates previous research that has employed both traditional and DL methodologies for the detection of vehicles, pedestrians, and road lanes, effectively linking these approaches with the evolving field of AVs. Moreover, this paper offers an in-depth analysis of the datasets commonly employed in AV research, with a specific focus on the detection of key elements in various environmental conditions, and then summarizes the evaluation matrix. We expect that this review paper will help scholars to gain a better understanding of this area of research.","<method>traditional approaches for object detection</method>, <method>deep learning (DL) approaches for object detection</method>, <method>traditional methodologies for detection of vehicles, pedestrians, and road lanes</method>, <method>deep learning (DL) methodologies for detection of vehicles, pedestrians, and road lanes</method>"
2024,https://openalex.org/W4396831262,Social Sciences,GPT-4 Turbo with Vision fails to outperform text-only GPT-4 Turbo in the Japan Diagnostic Radiology Board Examination,"Abstract Purpose To assess the performance of GPT-4 Turbo with Vision (GPT-4TV), OpenAI’s latest multimodal large language model, by comparing its ability to process both text and image inputs with that of the text-only GPT-4 Turbo (GPT-4 T) in the context of the Japan Diagnostic Radiology Board Examination (JDRBE). Materials and methods The dataset comprised questions from JDRBE 2021 and 2023. A total of six board-certified diagnostic radiologists discussed the questions and provided ground-truth answers by consulting relevant literature as necessary. The following questions were excluded: those lacking associated images, those with no unanimous agreement on answers, and those including images rejected by the OpenAI application programming interface. The inputs for GPT-4TV included both text and images, whereas those for GPT-4 T were entirely text. Both models were deployed on the dataset, and their performance was compared using McNemar’s exact test. The radiological credibility of the responses was assessed by two diagnostic radiologists through the assignment of legitimacy scores on a five-point Likert scale. These scores were subsequently used to compare model performance using Wilcoxon's signed-rank test. Results The dataset comprised 139 questions. GPT-4TV correctly answered 62 questions (45%), whereas GPT-4 T correctly answered 57 questions (41%). A statistical analysis found no significant performance difference between the two models (P = 0.44). The GPT-4TV responses received significantly lower legitimacy scores from both radiologists than the GPT-4 T responses. Conclusion No significant enhancement in accuracy was observed when using GPT-4TV with image input compared with that of using text-only GPT-4 T for JDRBE questions.","<method>GPT-4 Turbo with Vision (GPT-4TV)</method>, <method>GPT-4 Turbo (GPT-4 T)</method>"
2024,https://openalex.org/W4390755438,Social Sciences,A voting gray wolf optimizer-based ensemble learning models for intrusion detection in the Internet of Things,"Abstract The Internet of Things (IoT) has garnered considerable attention from academic and industrial circles as a pivotal technology in recent years. The escalation of security risks is observed to be associated with the growing interest in IoT applications. Intrusion detection systems (IDS) have been devised as viable instruments for identifying and averting malicious actions in this context. Several techniques described in academic papers are thought to be very accurate, but they cannot be used in the real world because the datasets used to build and test the models do not accurately reflect and simulate the IoT network. Existing methods, on the other hand, deal with these issues, but they are not good enough for commercial use because of their lack of precision, low detection rate, receiver operating characteristic (ROC), and false acceptance rate (FAR). The effectiveness of these solutions is predominantly dependent on individual learners and is consequently influenced by the inherent limitations of each learning algorithm. This study introduces a new approach for detecting intrusion attacks in an IoT network, which involves the use of an ensemble learning technique based on gray wolf optimizer (GWO). The novelty of this study lies in the proposed voting gray wolf optimizer (GWO) ensemble model, which incorporates two crucial components: a traffic analyzer and a classification phase engine. The model employs a voting technique to combine the probability averages of the base learners. Secondly, the combination of feature selection and feature extraction techniques is to reduce dimensionality. Thirdly, the utilization of GWO is employed to optimize the parameters of ensemble models. Similarly, the approach employs the most authentic intrusion detection datasets that are accessible and amalgamates multiple learners to generate ensemble learners. The hybridization of information gain (IG) and principal component analysis (PCA) was employed to reduce dimensionality. The study utilized a novel GWO ensemble learning approach that incorporated a decision tree, random forest, K-nearest neighbor, and multilayer perceptron for classification. To evaluate the efficacy of the proposed model, two authentic datasets, namely, BoT-IoT and UNSW-NB15, were scrutinized. The GWO-optimized ensemble model demonstrates superior accuracy when compared to other machine learning-based and deep learning models. Specifically, the model achieves an accuracy rate of 99.98%, a DR of 99.97%, a precision rate of 99.94%, an ROC rate of 99.99%, and an FAR rate of 1.30 on the BoT-IoT dataset. According to the experimental results, the proposed ensemble model optimized by GWO achieved an accuracy of 100%, a DR of 99.9%, a precision of 99.59%, an ROC of 99.40%, and an FAR of 1.5 when tested on the UNSW-NB15 dataset.","<method>ensemble learning technique</method>, <method>gray wolf optimizer (GWO)</method>, <method>voting technique</method>, <method>feature selection</method>, <method>feature extraction</method>, <method>information gain (IG)</method>, <method>principal component analysis (PCA)</method>, <method>decision tree</method>, <method>random forest</method>, <method>K-nearest neighbor</method>, <method>multilayer perceptron</method>"
2024,https://openalex.org/W4392520536,Social Sciences,Antithrombotic Therapy for VTE Disease,"The American College of Chest Physicians (CHEST) Antithrombotic Therapy for Venous Thromboembolism Disease evidence-based guidelines are now updated in a more frequent, focused manner. Guidance statements from the most recent full guidelines and two subsequent updates have not been gathered into a single source. An international panel of experts with experience in prior antithrombotic therapy guideline development reviewed the 2012 CHEST antithrombotic therapy guidelines and its two subsequent updates. All guideline statements and their associated patient, intervention, comparator, and outcome questions were assembled. A modified Delphi process was used to select statements considered relevant to current clinical care. The panel further endorsed minor phrasing changes to match the standard language for guidance statements using the modified Grading of Recommendations, Assessment, Development, and Evaluations (ie, GRADE) format endorsed by the CHEST Guidelines Oversight Committee. The panel appended comments after statements deemed as relevant, including suggesting that statements be updated in future guidelines because of interval evidence. We include 58 guidance statements from prior versions of the antithrombotic therapy guidelines, with updated phrasing as needed to adhere to contemporary nomenclature. Statements were classified as strong or weak recommendations based on high-certainty, moderate-certainty, and low-certainty evidence using GRADE methodology. The panel suggested that five statements are no longer relevant to current practice. As CHEST continues to update guidance statements relevant to antithrombotic therapy for VTE disease, this article serves as a unified collection of currenrtly relevant statements from the preceding three guidelines. Suggestions have been made to update specific statements in future publications.",No methods found.
2024,https://openalex.org/W4392865184,Social Sciences,Artificial intelligence and multimodal data fusion for smart healthcare: topic modeling and bibliometrics,"Abstract Advancements in artificial intelligence (AI) have driven extensive research into developing diverse multimodal data analysis approaches for smart healthcare. There is a scarcity of large-scale analysis of literature in this field based on quantitative approaches. This study performed a bibliometric and topic modeling examination on 683 articles from 2002 to 2022, focusing on research topics and trends, journals, countries/regions, institutions, authors, and scientific collaborations. Results showed that, firstly, the number of articles has grown from 1 in 2002 to 220 in 2022, with a majority being published in interdisciplinary journals that link healthcare and medical research and information technology and AI. Secondly, the significant rise in the quantity of research articles can be attributed to the increasing contribution of scholars from non-English speaking countries/regions and the noteworthy contributions made by authors in the USA and India. Thirdly, researchers show a high interest in diverse research issues, especially, cross-modality magnetic resonance imaging (MRI) for brain tumor analysis, cancer prognosis through multi-dimensional data analysis, and AI-assisted diagnostics and personalization in healthcare, with each topic experiencing a significant increase in research interest. There is an emerging trend towards issues such as applying generative adversarial networks and contrastive learning for multimodal medical image fusion and synthesis and utilizing the combined spatiotemporal resolution of functional MRI and electroencephalogram in a data-centric manner. This study is valuable in enhancing researchers’ and practitioners’ understanding of the present focal points and upcoming trajectories in AI-powered smart healthcare based on multimodal data analysis.","<method>generative adversarial networks</method>, <method>contrastive learning</method>"
2024,https://openalex.org/W4392884001,Social Sciences,Developing a Multi-Criteria Decision-Making model for nuclear power plant location selection using Fuzzy Analytic Hierarchy Process and Fuzzy VIKOR methods focused on socio-economic factors,"In response to its position as the fourth most populous country globally, Indonesia is exploring constructing nuclear power plants (NPPs) as a sustainable energy solution. A pivotal step in this initiative is selecting an appropriate NPP site. This study employs two Multi-Criteria Decision-Making (MCDM) methods, the Fuzzy Analytic Hierarchy Process (Fuzzy-AHP) and Fuzzy VIKOR, to identify the most suitable location for an NPP, focusing on socio-economic factors. The Fuzzy-AHP method is utilized to prioritize ten sub-criteria: transmission network, operating costs, economic impact, security, transportation network, legal considerations, the impact of tourism, land ownership, historical sites, and public acceptance. Following this, the Fuzzy VIKOR method leverages these prioritized criteria to evaluate two potential sites: East Kalimantan and West Kalimantan. The analysis reveals that security, transmission, and transportation networks emerge as the top priorities. The application of the Fuzzy VIKOR algorithm identifies West Kalimantan as the optimal site for NPP construction, evidenced by its lower VIKOR index of 0.3599, indicating a higher overall preference based on the evaluated criteria. The study demonstrates that the integration of Fuzzy-AHP and Fuzzy VIKOR methods prioritizes critical socio-economic factors and quantitatively assesses potential sites, offering a systematic and objective approach to support decision-making in NPP site selection.","<method>Fuzzy Analytic Hierarchy Process (Fuzzy-AHP)</method>, <method>Fuzzy VIKOR</method>"
2024,https://openalex.org/W4396905964,Social Sciences,A systematic review and meta-analysis of artificial intelligence versus clinicians for skin cancer diagnosis,"Scientific research of artificial intelligence (AI) in dermatology has increased exponentially. The objective of this study was to perform a systematic review and meta-analysis to evaluate the performance of AI algorithms for skin cancer classification in comparison to clinicians with different levels of expertise. Based on PRISMA guidelines, 3 electronic databases (PubMed, Embase, and Cochrane Library) were screened for relevant articles up to August 2022. The quality of the studies was assessed using QUADAS-2. A meta-analysis of sensitivity and specificity was performed for the accuracy of AI and clinicians. Fifty-three studies were included in the systematic review, and 19 met the inclusion criteria for the meta-analysis. Considering all studies and all subgroups of clinicians, we found a sensitivity (Sn) and specificity (Sp) of 87.0% and 77.1% for AI algorithms, respectively, and a Sn of 79.78% and Sp of 73.6% for all clinicians (overall); differences were statistically significant for both Sn and Sp. The difference between AI performance (Sn 92.5%, Sp 66.5%) vs. generalists (Sn 64.6%, Sp 72.8%), was greater, when compared with expert clinicians. Performance between AI algorithms (Sn 86.3%, Sp 78.4%) vs expert dermatologists (Sn 84.2%, Sp 74.4%) was clinically comparable. Limitations of AI algorithms in clinical practice should be considered, and future studies should focus on real-world settings, and towards AI-assistance.",No methods found.
2024,https://openalex.org/W4402890475,Social Sciences,"Foundation models in robotics: Applications, challenges, and the future","We survey applications of pretrained foundation models in robotics. Traditional deep learning models in robotics are trained on small datasets tailored for specific tasks, which limits their adaptability across diverse applications. In contrast, foundation models pretrained on internet-scale data appear to have superior generalization capabilities, and in some instances display an emergent ability to find zero-shot solutions to problems that are not present in the training data. Foundation models may hold the potential to enhance various components of the robot autonomy stack, from perception to decision-making and control. For example, large language models can generate code or provide common sense reasoning, while vision-language models enable open-vocabulary visual recognition. However, significant open research challenges remain, particularly around the scarcity of robot-relevant training data, safety guarantees and uncertainty quantification, and real-time execution. In this survey, we study recent papers that have used or built foundation models to solve robotics problems. We explore how foundation models contribute to improving robot capabilities in the domains of perception, decision-making, and control. We discuss the challenges hindering the adoption of foundation models in robot autonomy and provide opportunities and potential pathways for future advancements. The GitHub project corresponding to this paper can be found here: https://github.com/robotics-survey/Awesome-Robotics-Foundation-Models .","<method>pretrained foundation models</method>, <method>traditional deep learning models</method>, <method>large language models</method>, <method>vision-language models</method>"
2024,https://openalex.org/W4391454392,Social Sciences,UANet: An Uncertainty-Aware Network for Building Extraction From Remote Sensing Images,"Building extraction aims to segment building pixels from remote sensing images and plays an essential role in many applications, such as city planning and urban dynamic monitoring. Over the past few years, deep learning methods with encoder–decoder architectures have achieved remarkable performance due to their powerful feature representation capability. Nevertheless, due to the varying scales and styles of buildings, conventional deep learning models always suffer from uncertain predictions and cannot accurately distinguish the complete footprints of the building from the complex distribution of ground objects, leading to a large degree of omission and commission. In this paper, we realize the importance of uncertain prediction and propose a novel and straightforward Uncertainty-Aware Network (UANet) to alleviate this problem. Specifically, we first apply a general encoder–decoder network to obtain a building extraction map with relatively high uncertainty. Second, in order to aggregate the useful information in the highest-level features, we design a Prior Information Guide Module to guide the highest-level features in learning the prior information from the conventional extraction map. Third, based on the uncertain extraction map, we introduce an Uncertainty Rank Algorithm to measure the uncertainty level of each pixel belonging to the foreground and the background. We further combine this algorithm with the proposed Uncertainty-Aware Fusion Module to facilitate level-by-level feature refinement and obtain the final refined extraction map with low uncertainty. To verify the performance of our proposed UANet, we conduct extensive experiments on three public building datasets, including the WHU building dataset, the Massachusetts building dataset, and the Inria aerial image dataset. Results demonstrate that the proposed UANet outperforms other state-of-the-art algorithms by a large margin. The source code of the proposed UANet is available at https://github.com/Henryjiepanli/Uncertainty-aware-Network.","<method>deep learning methods with encoder–decoder architectures</method>, <method>encoder–decoder network</method>, <method>Uncertainty-Aware Network (UANet)</method>, <method>Prior Information Guide Module</method>, <method>Uncertainty Rank Algorithm</method>, <method>Uncertainty-Aware Fusion Module</method>"
2024,https://openalex.org/W4392239939,Social Sciences,The Future of Edge Computing for Healthcare Ecosystem,"Healthcare applications generate huge amounts of sensitive data which require proper storage, management, and analysis in order to derive meaningful information. Different computing solutions have been devised to store and process health data. Out of these, cloud computing has come to the forefront due to its capability to store and process massive amount of data. The data and processing applications are moving to the cloud owing to the benefits, such as on-demand scaling, reliability, and cost savings. However, the cloud platform can introduce latency challenges that can hamper performance of mission-critical applications, including violating several data privacy laws such as the Health Insurance Portability and Accountability Act (HIPAA). Edge computing is a new technology that promises to address these challenges by allowing devices in remote locations to process data locally at the ""edge"" of the network, either by the device or a local server, thereby achieving high computation, low latency, and data security requirements of the healthcare use cases. Though edge computing promises several benefits, it is a technology that is still in its infancy, and there are limitations on how much data can be processed at the edge, thereby making the choice of using edge computing challenging. In this chapter, we review the use of edge computing in healthcare. We performed a literature review to identify how edge computing has been used and the challenges it addresses, along with the opportunities that it creates. This chapter will be of interest to implementers and practitioners to understand the role of edge computing in the healthcare ecosystem.",No methods found.
2024,https://openalex.org/W4392241969,Social Sciences,All models are wrong and yours are useless: making clinical prediction models impactful for patients,"All models are wrong and yours are useless: making clinical prediction models impactful for patients Florian MarkowetzCheck for updates Most published clinical prediction models are never used in clinical practice and there is a huge gap between academic research and clinical implementation.Here, I propose ways for academic researchers to be proactive partners in improving clinical practice and to design models in ways that ultimately benefit patients.""All models are wrong, but some are useful"" is an aphorism attributed to the statistician George Box.There is humility in claiming your model is wrong, but there is also bravado in implying your model might be useful.And, honestly, I don't think it is.I think your model is useless.How would I know?I don't even know who you are.Well, it is a bet.A bet I am willing to take because the odds are ridiculously in my favour.I will explain what I mean in the context of clinical prediction models.My points apply to a wide range of preclinical models, both computational and biological, but my own core expertise is with clinical prediction tools.These are computational models from statistics, machine learning or AI that try to predict clinically relevant variables and ultimately aim to help doctors to treat patients better.The papers describing them make claims like ""this model can be used in the clinic""; generally softened with words like ""might"", ""could"", ""potential"", ""promise"", or other techniques to reduce accountability.The Box quote offers a yardstick to measure the success of these models; not by how correctly they describe reality but by how useful they are in helping patients.And in general, almost none of these tools ever help anyone.There is a wealth of systematic reviews in different fields to show how many models have been proposed and how few have even been validated, let alone been adopted in the clinic.For example, 408(!) models for chronic obstructive pulmonary disease were systematically reviewed 1 and as a summary the authors bleakly note ""several methodological pitfalls in their development and a low rate of external validation"".And whatever biomedical area you work in, your experiences will mirror this resultmany novel prediction models, little help for patients.I believe that a model designed to be used for patients is useless unless it is actually used for patients.",No methods found.
2024,https://openalex.org/W4392764934,Social Sciences,Sustainability-oriented crowdfunding: An integrative literature review,"Crowdfunding has emerged as an attractive financing option for sustainable entrepreneurship, where entrepreneurs must overcome considerable investor uncertainty in light of mixed social, ecological, and economic goals. The rapid emergence of studies on sustainability-oriented crowdfunding yielded a wide variety of theoretical perspectives and an abundance of empirical evidence due to a high dispersion across different research fields. Drawing on a systematic review and qualitative analysis of 157 articles, we map the existing research and develop a new and integrative framework to (1) organize key theoretical research levels (i.e., individual, transactional, and institutional), (2) identify multilevel antecedents, crowdfunding process dimensions, and short-vs. long-term outcomes, and (3) offer new and promising future research avenues. Our findings indicate a high degree of theoretical convergence at the individual and transactional level related to short-term crowdfunding outcomes, while research is limited concerning the institutional level, long-term outcomes, and the context of different crowdfunding forms. By highlighting the existing and missing linkages between the three levels related to sustainability-oriented crowdfunding processes, we contribute to the literature and guide future research to explore fruitful avenues in the field. Practical implications are gleaned from our findings for crowdfunding platforms, their supporting institutions, and the sustainability-oriented entrepreneurs seeking financing.",No methods found.
2024,https://openalex.org/W4393380945,Social Sciences,One-Step Multi-View Clustering With Diverse Representation,"Multi-View clustering has attracted broad attention due to its capacity to utilize consistent and complementary information among views. Although tremendous progress has been made recently, most existing methods undergo high complexity, preventing them from being applied to large-scale tasks. Multi-View clustering via matrix factorization is a representative to address this issue. However, most of them map the data matrices into a fixed dimension, limiting the model's expressiveness. Moreover, a range of methods suffers from a two-step process, i.e., multimodal learning and the subsequent <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""> <tex-math notation=""LaTeX"">$k$</tex-math> </inline-formula> -means, inevitably causing a suboptimal clustering result. In light of this, we propose a one-step multi-view clustering with diverse representation (OMVCDR) method, which incorporates multi-view learning and <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""> <tex-math notation=""LaTeX"">$k$</tex-math> </inline-formula> -means into a unified framework. Specifically, we first project original data matrices into various latent spaces to attain comprehensive information and auto-weight them in a self-supervised manner. Then, we directly use the information matrices under diverse dimensions to obtain consensus discrete clustering labels. The unified work of representation learning and clustering boosts the quality of the final results. Furthermore, we develop an efficient optimization algorithm with proven convergence to solve the resultant problem. Comprehensive experiments on various datasets demonstrate the promising clustering performance of our proposed method. The code is publicly available at https://github.com/wanxinhang/OMVCDR.","<method>Multi-View clustering via matrix factorization</method>, <method>k-means</method>, <method>one-step multi-view clustering with diverse representation (OMVCDR)</method>, <method>multi-view learning</method>, <method>self-supervised auto-weighting</method>, <method>representation learning</method>"
2024,https://openalex.org/W4396827149,Social Sciences,Rehearsal: Simulating Conflict to Teach Conflict Resolution,"Interpersonal conflict is an uncomfortable but unavoidable fact of life. Navigating conflict successfully is a skill—one that can be learned through deliberate practice—but few have access to effective training or feedback. To expand this access, we introduce Rehearsal, a system that allows users to rehearse conflicts with a believable simulated interlocutor, explore counterfactual ""what if?"" scenarios to identify alternative conversational paths, and learn through feedback on how and when to apply specific conflict strategies. Users can utilize Rehearsal to practice handling a variety of predefined conflict scenarios, from office disputes to relationship issues, or they can choose to create their own setting. To enable Rehearsal, we develop IRP prompting, a method of conditioning output of a large language model on the influential Interest-Rights-Power (IRP) theory from conflict resolution. Rehearsal uses IRP to generate utterances grounded in conflict resolution theory, guiding users towards counterfactual conflict resolution strategies that help de-escalate difficult conversations. In a between-subjects evaluation, 40 participants engaged in an actual conflict with a confederate after training. Compared to a control group with lecture material covering the same IRP theory, participants with simulated training from Rehearsal significantly improved their performance in the unaided conflict: they reduced their use of escalating competitive strategies by an average of 67%, while doubling their use of cooperative strategies. Overall, Rehearsal highlights the potential effectiveness of language models as tools for learning and practicing interpersonal skills.",<method>IRP prompting</method>
2024,https://openalex.org/W4399054302,Social Sciences,Artificial Intelligence in Point-of-Care Biosensing: Challenges and Opportunities,"The integration of artificial intelligence (AI) into point-of-care (POC) biosensing has the potential to revolutionize diagnostic methodologies by offering rapid, accurate, and accessible health assessment directly at the patient level. This review paper explores the transformative impact of AI technologies on POC biosensing, emphasizing recent computational advancements, ongoing challenges, and future prospects in the field. We provide an overview of core biosensing technologies and their use at the POC, highlighting ongoing issues and challenges that may be solved with AI. We follow with an overview of AI methodologies that can be applied to biosensing, including machine learning algorithms, neural networks, and data processing frameworks that facilitate real-time analytical decision-making. We explore the applications of AI at each stage of the biosensor development process, highlighting the diverse opportunities beyond simple data analysis procedures. We include a thorough analysis of outstanding challenges in the field of AI-assisted biosensing, focusing on the technical and ethical challenges regarding the widespread adoption of these technologies, such as data security, algorithmic bias, and regulatory compliance. Through this review, we aim to emphasize the role of AI in advancing POC biosensing and inform researchers, clinicians, and policymakers about the potential of these technologies in reshaping global healthcare landscapes.","<method>machine learning algorithms</method>, <method>neural networks</method>"
2024,https://openalex.org/W4399426804,Social Sciences,Evaluating the persuasive influence of political microtargeting with large language models,"Recent advancements in large language models (LLMs) have raised the prospect of scalable, automated, and fine-grained political microtargeting on a scale previously unseen; however, the persuasive influence of microtargeting with LLMs remains unclear. Here, we build a custom web application capable of integrating self-reported demographic and political data into GPT-4 prompts in real-time, facilitating the live creation of unique messages tailored to persuade individual users on four political issues. We then deploy this application in a preregistered randomized control experiment ( n = 8,587) to investigate the extent to which access to individual-level data increases the persuasive influence of GPT-4. Our approach yields two key findings. First, messages generated by GPT-4 were broadly persuasive, in some cases increasing support for an issue stance by up to 12 percentage points. Second, in aggregate, the persuasive impact of microtargeted messages was not statistically different from that of non-microtargeted messages (4.83 vs. 6.20 percentage points, respectively, P = 0.226). These trends hold even when manipulating the type and number of attributes used to tailor the message. These findings suggest—contrary to widespread speculation—that the influence of current LLMs may reside not in their ability to tailor messages to individuals but rather in the persuasiveness of their generic, nontargeted messages. We release our experimental dataset, GPTarget2024 , as an empirical baseline for future research.",<method>GPT-4</method>
2024,https://openalex.org/W4401434014,Social Sciences,Crafting personalized learning paths with AI for lifelong learning: a systematic literature review,"The rapid evolution of knowledge requires constantly acquiring and updating skills, making lifelong learning crucial. Despite decades of artificial intelligence, recent advances promote new solutions to personalize learning in this context. The purpose of this article is to explore the current state of research on the development of artificial intelligence-mediated solutions for the design of personalized learning paths. To achieve this, a systematic literature review (SRL) of 78 articles published between 2019 and 2024 from the Scopus and Web or Science databases was conducted, answering seven questions grouped into three themes: characteristics of the published research, context of the research, and type of solution analyzed. This study identified that: (a) the greatest production of scientific research on the topic is developed in China, India and the United States, (b) the focus is mainly directed towards the educational context at the higher education level with areas of opportunity for application in the work context, and (c) the development of adaptive learning technologies predominates; however, there is a growing interest in the application of generative language models. This article contributes to the growing interest and literature related to personalized learning under artificial intelligence mediated solutions that will serve as a basis for academic institutions and organizations to design programs under this model.","<method>adaptive learning technologies</method>, <method>generative language models</method>"
2024,https://openalex.org/W4392346526,Social Sciences,Incorporation of “Artificial Intelligence” for Objective Pain Assessment: A Comprehensive Review,"Pain is a significant health issue, and pain assessment is essential for proper diagnosis, follow-up, and effective management of pain. The conventional methods of pain assessment often suffer from subjectivity and variability. The main issue is to understand better how people experience pain. In recent years, artificial intelligence (AI) has been playing a growing role in improving clinical diagnosis and decision-making. The application of AI offers promising opportunities to improve the accuracy and efficiency of pain assessment. This review article provides an overview of the current state of AI in pain assessment and explores its potential for improving accuracy, efficiency, and personalized care. By examining the existing literature, research gaps, and future directions, this article aims to guide further advancements in the field of pain management. An online database search was conducted via multiple websites to identify the relevant articles. The inclusion criteria were English articles published between January 2014 and January 2024). Articles that were available as full text clinical trials, observational studies, review articles, systemic reviews, and meta-analyses were included in this review. The exclusion criteria were articles that were not in the English language, not available as free full text, those involving pediatric patients, case reports, and editorials. A total of (47) articles were included in this review. In conclusion, the application of AI in pain management could present promising solutions for pain assessment. AI can potentially increase the accuracy, precision, and efficiency of objective pain assessment.",No methods found.
2024,https://openalex.org/W4392817144,Social Sciences,Accessing care for Long Covid from the perspectives of patients and healthcare practitioners: A qualitative study,"Abstract Background Long Covid is an emerging long‐term condition, with those affected raising concerns about lack of healthcare support. Objective We conducted a qualitative study to identify facilitators and barriers to healthcare access for people with Long Covid, aiming to enhance our understanding of the specific nature of these barriers and how patient experiences may vary. Setting and Participants In the context of the Symptoms, Trajectory, Inequalities and Management: Understanding Long‐COVID to Address and Transform Existing Integrated Care Pathways (STIMULATE‐ICP) Delphi study, a nationally distributed online survey was conducted. Eight patients and eight healthcare practitioners (HCP) were interviewed via telephone or video call. Framework analysis, sensitised by the candidacy theory, was used to identify barriers and facilitators over four levels of access to care. Results Three themes were identified: (i) patients' efforts to navigate emerging pathways for Long Covid, (ii) the patient–HCP interaction and (iii) service resources and structural constraints. Barriers to specialist care included long waiting times, communication gaps across services and a lack of continuity in care. Facilitators included collaborative, patient‐centred approaches, patients' active role in their healthcare and blended approaches for appointments. The perspectives of both patients and HCPs largely aligned. Discussion The candidacy framework was valuable in understanding the experiences of people with Long Covid seeking access to healthcare. Individuals perceived themselves as eligible for care, but they often encountered obstacles in obtaining the expected level of care or, in some cases, did not receive it at all. Our findings are discussed in the context of the candidacy model through multiple processes of identification, negotiation, permeability and appearances at health services. These themes seem to be especially important for the emerging new pathway model and are relevant to both primary and secondary care. Conclusions This study highlights that despite these interviews being conducted two years after the start of the COVID‐19 pandemic, people with Long Covid still struggle to access healthcare, emphasising the ongoing need to provide equitable timely healthcare access for people with Long Covid. Patient or Public Contribution People with Long Covid advised on all stages of this research.",No methods found.
2024,https://openalex.org/W4390610074,Social Sciences,A longitudinal study on artificial intelligence adoption: understanding the drivers of ChatGPT usage behavior change in higher education,"As the field of artificial intelligence (AI) continues to progress, the use of AI-powered chatbots, such as ChatGPT, in higher education settings has gained significant attention. This paper addresses a well-defined problem pertaining to the critical need for a comprehensive examination of students' ChatGPT adoption in higher education. To examine such adoption, it is imperative to focus on measuring actual user behavior. While measuring students' ChatGPT usage behavior at a specific point in time can be valuable, a more holistic approach is necessary to understand the temporal dynamics of AI adoption. To address this need, a longitudinal survey was conducted, examining how students' ChatGPT usage behavior changes over time among students, and unveiling the drivers of such behavior change. The empirical examination of 222 Dutch higher education students revealed a significant decline in students' ChatGPT usage behavior over an 8 month period. This period was defined by two distinct data collection phases: the initial phase (T1) and a follow-up phase conducted 8 months later (T2). Furthermore, the results demonstrate that changes in trust, emotional creepiness, and Perceived Behavioral Control significantly predicted the observed change in usage behavior. The findings of this research carry significant academic and managerial implications, as they advance our comprehension of the temporal aspects of AI adoption in higher education. The findings also provide actionable guidance for AI developers and educational institutions seeking to optimize student engagement with AI technologies.",No methods found.
2024,https://openalex.org/W4390876710,Social Sciences,Utilisation of Deep Learning (DL) and Neural Networks (NN) Algorithms for Energy Power Generation: A Social Network and Bibliometric Analysis (2004-2022),"The research landscape on the applications of advanced computational tools (ACTs) such as machine/deep learning and neural network algorithms for energy and power generation (EPG) was critically examined through publication trends and bibliometrics data analysis. The Elsevier Scopus database and the PRISMA methodology were employed to identify and screen the published documents, whereas the bibliometric analysis software VOSviewer was used to analyse the co-authorships, citations, and keyword occurrences. The results showed that 152 documents have been published on the topic comprising conference proceedings (58.6%) and articles (41.4%) between 2004 and 2022. Publication trends analysis revealed the number of publications increased from 1 to 31 or by 3,000% over the same period, which was ascribed to the growing scientific interest and research impact of the topic. Stakeholder analysis revealed the top authors/researchers are Anvari M, Ghaderi SF and Saberi M, whereas the most prolific affiliation and nations actively engaged in the topic are the North China Electric Power University, and China, respectively. Conversely, the top funding agency actively backing research on the topic is the National Natural Science Foundation of China (NSFC). Co-authorship analysis revealed high levels of collaboration between researching nations compared to authors and affiliations. Hotspot analysis revealed three major thematic focus areas namely; Energy Grid Forecasting, Power Generation Control, and Intelligent Energy Optimization. In conclusion, the study showed that the application of ACTs in EPG is an active, multidisciplinary, and impact area of research with potential for more impactful contributions to research and society at large.","<method>machine learning</method>, <method>deep learning</method>, <method>neural network algorithms</method>"
2024,https://openalex.org/W4391687152,Social Sciences,"Traditional, complementary, and integrative medicine and artificial intelligence: Novel opportunities in healthcare","The convergence of traditional, complementary, and integrative medicine (TCIM) with artificial intelligence (AI) is a promising frontier in healthcare. TCIM is a patient-centric approach that combines conventional medicine with complementary therapies, emphasizing holistic well-being. AI can revolutionize healthcare through data-driven decision-making and personalized treatment plans. This article explores how AI technologies can complement and enhance TCIM, aligning with the shared objectives of researchers from both fields in improving patient outcomes, enhancing care quality, and promoting holistic wellness. This integration of TCIM and AI introduces exciting opportunities but also noteworthy challenges. AI may augment TCIM by assisting in early disease detection, providing personalized treatment plans, predicting health trends, and enhancing patient engagement. Challenges at the intersection of AI and TCIM include data privacy and security, regulatory complexities, maintaining the human touch in patient-provider relationships, and mitigating bias in AI algorithms. Patients' trust, informed consent, and legal accountability are all essential considerations. Future directions in AI-enhanced TCIM include advanced personalized medicine, understanding the efficacy of herbal remedies, and studying patient-provider interactions. Research on bias mitigation, patient acceptance, and trust in AI-driven TCIM healthcare is crucial. In this article, we outlined that the merging of TCIM and AI holds great promise in enhancing healthcare delivery, personalizing treatment plans, preventive care, and patient engagement. Addressing challenges and fostering collaboration between AI experts, TCIM practitioners, and policymakers, however, is vital to harnessing the full potential of this integration.",No methods found.
2024,https://openalex.org/W4392639228,Social Sciences,Application of Interval Valued Intuitionistic Fuzzy Uncertain MCDM Methodology for Ph.D Supervisor Selection Problem,"The selection of Ph.D (Doctor of Philosophy) supervisor is always a vital and interesting problem in academia and especially for students who want to carry out Ph.D. Nowadays, selecting a supervisor for Ph.D in a scientific manner becomes a challenge for any student because of the variety of options available to the scholar. In this context, the present study aims to formulate a model for Ph.D. supervisor selection from the offered alternatives in an academic institute. A hybrid multi-criteria decision making (MCDM) framework has been applied to select the suitable supervisor of the student's preferred criteria under interval-valued intuitionistic fuzzy (IVIF) scenario. The IVIF Analytic Hierarchy Process (AHP) has been employed to prioritize the criteria, whereas IVIF Technique for order preference by similarity to ideal solution (TOPSIS) technique is engaged to rank the available supervisors based on criteria weight. A set of eight criteria and five alternatives have been considered for modelling the problem. Moreover, the potential criteria are weighted and ranked by the multiple decision makers in the present study. To examine the consistency and robustness of the proposed integrated approach, sensitivity analysis and comparative analysis have been carried out. From all the analyses, it can be conferred that the suggested approach is quite useful to apply in different decision-making scenarios.","<method>IVIF Analytic Hierarchy Process (AHP)</method>, <method>IVIF Technique for order preference by similarity to ideal solution (TOPSIS)</method>"
2024,https://openalex.org/W4392925707,Social Sciences,LEGAL FRAMEWORKS AND TAX COMPLIANCE IN THE DIGITAL ECONOMY: A FINANCE PERSPECTIVE,"In the wake of rapid digitalization, the landscape of commerce has undergone a profound transformation, presenting unprecedented challenges to traditional tax systems and legal frameworks. This abstract examines the evolving dynamics of tax compliance within the digital economy through the lens of finance. The digital economy encompasses a broad spectrum of economic activities facilitated by digital technologies, including e-commerce, digital platforms, and virtual currencies. These innovations have blurred the boundaries of traditional tax jurisdictions, leading to complexities in determining tax liabilities and enforcement mechanisms. As such, the adequacy of existing legal frameworks in addressing tax challenges posed by the digital economy has come under scrutiny. From a finance perspective, ensuring tax compliance in the digital economy involves understanding the intricate interplay between technology, business models, and regulatory frameworks. Digital businesses often operate across multiple jurisdictions, exploiting loopholes and jurisdictional discrepancies to minimize tax obligations. Such practices have raised concerns regarding tax fairness and the erosion of tax bases, prompting policymakers to explore new regulatory approaches. One key aspect of addressing tax compliance in the digital economy is the development of international cooperation and coordination mechanisms. Given the transnational nature of digital transactions, effective tax enforcement requires collaboration among countries to combat tax evasion and profit shifting. Initiatives such as the Base Erosion and Profit Shifting (BEPS) project by the OECD seek to establish common standards and guidelines for taxing digital businesses. Moreover, the emergence of innovative technologies, such as blockchain and artificial intelligence, presents both opportunities and challenges for tax authorities. While these technologies offer potential solutions for enhancing tax administration and enforcement, they also introduce new complexities, such as the anonymity of transactions and the difficulty of tracking digital assets. Navigating the complexities of tax compliance in the digital economy requires a multifaceted approach that integrates legal, technological, and financial perspectives. By fostering international cooperation, leveraging technological innovations, and adapting regulatory frameworks, policymakers can mitigate tax challenges and promote a fair and sustainable tax system in the digital age.&#x0D; Keywords: Tax, Digital Economy, Finance, Legal, Review.",No methods found.
2024,https://openalex.org/W4393909951,Social Sciences,The governance of artificial intelligence in Canada: Findings and opportunities from a review of 84 AI governance initiatives,"In recent years, the effective governance of artificial intelligence (AI) systems has become a strategic necessity for many nations. Among those nations, Canada is particularly noteworthy: Canada was the first nation to implement a national AI strategy, and more recently, Canada's federal and provincial governments have designed and implemented a wide range of initiatives that attempt to intervene in a variety of potential impacts associated with AI systems. We present a semi-systematic review and synthesis of 84 of those AI governance initiatives. We find that those 84 initiatives predominantly focus on developing programs, policies, and strategic plans to intervene in industry and innovation, technology production and use, AI research, and public administration. Conversely, we find relatively little focus on developing ethics statements or standards, as well as little focus on intervening in social and workforce development services, AI education and training, and digital infrastructure. We suggest three opportunities for researchers and four opportunities for practitioners that, if enacted, would strengthen the overall state of Canadian AI governance. Our study contributes a novel macro-scale synthesis of AI governance initiatives within a national context, as well as practical opportunities for intervening in national AI governance challenges related to evaluation of initiative outcomes, public trust and participation in initiatives, AI impact representation in initiatives, and national unification.",No methods found.
2024,https://openalex.org/W4394967854,Social Sciences,Potential of Large Language Models in Health Care: Delphi Study,"Background A large language model (LLM) is a machine learning model inferred from text data that captures subtle patterns of language use in context. Modern LLMs are based on neural network architectures that incorporate transformer methods. They allow the model to relate words together through attention to multiple words in a text sequence. LLMs have been shown to be highly effective for a range of tasks in natural language processing (NLP), including classification and information extraction tasks and generative applications. Objective The aim of this adapted Delphi study was to collect researchers’ opinions on how LLMs might influence health care and on the strengths, weaknesses, opportunities, and threats of LLM use in health care. Methods We invited researchers in the fields of health informatics, nursing informatics, and medical NLP to share their opinions on LLM use in health care. We started the first round with open questions based on our strengths, weaknesses, opportunities, and threats framework. In the second and third round, the participants scored these items. Results The first, second, and third rounds had 28, 23, and 21 participants, respectively. Almost all participants (26/28, 93% in round 1 and 20/21, 95% in round 3) were affiliated with academic institutions. Agreement was reached on 103 items related to use cases, benefits, risks, reliability, adoption aspects, and the future of LLMs in health care. Participants offered several use cases, including supporting clinical tasks, documentation tasks, and medical research and education, and agreed that LLM-based systems will act as health assistants for patient education. The agreed-upon benefits included increased efficiency in data handling and extraction, improved automation of processes, improved quality of health care services and overall health outcomes, provision of personalized care, accelerated diagnosis and treatment processes, and improved interaction between patients and health care professionals. In total, 5 risks to health care in general were identified: cybersecurity breaches, the potential for patient misinformation, ethical concerns, the likelihood of biased decision-making, and the risk associated with inaccurate communication. Overconfidence in LLM-based systems was recognized as a risk to the medical profession. The 6 agreed-upon privacy risks included the use of unregulated cloud services that compromise data security, exposure of sensitive patient data, breaches of confidentiality, fraudulent use of information, vulnerabilities in data storage and communication, and inappropriate access or use of patient data. Conclusions Future research related to LLMs should not only focus on testing their possibilities for NLP-related tasks but also consider the workflows the models could contribute to and the requirements regarding quality, integration, and regulations needed for successful implementation in practice.","<method>large language model (LLM)</method>, <method>machine learning model</method>, <method>neural network architectures</method>, <method>transformer methods</method>"
2024,https://openalex.org/W4398796607,Social Sciences,Desirable Characteristics for AI Teaching Assistants in Programming Education,"Providing timely and personalized feedback to large numbers of students is a long-standing challenge in programming courses.Relying on human teaching assistants (TAs) has been extensively studied, revealing a number of potential shortcomings.These include inequitable access for students with low confidence when needing support, as well as situations where TAs provide direct solutions without helping students to develop their own problemsolving skills.With the advent of powerful large language models (LLMs), digital teaching assistants configured for programming contexts have emerged as an appealing and scalable way to provide instant, equitable, round-the-clock support.Although digital TAs can provide a variety of help for programming tasks, from high-level problem solving advice to direct solution generation, the effectiveness of such tools depends on their ability to promote meaningful learning experiences.If students find the guardrails implemented in digital TAs too constraining, or if other expectations are not met, they may seek assistance in ways that do not help them learn.Thus, it is essential to identify the features that students believe make digital teaching assistants valuable.We deployed an LLM-powered digital assistant in an introductory programming course and collected student feedback ( = 813) on the characteristics of the tool they perceived to be most important.Our results highlight that students value such tools for their ability to provide instant, engaging support, particularly during peak times such as before assessment deadlines.They also expressed a strong preference for features that enable them to retain autonomy in their learning journey, such as scaffolding that helps to guide them through problem-solving steps rather than simply being shown direct solutions.",<method>large language models (LLMs)</method>
2024,https://openalex.org/W4399421825,Social Sciences,Integrating deep learning techniques for personalized learning pathways in higher education,"The rapid improvement of artificial intelligence (AI) in the educational domain has opened new possibilities for enhancing the learning experiences for students. This research discusses the critical need for personalized education in higher education by integrating deep learning (DL) techniques to create customized learning pathways for students. This research intends to bridge the gap between constant educational content and dynamic student needs. This research presents an AI-driven adaptive learning platform implemented across four different courses and 300 students at a university in Faisalabad-Pakistan. A controlled experiment compares student outcomes between those using the AI platform and those undergoing traditional instruction. Quantitative results demonstrate a 25 % improvement in grades, test scores, and engagement for the AI group, with a statistical significance of a p-value of 0.00045. Qualitative feedback highlights enhanced experiences attributed to personalized pathways. The DL analysis of student performance data highlights key parameters, including enhanced learning outcomes and engagement metrices over time. Surveys reveal increased satisfaction compared to one-size-fits-all content. Unlike prior AI research lacking rigorous validation, our methodology and significant results deliver a concrete framework for institutions to implement personalized, AI-driven education at scale. This data-driven approach builds on previous attempts by tying adaptations to actual student needs, yielding measurable improvements in key outcomes. Overall, this work empirically validates that AI platforms leveraging robust analytics to provide customized and adaptive learning can significantly enhance student academic performance, engagement, and satisfaction compared to traditional approaches. These findings have insightful consequences for the future of higher education. The research contributes to the growing demand for AI in education research and provides a practical framework for institutions seeking to implement more adaptive and student-centric teaching methodologies.",<method>deep learning (DL) techniques</method>
2024,https://openalex.org/W4400853276,Social Sciences,Unveiling the dynamics of AI applications: A review of reviews using scientometrics and BERTopic modeling,"In a world that has rapidly transformed through the advent of artificial intelligence (AI), our systematic review, guided by the PRISMA protocol, investigates a decade of AI research, revealing insights into its evolution and impact. Our study, examining 3,767 articles, has drawn considerable attention, as evidenced by an impressive 63,577 citations, underscoring the scholarly community's profound engagement. Our study reveals a collaborative landscape with 18,189 contributing authors, reflecting a robust network of researchers advancing AI and machine learning applications. Review categories focus on systematic reviews and bibliometric analyses, indicating an increasing emphasis on comprehensive literature synthesis and quantitative analysis. The findings also suggest an opportunity to explore emerging methodologies such as topic modeling and meta-analysis. We dissect the state of the art presented in these reviews, finding themes throughout the broad scholarly discourse through thematic clustering and BERTopic modeling. Categorization of study articles across fields of research indicates dominance in Information and Computing Sciences, followed by Biomedical and Clinical Sciences. Subject categories reveal interconnected clusters across various sectors, notably in healthcare, engineering, business intelligence, and computational technologies. Semantic analysis via BERTopic revealed nineteen clusters mapped to themes such as AI in health innovations, AI for sustainable development, AI and deep learning, AI in education, and ethical considerations. Future research directions are suggested, emphasizing the need for intersectional bias mitigation, holistic health approaches, AI's role in environmental sustainability, and the ethical deployment of generative AI.","<method>topic modeling</method>, <method>meta-analysis</method>, <method>thematic clustering</method>, <method>BERTopic modeling</method>"
2024,https://openalex.org/W4403656816,Social Sciences,"Advancing the Sustainable Development Goals (SDGs) through artificial intelligence, machine learning, and deep learning","The use of Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL) significantly has the touch of transformational potential towards bringing the Sustainable Development Goals (SDGs) to be addressed in various industries. This research investigates the new developments and applications of these technologies in advancing sustainability programs in industry-intensive domains. Industries are beginning to undergo a major change by making today with the help of AI, ML, and DL that resources can be optimized, energy efficiency can be improved, and environmental impacts can be mitigated. A number of other trends - including predictive analytics and intelligent automation, allow for smarter and more efficient production, waste minimization and circular economy practices. AI-powered solutions are also now being used in the energy sector to maximize the generation of renewable energy, optimize grid management, and aid in the transition to low carbon energy systems. This will enable industries achieve better environmental benefits and higher operational efficiencies through big data analytics and IoT. AI and ML are also crucial in smart cities, urban planning, public services that delivery efficiency and overall support the sustainability agenda. The results reinforce the importance of strong regulatory structures and interdisciplinary collaboration to optimally leverage AI, ML, and DL to the SDGs, which will be intrinsic to designing for resilience and sustainability.","<method>Artificial Intelligence (AI)</method>, <method>Machine Learning (ML)</method>, <method>Deep Learning (DL)</method>, <method>predictive analytics</method>, <method>intelligent automation</method>"
2024,https://openalex.org/W4404105402,Social Sciences,Autonomous mobile robots for exploratory synthetic chemistry,"Abstract Autonomous laboratories can accelerate discoveries in chemical synthesis, but this requires automated measurements coupled with reliable decision-making 1,2 . Most autonomous laboratories involve bespoke automated equipment 3–6 , and reaction outcomes are often assessed using a single, hard-wired characterization technique 7 . Any decision-making algorithms 8 must then operate using this narrow range of characterization data 9,10 . By contrast, manual experiments tend to draw on a wider range of instruments to characterize reaction products, and decisions are rarely taken based on one measurement alone. Here we show that a synthesis laboratory can be integrated into an autonomous laboratory by using mobile robots 11–13 that operate equipment and make decisions in a human-like way. Our modular workflow combines mobile robots, an automated synthesis platform, a liquid chromatography–mass spectrometer and a benchtop nuclear magnetic resonance spectrometer. This allows robots to share existing laboratory equipment with human researchers without monopolizing it or requiring extensive redesign. A heuristic decision-maker processes the orthogonal measurement data, selecting successful reactions to take forward and automatically checking the reproducibility of any screening hits. We exemplify this approach in the three areas of structural diversification chemistry, supramolecular host–guest chemistry and photochemical synthesis. This strategy is particularly suited to exploratory chemistry that can yield multiple potential products, as for supramolecular assemblies, where we also extend the method to an autonomous function assay by evaluating host–guest binding properties.",<method>heuristic decision-maker</method>
2024,https://openalex.org/W4390542282,Social Sciences,Unveiling energy efficiency and renewable electricity’s role in achieving sustainable development goals 7 and 13 policies,"Germany has not yet made significant progress toward achieving SDGs 7 and 13. This challenge can be attributed to the underlying financialization problem in Germany, as well as implementation issues related to energy efficiency, particularly in coal and gas, and renewable energy consumption. Given these circumstances, Germany is facing challenges in reducing greenhouse gas emissions. Addressing this issue may necessitate a policy realignment, which is the primary focus of this study. In this context, our study employs various wavelet-based tools, including wavelet coherence, multivariate wavelet coherence, and wavelet-based causality, to delve into the co-movements, causality, and direction of causality between the load capacity factor (LF) and key factors such as energy efficiency (gas and coal), technological innovation, renewable electricity, and financial development, focusing on Germany during the period from 1990Q1 to 2020Q4. The results from wavelet coherence, corroborated by wavelet cohesion, indicate that in the short and medium term, improvements in coal efficiency, gas efficiency, financial development, and renewable energy are associated with an increase in LF, thereby contributing to ecological quality. Furthermore, the multivariate wavelet coherence results underscore the significant impact of considering or not considering the effect of a third variable in the interrelationship between LF and its determinants, with the effect being more pronounced when the third variable is taken into account. Although this policy framework primarily targets the objectives of SDG 13 and 7, its applicability extends to other EU nations. The primary significance of this study is its proposal of an SDG-focused policy framework.","<method>wavelet coherence</method>, <method>multivariate wavelet coherence</method>, <method>wavelet-based causality</method>"
2024,https://openalex.org/W4390742710,Social Sciences,Machine Learning as a Tool for Hypothesis Generation,"Abstract While hypothesis testing is a highly formalized activity, hypothesis generation remains largely informal. We propose a systematic procedure to generate novel hypotheses about human behavior, which uses the capacity of machine learning algorithms to notice patterns people might not. We illustrate the procedure with a concrete application: judge decisions about whom to jail. We begin with a striking fact: the defendant’s face alone matters greatly for the judge’s jailing decision. In fact, an algorithm given only the pixels in the defendant’s mug shot accounts for up to half of the predictable variation. We develop a procedure that allows human subjects to interact with this black-box algorithm to produce hypotheses about what in the face influences judge decisions. The procedure generates hypotheses that are both interpretable and novel: they are not explained by demographics (e.g., race) or existing psychology research, nor are they already known (even if tacitly) to people or experts. Though these results are specific, our procedure is general. It provides a way to produce novel, interpretable hypotheses from any high-dimensional data set (e.g., cell phones, satellites, online behavior, news headlines, corporate filings, and high-frequency time series). A central tenet of our article is that hypothesis generation is a valuable activity, and we hope this encourages future work in this largely “prescientific” stage of science.",<method>machine learning algorithms</method>
2024,https://openalex.org/W4392101515,Social Sciences,Integrating artificial intelligence into the modernization of traditional Chinese medicine industry: a review,"Traditional Chinese medicine (TCM) is the practical experience and summary of the Chinese nation for thousands of years. It shows great potential in treating various chronic diseases, complex diseases and major infectious diseases, and has gradually attracted the attention of people all over the world. However, due to the complexity of prescription and action mechanism of TCM, the development of TCM industry is still in a relatively conservative stage. With the rise of artificial intelligence technology in various fields, many scholars began to apply artificial intelligence technology to traditional Chinese medicine industry and made remarkable progress. This paper comprehensively summarizes the important role of artificial intelligence in the development of traditional Chinese medicine industry from various aspects, including new drug discovery, data mining, quality standardization and industry technology of traditional Chinese medicine. The limitations of artificial intelligence in these applications are also emphasized, including the lack of pharmacological research, database quality problems and the challenges brought by human-computer interaction. Nevertheless, the development of artificial intelligence has brought new opportunities and innovations to the modernization of traditional Chinese medicine. Integrating artificial intelligence technology into the comprehensive application of Chinese medicine industry is expected to overcome the major problems faced by traditional Chinese medicine industry and further promote the modernization of the whole traditional Chinese medicine industry.",No methods found.
2024,https://openalex.org/W4394838012,Social Sciences,Empathetic Algorithms: The Role of AI in Understanding and Enhancing Human Emotional Intelligence,"In an era where artificial intelligence (AI) seamlessly integrates into the fabric of daily life, understanding and enhancing human emotional intelligence (EI) through empathetic algorithms emerges as a frontier in technological advancement. This research paper explores the development and application of AI systems capable of recognizing, interpreting, and responding to human emotions in a manner that fosters emotional growth and understanding. Through a comprehensive literature review, this study identifies the theoretical underpinnings of emotional intelligence and examines the current landscape of empathetic algorithms. Employing a mixed-methods approach, including case studies and empirical analysis, the paper presents novel insights into how AI can be engineered to support emotional intelligence across various domains, such as healthcare, education, and customer service. Ethical considerations, including privacy, consent, and data security, are thoroughly evaluated to address potential societal implications. The findings suggest that empathetic algorithms hold significant promise in enhancing human emotional intelligence, albeit with challenges that necessitate careful ethical and technical scrutiny. The research culminates in proposing a set of guidelines for future developments in this field, emphasizing the need for interdisciplinary collaboration. This study not only contributes to the theoretical framework of empathetic algorithms but also paves the way for future innovations that prioritize emotional intelligence in the design and implementation of AI systems.",No methods found.
2024,https://openalex.org/W4394961856,Social Sciences,Applications and challenges of neural networks in otolaryngology (Review),"Artificial Intelligence (AI) has become a topic of interest that is frequently debated in all research fields. The medical field is no exception, where several unanswered questions remain. When and how this field can benefit from AI support in daily routines are the most frequently asked questions. The present review aims to present the types of neural networks (NNs) available for development, discussing their advantages, disadvantages and how they can be applied practically. In addition, the present review summarizes how NNs (combined with various other features) have already been applied in studies in the ear nose throat research field, from assisting diagnosis to treatment management. Although the answer to this question regarding AI remains elusive, understanding the basics and types of applicable NNs can lead to future studies possibly using more than one type of NN. This approach may bypass the actual limitations in accuracy and relevance of information generated by AI. The proposed studies, the majority of which used convolutional NNs, obtained accuracies varying 70-98%, with a number of studies having the AI trained on a limited number of cases (<100 patients). The lack of standardization in AI protocols for research negatively affects data homogeneity and transparency of databases.","<method>neural networks (NNs)</method>, <method>convolutional neural networks (convolutional NNs)</method>"
2024,https://openalex.org/W4396902951,Social Sciences,An overview of the perspectives used in health economic evaluations,"Abstract The term ‘perspective’ in the context of economic evaluations and costing studies in healthcare refers to the viewpoint that an analyst has adopted to define the types of costs and outcomes to consider in their studies. However, there are currently notable variations in terms of methodological recommendations, definitions, and applications of different perspectives, depending on the objective or intended user of the study. This can make it a complex area for stakeholders when interpreting these studies. Consequently, there is a need for a comprehensive overview regarding the different types of perspectives employed in such analyses, along with the corresponding implications of their use. This is particularly important, in the context of low-and-middle-income countries (LMICs), where practical guidelines may be less well-established and infrastructure for conducting economic evaluations may be more limited. This article addresses this gap by summarising the main types of perspectives commonly found in the literature to a broad audience (namely the patient, payer, health care providers, healthcare sector, health system, and societal perspectives), providing their most established definitions and outlining the corresponding implications of their uses in health economic studies, with examples particularly from LMIC settings. We then discuss important considerations when selecting the perspective and present key arguments to consider when deciding whether the societal perspective should be used. We conclude that there is no one-size-fits-all answer to what perspective should be used and the perspective chosen will be influenced by the context, policymakers'/stakeholders’ viewpoints, resource/data availability, and intended use of the analysis. Moving forward, considering the ongoing issues regarding the variation in terminology and practice in this area, we urge that more standardised definitions of the different perspectives and the boundaries between them are further developed to support future studies and guidelines, as well as to improve the interpretation and comparison of health economic evidence.",No methods found.
2024,https://openalex.org/W4397028793,Social Sciences,A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets,"Novel view synthesis has seen major advances in recent years, with 3D Gaussian splatting offering an excellent level of visual quality, fast training and real-time rendering. However, the resources needed for training and rendering inevitably limit the size of the captured scenes that can be represented with good visual quality. We introduce a hierarchy of 3D Gaussians that preserves visual quality for very large scenes, while offering an efficient Level-of-Detail (LOD) solution for efficient rendering of distant content with effective level selection and smooth transitions between levels. We introduce a divide-and-conquer approach that allows us to train very large scenes in independent chunks. We consolidate the chunks into a hierarchy that can be optimized to further improve visual quality of Gaussians merged into intermediate nodes. Very large captures typically have sparse coverage of the scene, presenting many challenges to the original 3D Gaussian splatting training method; we adapt and regularize training to account for these issues. We present a complete solution, that enables real-time rendering of very large scenes and can adapt to available resources thanks to our LOD method. We show results for captured scenes with up to tens of thousands of images with a simple and affordable rig, covering trajectories of up to several kilometers and lasting up to one hour.","<method>3D Gaussian splatting</method>, <method>hierarchy of 3D Gaussians</method>, <method>Level-of-Detail (LOD) solution</method>, <method>divide-and-conquer approach</method>, <method>training adaptation and regularization</method>"
2024,https://openalex.org/W4401508155,Social Sciences,REAL TIME OBJECT DETECTION,"The capacity for cameras and computational frameworks to screen a given region has been set up and worked upon by the specialist for quite a long while.The capacity of the human mind to just classify and comprehend what an object is upon a basic look is something that has interested numerous scientists for quite a while.Indeed, even with the progressions in the areas of Artificial insight also ML is as yet a troublesome errand for researchers to make an object following and recognition framework that can work productively in rapidly.We have had the option to make models in the realm of ML which have been prepared thoroughly with a huge number of pictures inside datasets to perceive and comprehend a few objects however we have still neglected to appropriately set up one equivalent to that of the human mind.Although, the downside of computational elements is their capacity to share feelings we have likewise had the option to comprehend the human brain has undeniably more computational force then one can envision.In our task, we will be pursuing making a calculation that will actually want to perceive and portray the entirety of the pictures inside a given edge.To achieve this, we will be building up a module utilizing Python and a webcam that catches the pictures in the type of casings and perceive objects.We will make a calculation that will contrast the object inside the casing with that of a dataset set up inside Amazon AWS to perceive and comprehend what the object is.Through this paper, we will discuss the sorts of programming used to execute to make this conceivable alongside our discoveries alongside the future extent of this innovation.","<method>object tracking</method>, <method>object recognition</method>, <method>machine learning models trained on image datasets</method>"
2024,https://openalex.org/W4390500790,Social Sciences,Adoption of smart technologies in the cruise tourism services: a systematic review and future research agenda,"Purpose The purpose of this paper is to systematically analyze existing studies related to the adoption of smart technologies in cruise tourism services, particularly robots, artificial intelligence, service automation and virtual reality. More specifically, the authors intend to highlight the current state of research on this topic, present the findings within a conceptual framework and propose a research agenda. Design/methodology/approach The relevant literature was extracted using two major electronic databases, web of science (WoS) and Scopus. The authors identified 31 articles from high-quality journals and used a systematic review and the VOSviewer software to analyze them. Findings Since 2014, there has been an increase in the number of studies related to smart technologies in cruise tourism services. At first, researchers focused on Royal Caribbean’s robotic bartender arm, whereas other technologies such as digital signage, self-service options, facial recognition and virtual culinary experiences received less attention. However, the interest in exploring these last smart technologies has grown significantly since 2019. The adoption of RAISA in the cruise tourism service (ASCT) framework was proposed, identifying five major domains: cruise robotic technology, technology innovation, cruise passengers’ engagement behavior, cruise passengers’ technology readiness and privacy perception and knowledge expertise. These domains provide valuable guidance for future research in this field. Originality/value To the best of the authors’ knowledge, this is the first study to systematically analyze literature on the adoption of new technologies in cruise tourism services, specifically focusing on the major technologies available on cruise ships.",<method>artificial intelligence</method>
2024,https://openalex.org/W4390610986,Social Sciences,Impacts of the advancement in artificial intelligence on laboratory medicine in low‐ and middle‐income countries: Challenges and recommendations—A literature review,"Abstract Background and Aims Artificial intelligence (AI) has emerged as a transformative force in laboratory medicine, promising significant advancements in healthcare delivery. This study explores the potential impact of AI on diagnostics and patient management within the context of laboratory medicine, with a particular focus on low‐ and middle‐income countries (LMICs). Methods In writing this article, we conducted a thorough search of databases such as PubMed, ResearchGate, Web of Science, Scopus, and Google Scholar within 20 years. The study examines AI's capabilities, including learning, reasoning, and decision‐making, mirroring human cognitive processes. It highlights AI's adeptness at processing vast data sets, identifying patterns, and expediting the extraction of actionable insights, particularly in medical imaging interpretation and laboratory test data analysis. The research emphasizes the potential benefits of AI in early disease detection, therapeutic interventions, and personalized treatment strategies. Results In the realm of laboratory medicine, AI demonstrates remarkable precision in interpreting medical images such as radiography, computed tomography, and magnetic resonance imaging. Its predictive analytical capabilities extend to forecasting patient trajectories and informing personalized treatment strategies using comprehensive data sets comprising clinical outcomes, patient records, and laboratory results. The study underscores the significance of AI in addressing healthcare challenges, especially in resource‐constrained LMICs. Conclusion While acknowledging the profound impact of AI on laboratory medicine in LMICs, the study recognizes challenges such as inadequate data availability, digital infrastructure deficiencies, and ethical considerations. Successful implementation necessitates substantial investments in digital infrastructure, the establishment of data‐sharing networks, and the formulation of regulatory frameworks. The study concludes that collaborative efforts among stakeholders, including international organizations, governments, and nongovernmental entities, are crucial for overcoming obstacles and responsibly integrating AI into laboratory medicine in LMICs. A comprehensive, coordinated approach is essential for realizing AI's transformative potential and advancing health care in LMICs.",No methods found.
2024,https://openalex.org/W4390847768,Social Sciences,Exploring the role of workforce agility on digital transformation: a systematic literature review,"Purpose Successful digital transformation requires a change in organisational structures, processes, capabilities and competencies. Digital transformation research is more influenced by the technology adaptation model and hence focuses on people's attitudes, behaviour and abilities. Recently, employee agility has attracted attention in the context of technology adoption and Industry 4.0. The current research explores the relationship between employee agility and digital technology adoption in the context of digital transformation by adopting the systematic literature review method. Design/methodology/approach Following the attitude–ability-behaviour–outcome framework, the research explored the specific agile ability, attitude and behaviour characteristics useful for digital transformation. Following the preferred reporting items for systematic reviews and meta-analyses (PRISMA) framework consisting of (1) initiation, (2) screening, (3) evaluation and (4) confirming inclusion (Ambika et al. , 2023), the study identified 19 papers from SCOPUS indexed journals. Findings The study result found that agile attitude characteristics such as collaborative mindset, computer self-efficacy, ambiguity aversiveness, etc. are influencing the digital transformation process. Agile abilities like basic computer knowledge, previous technical experience, cognitive abilities, innovation capability, digital competence training and supporting proper knowledge management practices also influence digital transformation. Finally, agile behaviour such as relationship building, knowledge-sharing behaviour, promoting values of learning, risk-taking and experimenting, rewarding innovativeness and customer-centric innovation and displaying adaptability, resilience and commitment to change, etc. are found to drive digital transformation. Originality/value Research on workforce agility and digital transformation is scarce. The current study contributes to benchmarking research by exploring specific agile attitudes, abilities and behaviour characteristics relevant to digital transformation, especially in the era of Industry 4.0.",No methods found.
2024,https://openalex.org/W4390913521,Social Sciences,A Review of Intraocular Lens Power Calculation Formulas Based on Artificial Intelligence,"Purpose: The proper selection of an intraocular lens power calculation formula is an essential aspect of cataract surgery. This study evaluated the accuracy of artificial intelligence-based formulas. Design: Systematic review. Methods: This review comprises articles evaluating the exactness of artificial intelligence-based formulas published from 2017 to July 2023. The papers were identified by a literature search of various databases (Pubmed/MEDLINE, Google Scholar, Crossref, Cochrane Library, Web of Science, and SciELO) using the terms “IOL formulas”, “FullMonte”, “Ladas”, “Hill-RBF”, “PEARL-DGS”, “Kane”, “Karmona”, “Hoffer QST”, and “Nallasamy”. In total, 25 peer-reviewed articles in English with the maximum sample and the largest number of compared formulas were examined. Results: The scores of the mean absolute error and percentage of patients within ±0.5 D and ±1.0 D were used to estimate the exactness of the formulas. In most studies the Kane formula obtained the smallest mean absolute error and the highest percentage of patients within ±0.5 D and ±1.0 D. Second place was typically achieved by the PEARL DGS formula. The limitations of the studies were also discussed. Conclusions: Kane seems to be the most accurate artificial intelligence-based formula. PEARL DGS also gives very good results. Hoffer QST, Karmona, and Nallasamy are the newest, and need further evaluation.","<method>FullMonte</method>, <method>Ladas</method>, <method>Hill-RBF</method>, <method>PEARL-DGS</method>, <method>Kane</method>, <method>Karmona</method>, <method>Hoffer QST</method>, <method>Nallasamy</method>"
2024,https://openalex.org/W4391002180,Social Sciences,Regional post-mining land use assessment: An interdisciplinary and multi-stakeholder approach,"Mine closure is regulated and planned on an individual site basis and selection of post-mining land uses (PMLUs) are commonly considered from that perspective. Some mining jurisdictions, such as Queensland in Australia, are starting to acknowledge the need for wider regional planning approaches in which PMLU selection considers regional and local planning strategies, the surrounding landscape and community views. Shifting thinking from site-specific planning to regional scale offers strategic advantages including the ability to consider options that are only viable at scale and to reinstate larger expanses of native bushland or functional agricultural land. Assessing and selecting PMLU options at regional scale requires an interdisciplinary approach that incorporates diverse technical, planning and social stakeholders. However, there is little guidance on how to conduct an assessment of this type at this scale. A collaborative regional PMLU assessment methodology is presented that addresses this gap. The methodology explicitly focuses on the complex contextual factors found in different mining regions to determine what PMLUs a region is capable of sustaining. The methodology provides a path for collaboration between mine operators, governments, regulators and regional stakeholders by providing data that supports their quest for viable post-mining futures. It enables regional stakeholders to start a conversation about what PMLUs are suitable for the region and to foster a collaborative approach to the first stage of PMLU decision-making. The methodology is intended to be a precursor to more detailed decision analysis. Importantly, it does not eliminate the need for planning and decision-making at the mine level.",No methods found.
2024,https://openalex.org/W4391065816,Social Sciences,Multi-criteria decision making for solar power - Wind power plant site selection using a GIS-intuitionistic fuzzy-based approach with an application in the Netherlands,"The development of a country cannot be realized only through the amount of energy it produces and its industrialization. In a country where its people are left homeless and poor, and its cultural and natural riches are destroyed, the electricity produced is not a measure of development on its own. Development and progress must be considered from a holistic perspective that includes the country's geographical structure, all its living creatures, culture, urban and social structure as a whole. In this respect, the transition to renewable energy is imperative. One of the most widely used renewable energies in the Netherlands is solar and wind energy. For these power plants, site selection is an important factor in reducing the installation cost of the wind and solar power plant and achieving maximum efficiency during operation. This paves the way for the study of a site selection problem. In this study, we first investigate possible locations for solar-wind power plant installation for 12 regions of the Netherlands, namely Noord Holland, Gelderland, Friesland, North Brabant, Drenthe, Groningen, Zeeland (Middelburg), Utrecht, Zuid Holland, Limburg, Over Ijssel and Flevoland, using GIS as a mapping method, and then apply a Intuitionistic fuzzy-based approach to the problem to obtain the optimal locations for both solar and wind energy. Furthermore, the results of two methods (GIS and Intuitionistic fuzzy-based approach) are compared to obtain more accurate results. The results show that 35317.2 km2 is suitable for solar power plant and 34844.5 km2 is suitable for wind turbine, but only 34875.8 km2 is suitable for solar-wind power plan installation.",<method>Intuitionistic fuzzy-based approach</method>
2024,https://openalex.org/W4391754120,Social Sciences,"Good models borrow, great models steal: intellectual property rights and generative AI","Abstract Two critical policy questions will determine the impact of generative artificial intelligence (AI) on the knowledge economy and the creative sector. The first concerns how we think about the training of such models—in particular, whether the creators or owners of the data that are “scraped” (lawfully or unlawfully, with or without permission) should be compensated for that use. The second question revolves around the ownership of the output generated by AI, which is continually improving in quality and scale. These topics fall in the realm of intellectual property, a legal framework designed to incentivize and reward only human creativity and innovation. For some years, however, Britain has maintained a distinct category for “computer-generated” outputs; on the input issue, the EU and Singapore have recently introduced exceptions allowing for text and data mining or computational data analysis of existing works. This article explores the broader implications of these policy choices, weighing the advantages of reducing the cost of content creation and the value of expertise against the potential risk to various careers and sectors of the economy, which might be rendered unsustainable. Lessons may be found in the music industry, which also went through a period of unrestrained piracy in the early digital era, epitomized by the rise and fall of the file-sharing service Napster. Similar litigation and legislation may help navigate the present uncertainty, along with an emerging market for “legitimate” models that respect the copyright of humans and are clear about the provenance of their own creations.",No methods found.
2024,https://openalex.org/W4392138827,Social Sciences,Development and application of emotion recognition technology — a systematic literature review,"Abstract Background There is a mutual influence between emotions and diseases. Thus, the subject of emotions has gained increasing attention. Objective The primary objective of this study was to conduct a comprehensive review of the developments in emotion recognition technology over the past decade. This review aimed to gain insights into the trends and real-world effects of emotion recognition technology by examining its practical applications in different settings, including hospitals and home environments. Methods This study followed the Preferred Reporting Items for Systematic Reviews (PRISMA) guidelines and included a search of 4 electronic databases, namely, PubMed, Web of Science, Google Scholar and IEEE Xplore, to identify eligible studies published between 2013 and 2023. The quality of the studies was assessed using the Critical Appraisal Skills Programme (CASP) criteria. The key information from the studies, including the study populations, application scenarios, and technological methods employed, was summarized and analyzed. Results In a systematic literature review of the 44 studies that we analyzed the development and impact of emotion recognition technology in the field of medicine from three distinct perspectives: “application scenarios,” “techniques of multiple modalities,” and “clinical applications.” The following three impacts were identified: (i) The advancement of emotion recognition technology has facilitated remote emotion recognition and treatment in hospital and home environments by healthcare professionals. (ii) There has been a shift from traditional subjective emotion assessment methods to multimodal emotion recognition methods that are grounded in objective physiological signals. This technological progress is expected to enhance the accuracy of medical diagnosis. (iii) The evolving relationship between emotions and disease throughout diagnosis, intervention, and treatment processes holds clinical significance for real-time emotion monitoring. Conclusion These findings indicate that the integration of emotion recognition technology with intelligent devices has led to the development of application systems and models, which provide technological support for the recognition of and interventions for emotions. However, the continuous recognition of emotional changes in dynamic or complex environments will be a focal point of future research.",No methods found.
2024,https://openalex.org/W4392516399,Social Sciences,Artificial intelligence in dermatology: advancements and challenges in skin of color,"Abstract Artificial intelligence (AI) uses algorithms and large language models in computers to simulate human‐like problem‐solving and decision‐making. AI programs have recently acquired widespread popularity in the field of dermatology through the application of online tools in the assessment, diagnosis, and treatment of skin conditions. A literature review was conducted using PubMed and Google Scholar analyzing recent literature (from the last 10 years through October 2023) to evaluate current AI programs in use for dermatologic purposes, identifying challenges in this technology when applied to skin of color (SOC), and proposing future steps to enhance the role of AI in dermatologic practice. Challenges surrounding AI and its application to SOC stem from the underrepresentation of SOC in datasets and issues with image quality and standardization. With these existing issues, current AI programs inevitably do worse at identifying lesions in SOC. Additionally, only 30% of the programs identified in this review had data reported on their use in dermatology, specifically in SOC. Significant development of these applications is required for the accurate depiction of darker skin tone images in datasets. More research is warranted in the future to better understand the efficacy of AI in aiding diagnosis and treatment options for SOC patients.","<method>algorithms</method>, <method>large language models</method>"
2024,https://openalex.org/W4392592756,Social Sciences,Beyond Discrimination: Generative AI Applications and Ethical Challenges in Forensic Psychiatry,"The advent and growing popularity of generative artificial intelligence (GenAI) holds the potential to revolutionise AI applications in forensic psychiatry and criminal justice, which traditionally relied on discriminative AI algorithms. Generative AI models mark a significant shift from the previously prevailing paradigm through their ability to generate seemingly new realistic data and analyse and integrate a vast amount of unstructured content from different data formats. This potential extends beyond reshaping conventional practices, like risk assessment, diagnostic support, and treatment and rehabilitation plans, to creating new opportunities in previously underexplored areas, such as training and education. This paper examines the transformative impact of generative artificial intelligence on AI applications in forensic psychiatry and criminal justice. First, it introduces generative AI and its prevalent models. Following this, it reviews the current applications of discriminative AI in forensic psychiatry. Subsequently, it presents a thorough exploration of the potential of generative AI to transform established practices and introduce novel applications through multimodal generative models, data generation and data augmentation. Finally, it provides a comprehensive overview of ethical and legal issues associated with deploying generative AI models, focusing on their impact on individuals as well as their broader societal implications. In conclusion, this paper aims to contribute to the ongoing discourse concerning the dynamic challenges of generative AI applications in forensic contexts, highlighting potential opportunities, risks, and challenges. It advocates for interdisciplinary collaboration and emphasises the necessity for thorough, responsible evaluations of generative AI models before widespread adoption into domains where decisions with substantial life-altering consequences are routinely made.","<method>generative artificial intelligence (GenAI)</method>, <method>discriminative AI algorithms</method>, <method>generative AI models</method>, <method>multimodal generative models</method>, <method>data generation</method>, <method>data augmentation</method>"
2024,https://openalex.org/W4392911004,Social Sciences,Unlocking the Potential of Artificial Intelligence in Fashion Design and E-Commerce Applications: The Case of Midjourney,"The fashion industry has shown increasing interest in applying artificial intelligence (AI), yet there is a significant gap in exploring the potential of emerging diffusion-modeling-based AI image-generation systems for fashion design and commerce. Therefore, this study aims to assess the effectiveness of Midjourney, one such AI system, in both fashion design and related commerce applications. We employed the action research approach with the Functional, Expressive, and Aesthetic (FEA) Consumer Needs Model as the theoretical framework. Our research comprised three stages: refining an initial idea into well-defined textual design concepts, facilitating concept development, and validating the preceding observations and reflections by creating a new line of hemp-based products that were evaluated by targeted consumers through an online survey. Findings reveal that this AI tool can assist fashion designers in creating both visually expressive attire and ready-to-wear products, meeting defined design criteria and consumer needs. Midjourney shows promise in streamlining the fashion design process by enhancing ideation and optimizing design details. Potential e-commercial applications of such AI systems were proposed, benefiting physical and digital fashion businesses. It is noted that, to date, the major limitations of using Midjourney encompass its restriction to only facilitating early fashion design stages and necessitating substantial involvement from designers.","<method>diffusion-modeling-based AI image-generation systems</method>, <method>Midjourney</method>"
2024,https://openalex.org/W4394627367,Social Sciences,ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments,"Vision-language navigation is a task that requires an agent to follow instructions to navigate in environments. It becomes increasingly crucial in the field of embodied AI, with potential applications in autonomous navigation, search and rescue, and human-robot interaction. In this paper, we propose to address a more practical yet challenging counterpart setting - vision-language navigation in continuous environments (VLN-CE). To develop a robust VLN-CE agent, we propose a new navigation framework, ETPNav, which focuses on two critical skills: 1) the capability to abstract environments and generate long-range navigation plans, and 2) the ability of obstacle-avoiding control in continuous environments. ETPNav performs online topological mapping of environments by self-organizing predicted waypoints along a traversed path, without prior environmental experience. It privileges the agent to break down the navigation procedure into high-level planning and low-level control. Concurrently, ETPNav utilizes a transformer-based cross-modal planner to generate navigation plans based on topological maps and instructions. The plan is then performed through an obstacle-avoiding controller that leverages a trial-and-error heuristic to prevent navigation from getting stuck in obstacles. Experimental results demonstrate the effectiveness of the proposed method. ETPNav yields more than 10% and 20% improvements over prior state-of-the-art on R2R-CE and RxR-CE datasets, respectively. Our code is available at <uri xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">https://github.com/MarSaKi/ETPNav</uri> .","<method>transformer-based cross-modal planner</method>, <method>trial-and-error heuristic</method>"
2024,https://openalex.org/W4400133228,Social Sciences,Driving Growth: The Integral Role of Small Businesses in the U.S. Economic Landscape,"This comprehensive research endeavor undertakes an exhaustive examination of the far-reaching influence exerted by small businesses on the intricate fabric of the U.S. economy. Employing a meticulously crafted methodology that combines random interviews with a diverse array of small and medium enterprises across the United States and an extensive review of secondary sources, this study endeavors to unravel the multifaceted effects and ramifications of small businesses. From serving as engines of job creation and catalysts for local economic development to driving industrial expansion and fostering innovation, the pivotal role played by small businesses in shaping the economic landscape of the nation becomes increasingly apparent. Furthermore, this research undertakes a critical analysis of the myriad challenges and negative externalities that often beset small businesses, offering incisive insights and strategic recommendations aimed at mitigating these obstacles and fortifying resilience. The robust findings derived from this study not only underscore the indispensable contributions of small businesses to the vitality and dynamism of the U.S. economy but also serve as a clarion call for policymakers, stakeholders, and entrepreneurs alike to redouble efforts in fostering an environment conducive to the sustained growth and prosperity of small businesses. By equipping stakeholders with actionable insights and tailored solutions, this research seeks to empower small businesses to navigate the complexities of the U.S. business landscape and thrive amidst evolving economic paradigms.",No methods found.
2024,https://openalex.org/W4390618081,Social Sciences,Making Sense of Machine Learning: A Review of Interpretation Techniques and Their Applications,"Transparency in AI models is essential for promoting human–AI collaboration and ensuring regulatory compliance. However, interpreting these models is a complex process influenced by various methods and datasets. This study presents a comprehensive overview of foundational interpretation techniques, meticulously referencing the original authors and emphasizing their pivotal contributions. Recognizing the seminal work of these pioneers is imperative for contextualizing the evolutionary trajectory of interpretation in the field of AI. Furthermore, this research offers a retrospective analysis of interpretation techniques, critically evaluating their inherent strengths and limitations. We categorize these techniques into model-based, representation-based, post hoc, and hybrid methods, delving into their diverse applications. Furthermore, we analyze publication trends over time to see how the adoption of advanced computational methods within various categories of interpretation techniques has shaped the development of AI interpretability over time. This analysis highlights a notable preference shift towards data-driven approaches in the field. Moreover, we consider crucial factors such as the suitability of these techniques for generating local or global insights and their compatibility with different data types, including images, text, and tabular data. This structured categorization serves as a guide for practitioners navigating the landscape of interpretation techniques in AI. In summary, this review not only synthesizes various interpretation techniques but also acknowledges the contributions of their original authors. By emphasizing the origins of these techniques, we aim to enhance AI model explainability and underscore the importance of recognizing biases, uncertainties, and limitations inherent in the methods and datasets. This approach promotes the ethical and practical use of interpretation insights, empowering AI practitioners, researchers, and professionals to make informed decisions when selecting techniques for responsible AI implementation in real-world scenarios.","<method>model-based methods</method>, <method>representation-based methods</method>, <method>post hoc methods</method>, <method>hybrid methods</method>, <method>data-driven approaches</method>"
2024,https://openalex.org/W4390938718,Social Sciences,A Credible and Fair Federated Learning Framework Based on Blockchain,"Federated learning enables cooperative computation between multiple participants while protecting user privacy. Currently, federated learning algorithms assume that all participants are trustworthy and their systems are secure. However, the following problems arise in real-world scenarios: (1) Malicious clients disrupt federated learning through model poisoning and data poisoning attacks. Although some research has proposed secure aggregation methods to solve this problem, most methods have limitations. (2) Due to the variance in data quality and computational resources among participants, rewards cannot be distributed equally. Some clients also exhibit free-rider behavior, seeking to cheat the reward system and manipulate global models. Evaluating client contribution and distributing rewards also present challenges. <p xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">To address these challenges, we design a trustworthy federated framework to ensure secure computing throughout the federated task process. First, we propose a malicious model detection method for secure model aggregation. Then, we also propose a fair method of assessing contribution to identify client-side free-riding behavior. Lastly, we develop a computation process grounded in blockchain and smart contracts to guarantee the trustworthiness and fairness of federated tasks. To validate the performance of our framework, we simulate different types of client attacks and contribution evaluation scenarios on several open-source datasets. The experiments show that our framework guarantees the federated task's credibility and achieves fair client contribution evaluation.","<method>malicious model detection method for secure model aggregation</method>, <method>fair method of assessing contribution to identify client-side free-riding behavior</method>"
2024,https://openalex.org/W4391723759,Social Sciences,Artificial Intelligence to Automate Network Meta-Analyses: Four Case Studies to Evaluate the Potential Application of Large Language Models,"The emergence of artificial intelligence, capable of human-level performance on some tasks, presents an opportunity to revolutionise development of systematic reviews and network meta-analyses (NMAs). In this pilot study, we aim to assess use of a large-language model (LLM, Generative Pre-trained Transformer 4 [GPT-4]) to automatically extract data from publications, write an R script to conduct an NMA and interpret the results. We considered four case studies involving binary and time-to-event outcomes in two disease areas, for which an NMA had previously been conducted manually. For each case study, a Python script was developed that communicated with the LLM via application programming interface (API) calls. The LLM was prompted to extract relevant data from publications, to create an R script to be used to run the NMA and then to produce a small report describing the analysis. The LLM had a > 99% success rate of accurately extracting data across 20 runs for each case study and could generate R scripts that could be run end-to-end without human input. It also produced good quality reports describing the disease area, analysis conducted, results obtained and a correct interpretation of the results. This study provides a promising indication of the feasibility of using current generation LLMs to automate data extraction, code generation and NMA result interpretation, which could result in significant time savings and reduce human error. This is provided that routine technical checks are performed, as recommend for human-conducted analyses. Whilst not currently 100% consistent, LLMs are likely to improve with time.","<method>large-language model (LLM, Generative Pre-trained Transformer 4 [GPT-4])</method>"
2024,https://openalex.org/W4392186815,Social Sciences,BioLORD-2023: semantic textual representations fusing large language models and clinical knowledge graph insights,"Abstract Objective In this study, we investigate the potential of large language models (LLMs) to complement biomedical knowledge graphs in the training of semantic models for the biomedical and clinical domains. Materials and Methods Drawing on the wealth of the Unified Medical Language System knowledge graph and harnessing cutting-edge LLMs, we propose a new state-of-the-art approach for obtaining high-fidelity representations of biomedical concepts and sentences, consisting of 3 steps: an improved contrastive learning phase, a novel self-distillation phase, and a weight averaging phase. Results Through rigorous evaluations of diverse downstream tasks, we demonstrate consistent and substantial improvements over the previous state of the art for semantic textual similarity (STS), biomedical concept representation (BCR), and clinically named entity linking, across 15+ datasets. Besides our new state-of-the-art biomedical model for English, we also distill and release a multilingual model compatible with 50+ languages and finetuned on 7 European languages. Discussion Many clinical pipelines can benefit from our latest models. Our new multilingual model enables a range of languages to benefit from our advancements in biomedical semantic representation learning, opening a new avenue for bioinformatics researchers around the world. As a result, we hope to see BioLORD-2023 becoming a precious tool for future biomedical applications. Conclusion In this article, we introduced BioLORD-2023, a state-of-the-art model for STS and BCR designed for the clinical domain.","<method>contrastive learning</method>, <method>self-distillation</method>, <method>weight averaging</method>"
2024,https://openalex.org/W4392851944,Social Sciences,Promoting the Transition towards Agriculture 4.0: A Systematic Literature Review on Drivers and Barriers,"In the modern era, the imperative of digitalisation to enhance competitiveness spans various sectors, with agriculture being no exception. Agriculture 4.0, strategically positioned to address challenges like climate change, food security, and resource preservation, holds the potential to increase productivity, profitability, and sustainability in agriculture. Despite the global accessibility to digital technologies, their adoption within the agriculture sector, especially among small and medium-sized farms, encounters obstacles. Realising the full potential of Agriculture 4.0 requires understanding the factors influencing technology adoption. To address this, the study conducts a systematic literature review using the PRISMA method, focusing on identifying the primary drivers and barriers associated with the implementation of Agriculture 4.0 technologies. The study is complemented by a network analysis of the identified drivers and barriers. A total of 42 articles from 2011 to 2023, sourced from the Scopus database, are examined. Individual and farm-related factors play a crucial role in driving the adoption of smart farming technologies, along with social trust and influence. Economic constraints and lack of infrastructure, such as internet access, emerge as significant barriers. The identified drivers and barriers can inform the development of strategies to promote the transition to Agriculture 4.0. Farmers stand to benefit from insights into potential advantages, required skills, and challenges, aiding informed decision-making in the adoption of Agriculture 4.0 technologies.",No methods found.
2024,https://openalex.org/W4393281169,Social Sciences,Artificial intelligence techniques in financial trading: A systematic literature review,"Artificial Intelligence (AI) approaches have been increasingly used in financial markets as technology advances. In this research paper, we conduct a Systematic Literature Review (SLR) that studies financial trading approaches through AI techniques. It reviews 143 research articles that implemented AI techniques in financial trading markets. Accordingly, it presents several findings and observations after reviewing the papers from the following perspectives: the financial trading market and the asset type, the trading analysis type considered along with the AI technique, and the AI techniques utilized in the trading market, the estimation and performance metrics of the proposed models. The selected research articles were published between 2015 and 2023, and this review addresses four RQs. After analyzing the selected research articles, we observed 8 financial markets used in building predictive models. Moreover, we found that technical analysis is more adopted compared to fundamental analysis. Furthermore, 16% of the selected research articles entirely automate the trading process. In addition, we identified 40 different AI techniques that are used as standalone and hybrid models. Among these techniques, deep learning techniques are the most frequently used in financial trading markets. Building prediction models for financial markets using AI is a promising field of research, and academics have already deployed several machine learning models. As a result of this evaluation, we provide recommendations and guidance to researchers.","<method>deep learning techniques</method>, <method>machine learning models</method>"
2024,https://openalex.org/W4394620240,Social Sciences,Human-AI interaction in skin cancer diagnosis: a systematic review and meta-analysis,"Abstract The development of diagnostic tools for skin cancer based on artificial intelligence (AI) is increasing rapidly and will likely soon be widely implemented in clinical use. Even though the performance of these algorithms is promising in theory, there is limited evidence on the impact of AI assistance on human diagnostic decisions. Therefore, the aim of this systematic review and meta-analysis was to study the effect of AI assistance on the accuracy of skin cancer diagnosis. We searched PubMed, Embase, IEE Xplore, Scopus and conference proceedings for articles from 1/1/2017 to 11/8/2022. We included studies comparing the performance of clinicians diagnosing at least one skin cancer with and without deep learning-based AI assistance. Summary estimates of sensitivity and specificity of diagnostic accuracy with versus without AI assistance were computed using a bivariate random effects model. We identified 2983 studies, of which ten were eligible for meta-analysis. For clinicians without AI assistance, pooled sensitivity was 74.8% (95% CI 68.6–80.1) and specificity was 81.5% (95% CI 73.9–87.3). For AI-assisted clinicians, the overall sensitivity was 81.1% (95% CI 74.4–86.5) and specificity was 86.1% (95% CI 79.2–90.9). AI benefitted medical professionals of all experience levels in subgroup analyses, with the largest improvement among non-dermatologists. No publication bias was detected, and sensitivity analysis revealed that the findings were robust. AI in the hands of clinicians has the potential to improve diagnostic accuracy in skin cancer diagnosis. Given that most studies were conducted in experimental settings, we encourage future studies to further investigate these potential benefits in real-life settings.","<method>deep learning-based AI assistance</method>, <method>bivariate random effects model</method>"
2024,https://openalex.org/W4396546585,Social Sciences,International Consensus Definition and Diagnostic Criteria for Generalized Pustular Psoriasis From the International Psoriasis Council,"Importance Generalized pustular psoriasis (GPP) lacks internationally accepted definitions and diagnostic criteria, impeding timely diagnosis and treatment and hindering cross-regional clinical and epidemiological study comparisons. Objective To develop an international consensus definition and diagnostic criteria for GPP using the modified Delphi method. Evidence Review The rarity of GPP presents a challenge in acquiring comprehensive published clinical data necessary for developing standardized definition and criteria. Instead of relying on a literature search, 43 statements that comprehensively addressed the fundamental aspects of the definitions and diagnostic criteria for GPP were formulated based on expert reviews of 64 challenging GPP cases. These statements were presented to a panel of 33 global GPP experts for voting, discussion, and refinements in 2 virtual consensus meetings. Consensus during voting was defined as at least 80% agreement; the definition and diagnostic criteria were accepted by all panelists after voting and in-depth discussion. Findings In the first and second modified Delphi round, 30 (91%) and 25 (76%) experts participated. In the initial Delphi round, consensus was achieved for 53% of the statements, leading to the approval of 23 statements that were utilized to develop the proposed definitions and diagnostic criteria for GPP. During the second Delphi round, the final definition established was, “Generalized Pustular Psoriasis is a systemic inflammatory disease characterized by cutaneous erythema and macroscopically visible sterile pustules.” It can occur with or without systemic symptoms, other psoriasis types, and laboratory abnormalities. GPP may manifest as an acute form with widespread pustules or a subacute variant with an annular phenotype. The identified essential criterion was, “Macroscopically visible sterile pustules on erythematous base and not restricted to the acral region or within psoriatic plaques.” Conclusions and Relevance The achievement of international consensus on the definition and diagnostic criteria for GPP underscores the importance of collaboration, innovative methodology, and expert engagement to address rare diseases. Although further validation is needed, these criteria can serve as a reference point for clinicians, researchers, and patients, which may contribute to more accurate diagnosis and improved management of GPP.",No methods found.
2024,https://openalex.org/W4400101773,Social Sciences,Jurisprudential analysis on substitute compensation in the Departament of Caldas: Contrast between Legal Security and the right to Social Security,"The substitute indemnity for the old-age pension stands as an economic benefit for the worker who failed to comply with the number of weeks required by the Law for the recognition of the old-age pension, having to profess Law 100 of 1993, erected a General Pension System divided into a medium premium regime with a defined benefit administered by the Colombian Pension Administrator – COLPENSIONES; and, an individual savings scheme with solidarity, where a group of private fund insurers proliferate. Now, the aforementioned norm established in article 151, that the General Pension System would enter into force for public servants at the departmental, municipal and district level as of June 30, 1995 (Law 100, 1993), therefore, in advance As of that date, this charge was at the head of the social security funds of the territorial entities, finding that some of them were not affiliated to said funds or despite being, the benefit was denied when it was specified that the introduction of this benefit did not applied to such employees. Thus, it is important from the pronouncements of the Constitutional Court, the Supreme Court of Justice, the contrast that is presented in the subject in question will be established, since it has been indicated by the aforementioned corporations that rights such as social security by the government of Caldas, by not recognizing the substitute compensation for employees terminated prior to June 30, 1995.",No methods found.
2024,https://openalex.org/W4400243314,Social Sciences,The Ethical Concerns of Artificial Intelligence in Urban Planning,"Problem, research strategy, and findings The integration of a artificial intelligence (AI) into urban planning presents potential ethical challenges, including concerns about bias, transparency, accountability, privacy, and misinformation. As planners rely more on AI for decision making, the potential for these systems to perpetuate biases, obscure decision-making processes, and infringe on privacy becomes more pronounced, potentially undermining public trust and excluding marginalized communities. We reviewed existing literature on AI ethics in urban planning, examining biases, transparency, accountability, and privacy issues. Our methodology synthesized findings from various studies, reports, and theoretical frameworks to highlight ethical concerns in AI-driven urban planning. Recommendations for ethical AI implementation emphasize transparency, inclusive data sets, public engagement, and robust ethical guidelines. Our research identified critical ethical concerns in AI-driven urban planning. Bias in AI systems can lead to unequal outcomes, disproportionately affecting marginalized communities. Transparency issues arise from the black box nature of AI, complicating understanding and trust in AI-driven decisions. Privacy concerns are heightened due to extensive data collection and potential misuse, raising the risk of surveillance and data breaches. Limitations include the availability of specific literature focused on AI ethics for urban planning and the evolving nature of AI technologies, suggesting a need for ongoing research and adaptive strategies. Human oversight and continuous monitoring are essential to ensure ethical practices, with an emphasis on community engagement and public education to foster trust and inclusivity.",No methods found.
2024,https://openalex.org/W4400807856,Social Sciences,On legal personhood of artificial intelligence,"Artificial intelligence is becoming an increasingly significant factor in social and economic life. As it is capable of performing a large portion of tasks as well as or even better and faster than humans, its use is becoming widespread, ranging from simple tasks in manufacturing plants to participating in online conclusion of contract and even in diagnostics and surgical procedures in medicine. Although it represents a huge potential for progress and development in society and the improvement of quality of life of an individual, it also brings significant risks. The question of the legal personhood of artificial intelligence was raised long before it became such a significant factor in legal transactions. However, in the last decade, it has become particularly relevant. Despite this, there is disagreement in the doctrine, both in terms of terminology and regarding whether artificial intelligence should be recognized as a legal subject or not. The paper presents various arguments on which authors advocating for the recognition of legal personhood of AI have based their stance, as well as reasons on which other authors base their skepticism towards expanding the circle of legal subjects in favor of artificial intelligence. At the end of the paper, based on the analysis of the presented views and arguments, the authors take a stance on whether and under what conditions legal personhood should be recognized to artificial intelligence.",No methods found.
2024,https://openalex.org/W4401726216,Social Sciences,Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model,"Large language models (LLMs) have made a significant impact on the fields of general artificial intelligence. General purpose LLMs exhibit strong logic and reasoning skills and general world knowledge but can sometimes generate misleading results when prompted on specific subject areas. LLMs trained with domain-specific knowledge can reduce the generation of misleading information (i.e. hallucinations) and enhance the precision of LLMs in specialized contexts. Training new LLMs on specific corpora however can be resource intensive. Here we explored the use of a retrieval-augmented generation (RAG) model which we tested on literature specific to a biomedical research area. OpenAI’s GPT-3.5, GPT-4, Microsoft’s Prometheus, and a custom RAG model were used to answer 19 questions pertaining to diffuse large B-cell lymphoma (DLBCL) disease biology and treatment. Eight independent reviewers assessed LLM responses based on accuracy, relevance, and readability, rating responses on a 3-point scale for each category. These scores were then used to compare LLM performance. The performance of the LLMs varied across scoring categories. On accuracy and relevance, the RAG model outperformed other models with higher scores on average and the most top scores across questions. GPT-4 was more comparable to the RAG model on relevance versus accuracy. By the same measures, GPT-4 and GPT-3.5 had the highest scores for readability of answers when compared to the other LLMs. GPT-4 and 3.5 also had more answers with hallucinations than the other LLMs, due to non-existent references and inaccurate responses to clinical questions. Our findings suggest that an oncology research-focused RAG model may outperform general-purpose LLMs in accuracy and relevance when answering subject-related questions. This framework can be tailored to Q&amp;A in other subject areas. Further research will help understand the impact of LLM architectures, RAG methodologies, and prompting techniques in answering questions across different subject areas.","<method>retrieval-augmented generation (RAG) model</method>, <method>OpenAI’s GPT-3.5</method>, <method>OpenAI’s GPT-4</method>, <method>Microsoft’s Prometheus</method>"
2024,https://openalex.org/W4403618367,Social Sciences,"Exploring Rich Subjective Quality Information for Image Quality
  Assessment in the Wild","Traditional in the wild image quality assessment (IQA) models are generally trained with the quality labels of mean opinion score (MOS), while missing the rich subjective quality information contained in the quality ratings, for example, the standard deviation of opinion scores (SOS) or even distribution of opinion scores (DOS). In this paper, we propose a novel IQA method named RichIQA to explore the rich subjective rating information beyond MOS to predict image quality in the wild. RichIQA is characterized by two key novel designs: (1) a three-stage image quality prediction network which exploits the powerful feature representation capability of the Convolutional vision Transformer (CvT) and mimics the short-term and long-term memory mechanisms of human brain; (2) a multi-label training strategy in which rich subjective quality information like MOS, SOS and DOS are concurrently used to train the quality prediction network. Powered by these two novel designs, RichIQA is able to predict the image quality in terms of a distribution, from which the mean image quality can be subsequently obtained. Extensive experimental results verify that the three-stage network is tailored to predict rich quality information, while the multi-label training strategy can fully exploit the potentials within subjective quality rating and enhance the prediction performance and generalizability of the network. RichIQA outperforms state-of-the-art competitors on multiple large-scale in the wild IQA databases with rich subjective rating labels. The code of RichIQA will be made publicly available on GitHub.","<method>Convolutional vision Transformer (CvT)</method>, <method>three-stage image quality prediction network</method>, <method>multi-label training strategy</method>"
2024,https://openalex.org/W4390506438,Social Sciences,Artificial intelligence for oral squamous cell carcinoma detection based on oral photographs: A comprehensive literature review,"Abstract Introduction Oral squamous cell carcinoma (OSCC) presents a significant global health challenge. The integration of artificial intelligence (AI) and computer vision holds promise for the early detection of OSCC through the analysis of digitized oral photographs. This literature review explores the landscape of AI‐driven OSCC automatic detection, assessing both the performance and limitations of the current state of the art. Materials and Methods An electronic search using several data base was conducted, and a systematic review performed in accordance with PRISMA guidelines (CRD42023441416). Results Several studies have demonstrated remarkable results for this task, consistently achieving sensitivity rates exceeding 85% and accuracy rates surpassing 90%, often encompassing around 1000 images. The review scrutinizes these studies, shedding light on their methodologies, including the use of recent machine learning and pattern recognition approaches coupled with different supervision strategies. However, comparing the results from different papers is challenging due to variations in the datasets used. Discussion Considering these findings, this review underscores the urgent need for more robust and reliable datasets in the field of OSCC detection. Furthermore, it highlights the potential of advanced techniques such as multi‐task learning, attention mechanisms, and ensemble learning as crucial tools in enhancing the accuracy and sensitivity of OSCC detection through oral photographs. Conclusion These insights collectively emphasize the transformative impact of AI‐driven approaches on early OSCC diagnosis, with the potential to significantly improve patient outcomes and healthcare practices.","<method>machine learning</method>, <method>pattern recognition</method>, <method>multi-task learning</method>, <method>attention mechanisms</method>, <method>ensemble learning</method>"
2024,https://openalex.org/W4390917123,Social Sciences,Decision-making for solar panel selection using Sugeno-Weber triangular norm-based on q-rung orthopair fuzzy information,"Solar power is an alternative energy derived from the sun. Solar power is more environmentally friendly and sustainable than burning fossil fuels which releases harmful greenhouse gas emissions. Therefore, this study aims to evaluate a reliable solar panel based on certain characteristics by incorporating the theory of the decision-making process. To serve this goal, this study discusses a well-known aggregation model of the q-rung orthopair fuzzy set, which is a broader and flexible environment of fuzzy sets and intuitionistic fuzzy sets used to handle unpredictable information of human opinions. The key components of this article are to demonstrate some realistic operations of Sugeno–Weber triangular norms considering q-rung orthopair fuzzy information. These operations provide authentic estimated information during the decision-making process. We developed a class of new aggregation operators using the q-rung orthopair fuzzy information system, including q-rung orthopair fuzzy Sugeno–Weber power weighted average and q-rung orthopair fuzzy Sugeno–Weber power weighted geometric operators. Some realistic characteristics and special cases are also demonstrated to show the compatibility of the proposed methodologies. An innovative approach to the multi-attribute decision-making problem is utilized to resolve different real-life applications considering various criteria or attributes. To show the intensity and applicability of the proposed approaches, we explored a numerical example for efficient solar panel selection based on the proposed methodologies. Furthermore, we presented a comprehensive comparison technique to compare the findings of the existing methods with the proposed aggregation approaches. Finally, the proposed research work is summarized, and the future prospects are discussed.","<method>q-rung orthopair fuzzy set</method>, <method>aggregation operators using the q-rung orthopair fuzzy information system</method>, <method>q-rung orthopair fuzzy Sugeno–Weber power weighted average</method>, <method>q-rung orthopair fuzzy Sugeno–Weber power weighted geometric operators</method>, <method>multi-attribute decision-making problem approach</method>"
2024,https://openalex.org/W4391074613,Social Sciences,"Application of artificial intelligence in medical education: A review of benefits, challenges, and solutions","Artificial intelligence (AI) holds indisputable potential in solving global challenges faced in healthcare provision, resulting in its ever-increasing utilization in various medical fields. This review aims to present a thorough evaluation of the advantages, challenges, and leading strategies for utilizing AI in the field of medical education. In order to access the latest studies and documents, a comprehensive literature search was conducted in Medline (via PubMed), Scopus, and Web of Science databases, using free keywords and MeSH terms representing artificial intelligence, medical education, curriculum, medical student, and their equivalents, without any restriction in time and language, until October 2023. The expanding integration of AI into medical practice, especially in recent decades, has also led to an increase in the use of different AI methods in medical education. However, integrating AI into medical education comes with advantages and obstacles. The advantages include objective student assessment, better clinical simulation organization, and enhanced education transparency. Conversely, the main challenges of AI integration in education are ethical and legal issues, scalability limitations, evaluating the effectiveness of these educational methods, and technical difficulties. Further research is needed to specifically identify the potential benefits and challenges, and analyze the proposed solutions for these challenges. Also, the academic curriculum should be evaluated periodically in order to improve the effectiveness of incorporating AI in medical education. La inteligencia artificial (IA) tiene un potencial indiscutible para resolver los desafíos globales que enfrentan en la prestación de servicios de salud, lo que resulta en su utilización cada vez mayor en diversos campos médicos. Esta revisión tiene como objetivo presentar una evaluación exhaustiva de las ventajas, los desafíos y las estrategias líderes para utilizar la IA en el campo de la educación médica. Para acceder a los estudios y documentos más recientes, se realizó una búsqueda bibliográfica exhaustiva en las bases de datos Medline (a través de PubMed), Scopus y Web of Science, utilizando palabras clave gratuitas y términos MeSH que representan inteligencia artificial, educación médica, plan de estudios y estudiantes de medicina, y sus equivalentes, sin restricción alguna de tiempo e idioma, hasta octubre de 2023. La creciente integración de la IA en la práctica médica, especialmente en las últimas décadas, también ha llevado a un aumento en el uso de diferentes métodos de IA en la educación médica. Sin embargo, integrar la IA en la educación médica conlleva ventajas y obstáculos. Las ventajas incluyen evaluación objetiva de los estudiantes, mejor organización de la simulación clínica y mayor transparencia educativa. Por el contrario, los principales desafíos de la integración de la IA en la educación son cuestiones éticas y legales, limitaciones de escalabilidad, evaluación de la efectividad de estos métodos educativos y dificultades técnicas. Se necesita más investigación para identificar específicamente los beneficios y desafíos potenciales, y analizar las soluciones propuestas para estos desafíos. Además, el currículo académico debe evaluarse periódicamente para mejorar la eficacia de la incorporación de la IA en la educación médica.",No methods found.
2024,https://openalex.org/W4391096835,Social Sciences,Diagnostic Performance Comparison between Generative AI and Physicians: A Systematic Review and Meta-Analysis,"Abstract Background The rapid advancement of generative artificial intelligence (AI) has led to the wide dissemination of models with exceptional understanding and generation of human language. Their integration into healthcare has shown potential for improving medical diagnostics, yet a comprehensive diagnostic performance evaluation of generative AI models and the comparison of their diagnostic performance with that of physicians has not been extensively explored. Methods In this systematic review and meta-analysis, a comprehensive search of Medline, Scopus, Web of Science, Cochrane Central, and MedRxiv was conducted for studies published from June 2018 through December 2023, focusing on those that validate generative AI models for diagnostic tasks. The risk of bias was assessed using the Prediction Model Study Risk of Bias Assessment Tool. Meta-regression was performed to summarize the performance of the models and to compare the accuracy of the models with that of physicians. Results The search resulted in 54 studies being included in the meta-analysis. Nine generative AI models were evaluated across 17 medical specialties. The quality assessment indicated a high risk of bias in the majority of studies, primarily due to small sample sizes. The overall accuracy for generative AI models across 54 studies was 56.9% (95% confidence interval [CI]: 51.0–62.7%). The meta-analysis demonstrated that, on average, physicians exceeded the accuracy of the models (difference in accuracy: 14.4% [95% CI: 4.9–23.8%], p-value =0.004). However, both Prometheus (Bing) and GPT-4 showed slightly better performance compared to non-experts (-2.3% [95% CI: -27.0–22.4%], p-value = 0.848 and -0.32% [95% CI: -14.4–13.7%], p-value = 0.962), but slightly underperformed when compared to experts (10.9% [95% CI: -13.1–35.0%], p-value = 0.356 and 12.9% [95% CI: 0.15–25.7%], p-value = 0.048). The sub-analysis revealed significantly improved accuracy in the fields of Gynecology, Pediatrics, Orthopedic surgery, Plastic surgery, and Otolaryngology, while showing reduced accuracy for Neurology, Psychiatry, Rheumatology, and Endocrinology compared to that of General Medicine. No significant heterogeneity was observed based on the risk of bias. Conclusions Generative AI exhibits promising diagnostic capabilities, with accuracy varying significantly by model and medical specialty. Although they have not reached the reliability of expert physicians, the findings suggest that generative AI models have the potential to enhance healthcare delivery and medical education, provided they are integrated with caution and their limitations are well-understood. Key Points Question: What is the diagnostic accuracy of generative AI models and how does this accuracy compare to that of physicians? Findings: This meta-analysis found that generative AI models have a pooled accuracy of 56.9% (95% confidence interval: 51.0–62.7%). The accuracy of expert physicians exceeds that of AI in all specialties, however, some generative AI models are comparable to non-expert physicians. Meaning: The diagnostic performance of generative AI models suggests that they do not match the level of experienced physicians but that they may have potential applications in healthcare delivery and medical education.",<method>generative AI models</method>
2024,https://openalex.org/W4391260165,Social Sciences,Artificial intelligence and corporate carbon neutrality: A qualitative exploration,"Abstract Many firms have established formal carbon neutrality (CN) targets in response to the increasing climate risk and related regulatory requirements. Subsequently, they have implemented various measures and adopted multiple approaches to attain these goals. Academic research has given due attention to firms' efforts in this direction. However, past studies have primarily focused on non‐digital and process‐oriented approaches to achieving CN, with the potential of digital technologies such as artificial intelligence (AI) remaining less explored. Our study aims to address this gap by qualitatively examining the use of AI for pursuing CN, drawing insights from firms with prior experience in the area. We analyzed the collected qualitative data to identify four key dimensions that capture different nuances of applying AI for achieving CN: (a) implementing AI for direct and indirect control of emissions, (b) accepting the strategic trade‐offs related to funding, data and systems concerns, and social priorities, (c) overcoming organizational and human‐related impediments, and (d) acknowledging the significant impact of AI in terms of gains in business model efficiency and measurable CN target attainment, which ultimately contribute to CN. Based on our findings, we propose a convergence–divergence model encompassing the positive aspects, inhibiting factors, synergies, and offsets necessary for firms to leverage AI to achieve net‐zero emissions effectively. Overall, our study contributes to the discourse on the utilization of AI for CN in a comprehensive manner.",No methods found.
2024,https://openalex.org/W4391560032,Social Sciences,Artificial Intelligence Language Model Performance for Rapid Intraoperative Queries in Plastic Surgery: ChatGPT and the Deep Inferior Epigastric Perforator Flap,"Background: The integration of artificial intelligence in healthcare has led to the development of large language models that can address various medical queries, including intraoperatively. This study investigates the potential of ChatGPT in addressing intraoperative questions during the deep inferior epigastric perforator flap procedure. Methods: A series of six intraoperative questions specific to the DIEP flap procedure, derived from real-world clinical scenarios, were proposed to ChatGPT. A panel of four experienced board-certified plastic surgeons evaluated ChatGPT’s performance in providing accurate, relevant, and comprehensible responses. Results: The Likert scale demonstrated to be medically accurate, systematic in presentation, and logical when providing alternative solutions. The mean readability score of the Flesch Reading Ease Score was 28.7 (±0.8), the Flesch–Kincaid Grade Level was 12.4 (±0.5), and the Coleman–Liau Index was 14.5 (±0.5). Suitability-wise, the DISCERN score of ChatGPT was 48 (±2.5) indicating suitable and comprehensible language for experts. Conclusions: Generative AI tools such as ChatGPT can serve as a supplementary tool for surgeons to offer valuable insights and foster intraoperative problem-solving abilities. However, it lacks consideration of individual patient factors and surgical nuances. Nevertheless, further refinement of its training data and rigorous scrutiny under experts to ensure the accuracy and up-to-date nature of the information holds the potential for it to be utilized in the surgical field.","<method>large language models</method>, <method>ChatGPT</method>"
2024,https://openalex.org/W4392111155,Social Sciences,Harnessing of Artificial Intelligence for the Diagnosis and Prevention of Hospital-Acquired Infections: A Systematic Review,"Healthcare-associated infections (HAIs) are the most common adverse events in healthcare and constitute a major global public health concern. Surveillance represents the foundation for the effective prevention and control of HAIs, yet conventional surveillance is costly and labor intensive. Artificial intelligence (AI) and machine learning (ML) have the potential to support the development of HAI surveillance algorithms for the understanding of HAI risk factors, the improvement of patient risk stratification as well as the prediction and timely detection and prevention of infections. AI-supported systems have so far been explored for clinical laboratory testing and imaging diagnosis, antimicrobial resistance profiling, antibiotic discovery and prediction-based clinical decision support tools in terms of HAIs. This review aims to provide a comprehensive summary of the current literature on AI applications in the field of HAIs and discuss the future potentials of this emerging technology in infection practice. Following the PRISMA guidelines, this study examined the articles in databases including PubMed and Scopus until November 2023, which were screened based on the inclusion and exclusion criteria, resulting in 162 included articles. By elucidating the advancements in the field, we aim to highlight the potential applications of AI in the field, report related issues and shortcomings and discuss the future directions.",No methods found.
2024,https://openalex.org/W4392447932,Social Sciences,Systematic Review of Large Language Models for Patient Care: Current Applications and Challenges,"Abstract The introduction of large language models (LLMs) into clinical practice promises to improve patient education and empowerment, thereby personalizing medical care and broadening access to medical knowledge. Despite the popularity of LLMs, there is a significant gap in systematized information on their use in patient care. Therefore, this systematic review aims to synthesize current applications and limitations of LLMs in patient care using a data-driven convergent synthesis approach. We searched 5 databases for qualitative, quantitative, and mixed methods articles on LLMs in patient care published between 2022 and 2023. From 4,349 initial records, 89 studies across 29 medical specialties were included, primarily examining models based on the GPT-3.5 (53.2%, n=66 of 124 different LLMs examined per study) and GPT-4 (26.6%, n=33/124) architectures in medical question answering, followed by patient information generation, including medical text summarization or translation, and clinical documentation. Our analysis delineates two primary domains of LLM limitations: design and output. Design limitations included 6 second-order and 12 third-order codes, such as lack of medical domain optimization, data transparency, and accessibility issues, while output limitations included 9 second-order and 32 third-order codes, for example, non-reproducibility, non-comprehensiveness, incorrectness, unsafety, and bias. In conclusion, this study is the first review to systematically map LLM applications and limitations in patient care, providing a foundational framework and taxonomy for their implementation and evaluation in healthcare settings.","<method>large language models (LLMs)</method>, <method>GPT-3.5</method>, <method>GPT-4</method>"
2024,https://openalex.org/W4393159297,Social Sciences,PathAsst: A Generative Foundation AI Assistant towards Artificial General Intelligence of Pathology,"As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, offering significant applications in interpreting natural images. However, the field of pathology has largely remained untapped, particularly in gathering high-quality data and designing comprehensive model frameworks. To bridge the gap in pathology MLLMs, we present PathAsst, a multimodal generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. The development of PathAsst involves three pivotal steps: data acquisition, CLIP model adaptation, and the training of PathAsst's multimodal generative capabilities. Firstly, we collect over 207K high-quality pathology image-text pairs from authoritative sources. Leveraging the advanced power of ChatGPT, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data specifically tailored for invoking eight pathology-specific sub-models we prepared, allowing the PathAsst to effectively collaborate with these models, enhancing its diagnostic ability. Secondly, by leveraging the collected data, we construct PathCLIP, a pathology-dedicated CLIP, to enhance PathAsst's capabilities in interpreting pathology images. Finally, we integrate PathCLIP with the Vicuna-13b and utilize pathology-specific instruction-tuning data to enhance the multimodal generation capacity of PathAsst and bolster its synergistic interactions with sub-models. The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes. We open-source our dataset, as well as a comprehensive toolkit for extensive pathology data collection and preprocessing at https://github.com/superjamessyx/Generative-Foundation-AI-Assistant-for-Pathology.","<method>CLIP model adaptation</method>, <method>multimodal generative foundation AI assistant</method>, <method>instruction-tuning</method>"
2024,https://openalex.org/W4393188639,Social Sciences,The Impact of Motherhood on Women’s Career Progression: A Scoping Review of Evidence-Based Interventions,"(1) Background: Despite the progress made by women in the workplace, mothers still face systemic barriers that prevent them from advancing professionally. This “motherhood penalty” involves a variety of discriminatory practices and experiences that mothers can face at work, including being held to stricter standards regarding salary and recruitment. Despite ongoing research on the association between motherhood and career outcomes, few studies specifically explore how motherhood impacts career advancement and, consequently, access to leadership. This scoping review seeks to gain an understanding of how motherhood impacts women’s career progression, and how interventions can address the underrepresentation of mothers in leadership. (2) Methods: Following the PRISMA-ScR framework, we analyzed 52 articles from 2010 to 2022, drawn from 10 databases. (3) Results: The results showed both negative and positive impacts of motherhood on career progression, affecting mothers’ attitudes, feelings, and behaviors and yielding changes in interpersonal relationships and work conditions. Intersectionality is highlighted, urging a nuanced examination of challenges faced by mothers from a diversity of backgrounds. Recommendations for interventions include individual and institutional efforts, comprising societal support structures, organizational policy changes, and cultural shifts. (4) Conclusions: This scoping review offers an updated perspective on a classic challenge, providing practical insights for a more inclusive and structural understanding of the career trajectories of working mothers.",No methods found.
2024,https://openalex.org/W4393222088,Social Sciences,Methodological insights into ChatGPT’s screening performance in systematic reviews,"Abstract Background The screening process for systematic reviews and meta-analyses in medical research is a labor-intensive and time-consuming task. While machine learning and deep learning have been applied to facilitate this process, these methods often require training data and user annotation. This study aims to assess the efficacy of ChatGPT, a large language model based on the Generative Pretrained Transformers (GPT) architecture, in automating the screening process for systematic reviews in radiology without the need for training data. Methods A prospective simulation study was conducted between May 2nd and 24th, 2023, comparing ChatGPT’s performance in screening abstracts against that of general physicians (GPs). A total of 1198 abstracts across three subfields of radiology were evaluated. Metrics such as sensitivity, specificity, positive and negative predictive values (PPV and NPV), workload saving, and others were employed. Statistical analyses included the Kappa coefficient for inter-rater agreement, ROC curve plotting, AUC calculation, and bootstrapping for p-values and confidence intervals. Results ChatGPT completed the screening process within an hour, while GPs took an average of 7–10 days. The AI model achieved a sensitivity of 95% and an NPV of 99%, slightly outperforming the GPs’ sensitive consensus (i.e., including records if at least one person includes them). It also exhibited remarkably low false negative counts and high workload savings, ranging from 40 to 83%. However, ChatGPT had lower specificity and PPV compared to human raters. The average Kappa agreement between ChatGPT and other raters was 0.27. Conclusions ChatGPT shows promise in automating the article screening phase of systematic reviews, achieving high sensitivity and workload savings. While not entirely replacing human expertise, it could serve as an efficient first-line screening tool, particularly in reducing the burden on human resources. Further studies are needed to fine-tune its capabilities and validate its utility across different medical subfields.","<method>machine learning</method>, <method>deep learning</method>, <method>ChatGPT</method>, <method>Generative Pretrained Transformers (GPT) architecture</method>"
2024,https://openalex.org/W4393226952,Social Sciences,"Generative artificial intelligence (AI) in higher education: a comprehensive review of challenges, opportunities, and implications","This paper explores recent advancements and implications of artificial intelligence (AI) technology, with a specific focus on Large Language Models (LLMs) like ChatGPT 3.5, within the realm of higher education. Through a review of the academic literature, this paper highlights the unprecedented growth of these models and their wide-reaching impact across various sectors. The discussion sheds light on the complex issues and potential benefits presented by LLMs, providing a overview of the field's current state. In the context of higher education, the paper explores the challenges and opportunities posed by LLMs. These include issues related to educational assessment, potential threats to academic integrity, privacy concerns, the propagation of misinformation, EDI aspects, copyright concerns and inherent biases within the models. While these challenges are multifaceted and significant, the paper emphasizes the availability of strategies to address them effectively and facilitate the successful adoption of LLMs in educational settings. Furthermore, the paper recognises the potential opportunities to transform higher education. It emphasises the need to update assessment policies, develop guidelines for staff and students, scaffold AI skills development, and find ways to leverage technology in the classroom. By proactively pursuing these steps, higher education institutions (HEIs) can harness the full potential of LLMs while managing their adoption responsibly. In conclusion, the paper urges HEIs to allocate resources to handle the adoption of LLMs effectively. This includes ensuring staff AI readiness and taking steps to modify their study programmes to align with the evolving educational landscape influenced by emerging technologies.",No methods found.
2024,https://openalex.org/W4394835724,Social Sciences,Machine Learning-Assisted Design of Advanced Polymeric Materials,"ConspectusPolymeric material research is encountering a new paradigm driven by machine learning (ML) and big data. The ML-assisted design has proven to be a successful approach for designing novel high-performance polymeric materials. This goal is mainly achieved through the following procedure: structure representation and database construction, establishment of a ML-based property prediction model, virtual design and high-throughput screening. The key to this approach lies in training ML models that delineate structure–property relationships based on available polymer data (e.g., structure, component, and property data), enabling the screening of promising polymers that satisfy the targeted property requirements. However, the relative scarcity of high-quality polymer data and the complex polymeric multiscale structure–property relationships pose challenges for this ML-assisted design method, such as data and modeling challenges.In this Account, we summarize the state-of-the-art advancements concerning the ML-assisted design of polymeric materials. Regarding structure representation and database construction, the digital representations of polymers are the predominant methods in cheminformatics along with some newly developed methods that integrate the polymeric multiscale structure characteristics. When establishing a ML-based property prediction model, the key is choosing and optimizing ML models to attain high-precision predictions across a vast chemical structure space. Advanced ML algorithms, such as transfer learning and multitask learning, have been utilized to address the data and modeling challenges. During the ML-assisted screening process, by defining and combining polymer genes, virtual polymer candidates are generated, and subsequently, their properties are predicted and high-throughput screened using ML property prediction models. Finally, the promising polymers identified through this approach are verified by computer simulations and experiments.We provide an overview of our recent efforts toward developing ML-assisted design approaches for discovering advanced polymeric materials and emphasize the intricate nature of polymer structural design. To well describe the multiscale structures of polymers, new structure representation methods, such as polymer fingerprint and cross-linking descriptors, were developed. Moreover, a multifidelity learning method was proposed to leverage the multisource isomerous polymer data from experiments and simulations. Additionally, graph neural networks and Bayesian optimization methods have been developed and applied for predicting polymer properties as well as designing polymer structures and compositions.Finally, we identify the current challenges and point out the development directions in this emerging field. It is highly desirable to establish new structure representation and advanced ML modeling methods for polymeric materials, particularly when constructing polymer large models based on chemical language. Through this Account, we seek to stimulate further interest and foster active collaborations for developing ML-assisted design approaches and realizing the innovation of advanced polymeric materials.","<method>transfer learning</method>, <method>multitask learning</method>, <method>multifidelity learning</method>, <method>graph neural networks</method>, <method>Bayesian optimization</method>"
2024,https://openalex.org/W4394989026,Social Sciences,Challenges and perspectives of air pollution control in China,"Abstract Air pollution is one of the most challenging environmental issues in the world. China has achieved remarkable success in improving air quality in last decade as a result of aggressive air pollution control policies. However, the average fine particulate matter (PM 2.5 ) concentration in China is still about six times of the World Health Organization (WHO) Global Air Quality Guidelines (AQG) and causing significant human health risks. Extreme emission reductions of multiple air pollutants are required for China to achieve the AQG. Here we identify the major challenges in future air quality improvement and propose corresponding control strategies. The main challenges include the persistently high health risk attributed to PM 2.5 pollution, the excessively loose air quality standards, and coordinated control of air pollution, greenhouse gases (GHGs) emissions and emerging pollutants. To further improve air quality and protect human health, a health-oriented air pollution control strategy shall be implemented by tightening the air quality standards as well as optimizing emission reduction pathways based on the health risks of various sources. In the meantime, an “one-atmosphere” concept shall be adopted to strengthen the synergistic control of air pollutants and GHGs and the control of non-combustion sources and emerging pollutants shall be enhanced.",No methods found.
2024,https://openalex.org/W4395663988,Social Sciences,Navigating the Power of Artificial Intelligence in Risk Management: A Comparative Analysis,"This study presents a responsive analysis of the role of artificial intelligence (AI) in risk management, contrasting traditional approaches with those augmented by AI and highlighting the challenges and opportunities that emerge. AI, intense learning methodologies such as convolutional neural networks (CNNs), have been identified as pivotal in extracting meaningful insights from image data, a form of analysis that holds significant potential in identifying and managing risks across various industries. The research methodology involves a strategic selection and processing of images for analysis and introduces three case studies that serve as benchmarks for evaluation. These case studies showcase the application of AI, in place of image processing capabilities, to identify hazards, evaluate risks, and suggest control measures. The comparative evaluation focuses on the accuracy, relevance, and practicality of the AI-generated findings alongside the system’s response time and comprehensive understanding of the context. Results reveal that AI can significantly enhance risk assessment processes, offering rapid and detailed insights. However, the study also recognises the intrinsic limitations of AI in contextual interpretation, advocating for a synergy between technological and domain-specific expertise. The conclusion underscores the transformative potential of AI in risk management, supporting continued research to further integrate AI effectively into risk assessment frameworks.",<method>convolutional neural networks (CNNs)</method>
2024,https://openalex.org/W4398169659,Social Sciences,The Sociodemographic Biases in Machine Learning Algorithms: A Biomedical Informatics Perspective,"Artificial intelligence models represented in machine learning algorithms are promising tools for risk assessment used to guide clinical and other health care decisions. Machine learning algorithms, however, may house biases that propagate stereotypes, inequities, and discrimination that contribute to socioeconomic health care disparities. The biases include those related to some sociodemographic characteristics such as race, ethnicity, gender, age, insurance, and socioeconomic status from the use of erroneous electronic health record data. Additionally, there is concern that training data and algorithmic biases in large language models pose potential drawbacks. These biases affect the lives and livelihoods of a significant percentage of the population in the United States and globally. The social and economic consequences of the associated backlash cannot be underestimated. Here, we outline some of the sociodemographic, training data, and algorithmic biases that undermine sound health care risk assessment and medical decision-making that should be addressed in the health care system. We present a perspective and overview of these biases by gender, race, ethnicity, age, historically marginalized communities, algorithmic bias, biased evaluations, implicit bias, selection/sampling bias, socioeconomic status biases, biased data distributions, cultural biases and insurance status bias, conformation bias, information bias and anchoring biases and make recommendations to improve large language model training data, including de-biasing techniques such as counterfactual role-reversed sentences during knowledge distillation, fine-tuning, prefix attachment at training time, the use of toxicity classifiers, retrieval augmented generation and algorithmic modification to mitigate the biases moving forward.","<method>knowledge distillation</method>, <method>fine-tuning</method>, <method>prefix attachment at training time</method>, <method>toxicity classifiers</method>, <method>retrieval augmented generation</method>, <method>algorithmic modification</method>"
2024,https://openalex.org/W4399244247,Social Sciences,Investigating influencing factors of learning satisfaction in AI ChatGPT for research: University students perspective,"This study investigates the determinants of ChatGPT adoption among university students and its impact on learning satisfaction. Utilizing the Technology Acceptance Model (TAM) and incorporating insights from interaction learning, collaborative learning, and information quality, a structural equation modeling approach was employed. This research collected valuable responses from 262 students at King Faisal University in Saudi Arabia through the use of self-report questionnaires. The data's reliability and validity were assessed using confirmation factor analysis, followed by path analysis to explore the hypotheses in the proposed model. The results indicate the pivotal roles of interaction learning and collaborative learning in fostering ChatGPT adoption. Social interaction played a significant role, as researchers engaging in conversations and knowledge-sharing expressed increased comfort with ChatGPT. Information quality was found to substantially influence researchers' decisions to continue using ChatGPT, emphasizing the need for ongoing improvement in the accuracy and relevance of content provided. Perceived ease of use and perceived usefulness played intermediary roles in linking ChatGPT engagement to learning satisfaction. User-friendly interfaces and perceived utility were identified as crucial factors affecting overall satisfaction levels. Notably, ChatGPT positively impacted learning motivation, indicating its potential to enhance student engagement and interest in learning. The study's findings have implications for educational practitioners seeking to improve the implementation of AI technologies in university students, emphasizing user-friendly design, collaborative learning, and factors influencing satisfaction. The study concludes with insights into the complex interplay between AI-powered tools, learning objectives, and motivation, highlighting the need for continued research to comprehensively understand these dynamics. This study investigates the determinants of ChatGPT adoption among university students and its impact on learning satisfaction. Utilizing the Technology Acceptance Model (TAM) and incorporating insights from interaction learning, collaborative learning, and information quality, a structural equation modeling approach was employed. This research collected valuable responses from 262 students at King Faisal University in Saudi Arabia through the use of self-report questionnaires. The data's reliability and validity were assessed using confirmation factor analysis, followed by path analysis to explore the hypotheses in the proposed model. The results indicate the pivotal roles of interaction learning and collaborative learning in fostering ChatGPT adoption. Social interaction played a significant role, as researchers engaging in conversations and knowledge-sharing expressed increased comfort with ChatGPT. Information quality was found to substantially influence researchers' decisions to continue using ChatGPT, emphasizing the need for ongoing improvement in the accuracy and relevance of content provided. Perceived ease of use and perceived usefulness played intermediary roles in linking ChatGPT engagement to learning satisfaction. User-friendly interfaces and perceived utility were identified as crucial factors affecting overall satisfaction levels. Notably, ChatGPT positively impacted learning motivation, indicating its potential to enhance student engagement and interest in learning. The study's findings have implications for educational practitioners seeking to improve the implementation of AI technologies in university students, emphasizing user-friendly design, collaborative learning, and factors influencing satisfaction. The study concludes with insights into the complex interplay between AI-powered tools, learning objectives, and motivation, highlighting the need for continued research to comprehensively understand these dynamics.","<method>Technology Acceptance Model (TAM)</method>, <method>structural equation modeling</method>, <method>confirmation factor analysis</method>, <method>path analysis</method>"
2024,https://openalex.org/W4399363436,Social Sciences,Collective Constitutional AI: Aligning a Language Model with Public Input,"There is growing consensus that language model (LM) developers should not be the sole deciders of LM behavior, creating a need for methods that enable the broader public to collectively shape the behavior of LM systems that affect them. To address this need, we present Collective Constitutional AI (CCAI): a multi-stage process for sourcing and integrating public input into LMs—from identifying a target population to sourcing principles to training and evaluating a model. We demonstrate the real-world practicality of this approach by creating what is, to our knowledge, the first LM fine-tuned with collectively sourced public input and evaluating this model against a baseline model trained with established principles from a LM developer. Our quantitative evaluations demonstrate several benefits of our approach: the CCAI-trained model shows lower bias across nine social dimensions compared to the baseline model, while maintaining equivalent performance on language, math, and helpful-harmless evaluations. Qualitative comparisons of the models suggest that the models differ on the basis of their respective constitutions, e.g., when prompted with contentious topics, the CCAI-trained model tends to generate responses that reframe the matter positively instead of a refusal. These results demonstrate a promising, tractable pathway toward publicly informed development of language models.","<method>Collective Constitutional AI (CCAI)</method>, <method>fine-tuning</method>"
2024,https://openalex.org/W4399715357,Social Sciences,AI-POWERED FRAUD DETECTION IN BANKING: SAFEGUARDING FINANCIAL TRANSACTIONS,"The banking industry's metamorphosis through digitalization has unquestionably revolutionized accessibility and convenience for customers worldwide. However, this paradigm shift has ushered in a new era of challenges, most notably in the realm of cybersecurity. Conventional rule-based fraud detection strategies have struggled to keep pace with the rapid evolution of cyber threats, prompting a surge of interest in more adaptive approaches like unsupervised learning. Furthermore, the COVID-19 pandemic has exacerbated the issue of bank fraud due to the widespread transition to online platforms and the proliferation of charitable funds, which present ripe opportunities for exploitation by cybercriminals. In response to these pressing concerns, this study delves into the realm of machine learning algorithms for the analysis and identification of fraudulent banking transactions. Notably, it contributes scientific novelty by developing models specifically tailored to this purpose and implementing innovative preprocessing techniques to enhance detection accuracy. Utilizing a diverse array of algorithms, including Random Forest, K-Nearest Neighbor (KNN), Naïve Bayes, Decision Trees, and Logistic Regression, the study showcases promising results. In particular, logistic regression and decision tree models exhibit impressive accuracy and Area Under the Curve (AUC) values of approximately 0.98, 0.97 and 0.95, 0.94, respectively. Given the pervasive nature of banking fraud in our digital society, the utilization of artificial intelligence algorithms for fraud detection stands as a critical and timely endeavor, promising enhanced security and trust in the financial ecosystem.","<method>unsupervised learning</method>, <method>Random Forest</method>, <method>K-Nearest Neighbor (KNN)</method>, <method>Naïve Bayes</method>, <method>Decision Trees</method>, <method>Logistic Regression</method>"
2024,https://openalex.org/W4400118905,Social Sciences,The Application of Space Syntax to Enhance Sociability in Public Urban Spaces: A Systematic Review,"Public urban spaces are vital settings for fostering social interaction among people. However, understanding how spatial layouts can promote positive social behaviors remains a critical and debated challenge for urban designers and planners aiming to create socially sustainable environments. Space syntax, a well-established theory and research method, explores the influence of spatial configurations on social aspects. Despite its significant contributions, there is a lack of comprehensive systematic reviews evaluating its effectiveness in enhancing social interaction within urban public spaces. This study aims to identify the existing scientific gaps in the domain of space syntax studies, with a primary focus on sociability in public urban spaces. Following the PRISMA framework, a thorough literature search was conducted in the Scopus database, yielding 1107 relevant articles. After applying screening and eligibility criteria, 26 articles were selected for in-depth review. This review adopted a novel approach to synthesizing and analyzing the findings for identifying underexplored scientific gaps. The findings suggested a wide variety of research gaps to address, encompassing evidence, knowledge, practical, methodological, empirical, theoretical, and target populations to provide a thorough overview of the current state of knowledge in this field. In conclusion, by exploring the interplay between space syntax and design elements such as the urban infrastructure, landscaping, and microclimate in these areas, future research can bridge this gap, particularly when considering a cross-cultural lens. This study underscores the importance of space syntax in promoting social interaction in urban public spaces, offering a robust foundation for future research and practical applications to create more socially engaging environments.",No methods found.
2024,https://openalex.org/W4401059011,Social Sciences,A review of control strategies for proton exchange membrane (PEM) fuel cells and water electrolysers: From automation to autonomy,"Proton exchange membrane (PEM) based electrochemical systems have the capability to operate in fuel cell (PEMFC) and water electrolyser (PEMWE) modes, enabling efficient hydrogen energy utilisation and green hydrogen production. In addition to the essential cell stacks, the system of PEMFC or PEMWE consists of four sub-systems for managing gas supply, power, thermal, and water, respectively. Due to the system's complexity, even a small fluctuation in a certain sub-system can result in an unexpected response, leading to a reduced performance and stability. To improve the system's robustness and responsiveness, considerable efforts have been dedicated to developing advanced control strategies. This paper comprehensively reviews various control strategies proposed in literature, revealing that traditional control methods are widely employed in PEMFC and PEMWE due to their simplicity, yet they suffer from limitations in accuracy. Conversely, advanced control methods offer high accuracy but are hindered by poor dynamic performance. This paper highlights the recent advancements in control strategies incorporating machine learning algorithms. Additionally, the paper provides a perspective on the future development of control strategies, suggesting that hybrid control methods should be used for future research to leverage the strength of both sides. Notably, it emphasises the role of artificial intelligence (AI) in advancing control strategies, demonstrating its significant potential in facilitating the transition from automation to autonomy.","<method>machine learning algorithms</method>, <method>artificial intelligence (AI)</method>, <method>hybrid control methods</method>"
2024,https://openalex.org/W4390933379,Social Sciences,"A Systematic Review of Graph Neural Network in Healthcare-Based Applications: Recent Advances, Trends, and Future Directions","Graph neural network (GNN) is a formidable deep learning framework that enables the analysis and modeling of intricate relationships present in data structured as graphs. In recent years, a burgeoning interest has arisen in exploiting the latent capabilities of GNN for healthcare-based applications, capitalizing on their aptitude for modeling complex relationships and unearthing profound insights from graph-structured data. However, to the best of our knowledge, no study has systemically reviewed the GNN studies conducted in the healthcare domain. This study has furnished an all-encompassing and erudite overview of the prevailing cutting-edge research on GNN in healthcare. Through analysis and assimilation of studies, current research trends, recurrent challenges, and promising future opportunities in GNN for healthcare applications have been identified. China emerged as the leading country to conduct GNN-based studies in the healthcare domain, followed by the USA, UK, and Turkey. Among various aspects of healthcare, disease prediction and drug discovery emerge as the most prominent areas of focus for GNN application, indicating the potential of GNN for advancing diagnostic and therapeutic approaches. This study proposed research questions regarding diverse aspects of GNN in the healthcare domain and addressed them through an in-depth analysis. This study can provide practitioners and researchers with profound insights into the current landscape of GNN applications in healthcare and can guide healthcare institutes, researchers, and governments by demonstrating the ways in which GNN can contribute to the development of effective and efficient healthcare systems.",<method>Graph neural network (GNN)</method>
2024,https://openalex.org/W4391025321,Social Sciences,The examination of the relationship between learning motivation and learning effectiveness: a mediation model of learning engagement,"Abstract In the past decade, China has entered the process of universalization of higher education, however, with the popularization of higher education, the issue of education quality has become more important than ever. Therefore, the main objective of this study was to explore the relationship between learning motivation and learning effectiveness, the mediation effect of learning engagement on learning motivation and learning effectiveness, and the moderation effect of students’ personality traits on the relationship between learning motivation and learning effectiveness. It was found that (1) Learning motivation had a significant positive impact on learning effectiveness; (2) Learning motivation had a significant positive effect on learning engagement; (3) Learning engagement had a significant positive impact on learning effectiveness; (4) Learning engagement had a partial mediation effect on learning motivation and learning effectiveness. (5) Personality traits had a moderation effect on the relationship between learning motivation and learning effectiveness. It was suggested that college students should stimulate their motivation for independent learning, enhance their motivation for learning, and cultivate the ability to learn independently in learning. At the same time, in the classroom, teachers should innovate teaching methods, mobilize students’ interest in learning, and cultivate students’ initiative. In addition, schools should actively carry out themed lectures to cultivate students’ correct learning attitudes. In terms of the influence of personality traits on the relationship between learning motivation and learning effectiveness should not be ignored, and appropriate attention should be paid to improving learning effectiveness. Different personality traits should be further explored.",No methods found.
2024,https://openalex.org/W4404367645,Social Sciences,Plan of action to strengthen the surveillance and control of leishmaniasis in the Americas 2023-2030,"In order to update, systematize and monitor the leishmaniasis actions at the operational level, this new version of the Plan of Action for the period of 2023-2030 has been developed, which contains actions for the surveillance, assistance and control of leishmaniases in the Region. The main elements discussed include evidence, cost, cost-effectiveness of available interventions, access to and analysis of epidemiological and process data, as well as the organization of health services in the countries of the Americas. In addition, the goals and indicators of the Plan were developed based on technical discussions, consensus, orientations and suggestions made by experts, researchers and professionals responsible for actions to fight the disease in endemic countries. The proposed targets for leishmaniases control in the Region are ambitious and will require countries and partners interested in providing support to work hard and achieve them. The actions proposed in this Plan were developed based on the diagnosis of the disease situation in the Region, where leishmaniasis was initially divided into two groups according to its clinical form: cutaneous and mucosal leishmaniasis and visceral leishmaniasis. Both differ in the species of Leishmania, vectors, reservoirs, and clinical presentation, as well as in the socioeconomic and environmental characteristics that determine the risks and different transmission cycles, leading to different surveillance and control actions. These actions are focused on early diagnosis, adequate treatment and follow-up of affected people, surveillance, prevention, and control of human cases, vector, and reservoirs, when required, along with education and communication efforts.",No methods found.
2024,https://openalex.org/W4391230666,Social Sciences,End of selection criteria based on sexual orientation: An international symposium on alternatives to donation deferral,"Abstract Background and Objectives Until recently, gay, bisexual and other men who have sex with men (MSM) were deferred from donating blood for 3–12 months since the last male‐to‐male sexual contact. This MSM deferral has been discontinued by several high‐income countries (HIC) that now perform gender‐neutral donor selection. Materials and Methods An international symposium (held on 20‐04‐2023) gathered experts from seven HICs to (1) discuss how this paradigm shift might affect the mitigation strategies for transfusion‐transmitted infections and (2) address the challenges related to gender‐neutral donor selection. Results Most countries employed a similar approach for implementing a gender‐neutral donor selection policy: key stakeholders were consulted; the transition was bridged by time‐limited deferrals; donor compliance was monitored; and questions or remarks on anal sex and the number and/or type of sexual partners were often added. Many countries have now adopted a gender‐neutral approach in which questions on pre‐ and post‐exposure prophylaxis for human immunodeficiency virus (HIV) have been added (or retained, when already in place). Other countries used mitigation strategies, such as plasma quarantine or pathogen reduction technologies for plasma and/or platelets. Conclusion The experience with gender‐neutral donor selection has been largely positive among the countries covered herein and seems to be acceptable to stakeholders, donors and staff. The post‐implementation surveillance data collected so far appear reassuring with regards to safety, although longer observation periods are necessary. The putative risks associated with HIV antiretrovirals should be further investigated.",No methods found.
2024,https://openalex.org/W4391825238,Social Sciences,A global perspective on stepping down chronic spontaneous urticaria treatment: Results of the Urticaria Centers of Reference and Excellence SDown‐CSU study,"Abstract Background Although there have been significant advances in the treatment of chronic spontaneous urticaria (CSU) in recent years, there remains a lack of clear guidance on when and how to step down treatment in responders. This study aims to investigate stepping down approaches of different steps of CSU treatment from a global perspective. Methods “Stepping down chronic spontaneous urticaria treatment” (SDown‐CSU) is an international, multicenter, observational, cross‐sectional, survey‐based study of the Urticaria Centers of Reference and Excellence (UCARE) network. The questionnaire included 48 questions completed by physicians in the UCARE network. Results Surveys completed by 103 physicians from 81 UCAREs and 34 countries were analyzed. Seventy‐eight percent of the participants responded that they had a national urticaria management guideline written by their professional societies and 28% responded that they had to operate under a regulatory guideline proposed by central health funding organizations. Seventy‐two and 58.7% of these national recommendations do not contain any detailed information on when and/or how CSU treatment should be discontinued. There was a lack of detailed information on antihistamines and cyclosporine in particular. A predefined maximum duration was generally not applicable to omalizumab and cyclosporine (81% and 82%, respectively). Nearly all UCAREs step down omalizumab within 6 months from the first controlled status and 42% discontinue cyclosporine after 6 months regardless of the control status. Conclusions The findings from the SDown‐CSU study clearly highlight a global need for guidance on the process of stepping down treatment in CSU. Additionally, the study offers a step‐down algorithm applicable to all stages of CSU treatment.",No methods found.
2024,https://openalex.org/W4390879308,Social Sciences,"Between now and later: a mixed methods study of HPV vaccination delay among Chinese caregivers in urban Chengdu, China","Abstract Background Adolescent girls in China have a low HPV vaccination rate. Although vaccination is recommended by the Chinese health authorities, the cost is not covered by the national immunisation programme. Vaccination delay, among other reasons such as supply shortage and poor affordability, may contribute to low uptake. This sequential mixed methods study aimed to identify potential factors of delayed HPV vaccination among Chinese adolescent girls. Methods Quantitative data about the attitudes and perceptions of HPV vaccination were collected from 100 caregivers of 14–18-year-old girls using an online survey in Chengdu, China. The survey data informed a subsequent qualitative study using four focus group discussions. We conducted a descriptive analysis of the survey data and a thematic analysis of the qualitative data. The findings were interpreted using a health behaviour model adapted from the Health Belief Model and the Andersen’s Behavioural Model for Health Services Use. Results A total of 100 caregivers – 85 were mothers and 15 were fathers – participated in the survey; 21 caregivers joined focus group discussions. When asked about their intended course of action if the 9vHPV vaccine was out-of-stock, 74% chose to delay until the 9vHPV vaccine is available while 26% would consider 2vHPV or 4vHPV vaccines or seek alternative ways to procure the vaccine. Qualitative results confirmed that caregivers preferred delaying HPV vaccination for adolescent girls. The intent to delay was influenced by systemic barriers such as supply shortage and individual-level factors such as a preference for the 9vHPV vaccine, safety concerns, inadequate health communication, and the belief that adolescents were unlikely to be sexually active. Conclusion In urban areas, Chinese caregivers’ intent to delay vaccination in favour of 9vHPV vaccine over receiving the more accessible options was influenced by a mix of individual and contextual factors. Focussed health communication strategies are needed to accelerate HPV vaccination among adolescents.",No methods found.
2024,https://openalex.org/W4393904218,Social Sciences,Multilevel analysis of COVID-19 vaccination intention: the moderating role of economic and cultural country characteristics,"Abstract Background Predictors of COVID-19 (coronavirus) vaccination have been extensively researched; however, the contextual factors contributing to understanding vaccination intention remain largely unexplored. The present study aimed to investigate the moderating role of economic development (Gross domestic product - GDP per capita), economic inequality (Gini index), the perceived corruption index and Hofstede’s measurements of cultural values—index of individualism/collectivism and power distance index—in the relationship between determinants of satisfaction with the healthcare system, trust in political institutions, conspiracy beliefs and COVID-19 vaccination intention. Methods A multilevel modelling approach was employed on a sample of approximately 51 000 individuals nested within 26 countries. Data were drawn from the European Social Survey Round 10. The model examined the effect of individual- and country-level predictors and their interaction on vaccination intention. Results Satisfaction with the healthcare system had a stronger positive effect on intention to get vaccinated in countries with lower perceived corruption and more individualistic countries. Trust in political institutions had a stronger positive effect on vaccination intention in countries with higher economic development and lower perceived corruption, while a negative effect of conspiracy beliefs on vaccination intention was stronger in countries with lower economic development, higher perceived corruption and a more collectivistic cultural orientation. Conclusion Our findings highlight the importance of considering individual and contextual factors when addressing vaccination intention.",<method>multilevel modelling</method>
2024,https://openalex.org/W4390580521,Social Sciences,Public's perspective on COVID-19 adenovirus vector vaccines after thrombosis with thrombocytopenia syndrome (TTS) reports and associated regulatory actions – A cross-sectional study in six EU member states,"In 2021, thrombosis with thrombocytopenia syndrome (TTS) was confirmed by the European Medicines Agency (EMA) as a rare side effect of the COVID-19 adenovirus vector vaccines Vaxzevria® and Jcovden®. This study aimed to describe the public's knowledge of TTS and how it affected the willingness to be vaccinated with COVID-19 vaccines and other vaccines in six European countries. From June to October of 2022, a multi-country cross-sectional online survey was conducted in Denmark, Greece, Latvia, Netherlands, Portugal, and Slovenia. The minimum target of participants to be recruited was based on the size of the country's population. The results were analysed descriptively. In total, 3794 respondents were included in the analysis; across the six countries, 33.3 %–68.3 % reported being familiar with signs and symptoms of TTS, although 3.1–61.4 % of those were able to identify the symptoms correctly. The reported changes in willingness to be vaccinated against COVID-19 and with other vaccines varied per country. The largest reported change in the willingness to be vaccinated with Vaxzevria® and Jcovden® was observed in Denmark (61.2 %), while the willingness to be vaccinated with other COVID-19 vaccines changed most in Slovenia (30.4 %). The smallest decrease in willingness towards future vaccination against COVID-19 was reported in the Netherlands (20.9 %) contrasting with the largest decrease observed in Latvia (69.1 %). Knowledge about TTS seemed to have influenced the public's opinion in Europe resulting in less willingness to be vaccinated with Vaxzevria® and Jcovden®. Willingness for vaccination against COVID-19 with other vaccines and widespread use of vaccines to prevent other diseases also differed and seemed to be determined by the approaches taken by national health authorities when reacting to and communicating about COVID-19 vaccination risks. Further investigation of optimal risk communication strategies is warranted.",No methods found.
2024,https://openalex.org/W4391881153,Social Sciences,Determinants of HPV vaccine uptake intentions in Chinese clinical interns: an extended theory of planned behavior approach,"This study aims to utilize the extended Theory of Planned Behavior (TPB) model to examine the intentions of clinical interns in China towards Human papillomaviruses (HPV) vaccination. It also fills a significant gap in the literature concerning vaccine acceptance in this specific population. This cross-sectional study was carried out with clinical interns in Shandong Province, China, with a total of 1,619 participants. Data were collected through self-reported questionnaires, including demographic characteristics, TPB variables, and HPV-related health knowledge. Hierarchical regression analysis was employed to identify key factors influencing vaccination intentions, and Structural Equation Modeling (SEM) was used to analyze the interrelationships between these factors. This study initially identified key predictors affecting clinical interns' intentions to receive the HPV vaccine through hierarchical regression analysis. The preliminary model, which accounted for demographic factors, revealed foundational impacts of household income and HPV-related clinical experience on intentions. After integrating TPB variables-attitude, subjective norm, perceived behavioral control, and HPV-related health knowledge-the model's explanatory power was enhanced to 37.30%. SEM analysis focused on the interplay among TPB constructs and extended variables, confirming their significance in forming vaccination intentions, with subjective norm having the most substantial impact (β = 0.375, p < 0.001). The extended TPB model explained over half of the variance in vaccination intentions, substantiating the hypotheses and revealing the psychological determinants behind clinical interns' decision-making for HPV vaccination. The extended TPB model from this study effectively explains the vaccination intentions among clinical interns for HPV, offering theoretical support for public health strategies and educational interventions targeting this group. These findings are of significant importance for public health practice and future health promotion strategies.",No methods found.
2024,https://openalex.org/W4392095716,Social Sciences,From Qualitative Research to Quantitative Preference Elicitation: An Example in Invasive Meningococcal Disease,"Qualitative research is fundamental for designing discrete choice experiments (DCEs) but is often underreported in the preference literature. We developed a DCE to elicit preferences for vaccination against invasive meningococcal disease (IMD) among adolescents and young people (AYP) and parents and legal guardians (PLG) in the United States. This article reports the targeted literature review and qualitative interviews that informed the DCE design and demonstrates how to apply the recent reporting guidelines for qualitative developmental work in preference studies. This study included two parts: a targeted literature review and qualitative interviews. The Medline and Embase databases were searched for quantitative and qualitative studies on IMD and immunization. The results of the targeted literature review informed a qualitative interview guide. Sixty-minute, online, semi-structured interviews with AYP and PLG were used to identify themes related to willingness to be vaccinated against IMD. Participants were recruited through a third-party recruiter's database and commercial online panels. Interviews included vignettes about IMD and vaccinations and three thresholding exercises examining the effect of incidence rate, disability rate, and fatality rate on vaccination preferences. Participant responses related to the themes were counted. The targeted literature review identified 31 concepts that were synthesized into six topics for the qualitative interviews. Twenty AYP aged 16–23 years and 20 PLG of adolescents aged 11–17 years were interviewed. Four themes related to willingness to be vaccinated emerged: attitudes towards vaccination, knowledge and information, perception of IMD, and vaccine attributes. Most participants were concerned about IMD (AYP 60%; PLG 85%) and had positive views of vaccination (AYP 80%; PLG 60%). Ninety percent of AYP and 75% of PLG always chose vaccination over no vaccination, independent of IMD incidence rate, disability rate, or fatality rate. Willingness to be vaccinated against IMD was affected by vaccine attributes but largely insensitive to IMD incidence and severity. This article provides an example of how to apply the recent reporting guidelines for qualitative developmental work in preference studies, with 21 out of 22 items in the guidelines being considered.",No methods found.
2024,https://openalex.org/W4390765902,Social Sciences,Pseudo-spectral angle mapping for automated pixel-level analysis of highly multiplexed tissue image data,"Abstract The rapid development of highly multiplexed microscopy systems has enabled the study of cells embedded within their native tissue, which is providing exciting insights into the spatial features of human disease [1]. However, computational methods for analyzing these high-content images are still emerging, and there is a need for more robust and generalizable tools for evaluating the cellular constituents and underlying stroma captured by high-plex imaging [2]. To address this need, we have adapted spectral angle mapping – an algorithm used widely in hyperspectral image analysis – to compress the channel dimension of high-plex immunofluorescence images. As many high-plex immunofluorescence imaging experiments probe unique sets of protein markers, existing cell and pixel classification models do not typically generalize well. Pseudospectral angle mapping (pSAM) uses reference pseudospectra – or pixel vectors – to assign each pixel in an image a similarity score to several cell class reference vectors, which are defined by each unique staining panel. Here, we demonstrate that the class maps provided by pSAM can directly provide insight into the prevalence of each class defined by reference pseudospectra. In a dataset of high-plex images of colon biopsies from patients with gut autoimmune conditions, sixteen pSAM class representation maps were combined with instance segmentation of cells to provide cell class predictions. Finally, pSAM detected a diverse set of structure and immune cells when applied to a novel dataset of kidney biopsies imaged with a 43-marker panel. In summary, pSAM provides a powerful and readily generalizable method for evaluating high-plex immunofluorescence image data. Significance Statement Understanding the cellular constituents captured by highly multiplexed tissue imaging is a major limitation affecting the usability of these novel imaging methods. Many imaging experiments have uniquely designed staining panels, reducing the generalizability of cell classification models to new datasets. We present pseudospectral angle mapping (pSAM), which can compress high-dimensional image data into class representations. We demonstrate that the class representations generated by pSAM can be used to interpret high-plex image data and guide cell classification. Importantly, we also demonstrate that pSAM can generalize to new image datasets—collected with a different staining panel in samples from different tissues—without manual image annotation, subjective intensity gating, or re-training an algorithm.","<method>spectral angle mapping</method>, <method>pseudospectral angle mapping (pSAM)</method>"
2024,https://openalex.org/W4401576339,Social Sciences,Perspectives and challenges in developing and implementing integrated dengue surveillance tools and technology in Thailand: a qualitative study,"Background Dengue remains a persistent public health concern, especially in tropical and sub-tropical countries like Thailand. The development and utilization of quantitative tools and information technology show significant promise for enhancing public health policy decisions in integrated dengue control. However, the effective implementation of these tools faces multifaceted challenges and barriers that are relatively underexplored. Methods This qualitative study employed in-depth interviews to gain a better understanding of the experiences and challenges of quantitative tool development and implementation with key stakeholders involved in dengue control in Thailand, using a phenomenological framework. A diverse range of participants, including public health workers and dengue control experts, participated in these interviews. The collected interview data were systematically managed and investigated using thematic analysis to extract meaningful insights. Results The ability to collect dengue surveillance data and conduct ongoing analyses were contingent upon the availability of individuals possessing essential digital literacy and analytical skills, which were often in short supply. Furthermore, effective space-time early warning and precise data collection were hindered by the absence of user-friendly tools, efficient reporting systems, and complexities in data integration. Additionally, the study underscored the importance of the crucial role of community involvement and collaboration among organizations involved in integrated dengue surveillance, control and quantitative tool development. Conclusions This study employed a qualitative approach to gain a deeper understanding of the contextual intricacies surrounding the development and implementation of quantitative tools, which, despite their potential for strengthening public health policy decisions in dengue control, remain relatively unexplored in the Thai context. The findings yield valuable insights and recommendations for the development and utilization of quantitative tools to support dengue control in Thailand. This information also has the potential to support use of such tools to exert impact beyond dengue to a broader spectrum of diseases.",No methods found.
